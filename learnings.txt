```
# Knowledge Integration: Dataset-Specific Learnings

This document serves as a continuously updated log of learnings specific to the meeting scheduling task within the context of expert consultations. The goal is to accumulate knowledge about this dataset, track effective/ineffective strategies, and guide future research directions.

## 1. DATASET PATTERNS & CHARACTERISTICS

*   **Consistent Question Structure:** The dataset presents meeting scheduling problems framed as expert consultations. Each question follows a consistent structure: a task description introduced by a standardized preamble ("You are an expert at scheduling meetings..."), followed by a "TASK:" section that explicitly states the meeting requirements (participants, duration, time constraints), participant schedules, and finally, specific preferences.
    *   *Example:* Questions consistently start with the role assignment, followed by the scheduling task details and participant schedules.
*   **Schedule Representation:** Schedules are described in natural language, detailing busy times for each participant, often with specific days and time ranges. These schedules are provided *after* the task description. The format varies, sometimes providing consolidated schedules, sometimes individual schedules, which adds to the parsing complexity. The language used to describe these blocks can vary (e.g., "busy on Monday during...", "blocked their calendar on...", "has meetings on...").
    *   *Example:* "David is busy all day on Tuesday and Wednesday morning."
*   **Constraint Variability:** The constraints vary in complexity, including preferences ("Zachary would rather not meet on Thursday"), explicit non-availability, and different sets of days to choose from. Constraints can include specific days, time ranges, meeting durations, and participant preferences (e.g., "would rather not meet on..."). These preferences introduce an element of soft constraint satisfaction in addition to the hard constraints of existing schedules.
    *   *Example:* "Zachary would rather not meet on Thursday, and the meeting must be 30 minutes long."
*   **Semi-Structured Schedule Text:** Participant schedules are presented in a semi-structured text format, often spanning multiple lines and potentially including schedules for multiple days.
    *   *Example:* A participant's schedule might be spread across multiple lines, listing different busy times for different days of the week.
*   **Multiple Participants:** Problems involve scheduling meetings for multiple participants, increasing the complexity of conflict resolution.
    *   *Example:* "Schedule a meeting for David, Zachary, and Michael..."
*   **Standardized Preamble:** Each question starts with a standardized preamble assigning the role ("You are an expert at scheduling meetings...").
    *   *Example:* "You are an expert at scheduling meetings. Please schedule the following meeting..."
*   **Explicit TASK Section:** A "TASK:" section explicitly states the meeting requirements (participants, duration, time constraints).
    *   *Example:* "TASK: Schedule a meeting for David, Zachary, and Michael for 30 minutes. Zachary would rather not meet on Thursday."
*   **Guaranteed Solution:** The questions explicitly state "Note there exists a solution that works with existing schedule of every participant," guaranteeing a feasible meeting time within the provided constraints.

## 2. EFFECTIVE TASK-SPECIFIC STRATEGIES

*   *Currently, no strategies have proven effective on this dataset.* All approaches have resulted in 0% accuracy. The key is improving information extraction by moving away from regex-based parsing and using LLM calls instead.

## 3. COMMON FAILURE MODES ON THIS DATASET

*   **Inaccurate Schedule Extraction:** The primary failure mode is the inaccurate extraction of participant schedules. The LLM struggles to correctly parse the semi-structured text format, often misinterpreting available and unavailable time slots. This leads to proposing meeting times that conflict with existing schedules. The LLM fails to understand the combined schedule format and can't extract available times accurately.
    *   *Example:* Proposing a meeting time when a participant is explicitly stated to be busy during that time.
*   **Inability to Parse Participant Lists:** The `extract_meeting_info` function fails to correctly identify and extract the list of participants from the "TASK:" section. The model consistently fails to extract this essential information.
    *   *Example:* The LLM extracting only one participant when the task clearly states multiple participants. Resulting in "Could not extract meeting information" error.
*   **Schedule Parsing Failure:** The `extract_schedules` function fails to understand and parse the complex, natural language descriptions of participant schedules. Time ranges, days of the week, and participant names within the schedules are not reliably extracted.
    *   *Example:* Failing to recognize "Tuesday morning" as a busy time for a specific participant.
*   **Misinterpretation of Constraints:** The LLM sometimes fails to fully incorporate all constraints when determining available times, overlooking participant preferences or misinterpreting the available days.
    *   *Example:* Scheduling a meeting on a day that a participant "would rather not meet."
*   **Output Format Errors:** The output is often a conversational breakdown of the problem rather than a concise proposed meeting time in the required format ("Here is the proposed time: [Day], [Start Time] - [End Time]").
    *   *Example:* Outputting a paragraph explaining the reasoning process instead of the required formatted time.
*   **Cascading Failures:** Information extraction failures at the initial stages lead to subsequent stages receiving incomplete/incorrect data, causing a complete system breakdown.
*   **Rigid Parsing Failure:** The system's hardcoded regex or rule-based parsing in the `information_extraction` module is highly susceptible to even minor variations in the way participant schedules are described. If the text deviates from the anticipated format (e.g., a slightly different wording for a blocked time), the extraction process fails, resulting in the "Could not extract information" error.
*   **Lack of Robustness to Natural Language Variation:** The dataset uses natural language to describe participant schedules, including variations in phrasing. The current approach does not handle this variability, causing parsing failures when the input text differs slightly from the expected patterns.
*    **Complete System Failure:** The "Could not extract information" error cascades through the entire system. When initial information extraction fails, subsequent steps (verification, time slot search) cannot proceed, leading to complete failure.

## 4. EXPERIMENT LOG & FINDINGS

*   **Iteration 0:**
    *   **Approach:** Example-based prompting for schedule extraction and conflict resolution. Attempted a fully LLM-driven scheduling system with iterative problem solving and verification steps.
    *   **Result:** 0% accuracy. The LLM struggled to generalize from the examples to accurately parse the varying schedule formats. The iterative problem-solving approach was fragile due to information extraction failures. Hypothesis that a fully LLM-driven approach could solve the problem was rejected.
*   **Iteration 1:**
    *   **Approach:** Chained LLM calls, where each function relies on the output of the previous call, to extract meeting information, schedules, and find available times.
    *   **Result:** 0% accuracy. The LLM failed to parse participant lists and extract schedules from the natural language descriptions. The experiment highlighted the LLM's limitations in understanding complex scheduling descriptions without more targeted instruction or a more robust information extraction strategy. The hypothesis that a chained LLM approach could effectively solve this scheduling problem *in its current implementation* was rejected. The LLM's semantic understanding is insufficient.
*   **Iteration 2:**
    *   **Approach:** The approach continued to rely on chained LLM calls, but retained the problematic regex or rule-based parsing logic in the `information_extraction` module.
    *   **Result:** 0% accuracy. Confirmed that rigid parsing is unsuitable for this natural language dataset. The experiment highlights that a flawed information extraction process in an LLM-driven architecture leads to total system failure.

## 5. NEXT RESEARCH DIRECTIONS

*   **Refactor Information Extraction:** The entire approach needs to be overhauled, with a focus on improving information extraction. This is the key bottleneck.
    *   **Multi-Agent Approach:** Create separate agents with specialized roles:
        *   **Participant Agent:** Identifies and extracts all participants mentioned in the task.
        *   **Schedule Agent:** Extracts the schedules for each participant.
        *   **Constraint Agent:** Extracts explicit preferences, duration or day constraints.
    *   **Enhanced Prompting:** Employ more precise and structured prompts for each agent. Specifically for the Schedule Agent:
        *   Use Few-Shot learning, demonstrating how to correctly extract schedules from sample texts.
        *   Explicitly ask the model to identify *which* participant each schedule belongs to.
    *   **Data Validation:** Implement validation steps after each extraction stage to check for completeness and correctness. If validation fails, trigger a re-extraction with a refined prompt.
*   **Structured Data Representation:** Move away from passing raw text between LLM calls. Instead, use structured data formats (dictionaries, lists) to represent extracted information and constraints.
*   **Prioritize LLM Reasoning over Hardcoded Logic:** The extraction process must be based on robust LLM calls and not fragile regex parsing. The process must be based on extracting explicit information.
*   **Focus on Robust Schedule Extraction:** Prioritize improving the accuracy of schedule extraction. Instead of example-based prompting, use LLMs in a structured extraction manner. Decompose the extraction: First, identify each participant and the days being considered. Second, extract the blocked time slots for each participant on each day. Third, transform the information into a consistent and easily processable data structure (e.g., a dictionary with participants as keys and a list of blocked time intervals as values).
*   **Structured Output:** Ensure that the final output is strictly formatted as "Here is the proposed time: [Day], [Start Time] - [End Time]". This requires explicit instruction and potentially a final formatting function.
*   **Implement a Verification Agent:** Introduce a "verifier" agent that checks the proposed meeting time against the extracted schedules and constraints *before* presenting the final answer. This agent should flag any conflicts or constraint violations.
*   **Simplify initial prompt:** The initial prompt is too conversational and the llm spends too much time breaking down the problem and less time presenting a structured answer.
*   **Replace parsing with LLM calls:** Replace the brittle parsing logic with calls to the LLM to extract participant names, schedules and constraints. For example, use a function that looks like `extract_schedules_with_llm(text, "extract the schedules for each person")` instead of complex regex.
*   **Implement Information Verification:** Following information extraction, implement a LLM-driven verification step to check the consistency and completeness of the extracted information. If the extracted information is inconsistent or incomplete, instruct the LLM to rectify these issues by re-examining the input text or inferring missing details.
*   **Focus on Error Handling:** Implement robust error handling to gracefully manage extraction failures. If initial extraction fails, instead of halting, instruct the LLM to attempt extraction using alternative prompts or strategies.
```