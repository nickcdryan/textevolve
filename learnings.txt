```
# DATASET-SPECIFIC LEARNINGS & EXPERIMENT LOG

This document tracks our evolving understanding and experimental findings related to the question answering task on the current dataset. It serves as a long-term memory to guide future research and development efforts.

## 1. DATASET PATTERNS & CHARACTERISTICS

*   **General:**
    *   The dataset consists of passages describing events (primarily American football games or political events), followed by fact-based questions requiring information retrieval directly from the passage. No external knowledge is needed.
    *   Questions often involve numerical reasoning (arithmetic, comparisons), general knowledge application, and date/time calculations.
    *   Passages can be lengthy, requiring careful reading. Case sensitivity is observed.
    *   Questions often involve comparing or relating two or more pieces of information in the passage, frequently asking for numerical information (quantities, dates, names), requiring precise extraction and comparison.
    *   Passages often present multiple entities or events; questions require distinguishing between them and relating correct information.
    *   Questions may require synthesizing information from multiple sentences or inferring from disparate facts.
    *   Many questions involve interpreting percentages or proportions, requiring correct identification of the base number.
    *   Questions often test the understanding of specific terminology within the passage's context.
    *   The dataset presents questions that require **numerical reasoning and contextual linking**. Many questions ask for percentage differences or values derived from information spread across sentences.
    *   Questions frequently involve **extracting and comparing numerical values**. The system needs to identify the correct numerical values from the passage and perform arithmetic operations.
    *   The dataset contains questions where the **answer is not explicitly stated** but requires inferential reasoning based on the context provided in the passage. Sometimes the answer can't be found in the text.
    *   **Complex Reasoning:** The questions frequently require integrating information from multiple parts of the passage, going beyond simple fact retrieval.
    *   Questions frequently ask for specific values, but the system sometimes hallucinates other relevant pieces of information when it should've been concise.
    *   Questions require precise information extraction.
    *   Answers are usually directly stated but may require simple comparisons (e.g., "more than," "less than") or filtering based on numerical thresholds.
    *   The passages contain a mix of quantitative data (percentages, numbers) and categorical information (names of groups, industries).
    *   Questions often involve identifying groups or categories that meet specific quantitative criteria.
    *   There is a strong emphasis on accurate value extraction, with the need to isolate and compare specific numbers (percentages in the racial makeup question, employment numbers in the industry comparison).
    *   **Calculation/Extraction Focus:** A significant portion of the questions require performing calculations (e.g., summing scores) or extracting specific numerical data from the passage rather than just simple fact retrieval. The model often fails to present the *result* of the calculation, instead restating *what* calculation needs to be done.
    *   **Implicit Information:** The questions frequently require synthesizing information from multiple parts of the passage. The necessary facts are not always explicitly stated in one sentence.

*   **Sports Context:** The dataset heavily features sports passages, often game recaps or summaries. The questions frequently ask about specific numerical details like yardage, scores, or player statistics. Passages are dense with numbers, requiring precise parsing. Basic sports terminology (football positions, scoring) is needed. Questions about sports narratives can induce errors because they require understanding of event order and accurate extraction of details.
    *   Questions are primarily fact-retrieval based on a single passage of text describing a sports game (American football).
    *   Questions frequently ask about specific players and their actions during the game (e.g., who scored, who intercepted, how many points).
    *   **Sports-Related Numerical Data:** A noticeable portion of the dataset revolves around sports passages and numerical details (scores, yardage, etc.), making accurate numerical reasoning critical.

*   **Multiple Relevant Instances:** Questions often implicitly require the model to identify *all* instances of a particular event or object within the passage (e.g., "What yard line did both teams score from?").
*   **Implicit Comparisons/Filters:** Some questions demand the model to perform a calculation (e.g., difference, sum) or filter based on a condition after extracting information (e.g., "How many more households were there with couples married without children than households married with children?").

*   **Entity Relationship Extraction:**
    *   The dataset presents questions requiring identifying relationships between entities within a passage. The target relationship often involves actions (e.g., seizing power, causing casualties) and the entities involved (agent and target).
    *   Questions frequently require pinpointing the target/subject of an action, even when multiple related entities are mentioned in close proximity. This demands precise understanding of sentence structure and pronoun references. The model confuses the person who *preceded* the power seizure with the *victim* of the power seizure (e.g., confusing King William II of Sicily with Queen Joan). The system fails to correctly identify the subject of an action.
    *   The passages are concise, but contain sufficient detail that incorrect attribution is easy.

*   **Date/Time Reasoning:**
    *   A recurring pattern is the need to calculate durations (years, months, days) based on dates or time spans mentioned in the passage. This requires precise arithmetic and attention to inclusivity/exclusivity. A significant portion of the questions requires understanding and comparing dates and events described in the passage. The questions often ask "Which happened later?" or require calculating time differences. A recurring failure mode involves incorrect calculation of the number of months between two dates mentioned in the passage.
    *   **Temporal Reasoning:** Some questions require understanding and comparing events that occur at different times in the passage. Chronological order of events is important. Time-related keywords (e.g., "final," "first," "later," "second") help narrow the search.
    *   The passages contain a lot of temporal information (first quarter, second quarter, etc.) which is crucial for answering many of the questions.

*   **Explicit Information Retrieval:** The questions often require directly extracting and processing information explicitly stated in the passage.
*   **Numeric Answers:** Many questions seek numeric answers, making them easily verifiable but also sensitive to arithmetic errors.
*   **Question Structure:** All questions follow the format: `"PASSAGE:\n[passage text]\n\nQUESTION: [question text]"`. Many questions involve numerical reasoning and comparison.
*   **Answer Structure:** Answers are typically short phrases, numbers, or names.
*   **Passage Structure:** Passages vary in content (sports summaries, scientific descriptions like astronomy, political events). Passages are dense with numbers.
*   **Domain Knowledge:** General reading comprehension is crucial. For demographic examples, basic understanding of fertility rates is needed.
*   **Question Types:** Entity Extraction, Numerical Comparison, Counting.
*   **Reasoning Types:** Direct Extraction, Simple Arithmetic, Logical Deduction, Multi-Sentence Reasoning.
*   **Non-Obvious Patterns:** Chronological order of events is important. Time-related keywords help narrow the search.
*   **Edge Cases/Complexities:** Passages with ambiguous or contradictory information; questions requiring more complex arithmetic operations; questions with implicit rather than explicit answers; handling of units consistently.

## 2. EFFECTIVE TASK-SPECIFIC STRATEGIES

*   **Decompose, Extract, Synthesize:** A modular approach utilizing a question decomposer, information extraction expert, and answer synthesis expert shows promise.
*   **Chain-of-Thought Reasoning (CoT):** Improves interpretability and helps guide the model to extract the right type of information and perform simple comparisons. The ability to break down the question seems to reduce irrelevant information. Effectiveness is limited by the system's ability to accurately understand the question's intent and extract relevant numerical data. While conceptually sound, it doesn't guarantee accurate numerical reasoning or adherence to concise answer formats, even with validation steps.
*   **Zero-Shot Reasoning:** Directly ask the LLM to answer the question based on the passage, using a well-crafted prompt.
*   **Question-Type Determination and Specialized Processing:** Determine the question type (numerical vs. general) and use specialized processing. Failure in question type determination renders the entire system useless.
*   **LLM-Driven Techniques with Verification:** Combining LLM-driven techniques with verification steps at each stage is a successful pattern. Validation is inadequate for semantic errors. Explicit validation steps included in the script (within `decompose_question`, `extract_information`, and `synthesize_answer`) contribute to maintaining accuracy and filtering potentially incorrect extractions. However, these validations do not always succeed.
*   **Dual Verification with Iterative Refinement:** Appears promising, providing a solid base for improvement, but may be too sensitive and identifies a wide range of related answers.
*   **Dynamic Approach Selection:** Adapting the strategy based on the type of question being asked proves important.
*   **Validating intermediate steps:** Decomposition and extraction likely contributes to the overall accuracy.
*   **Providing examples in LLM prompts improves performance.** Explicit examples in the prompt, while intended to guide the LLM, are not always sufficient to overcome the numerical reasoning and over-inclusion issues.
*   **Explicitly prompting the LLM to show its work (intermediate calculations) helps in debugging.**
*   **Question Clarification & Focused Extraction:** Clarifying the question's intent before extraction *should* ideally improve precision.
*   **Iterative Question Refinement with Contextual Expansion (Ineffective in current implementation):** The "Iterative Question Refinement with Contextual Expansion" strategy, in its current implementation, is ineffective for this dataset. The low accuracy and consistent error patterns indicate a fundamental flaw in the approach's ability to translate understanding into execution (Iteration 21).

## 3. COMMON FAILURE MODES ON THIS DATASET

*   **API Connectivity Failure:** Inability to connect to the Gemini API, preventing any processing.
*   **General Script Errors:** `NameError: name 'call_llm' is not defined`.
*   **Root Cause: Incorrect Question Type Determination:** The `determine_question_type()` function consistently returns an error. The question type determination module is a single point of failure.
*   **Temporal Reasoning Errors:** Incorrectly ordering events, calculating time differences, and making off-by-one errors in duration calculation. Misinterpretation of Inclusion/Exclusion. Difficulty determining the *first* or *last* instance of an event.
*   **Answer Synthesis Failure:** Inability to synthesize a valid answer, even when the correct information is extracted. Struggles to provide complete answers, especially when context, units, or modifiers are needed. Struggles to filter out irrelevant entities.
*   **Over-generation of Instances:** The "Dual Verification with Iterative Refinement" approach often retrieves *all* possible instances related to the question from the passage, even when not all are directly relevant. This leads to overly detailed and noisy answers. Inability to apply precise constraints during information extraction. The system identifies both players who intercepted passes, but the question implicitly or explicitly only asks for one (Over-inclusive Extraction).
*   **Lack of Aggregation/Filtering:** The system struggles to perform necessary filtering or aggregation after extracting the information.
*   **Sports narrative complexity:** Requires understanding of event order and accurate extraction of details.
*   **Validation sensitivity:** Validation checks can be overly strict, but are still often insufficient. The validation step isn't always effective in catching and correcting numerical or over-inclusion errors. Validation may be too simplistic and the final answer may still be wrong. LLM validation is also ineffective as the LLM struggles to distinguish between a description of a calculation and the calculated result.
*   **Information Extraction Failure:** Inaccurate or incomplete extracted information. Pulls *additional* correct information when only *specific* information is requested. Numerical extraction errors.
*   **Ambiguous Passages:** Passages with ambiguous or contradictory information.
*   **Missing Information:** If the answer cannot be found in the passage. LLM incorrectly claims that information is missing when it's present in the passage. This suggests an issue with information extraction and entity recognition, possibly due to the complexity of the passage or the way the question is phrased (Iteration 21).
*   **Lack of Arithmetic Reasoning:** Inability to perform arithmetic operations, particularly simple addition.
*   **Calculation errors:** Performing incorrect arithmetic operations on extracted numbers. Struggles with simple arithmetic (e.g., subtraction). The LLM may not be performing the calculation accurately or extracting the correct numbers to begin with.
*   **Synthesis errors:** Generating a response that is not a coherent or accurate answer to the question.
*   **Question type misclassification**.
*   **Unit Handling:** Lack of proper unit handling leads to incomplete or misinterpreted answers.
*   **Misinterpretation of Question Intent**.
*   **Incorrect Numerical Calculation.**
*   **Lack of Verification:** Lacks a final answer verification step to check the reasonableness of the answer.
*   **Numerical Extraction in Dense Passages:** Difficulty in accurately extracting numerical values and associating them with the correct entities.
*    **Incorrect Subject Identification:** The system fails to correctly identify the subject of an action.
*   **Proximity Bias:** The LLM seems to exhibit proximity bias, selecting entities mentioned closer to the action verb.
*   **Lack of Deep Understanding of Numerical Comparisons:** Appears to sometimes fail to associate the numbers with the correct context, leading to incorrect comparisons.
*   **Lack of "Who" Question Understanding:** The system does not appear to know how to resolve "who" questions.
*   **Semantic Equivalence:** Errors occur when the system gives an answer that is not semantically equivalent to the reference answer, for example, "Novgorod" vs "Novgorodians".
*   **Misinterpretation of context**.
*   **Incorrect extractions**.
*   **Incorrect formatting:** Added extra spaces in the answer.
*   **Over-Inclusion of Information:** The system includes unnecessary or incorrect information in the answer, deviating from the precise requirement of the question. The LLM might lack precise instruction following skills, be unsure of what information is truly needed, or it might try to "over-answer" to be helpful (but missing the goal). *Example: In the "racial groups" question, including "White American" when only the highest percentage is requested. The group is present in the passage but doesn't meet the implicit filtering criteria.* The "greediness" issue suggests an over-reliance on potentially valid details, even if they aren't strictly required. This might stem from the LLM's general knowledge or tendency to provide comprehensive answers rather than minimal, targeted ones.
*   **"Description Instead of Value" Error:** The primary failure mode is the LLM identifying the *correct operation to perform* or the *correct information to extract* but failing to actually *execute* that operation and provide the numerical answer or extracted entity. For example, it correctly identifies that it needs to sum the scores from the second half, but it then outputs "Total score in the second half of the game" instead of the calculated total. This indicates a breakdown in the final execution or synthesis step (Iteration 21).
*   **Multi-Step Reasoning Breakdown:** When questions require multi-step reasoning (e.g., identifying multiple touchdowns and then the player who scored more than one), the system struggles to maintain accuracy across all steps. It might correctly identify individual events but fail to synthesize them to answer the overall question (Iteration 21).

## 4. EXPERIMENT LOG & FINDINGS

*   **2025-05-17 13:35:33: INITIAL DATASET ANALYSIS**: Comprehensive analysis of dataset characteristics, challenges, potential approaches, and implementation recommendations. Highlighted potential solution strategies: Keyword-Based Retrieval, LLM-Based Extraction & Reasoning, Hybrid Approach. Outlined decomposition steps: Question Analysis, Passage Filtering, Information Extraction, Answer Generation. Proposed validation techniques: Consistency Checks, Unit Analysis, Numerical Validation, Fact Verification. Prompt engineering techniques for effective text processing were proposed, including chain-of-thought prompting.
*   **ITERATION 0:** Accuracy: 0.67. Goal: Improve performance using question decomposition and reasoning. Finding: Question decomposition has challenges with reliable answer synthesis. Frequent validation failures indicate problems with downstream steps. Sports-related questions involving temporal reasoning pose significant challenges. Insight: Validation is helpful for diagnosing system performance, specifically highlighting failures in answer synthesis.
*   **ITERATION 1:** Accuracy: 1.00
*   **ITERATION 2:** Accuracy: 1.00. Goal: Improve performance using question-type determination and specialized processing. Finding: High accuracy suggests question-type determination followed by specialized processing is highly effective. Dynamic approach selection emphasizes the importance of adapting the strategy based on the type of question being asked. Caveat: The lack of error cases makes it impossible to determine the system's limitations definitively.
*   **ITERATION 3:** CoT is beneficial. Limitation: Lack of arithmetic reasoning capabilities. Individual role definitions for the LLMs (question decomposer, information extractor, answer synthesizer) needs more definition.
*   **ITERATION 4:** Accuracy: 1.0. Goal: Improve answer completeness, focusing on context, units, and modifiers. Achieving 1.0 accuracy suggests that the overall strategy of chain-of-thought, role-based agents, and validation is effective. The primary area for improvement is in the completeness and context of the final answer.
*   **ITERATION 5:** Chain-of-thought benefits are diminished by semantic errors in question interpretation. Verification steps are inadequate for addressing semantic errors.
*   **ITERATION 6:** Accuracy: 0.80. Goal: Improve semantic understanding and numerical reasoning by leveraging chain-of-thought, prompt examples, and showing work. Chain-of-thought with question decomposition, information extraction, and answer synthesis showed promise. Providing examples in LLM prompts seems to improve performance. Explicitly prompting the LLM to show its work (intermediate calculations) helped in debugging. Inconsistent and unreliable arithmetic calculations are a major failure mode. The LLM sometimes fails to correctly interpret or apply units. The system lacks a final answer verification step to check the reasonableness of the answer. Arithmetic and unit errors indicate a need for more robust numerical reasoning capabilities.
*   **ITERATION 7:** Accuracy: 0.0. Goal: Improve semantic understanding and numerical reasoning capabilities. This iteration provided no usable data due to the undefined `call_llm` function. The script is fundamentally broken without the `call_llm` function.
*   **ITERATION 8:** Accuracy: 0.0. Goal: Improve semantic understanding and numerical reasoning capabilities. The `determine_question_type()` function consistently failed, leading to a complete system failure.
*   **ITERATION 9:** Goal: Improve handling of date/time reasoning, specifically duration calculations. The "Reading Comprehension Expert" agent with self-debate, in its current form, is not robust enough to prevent common arithmetic errors, especially off-by-one errors in duration calculation. The agent's general reasoning capabilities are insufficient for accurate duration calculations. Self-Debate Alone Is Insufficient: The agent identifies the potential for errors but fails to consistently avoid them.
*   **ITERATION 10:** Accuracy: 0.80
*   **2025-05-17 20:26:38: SCRIPT ERROR:** Error detected during script repair (attempt 1): ERROR: Script requires external information to provide an answer.
*   **2025-05-17 20:26:49: SCRIPT ERROR:** Error detected during script repair (attempt 2): ERROR: No passage provided.
*   **ITERATION 11:** Accuracy: 0.80. Goal: Improve accuracy by clarifying the question's intent before information extraction. Clarifying the question isn't enough; the extraction phase needs to be more robust to identifying the correct entity related to a specific action. Failure Example: Incorrect subject identification. The LLM seems to exhibit proximity bias, selecting entities mentioned closer to the action verb.
*   **ITERATION 12:** Accuracy: 0.60. Goal: Improve performance with better handling of temporal and numerical reasoning by leveraging question decomposition and validation.
*   **ITERATION 13:** Accuracy: 0.70. Goal: Improve performance with an exploitation strategy with chain-of-thought reasoning. The use of examples in LLM prompts alone is insufficient to overcome the challenges of numerical reasoning and constraint application.
*   **ITERATION 14:** Accuracy: 0.60. Goal: Improve performance with an exploitation strategy with chain-of-thought reasoning. The chain-of-thought approach helps, but the individual components need to be more robust, especially in handling numerical data and conditional filtering. Failure Examples: "Billy Cundiff vs. Sebastian Janikowski" question: Incorrect identification of field goal lengths. "Countries on the 2014 Summer tour" question: Incorrect inclusion of "Germany" due to failure to properly filter based on the "more than one date" condition.
*   **ITERATION 15:** Accuracy: 0.70. Reliance on chain-of-thought prompting is not sufficient to guarantee accurate numerical extraction and calculation. Failure Examples: "How many more yards was Nate Kaeding's second field goal over his first?": The system extracted "27" instead of calculating the difference between 51 and 24. Semantic equivalence: Errors occur when the system gives an answer that is not semantically equivalent to the reference answer, for example, "Novgorod" vs "Novgorodians".
*   **ITERATION 16:** Accuracy: 0.80. The Dual Verification with Iterative Refinement approach appears promising but needs refinement to avoid over-generation of instances and improve aggregation/filtering.
*   **ITERATION 17:** Accuracy: 0.80. Exploitation strategy demonstrates the potential of the overall architecture. The high accuracy confirms the basic approach of question decomposition, information extraction, and answer synthesis is a viable strategy for this dataset. Improving the precision of the information extraction step is crucial for further improving performance. System identifies both players who intercepted passes, but the question implicitly or explicitly only asks for one (Over-inclusive Extraction).
*   **2025-05-17 20:47:28: SCRIPT ERROR:** Error detected during script repair (attempt 1): ERROR: Script failed due to missing attribute in google.genai module and question decomposition failure.
*   **2025-05-17 20:47:42: SCRIPT ERROR:** Error detected during script repair (attempt 2): ERROR: Gemini API call failed with 404 error, and question decomposition failed.
*   **2025-05-17 20:47:53: SCRIPT ERROR:** Error detected during script repair (attempt 3): ERROR: Unexpected keyword argument 'model' and failed question decomposition.
*   **ITERATION 18:** Due to the complete failure to connect to the LLM, no strategies were actually tested. The core failure stemmed from the inability to connect to and utilize the Gemini API. API failure resulted in a cascading failure; because question decomposition couldn't be performed, the entire pipeline halted.
*   **ITERATION 19:** The "chain-of-thought" approach, explicit examples in the prompt, and validation steps were not sufficient to guarantee accurate numerical reasoning or adherence to concise answer formats. Example failure: The system struggled with simple arithmetic (subtraction in the provided example) when asked to calculate the difference between two field goal lengths, and it included unnecessary information in the answer.
*   **ITERATION 20:** Accuracy: 0.80. The chain-of-thought approach with validation shows promise, but the specific failure mode of including unnecessary information highlights the need for more stringent filtering mechanisms. An example is the inclusion of "White American" in the racial groups question when only the highest percentage is required. This suggests an over-reliance on potentially valid details, even if they aren't strictly required.
*   **ITERATION 21:** "Iterative Question Refinement with Contextual Expansion" strategy proved ineffective. A primary failure mode is the LLM identifying the correct operation to perform or the correct information to extract but failing to actually execute that operation and provide the numerical answer or extracted entity. The LLM incorrectly claims that information is missing when it's present in the passage. Multi-step reasoning also fails in this iteration. LLM-based validation steps are not effectively preventing the "description instead of value" errors from propagating.

## 5. NEXT RESEARCH DIRECTIONS

*   **Address API Connectivity immediately:**
    *   Verify API keys, authentication methods, and network configurations.
    *   Implement Robust Error Handling: Include more specific error handling and logging around the API call to immediately identify connectivity problems with descriptive error messages.
    *   Test API Connectivity Separately: Create a simple test to verify API connectivity.
    *   Fallback Mechanism: Consider a fallback mechanism (e.g., a local, smaller model) to provide a basic response in case of API failures.
*   **Address `call_llm` and `determine_question_type()` immediately:**
    *   **Define or import `call_llm` immediately.**
    *   **Critically, redesign/debug `determine_question_type()`.** Consider:
        *   **More examples:** Providing significantly more examples to the LLM prompt.
        *   **Simpler prompt:** Attempting a simpler prompt focused solely on question classification.
        *   **Fine-tuning:** Fine-tuning a smaller, specialized LLM on question type classification.
        *   **Direct few-shot classification:** Prompt the LLM to directly classify questions with a few examples without chain-of-thought.
    *   **Implement Validation & Fallback:** Add validation logic to `determine_question_type()` with a fallback mechanism.
*   **Strengthen Numerical Reasoning:**
    *   Implement a post-processing step that uses a calculator or numerical reasoning module to verify and correct numerical calculations performed by the LLM. This external tool should be directly responsible for the calculation requested in the question.
    *   Add prompt examples that explicitly demonstrate the desired format for numerical answers and what level of precision is expected.
*   **Improve Answer Conciseness:**
    *   Refine the `synthesize_answer` agent's instructions to emphasize brevity and strict adherence to the question's requirements. The instructions should directly instruct it to avoid extraneous information.
    *   Include negative examples in the prompt that demonstrate what *not* to include in the answer (e.g., "Question: X. Incorrect Answer: X and Y. Correct Answer: X").
*   **Enhance Validation:**
    *   Modify the validation step to specifically check for both numerical accuracy and conciseness. The validation prompt should include explicit checks for extraneous information.
    *   Consider using a separate LLM or a rule-based system for validation that focuses *only* on the specific criteria (numerical correctness, conciseness). This could be more reliable than relying on the same LLM that generated the initial answer.
*   **Refine Information Extraction with Strict Constraints:** Modify the `extract_information` function to incorporate more explicit constraints based on the sub-question being answered. This could involve adding more negative examples in the prompt, or using a more sophisticated filtering mechanism. The LLM needs to be better at determining which of the two players meets the *implicit* criteria in the question.
*   **Enhance Temporal Reasoning:** Explicitly incorporate temporal reasoning into the question decomposition and information extraction steps. When a question involves time, ensure the system identifies and uses relevant temporal clues from the passage to filter extracted information.
*   **Improve Verification Step:** Add specific checks to the verification prompts to explicitly ask if the answer is complete and contains no extraneous information.
*   **Introduce a Filtering/Selection Step:** Add a dedicated filtering/selection step *after* information extraction but *before* answer synthesis. This step would be responsible for applying the final constraints and selecting the most relevant information based on the question's requirements.
*   **Error Handling:** Implement better error handling throughout the system to provide more informative debugging messages.
*   **Analyze the results of the re-run. Focus on the accuracy of `determine_question_type`, `extract_numerical_info`, and `extract_information`.**
*   **If the above modules are problematic, refine the prompts used by `call_llm` within each of those functions, providing more examples specific to football game summaries and question types.**
*   **Implement Explicit Aggregation:** For questions requiring aggregation, incorporate a dedicated aggregation step in the `synthesize_answer` function.
*   **Add Unit Verification:** Implement a module that checks the predicted unit against the expected unit based on the question.
*   **Implement Answer Sanity Checks:** Add a final step to evaluate the answer for sanity.
*   **Enhance training examples for numerical and unit understanding:** Fine-tune the LLM with more examples that focus on numerical reasoning and unit conversions.
*   **Test on edge cases:** Create specific test cases that target potential arithmetic errors, incorrect unit handling, and misunderstanding of specific terminology.
*   **Enhance Semantic Understanding.**
*   **Improve Numerical Reasoning.**
*   **Refine Verification Strategies:** Adapt the verification steps to specifically target potential semantic errors. Enhance Semantic Validation. Refine the validation call to the LLM to include a more robust assessment of the semantic equivalence between the system and golden answers.
*   **Enhance the answer synthesis stage:** Modify the prompt for the answer synthesis agent to explicitly request that it include all relevant context, units, and modifiers from the original passage.
*   **Post-processing for completeness.**
*   **Arithmetic Reasoning Module.**
*   **Refine Answer Synthesis Prompt.**
*   **Include Examples of Arithmetic Reasoning in Prompts.**
*   **Evaluate Different LLMs for Numerical Reasoning.**
*   **Answer Synthesis Improvement.**
*   **Refine Validation Logic.**
*   **Evaluate Information Extraction Success.**
*   **Dataset Split & Analysis.**
*   **Prompt Engineering.**
*   **Context Window Management.**
*   **Error Analysis.**
*   **External Knowledge Integration.**
*   **Introduce Complexity.**
*   **Introduce Unit Tests.**
*   **Verification specifically for temporal calculations.**
*   **Analyze successes.**
*   **Refine Extraction Prompts:** Modify the `extract_information` prompt to explicitly instruct the LLM to focus on identifying the *subject* of the action described in the question, not just any related entity. Include examples of correct and incorrect subject identification.
*   **Add Subject-Verb-Object Triplet Identification.**
*   **Improve Clarification of "Who" questions.**
*   **Implement a "reasoning chain".**
*   **Add more error examples.**
*   **Enhance Numerical Understanding and Comparison.**
*   **Refine Validation.**
*   **Address Quantifier Issues.**
*   **Enhance numerical reasoning capabilities.**
*   **Improve context understanding and constraint handling.**
*   **Improve the answer formatting.**
*   **Strengthen Information Extraction for Numerical Data.**
*   **Strengthen Calculation Logic.**
*   **Improve Conditional Filtering in Answer Synthesis.**
*   **Dataset Augmentation (Potential):** If possible, create synthetic data or augment the existing dataset with more examples that specifically test numerical reasoning and concise answering. This would provide a more targeted training set for addressing the identified failure modes.
*   **Explicitly train on examples involving Arithmetic Reasoning.**
*   **Refine Answer Synthesis Validation:** Strengthen the `synthesize_answer` function's validation step to explicitly penalize or remove information that isn't *directly* required by the question. Focus on filtering based on the specific criteria outlined in the question, not just on overall relevance.
*   **Implement a "Minimal Answer" Constraint:** Add an explicit instruction or constraint to the LLM interaction (perhaps via prompt engineering) emphasizing the need to provide the *smallest possible* set of information that answers the question accurately. For example, add "Answer the question using the fewest possible words".
*   **Add Negative Constraints to Prompts:** Experiment with prompts that explicitly exclude the common failure mode, such as "Only list groups that exceed 30% and exclude any that do not". This may help the LLM to focus its attention on the exact filtering criterion.
*   **Strengthen Execution/Calculation Stage:** The most critical adaptation is to reinforce the final "synthesize_answer" stage to ensure it outputs the *calculated numerical value* or *extracted entity* rather than just a description of the required action. This might involve more explicit system instructions, examples of correct output format, or a separate calculation module.
*   **Implement Unit Tests for Calculation:** Add unit tests that explicitly check for the numerical output in the synthesize_answer phase. This is *especially* important because the LLM validation has proven insufficient to catch these errors.
*   **Re-evaluate Problem Decomposition:** Simplify problem decomposition. Re-evaluate the problem decomposition into four distinct stages. It's possible that the strict separation is hindering the flow of information and contributing to the execution failures. Consider consolidating stages or allowing for more interaction between them.
```