
            You are creating a NEW Python script by SYNTHESIZING the best elements from multiple successful approaches.
            Your goal is to identify what makes each approach successful and combine these strengths into a superior hybrid solution.
    
            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "What is the resolution of the Cat B15 Q in pixels?",
    "answer": "480 x 800"
  },
  {
    "id": 1,
    "question": "On which day, month, and year did the Hubble Telescope enter a partial safe mode following suspected hardware problems in its most advanced instrument, the Wide Field Camera 3 instrument?",
    "answer": "January 8, 2019"
  },
  {
    "id": 2,
    "question": "On what date (day/month/year) was Makhdum Khusro Bakhtyar (Pakistani politician) inducted into the Federal Cabinet of Prime Minister Shaukat Aziz?",
    "answer": "4 September 2004"
  }
]
    
            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 10
        - Current explore/exploit balance: 18/55
        - Best accuracy achieved: 0.67 (iteration 2)

        APPROACH HISTORY (last 10 iterations):
        [
  {
    "iteration": 0,
    "strategy": "baseline",
    "accuracy": 0.1,
    "approach": "Simple baseline script: Direct LLM call without sophisticated techniques"
  },
  {
    "iteration": 1,
    "strategy": "explore",
    "accuracy": 0.0,
    "approach": "This script answers questions using LLMs by decomposing the problem into entity/relationship extraction, search query generation, information retrieval, answer generation, and answer validation. Each step leverages the Gemini LLM with a specific system instruction to act as a specialized agent (e.g., information extractor, query generator). The script uses a few functions: `call_llm` for interacting with the Gemini model, `extract_entities_and_relationships` to identify key information, `generate_search_query` to create a search query, `retrieve_information` to simulate a search engine, `generate_answer` to formulate an answer, and `validate_answer` to check the answer's correctness. The overall workflow involves sequentially calling these functions, passing the output of one as input to the next, ultimately returning the validated answer or an error message."
  },
  {
    "iteration": 2,
    "strategy": "exploit",
    "accuracy": 0.6666666666666666,
    "approach": "The script employs a validation-driven problem-solving approach using the Gemini LLM, where a problem is first solved and then iteratively refined based on validation feedback. The problem is decomposed into solution generation and solution validation steps. There are two agent roles: a solver and a validator, each with specific system instructions. `call_llm` is used to interact with the LLM by passing prompts and instructions, while `solve_with_validation_loop` manages the iterative solving and validation process, and `main` is the entry point that calls `solve_with_validation_loop`. The overall workflow involves generating an initial solution with `call_llm` using the solver agent, validating it with `call_llm` using the validator agent, and refining the solution using `call_llm` with feedback until it's deemed valid or the maximum number of attempts is reached."
  },
  {
    "iteration": 3,
    "strategy": "explore",
    "accuracy": 0.3333333333333333,
    "approach": "The script uses LLM-based information retrieval and answer generation. It decomposes the problem into retrieving relevant context and then generating an answer using that context. The `retrieve_relevant_context` function acts as a search agent, generating search queries, simulating search, and validating the retrieved context for relevance. The `generate_answer_with_context` function acts as an expert at answering questions using the provided context.\n\nThe `call_llm` function is used to interact with the Gemini LLM, taking prompts and system instructions as input. The `retrieve_relevant_context` calls `call_llm` to generate search queries, simulate search, and validate context relevance; this function is then called by the `main` function, which then calls `generate_answer_with_context`, which then calls `call_llm` to create an answer using the retrieved context, finally returning it. The `main` function orchestrates the workflow by calling both functions sequentially."
  },
  {
    "iteration": 4,
    "strategy": "explore",
    "accuracy": 0.6666666666666666,
    "approach": "The script implements a retrieval-augmented generation (RAG) approach to answer questions using an LLM. It decomposes the problem into query generation, search snippet validation, and answer generation. The `generate_query_and_validate` function uses an LLM with the \"expert at generating effective search queries\" role to create a search query and then validates the query using another LLM call with the role \"expert at validating search snippets\" before proceeding; It uses a few-shot validation technique. The `generate_answer_with_snippets` function uses an LLM with the role \"expert at answering questions given relevant search snippets\" to formulate an answer based on the validated search snippets.\n\nThe overall workflow begins in `main` which calls `generate_query_and_validate` with a question, which in turn uses `call_llm` to generate a search query and validate the results. `main` then calls `generate_answer_with_snippets` with the original question and the validated search snippets, which in turn uses `call_llm` to generate the final answer. The `call_llm` function is used throughout the script to interface with the Gemini LLM for query generation, snippet validation, and answer generation."
  },
  {
    "iteration": 5,
    "strategy": "exploit",
    "accuracy": 0.0,
    "approach": "The script implements a RAG-based approach with a validation loop for answering questions. It decomposes the problem into query generation, search snippet retrieval (simulated), answer generation, and solution validation. The script uses two agent roles: a problem solver/answer generator and a validator, both driven by the `call_llm` function.\n\nKey functions:\n*   `call_llm`: Interacts with the Gemini model.\n*   `generate_query_and_validate`: Generates and validates a search query against search snippets to ensure relevance.\n*   `generate_answer_with_snippets`: Generates an answer based on the provided search snippets.\n*   `solve_with_validation_loop`: Orchestrates the RAG process, incorporating a validation loop to refine the answer.\n*   `main`: Calls `solve_with_validation_loop` to return an answer to the user's question\n\nThe workflow starts with `solve_with_validation_loop`, which calls `generate_query_and_validate` and `generate_answer_with_snippets` to get an initial answer. This answer is then iteratively validated, and if found invalid, the query and answer generation steps are rerun."
  },
  {
    "iteration": 6,
    "strategy": "exploit",
    "accuracy": 0.3333333333333333,
    "approach": "The script uses a combination of RAG and a validation loop to answer questions. It first generates a search query, retrieves and validates search snippets, and then generates an initial answer using these snippets. The answer is then iteratively validated and refined through a validation loop, with the LLM acting as a validator and a refiner, until a valid answer is found or the maximum attempts are reached. The functions used include `call_llm` for LLM interaction, `generate_query_and_validate` for RAG, `generate_answer_with_snippets` for answer generation, `solve_with_validation_loop` for the iterative process, and `main` to orchestrate everything."
  },
  {
    "iteration": 7,
    "strategy": "explore",
    "accuracy": 0.6666666666666666,
    "approach": "The script implements LLM-Guided Recursive Decomposition & Verification (LLM-RDRV) to answer complex questions. It decomposes the original question into sub-questions, answers each sub-question individually, verifies the answers, and synthesizes them into a final answer. This involves agent roles like question decomposer, answerer, and validator. The functions used are `call_llm`, `decompose_question`, `answer_sub_question`, `verify_answer`, `synthesize_answers`, and `main`. The `main` function orchestrates the process by calling `decompose_question` to break down the initial question, then iterates through the sub-questions, using `answer_sub_question` to find answers, and `verify_answer` to check the validity of each response before finally using `synthesize_answers` to give the final output."
  },
  {
    "iteration": 8,
    "strategy": "explore",
    "accuracy": 0.6666666666666666,
    "approach": "The script uses LLM-Guided Iterative Context Expansion & Focused Summarization (LLM-ICE-FS) to answer questions. It decomposes the problem into entity extraction, iterative context expansion, focused summarization, and answer verification. The agent roles include an entity extractor, information gatherer, summarizer, and validator, all implemented via prompting the LLM with specific system instructions.\n\nKey functions include: `extract_key_entities` (extracts entities from the question), `expand_context` (gathers information about entities), `summarize_context` (summarizes the context to answer the question), and `verify_answer` (verifies the answer). The overall workflow involves first extracting entities, then iteratively expanding the context around those entities, summarizing the context to generate an answer, and finally verifying the answer for accuracy."
  },
  {
    "iteration": 9,
    "strategy": "exploit",
    "accuracy": 0.6666666666666666,
    "approach": "This script uses a combination of LLM-based techniques including chain-of-thought reasoning, retrieval-augmented generation (RAG), and iterative refinement with validation to answer questions. The problem is decomposed into query generation, search snippet retrieval, relevance validation, and answer generation. The script employs agent roles such as a query generator, search validator, problem solver, and solution validator.\n\nThe functions used are:\n*   `call_llm`: Makes calls to the Gemini LLM.\n*   `generate_query_and_validate`: Generates a search query and validates its effectiveness using LLM-based relevance checking.\n*   `generate_answer_with_snippets`: Generates an answer given search snippets\n*   `solve_with_validation_loop`: Solves the problem with iterative refinement via validation feedback; it calls `generate_query_and_validate` to obtain search snippets to be used by `generate_answer_with_snippets` and the LLM to provide/validate the answer.\n\nThe overall workflow involves generating an initial solution (potentially using search snippets), validating it, and iteratively refining it based on validation feedback until a valid solution is found or the maximum number of attempts is reached."
  }
]

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 3 iterations):
        [
  {
    "iteration": 0,
    "issue": "The most critical problem is the system's reliance on an unreliable knowledge source which leads to the retrieval and provision of factually incorrect information. The lack of a verification mechanism exacerbates this issue, as the system blindly trusts the incorrect information."
  },
  {
    "iteration": 1,
    "issue": "The most critical problem is the **inaccurate and unreliable information extraction process, coupled with insufficient validation and error detection**. The system needs to be significantly improved in its ability to pinpoint the specific information required to answer the question correctly and verify that the retrieved information is accurate and relevant."
  },
  {
    "iteration": 2,
    "issue": "The primary issue is **inaccurate knowledge retrieval**. The system provides a definite answer that is factually incorrect, indicating a flaw in its information gathering or database. This highlights the need for improved source reliability and validation."
  },
  {
    "iteration": 3,
    "issue": "The primary issue is **inaccurate factual recall, particularly involving dates and specific names**. This leads to the system providing incorrect answers even when it understands the question's intent. The root cause could stem from flaws in the training data, the knowledge retrieval mechanism, or the final answer selection process."
  },
  {
    "iteration": 4,
    "issue": "The single most critical problem is the **inadequate retrieval of relevant information from search snippets**. The search queries and information extraction methods used by the system are not specific or robust enough to find the required details, leading to \"information not present\" answers even when the information is potentially available."
  },
  {
    "iteration": 5,
    "issue": "The primary issue is **inaccurate and/or incomplete information retrieval from the knowledge source.** This manifests as providing imprecise answers, failing to find existing answers, or providing incorrect specific details. The system's retrieval mechanism needs to be improved to ensure accuracy and completeness."
  },
  {
    "iteration": 6,
    "issue": "The system's inability to extract precise information, specifically dates and time periods, from the available data is the most critical problem. It relies too heavily on exact matches and cannot synthesize or infer answers when precise details are missing."
  },
  {
    "iteration": 7,
    "issue": "The primary issue is the system's inability to systematically and reliably process generated sub-questions and integrate the answers to derive the final response. The sub-question generation is effective, but the execution and synthesis steps are flawed."
  },
  {
    "iteration": 8,
    "issue": "The primary issue is the system's premature conclusion that information is unavailable coupled with a flawed validation process that confirms this incorrect conclusion. This leads to the system failing to find and provide correct answers that require more in-depth search or inference."
  },
  {
    "iteration": 9,
    "issue": "The primary issue is the system's failure to retrieve the necessary information from the available knowledge sources to answer the question about Makhdum Khusro Bakhtyar's induction date."
  }
]

        TARGETED IMPROVEMENTS:
        [
  "Confidence Scoring and Thresholding:** Develop a confidence scoring mechanism for potential answers and set a threshold for acceptance. Answers below the threshold should be rejected, and the system should indicate its uncertainty or seek clarification.",
  "Stage 3: Verification and Validation:** Verify the accuracy and consistency of the filtered information by cross-referencing with other sources, applying logical rules, or using external validation tools.",
  "Stage 1: Focused Retrieval:** Retrieve a narrow set of potentially relevant information based on keywords and entity recognition.",
  "Improved Ranking Algorithm:** Experiment with different ranking algorithms to prioritize answers based on factors such as source credibility, frequency of mention, and relevance to the question's context. For example, prioritize information extracted from reputable encyclopedias or biographical databases.",
  "Fact Verification Module:** Integrate a fact verification module that cross-references potential answers against multiple external sources to assess their accuracy and reliability.",
  "Stage 2: Semantic Filtering:** Filter the retrieved information based on semantic relevance to the question. Use techniques like sentence similarity or knowledge graph traversal to identify the most relevant pieces of information.",
  "Data Verification and Augmentation:** Implement automated and manual processes to verify and correct information in the knowledge base. Focus on fact-checking and updating numerical data (especially dates). Consider augmenting the knowledge base with more comprehensive biographical data, especially for prominent figures."
]
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity ✓
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity ✓
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity ✓
    4. All operations have O(1) average time complexity ✓
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        
    
            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        ```
# Dataset-Specific Experiment Log: Question Answering

This document serves as a continuously updated log of patterns, strategies, and findings related to the question-answering task for this specific dataset. It prioritizes concrete, task-specific insights over general principles.

## 1. DATASET PATTERNS & CHARACTERISTICS

*   **Question Type Dominance:** Predominantly *Who* questions, seeking individuals/groups associated with events, creations, or awards (e.g., "Who created Groove Coaster?"). "What" questions can focus on parts of names (e.g., "What was the first name of Ralph E. Oesper?").
*   **Answer Type:** Short-form, factual names, groups, dates/numbers, or numerical facts (e.g., "How many losses...?"). Precise details are crucial.
*   **Knowledge Breadth:** Wide range of topics (music production, chemistry history, oceanography, TV series details). Broad domain knowledge is needed.
*   **Question Specificity:** Varies from precise to requiring interpretation.
*   **Structure and Format:** Questions are natural language sentences; answers are noun phrases or names. Each entry has an ID field.
*   **Reasoning Type:** Primarily fact retrieval; answers are facts needing extraction from a knowledge source.
*   **Entity and Relationship Focus:** Questions contain entities (person, place, organization) and relationships (purchased, announced). The system needs to track and correlate information from potentially disparate sources across time or classification changes.
*   **Date Sensitivity:** Correctness is highly sensitive to dates; even slight variations are incorrect. Distinguish "October 20" vs. "21 of October." Questions demand precise factual recall, including years ("In which year did...") and months ("In which month of 2005...").
*   **Precise Date & Identity Focus:** Pinpoint accuracy is needed regarding dates or individual identities. Partial/approximate answers are incorrect.
*   **Factual Recall, Not Reasoning:** Questions primarily test factual recall rather than complex reasoning. Answers are likely directly stated in the knowledge source, but finding the exact right snippet is crucial.
*   **Varied Temporal Scope:** Questions span a range of historical periods.
*   **Need for Completeness:** Answers must include all parts requested in the question (e.g., day, month, and year when asked for).
*   **Complex Relationships:** Requires identifying relationships between entities across different time periods or classification schemes (e.g., genus changes, visit dates).
*   **Implicit Assumptions:** Some questions rely on implicit assumptions or background knowledge.
*   **Comparative Reasoning:** Requires the system to compare and contrast information (e.g., "moved *to* from *Turdus* *before* finally being classified").
*   **Entity Recognition & Disambiguation:** Questions involve named entities (people, organizations, awards) that require correct identification and disambiguation.
*   **Complex Relational Queries:** Understanding the relationship between multiple entities (e.g., person, achievement, organization) is key.
*   **Compound Information:** Questions often contain multiple pieces of information (name, title, location), requiring accurate integration and filtering. Example: "In which month of 2005 was the Lal Bahadur Shastri Dam... completed?"
*   **Ambiguity in "First" or "Pioneer" Questions:** Questions may have multiple valid answers or lack a definitive answer.
*   **Temporal Specificity:** Many questions require extraction of specific dates or time periods. Questions often involve specific dates, years, or periods (e.g., "...spend the year 1973-74...").
*   **Entity-Rich Context:** The questions frequently include detailed contextual information about the entities involved (e.g., "Satyanarayan Gangaram Pitroda (an Indian telecommunication engineer and entrepreneur)").
*   **Fact Verification Challenge:** The questions target factual knowledge that may require precise lookup.
*   **Specific Patch/Version Queries:** A significant portion of the questions requires pinpointing exact versions or patches for specific changes within games or software (e.g., "In what patch did..."). Demands precise information retrieval and accurate matching.
*   **Specific Fictional Worlds:** The dataset contains questions heavily reliant on knowledge of specific fictional worlds (e.g., "Severance").
*   **Relationship Inference:** Some questions (e.g., parent-child relationships) require inference based on the provided context, rather than direct factual recall.
*   **Date Specific Questions:** (ITERATION 9 ADDITION) Significant portion of the questions require retrieving specific dates (day/month/year) associated with events or people. This demands precise information retrieval.
*   **Entity Recognition Importance:** (ITERATION 9 ADDITION) The questions often revolve around named entities (people, organizations, telescopes) requiring accurate entity recognition and linking within the knowledge source.
*   **Complex Relational Reasoning:** (ITERATION 9 ADDITION) Some questions require understanding and linking relationships between entities, such as a politician and the Prime Minister under whom they served.

## 2. EFFECTIVE TASK-SPECIFIC STRATEGIES

*   **NONE:** (Iteration 5 result) No aspect of the current strategy is working effectively for this dataset.
*   **Ineffective:** Direct LLM question answering without knowledge retrieval or verification (Baseline Experiment). Accuracy was only 10%.
*   **Ineffective:** Simple chain-of-thought approach using specialized LLM agents for each step (extraction, query generation, retrieval, answer generation, validation). Accuracy was 0%.
*   **Ineffective:** RAG implementation with current settings and the "explore" strategy (Iteration 4). RAG architecture with current validation loop (Iteration 5).
*   **Ineffective:** The validation loop alone, even with RAG (Iterations 2 and 5).
*   **Ineffective:** The RAG architecture, with its current validation loop, (Iteration 5). The `generate_query_and_validate` function isn't ensuring that the generated queries are precise enough to retrieve the exact information needed.
*   **Ineffective:** The accuracy of 0.67 in Iteration 7 indicates that simply decomposing the question and answering sub-questions is insufficient. The verification and, critically, the synthesis stages are hindering performance.
*    **Potentially Effective:** While overall accuracy is only 0.67, the LLM-ICE-FS (LLM Iterative Context Expansion with Focused Summarization) strategy from Iteration 8 does show promise in principle.
*   **Potentially Effective:** The validation loop approach has promise, but it needs to be coupled with more robust information retrieval (Iterations 2 and 5).
*   **Potentially Effective:** Decomposing the question is likely useful.
*   **Potentially Effective:** (ITERATION 9 ADDITION) The use of RAG (Retrieval-Augmented Generation) is promising. The framework itself allows for iterative refinement by generating a query, using it to find relevant information, then validating.
*   **Untested:** Knowledge Base Retrieval (using LLM to formulate queries for external knowledge bases).
*   **Untested:** Hybrid Approach (LLM for query rephrasing and search results informing answer generation).
*   **Untested:** Entity and Relation Extraction before Knowledge Retrieval.
*   **Untested:** Structured Query Generation (e.g., SPARQL).
*   **Untested:** Few-Shot Learning, Chain-of-Thought Prompting, Answer Verification Prompting, Specialized "Who" question prompts.

## 3. COMMON FAILURE MODES ON THIS DATASET

*   **Factual Inaccuracy (Hallucination):** LLM provides incorrect facts (e.g., details about "Barcelona corners" or the "Belmont purchaser").
*   **Date Discrepancies:** Small differences in dates are marked as incorrect (e.g., "October 20" vs. "21 of October").
*   **Incomplete Answers:** LLM fails to provide all parts of the answer (e.g., providing only the month and year).
*   **Incorrect Entity/Relationship Extraction:** The system fails to accurately extract the precise entities and relationships (e.g., "American University" instead of something relevant, or "Robert P. Sharp" instead of "Carl Owen Dunbar").
*   **Inability to Discern Temporal Order:** The system struggles to correctly identify the sequence of events (e.g., the ruby-throated bulbul question).
*   **Inaccurate Information Retrieval & Validation:** The system struggles to validate if information retrieved is in fact valid (Iterations 1, 3, 4, 5).
*   **Lack of Reliable Source Attribution:** Difficult to determine the source of the error, making debugging challenging.
*   **Incorrect Year Retrieval:** The system incorrectly retrieves the year (e.g., "1999" instead of "2013" for Maharaj Kishan Bhan in Iteration 3).
*   **Handling Ambiguity in "First" Questions:** The system struggled with questions seeking the "first" of something in Iteration 3.
*   **Context Validation Inadequacy:** The LLM struggles to validate the context against the question (Iterations 3, 4, 5).
*   **Script Errors:** Script errors encountered during attempted repairs (e.g., `ERROR: Answer not found in context`, `ERROR: Gemini API call failed...`, `ERROR: Could not find the answer.`).
*   **Failure to Extract Numerical Answers:** RAG approach fails when questions require numerical information (Iteration 4).
*   **Granularity Mismatch in Date Retrieval:** The system fails when a question demands a precise date (e.g., "November 30, 1949") but the retrieved information provides a broader range (e.g., "1949 to February 21, 1974"). (Iteration 5).
*   **"Answer Not Found" Errors for Existing Answers:** The system incorrectly reports "Answer Not found" even when the correct answer exists (Iteration 5).
*   **Query Validation Ineffective:** The `generate_query_and_validate` function isn't ensuring that the generated queries are precise enough (Iteration 5).
*   **Date Extraction Bottleneck:** The system fails when it cannot find the exact date or time period mentioned in the golden answer within the retrieved snippets.
*   **Insufficient Temporal Reasoning:** The system acknowledges Otto Schluter was a professor but cannot extract the date range (1911-1959).
*   **Inability to Retrieve Patch Numbers:** The "Mechanical Glove" example demonstrates a failure to retrieve the specific patch number (1.2.3).
*   **Answer Synthesis Breakdown:** The core issue is the failure to integrate the sub-question answers effectively (Iteration 7). The system can decompose the question, but struggles to synthesize a coherent and accurate final answer.
*   **Premature "Not Revealed" Conclusion:** The system incorrectly concludes that information is unavailable too quickly, especially when questions require inference or dealing with niche fictional settings (Iteration 8).
*   **Insufficient Contextual Depth:** The system likely doesn't expand the context deeply enough to uncover the relationships required for inference (Iteration 8).
*   **Reliance on Negative Constraints:** The prompt relies on negative constraints in the validation step, leading to false negatives (Iteration 8).
*    **Failure to Retrieve Specific Dates:** (ITERATION 9 ADDITION) The primary failure, exemplified by the Makhdum Khusro Bakhtyar question, highlights the system's difficulty in retrieving specific dates, even when the information might be present in the knowledge source. The system returns "Answer not found" instead of extracting the correct date (4 September 2004).

## 4. EXPERIMENT LOG & FINDINGS

*   **Iteration 0 (Baseline):**
    *   **Approach:** Direct LLM call with a basic prompt.
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** 10%.
    *   **Key Findings:** Direct LLM prompting is insufficient. Requires knowledge retrieval and verification.
    *   **Error Analysis:** Factual inaccuracies and date discrepancies.

*   **Iteration 1:**
    *   **Approach:** Chain-of-thought approach using specialized LLM agents.
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** 0%.
    *   **Key Findings:** Simple chain-of-thought insufficient. Inaccuracies at the extraction stage propagate.
    *   **Error Analysis:** Incorrect entity/relationship extraction, inability to discern temporal order, and inaccurate information retrieval and validation.

*   **Iteration 2:**
    *   **Approach:** Validation loop with specialized LLM agents.
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** Not explicitly recorded, but noted that validation alone was insufficient.
    *   **Key Findings:** The validation loop did not catch the factual inaccuracy in the example.
    *   **Error Analysis:** Incorrect Factual Recall. Validation alone is insufficient. Need for External Knowledge Injection. Lack of Reliable Source Attribution.

*   **Iteration 3:**
    *   **Approach:** LLM-based information retrieval and answer generation.
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** 0.33%.
    *   **Key Findings:** The approach does not achieve satisfactory accuracy on this dataset.
    *   **Error Analysis:** Incorrect Year Retrieval. Handling Ambiguity in "First" Questions. Context Validation Inadequacy.

*   **Iteration 4:**
    *   **Approach:** RAG implementation. "Explore" strategy used.
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** 0.67%.
    *   **Key Findings:** The "explore" strategy with the current RAG implementation is not effective for this dataset.
    *   **Error Analysis:** Failure to extract numerical answers. Overly conservative snippet validation.

*   **Iteration 5:**
    *   **Approach:** RAG architecture with a validation loop.
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** 0.00%.
    *   **Key Findings:** The RAG architecture is failing to provide accurate answers. The validation loop isn't effective. The `generate_query_and_validate` function isn't ensuring query precision.
    *   **Error Analysis:** Granularity Mismatch in Date Retrieval. "Answer Not Found" Errors. Incorrect Entity Resolution. Query Validation Ineffective. Validation Fails to Catch Inaccuracies.

*   **Iteration 6:**
    *   **Approach:** RAG architecture with validation loop.
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** Not explicitly recorded, due to identified errors.
    *   **Key Findings:** The validation loop alone is insufficient.
    *   **Error Analysis:** Date Extraction Bottleneck. Insufficient Temporal Reasoning.

*   **Iteration 7:**
    *   **Approach:** Decomposition Strategy.
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** 0.67%.
    *   **Key Findings:** Simply decomposing the question and answering sub-questions is insufficient. The synthesis stages are hindering performance.
    *   **Error Analysis:** Inability to Retrieve Patch Numbers. Answer Synthesis Breakdown.
    *   **Script Errors:** `Error detected during script repair (attempt 1): ERROR: Could not find the answer.`

*   **Iteration 8:**
    *   **Approach:** LLM Iterative Context Expansion with Focused Summarization (LLM-ICE-FS).
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** 0.67%.
    *   **Key Findings:** While overall accuracy is only 0.67, the LLM-ICE-FS strategy does show promise in principle.
    *   **Error Analysis:** Premature "Not Revealed" Conclusion. Insufficient Contextual Depth. Relies on negative constraints in the validation step, leading to false negatives.

*   **Iteration 9:**
    *   **Approach:** RAG framework (details from previous iterations apply).
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** 0.67%.
    *   **Key Findings:** The RAG framework is partially effective, but refinement is crucial. The error pattern suggests the retrieval and/or extraction of information related to specific dates is a weakness.
    *   **Error Analysis:** Failure to Retrieve Specific Dates. The LLM is not properly extracting the date from the search snippets.

## 5. NEXT RESEARCH DIRECTIONS

*   **Refine Sub-Question Synthesis:** Focus on improving the `synthesize_answers` function. Implement strategies to ensure the individual answers to sub-questions are integrated logically and accurately.
*   **Improve Patch Number Retrieval:** Enhance the system's ability to identify and extract patch numbers.
*   **Implement Intermediate Reasoning Checks:** Add checks after the `answer_sub_question` step to ensure the answer is of the correct format and type.
*   **Test Different Decomposition Strategies:** Experiment with different approaches to decomposing the original question.
*   **Enhance Temporal Reasoning:** Implement a module specifically designed to extract and reason about dates and time periods.
*   **Implement Date Inference:** Extend the system to infer dates or date ranges from contextual clues.
*   **Fine-tune LLM on Temporal Tasks:** Fine-tune the base LLM on a dataset of question-answer pairs where the answers involve specific dates or time periods.
*   **Query Expansion for Temporal Information:** Augment the search query to explicitly request temporal information.
*   **Improve Query Formulation for Precision:** Focus on refining the query generation process to create more specific and targeted queries, especially when questions involve dates or named entities.
*   **Enhance Retrieval Granularity:** Implement techniques to improve the granularity of the retrieval process.
*   **Strengthen Entity Resolution:** Integrate entity linking or named entity recognition (NER) techniques.
*   **Refine Validation Logic:** Re-evaluate the validation criteria and implementation. Remove reliance on negative constraints.
*   **Dataset Augmentation for Negative Examples:** Augment the dataset with negative examples.
*   **Improve Search Query Specificity:** Refine the prompt for query generation to emphasize the need for queries that specifically target numerical answers.
*   **Enhance Information Extraction:** Implement a more robust information extraction mechanism to identify and extract numerical answers.
*   **Refine Snippet Validation:** Loosen the validation criteria or explore alternative validation strategies. Specifically, explore if retrieving more snippets will help, then re-rank them.
*   **Implement Numerical Reasoning Checks:** After extracting a numerical answer, add a simple check to ensure it makes sense.
*   **Implement Knowledge Retrieval:** Integrate a search engine or knowledge base to retrieve supporting information *before* answer generation.
*   **Implement Answer Verification:** Verify the LLM's answer against a reliable external source.
*   **Date Normalization:** Standardize date formats.
*   **Prompt Engineering for Completeness:** Revise the prompt to ensure the model provides all parts of the answer or responds with an appropriate error message.
*   **Explore Hybrid Approach:** Use LLM for query rephrasing to improve search engine results, and then use those results to generate an answer.
*   **Enhance Entity and Relationship Extraction:** Implement more robust methods for entity and relationship extraction, potentially using named entity recognition (NER) models finetuned on similar datasets.
*   **Incorporate Temporal Reasoning:** Add a dedicated temporal reasoning module to track changes over time.
*   **Improve Answer Validation:** Implement more rigorous answer validation techniques, such as cross-referencing information from multiple sources and using a separate validation model to assess the answer's correctness.
*   **Iterative Refinement with Feedback:** Create a feedback loop where incorrect answers are analyzed to identify the specific errors made by each component in the pipeline.
*   **Test Structured Query Generation:** Convert the natural language question into a structured query (e.g., SPARQL).
*   **Investigate Few-Shot Learning:** Provide the LLM with examples of question-answer pairs.
*   **Evaluate Chain-of-Thought Prompting:** Prompt the LLM to explain its reasoning process step-by-step.
*   **Develop Answer Verification Prompting:** Use a separate prompt to ask the LLM to verify the answer's accuracy.
*   **Create Specialized "Who" Question Prompts:** Given the dominance of "Who" questions, design highly optimized prompts.
*   **Track Latency:** Measure and log the runtime (latency) of each experiment.
*   **Analyze Failure Cases:** Perform detailed analysis of failure cases to identify patterns and refine strategies.
*   **Implement Fact-Checking Mechanism:** Integrate a mechanism to fact-check the LLM's answers.
*   **Implement Source Tracking:** Modify the `call_llm` function to include source tracking.
*   **RAG Implementation**: Explore Retrieval-Augmented Generation (RAG).
*   **Improve Context Validation:** Implement a more robust context validation mechanism in the `retrieve_relevant_context` function.
*   **Fine-tune Answer Selection Logic:** Refine the answer selection logic in the `generate_answer_with_context` function to prioritize precise factual matches.
*   **Prompt Engineering for Date Retrieval:** Optimize the prompts used for generating search queries and validating context to emphasize the importance of accurate date retrieval.
*   **Prompt Engineering for validation:** Give the LLM the ability to "check its work" by validating its initial answer against the original question and the retrieved context.
*   **Adjust Confidence Thresholds:** Tune the thresholds for concluding that information is unavailable.
*   **Improve Context Expansion Depth/Breadth:** Increase the number of iterations in the `expand_context` function or broaden the search queries to explore more potential sources.
*   **Inference-Focused Summarization:** Modify the `summarize_context` prompt to explicitly instruct the LLM to identify and infer relationships between entities, not just summarize facts.
*   **Fictional World Specialization:** Consider a branch of the system specifically designed to handle questions about fictional works.
*   **Improve Date Extraction in `generate_answer_with_snippets`**: (ITERATION 9 ADDITION) Modify `generate_answer_with_snippets` to prioritize and explicitly extract dates from the retrieved snippets when the question asks for a date. Use regular expressions or date parsing libraries within the function to identify and format dates correctly.
*   **Enhance Query Specificity for Dates**: (ITERATION 9 ADDITION) Refine the `generate_query_and_validate` function to generate more specific queries when the question asks for a date. Include terms like "date of induction", "sworn in on", etc. to guide the search towards date-related information.
*   **Post-processing Date Validation**: (ITERATION 9 ADDITION) Add a post-processing step in the validation loop to check if the generated answer contains a valid date when the question expects one. This provides an additional layer of validation and helps identify cases where the LLM fails to extract the date.
*   **Error Analysis on Retrieval Content**: (ITERATION 9 ADDITION) Analyze the search snippets retrieved for the failed cases (e.g., the Makhdum Khusro Bakhtyar question). Determine if the relevant information (the correct date) was actually present in the retrieved snippets. If not, the query generation needs improvement. If the information *was* present, the answer generation/extraction needs improvement.
```
        
    
            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        
    
            MULTIPLE TOP PERFORMING APPROACHES TO SYNTHESIZE:
            
=== TOP PERFORMING APPROACH #1 ===
Iteration: 2
Accuracy: 0.67
Approach Summary: The script employs a validation-driven problem-solving approach using the Gemini LLM, where a problem is first solved and then iteratively refined based on validation feedback. The problem is decomposed into solution generation and solution validation steps. There are two agent roles: a solver and a validator, each with specific system instructions. `call_llm` is used to interact with the LLM by passing prompts and instructions, while `solve_with_validation_loop` manages the iterative solving and validation process, and `main` is the entry point that calls `solve_with_validation_loop`. The overall workflow involves generating an initial solution with `call_llm` using the solver agent, validating it with `call_llm` using the validator agent, and refining the solution using `call_llm` with feedback until it's deemed valid or the maximum number of attempts is reached.

FULL SCRIPT CODE:
```python
import os
import re
import json
import math # for react
from google import genai
from google.genai import types

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop.
    This is based on successful patterns from previous iterations, particularly in Iteration 0,
    but enhanced with iterative refinement for better accuracy."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions. Focus on factual accuracy and completeness."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements, ensuring factual correctness and completeness."

    # Initial solution generation - Enhanced with multi-example prompting
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements. Focus on factually accurate and complete answers.

    Example 1:
    Problem: What is the name of the university where Ana Figueroa, a political activist and government official, studies and graduates from?
    Solution: University of Chile

    Example 2:
    Problem: Which genus was the ruby-throated bulbul moved to from *Turdus* before finally being classified in the genus *Rubigula*?
    Solution: Genus Pycnonotus

    Example 3:
    Problem: In what year did Etta Cone last visit Europe?
    Solution: 1938

    Problem:
    {problem}
    """

    solution = call_llm(solution_prompt, system_instruction_solver)

    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution - Enhanced with specific validation examples
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem. Ensure factual correctness and completeness.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues, including specific factual errors or omissions]".

        Example 1:
        Problem: What is the capital of France?
        Solution: Paris
        Validation: VALID: The capital of France is indeed Paris.

        Example 2:
        Problem: Who painted the Mona Lisa?
        Solution: Leonardo DaVinci
        Validation: VALID: The Mona Lisa was painted by Leonardo da Vinci.

        Example 3:
        Problem: What year did World War II begin?
        Solution: 1940
        Validation: INVALID: World War II began in 1939, not 1940.

        Problem:
        {problem}

        Proposed Solution:
        {solution}
        """

        validation_result = call_llm(validation_prompt, system_instruction_validator)

        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution

        # If invalid, refine the solution - Provides multi-example based feedback to ensure robust refinement
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed. Ensure that you only use information from the original problem in your response, and ensure that the response is factually correct and as complete as possible.

        Problem:
        {problem}

        Your previous solution:
        {solution}

        Validation feedback:
        {validation_result}

        Example of a corrected solution based on validation feedback:

        Problem: When did the Titanic sink?
        Your previous solution: April 1912
        Validation Feedback: INVALID: The Titanic sank on April 15, 1912, include the day.

        Corrected Solution: April 15, 1912

        Please provide a completely revised solution that addresses all the issues mentioned. Be as factual as possible. Do not attempt to create new information that is not present in the original response.
        """

        solution = call_llm(refined_prompt, system_instruction_solver)

    return solution

def main(question):
    """
    Main function that orchestrates the solution process using solve_with_validation_loop.
    This function now incorporates the iterative validation loop for enhanced accuracy.
    This is a hybrid approach combining elements from Iteration 0 (direct LLM call) with the idea
    of iterative refinement from later iterations.
    """
    answer = solve_with_validation_loop(question)
    return answer
```

=== TOP PERFORMING APPROACH #2 ===
Iteration: 4
Accuracy: 0.67
Approach Summary: The script implements a retrieval-augmented generation (RAG) approach to answer questions using an LLM. It decomposes the problem into query generation, search snippet validation, and answer generation. The `generate_query_and_validate` function uses an LLM with the "expert at generating effective search queries" role to create a search query and then validates the query using another LLM call with the role "expert at validating search snippets" before proceeding; It uses a few-shot validation technique. The `generate_answer_with_snippets` function uses an LLM with the role "expert at answering questions given relevant search snippets" to formulate an answer based on the validated search snippets.

The overall workflow begins in `main` which calls `generate_query_and_validate` with a question, which in turn uses `call_llm` to generate a search query and validate the results. `main` then calls `generate_answer_with_snippets` with the original question and the validated search snippets, which in turn uses `call_llm` to generate the final answer. The `call_llm` function is used throughout the script to interface with the Gemini LLM for query generation, snippet validation, and answer generation.

FULL SCRIPT CODE:
```python
import os
import re
import math # for react
from google import genai
from google.genai import types

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def generate_query_and_validate(question, max_attempts=3):
    """
    Generates a search query from a question and validates its effectiveness by assessing
    if the top search snippets contain key entities and relationships needed to answer the question.
    Returns both the generated query and top search snippets.
    """
    system_instruction_query_gen = "You are an expert at generating effective search queries that help answer questions."
    system_instruction_search_validator = "You are an expert at validating whether a set of search snippets are relevant to answering the question"
    # Hypothesis: By generating and validating the query BEFORE retrieving the information, we can significantly improve the information retrieval and hallucination problems that are causing the pipeline to fail
    for attempt in range(max_attempts):
        # Step 1: Generate Search Query with Examples
        query_prompt = f"""
        Generate a search query to retrieve information needed to answer the question.

        Example 1:
        Question: What was the first name of Ralph E. Oesper?
        Search Query: Ralph E. Oesper first name

        Example 2:
        Question: In which year did Maharaj Kishan Bhan receive the Padma Bhushan for civil services?
        Search Query: Maharaj Kishan Bhan Padma Bhushan year

        Question: {question}
        Search Query:
        """
        search_query = call_llm(query_prompt, system_instruction_query_gen)
        # Step 2: Simulate Retrieving Top Search Snippets - IMPORTANT: IN A REAL SYSTEM THIS WOULD BE SEARCH API
        search_snippets = call_llm(f"Provide top 3 search snippets for: {search_query}", "You are a helpful search engine providing realistic search results.")

        # Step 3: Validate Relevance of Search Snippets with Examples
        validation_prompt = f"""
        Determine if the following search snippets are relevant to answering the question. If they are, respond with "RELEVANT: [brief explanation]". If not, respond with "IRRELEVANT: [detailed explanation]".

        Example 1:
        Question: What was the first name of Ralph E. Oesper?
        Search Snippets: Ralph Oesper was a professor...; His middle name was E...; There is no information on his first name.
        Validation: IRRELEVANT: The snippets don't reveal his first name.

        Example 2:
        Question: In which year did Maharaj Kishan Bhan receive the Padma Bhushan for civil services?
        Search Snippets: Maharaj Kishan Bhan received the Padma Bhushan in 2013; He was a scientist; He worked in civil services.
        Validation: RELEVANT: Snippets contain MKB and the year he received the award

        Question: {question}
        Search Snippets: {search_snippets}
        Validation:
        """
        validation_result = call_llm(validation_prompt, system_instruction_search_validator)

        if "RELEVANT:" in validation_result:
            return search_query, search_snippets # Return both the search query and relevant context
        else:
            print(f"Attempt {attempt + 1}: Search snippets deemed irrelevant. Trying again...")

    return None, None  # Return None if no relevant context is found
def generate_answer_with_snippets(question, search_snippets):
    """
    Generates an answer using the validated search snippets, ensuring that the answer
    is directly supported by the information in the snippets.
    """
    system_instruction = "You are an expert at answering question given relevant search snippets"
    # Now we leverage the search snippets to answer the question directly
    answer_prompt = f"""
    Answer the question using ONLY the information present in the search snippets.

    Example 1:
    Question: What was the first name of Ralph E. Oesper?
    Search Snippets: No results found.
    Answer: Answer not found.

    Example 2:
    Question: In which year did Maharaj Kishan Bhan receive the Padma Bhushan for civil services?
    Search Snippets: Maharaj Kishan Bhan was awarded the Padma Bhushan in 2013.; He was a famous scientist.
    Answer: 2013

    Question: {question}
    Search Snippets: {search_snippets}
    Answer:
    """
    answer = call_llm(answer_prompt, system_instruction)
    return answer

def main(question):
    """
    Main function to orchestrate the validated query generation, information retrieval (simulated),
    and answer generation process.
    """
    search_query, search_snippets = generate_query_and_validate(question)

    if search_query and search_snippets:
        answer = generate_answer_with_snippets(question, search_snippets)
        return answer
    else:
        return "Answer not found." # If not able to retrieve reliable context then return not found
```

=== TOP PERFORMING APPROACH #3 ===
Iteration: 7
Accuracy: 0.67
Approach Summary: The script implements LLM-Guided Recursive Decomposition & Verification (LLM-RDRV) to answer complex questions. It decomposes the original question into sub-questions, answers each sub-question individually, verifies the answers, and synthesizes them into a final answer. This involves agent roles like question decomposer, answerer, and validator. The functions used are `call_llm`, `decompose_question`, `answer_sub_question`, `verify_answer`, `synthesize_answers`, and `main`. The `main` function orchestrates the process by calling `decompose_question` to break down the initial question, then iterates through the sub-questions, using `answer_sub_question` to find answers, and `verify_answer` to check the validity of each response before finally using `synthesize_answers` to give the final output.

FULL SCRIPT CODE:
```python
import os
import re
import math # for react
from google import genai
from google.genai import types

# This script introduces a new approach: LLM-Guided Recursive Decomposition & Verification (LLM-RDRV)
# Hypothesis: By recursively decomposing complex questions into simpler sub-questions and verifying each intermediate answer,
# we can improve accuracy and handle complex queries more effectively.

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def decompose_question(question):
    """Decomposes a complex question into simpler, answerable sub-questions."""
    system_instruction = "You are an expert at breaking down complex questions into simpler sub-questions."
    prompt = f"""
    Decompose the following complex question into simpler, independent sub-questions that can be answered individually.

    Example 1:
    Complex Question: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    Sub-Questions:
    1. What country is the Great Barrier Reef located in?
    2. What is the capital of Australia?
    3. What is the population of Canberra?

    Example 2:
    Complex Question: In which month and year was Satyanarayan Gangaram Pitroda appointed as advisor to the Indian Prime Minister, and what was his rank?
    Sub-Questions:
    1. In which month and year was Satyanarayan Gangaram Pitroda appointed as advisor to the Indian Prime Minister?
    2. What was Satyanarayan Gangaram Pitroda's rank as advisor?

    Question: {question}
    Sub-Questions:
    """
    return call_llm(prompt, system_instruction)

def answer_sub_question(sub_question):
    """Answers a single sub-question using a direct LLM call."""
    system_instruction = "You are an expert at answering questions directly."
    prompt = f"""
    Answer the following question concisely and accurately.

    Example 1:
    Question: What is the capital of France?
    Answer: Paris

    Example 2:
    Question: In what year did World War II begin?
    Answer: 1939

    Question: {sub_question}
    Answer:
    """
    return call_llm(prompt, system_instruction)

def verify_answer(question, answer):
    """Verifies the answer against the original question to ensure relevance and accuracy."""
    system_instruction = "You are a critical validator who checks if an answer is factually correct and relevant to the question."
    prompt = f"""
    Verify if the following answer accurately and completely answers the question. Respond with VALID or INVALID, followed by a brief explanation.

    Example 1:
    Question: What is the capital of France?
    Answer: Paris
    Verification: VALID: Paris is indeed the capital of France.

    Example 2:
    Question: In what year did World War II begin?
    Answer: 1940
    Verification: INVALID: World War II began in 1939.

    Question: {question}
    Answer: {answer}
    Verification:
    """
    return call_llm(prompt, system_instruction)

def synthesize_answers(original_question, sub_questions_answers):
    """Synthesizes the answers to the sub-questions into a coherent answer to the original question."""
    system_instruction = "You are an expert at synthesizing information to answer complex questions."
    prompt = f"""
    Synthesize the following answers to sub-questions into a coherent and complete answer to the original question.

    Example 1:
    Original Question: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    Sub-Questions and Answers:
    1. What country is the Great Barrier Reef located in? Answer: Australia
    2. What is the capital of Australia? Answer: Canberra
    3. What is the population of Canberra? Answer: 431,500
    Synthesized Answer: The capital of Australia, where the Great Barrier Reef is located, is Canberra, and its population is 431,500.

   Example 2:
    Original Question: In which month and year was Satyanarayan Gangaram Pitroda appointed as advisor to the Indian Prime Minister, and what was his rank?
    Sub-Questions and Answers:
    1. In which month and year was Satyanarayan Gangaram Pitroda appointed as advisor to the Indian Prime Minister? Answer: October 2009
    2. What was Satyanarayan Gangaram Pitroda's rank as advisor? Answer: Cabinet Minister
    Synthesized Answer: Satyanarayan Gangaram Pitroda was appointed as advisor to the Indian Prime Minister in October 2009 with the rank of Cabinet Minister.

    Original Question: {original_question}
    Sub-Questions and Answers:
    {sub_questions_answers}
    Synthesized Answer:
    """
    return call_llm(prompt, system_instruction)

def main(question):
    """Main function to orchestrate the LLM-Guided Recursive Decomposition & Verification process."""
    # Step 1: Decompose the question
    sub_questions = decompose_question(question)
    print(f"Sub-questions: {sub_questions}")

    # Step 2: Answer each sub-question
    sub_questions_list = sub_questions.split("\n")
    sub_questions_answers = []
    all_valid = True

    for i, sub_question in enumerate(sub_questions_list):
        if sub_question.strip(): # Skip empty lines
            answer = answer_sub_question(sub_question)
            verification = verify_answer(sub_question, answer)
            print(f"Verification result: {verification}")
            if "INVALID" not in verification:
                sub_questions_answers.append(f"{i+1}. {sub_question} Answer: {answer}")
            else:
                all_valid = False
                break

    # Step 3: Synthesize the answers
    if all_valid:
        synthesized_answer = synthesize_answers(question, "\n".join(sub_questions_answers))
        print(f"Synthesized answer: {synthesized_answer}")

        return synthesized_answer
    else:
        return "Could not find the answer."
```

    
            EXPLOITATION SYNTHESIS GUIDANCE:
            1. ANALYZE EACH TOP SCRIPT to identify:
               - What specific techniques make each approach successful?
               - What unique strengths does each approach have?
               - What weaknesses or limitations does each approach have?
               - Which components could be combined effectively?
    
            2. IDENTIFY SYNTHESIS OPPORTUNITIES:
               - Which successful techniques from different scripts could work together?
               - How can you combine the best reasoning patterns from multiple approaches?
               - What hybrid approach would leverage strengths while avoiding weaknesses?
               - Can you create a multi-stage pipeline using the best parts of each?
    
            3. CREATE A HYBRID APPROACH that:
               - Takes the most effective reasoning techniques from each top script
               - Combines different successful verification/validation strategies
               - Integrates the best error handling approaches
               - Merges effective prompt engineering techniques from multiple scripts
               - Creates a more robust solution than any individual approach
    
            4. SPECIFIC SYNTHESIS STRATEGIES:
               - If Script A excels at information extraction and Script B excels at reasoning, combine both
               - If Script A has great verification and Script B has great generation, merge the pipelines
               - If multiple scripts use different successful prompting styles, create a multi-perspective approach
               - If different scripts handle different types of errors well, create comprehensive error handling
    
            5. AVOID SIMPLE COPYING:
               - Don't just take one script and make minor changes
               - Don't just concatenate approaches without thoughtful integration
               - Create something that's genuinely better than the sum of its parts
               - Ensure the hybrid approach addresses weaknesses that individual scripts had
    
            CRITICAL REQUIREMENTS FOR SYNTHESIS:
            1. The script MUST be a true hybrid that combines elements from multiple top approaches
            2. Include a clear comment explaining which elements came from which approaches
            3. EVERY LLM PROMPT must include embedded examples showing:
               - Sample input similar to the dataset
               - Expected reasoning steps
               - Desired output format
            4. The hybrid should be more robust than any individual approach
            5. Address the weaknesses identified in the capability assessment through synthesis
    
            Here's how to call the Gemini API. Use this example without modification:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
    
            SYNTHESIS IMPLEMENTATION:
            - Create a main function that orchestrates the combined approach
            - Integrate the best reasoning patterns from multiple scripts
            - Combine the most effective verification strategies
            - Merge successful prompt engineering techniques
            - Create comprehensive error handling that addresses issues from all approaches
    
            Return a COMPLETE, RUNNABLE Python script that represents a true synthesis of the top approaches:
            1. Has a main function that takes a question string as input and returns the answer string
            2. Combines reasoning techniques from multiple successful scripts
            3. Integrates the best verification and error handling from different approaches
            4. Includes embedded examples in EVERY LLM prompt
            5. Is COMPLETE - no missing code, no "..." placeholders
            6. Closes all string literals properly
            7. Includes comments explaining which techniques came from which top scripts
    
            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            CREATE A TRUE HYBRID THAT'S BETTER THAN ANY INDIVIDUAL APPROACH!
            