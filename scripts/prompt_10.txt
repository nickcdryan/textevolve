
            You are improving a Python script that solves problems from a dataset.
            Your goal is to REFINE and ENHANCE the best performing approaches by combining their strengths and addressing specific weaknesses identified in error analysis.

            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "PASSAGE: The club had not gained a major trophy since the 2005 FA Cup until 17 May 2014 when, spearheaded by then club-record acquisition Mesut \u00d6zil, Arsenal beat Hull City A.F.C. in the 2014 FA Cup Final, coming back from a 2-0 deficit to win the match 3-2. A year later, Arsenal appeared in the FA Cup final for the second time in a row, defeating Aston Villa F.C. 4-0 in the 2015 FA Cup Final and becoming the most successful club in the tournaments history with 12 titles, a record which Manchester United would tie the following season. Arsenal later won the 2016-17 FA Cup for a record 13th time, defeating Chelsea 2-1 in the 2017 FA Cup Final and once more becoming the outright leader in terms of FA Cups won. The victory also saw Wenger become the first manager in English football history to win seven FA Cups. However, in that same season, Arsenal finished in the fifth position in the league, the first time they had finished outside the top four since before Wenger arrived in 1996. After another unspectacular league season the following year, Wenger announced his departure from the club on 20 April 2018, after 22 years as manager. His decision was met by responses of praise throughout English and world football from many pundits and former players, who also thanked him for developing them as people. His final home match in charge was a 5-0 win over Burnley where his entrance was met to a standing ovation by supporters. The final match under the Wenger era was a 1-0 away victory against Huddersfield.\n\nQUESTION: In what year did Arensal regain their record of most titles won?",
    "answer": "2017"
  },
  {
    "id": 1,
    "question": "PASSAGE: According to the FBI, a city to city comparison of crime rates is not meaningful, because recording practices vary from city to city, citizens report different percentages of crimes from one city to the next, and the actual number of people physically present in a city is unknown. With that in mind, Dallass violent crime rate (12.06 per 1,000 people) is lower than St Louis (24.81), Detroit (24.22), Baltimore (16.96), Philadelphia (15.62), Cleveland (15.47), Miami (15.09), Washington, D.C.. (14.48), Kansas City, Missouri (14.44) and Boston (13.39). However, Houston (11.69), Los Angeles (7.87), and New York City (6.38) have lower violent crime rates than Dallas.\n\nQUESTION: Which cities had a violent crime rate lower than 8?",
    "answer": "Los Angeles"
  },
  {
    "id": 2,
    "question": "PASSAGE: In week 4, the Bears played against the Dallas Cowboys on Monday Night Football in Dallas. After a scoreless first quarter and Robbie Gould's field goal, Charles Tillman intercepted Tony Romo, returning the pick for a touchdown. The interception would be the first of Romo's five interceptions. Romo would later hit Miles Austin for a 10-yard touchdown. In the second half, Cutler was able to hit Devin Hester on a 34-yard touchdown pass to extend the Bears lead. Later, Romo's pass was intercepted by Bears linebacker Lance Briggs, who then returned the interception for a touchdown to increase the lead 24-7. The pick-six marked Bears' fourteenth forced turnover, which led the league, and also leads the league in interceptions (11). Cutler would then throw another touchdown pass to Brandon Marshall. Cutler would have his highest performance of the season, completing 18 of 24 passes for 275 yards, along with two touchdowns. His 140.1 passer rating was the third highest of his career. Marshall caught seven passes for 138 yards, a season-best. With 34 seconds left in the game, Romo was replaced by former Bears quarterback Kyle Orton, who threw a 5-yard touchdown pass to Jason Witten, and the Cowboys had a two-point conversion, but the Bears would then win 34-18. With the win, the Bears shared the NFC North lead with the Minnesota Vikings with a 3-1 record.\n\nQUESTION: Which receiver had over 125 yards receiving?",
    "answer": "Brandon Marshall"
  }
]

            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 10
        - Current explore/exploit balance: 30/70
        - Best accuracy achieved: 1.00 (iteration 1)

        APPROACH HISTORY (last 10 iterations):
        [
  {
    "iteration": 0,
    "strategy": "Exploration",
    "accuracy": 0.3333333333333333,
    "approach": "This script uses a chain-of-thought approach, leveraging LLMs to answer questions by first determining the question type, then extracting relevant information, generating an answer, and finally verifying the answer. Each step uses a specific LLM agent role (e.g., question type classifier, information extractor) defined by system instructions to decompose the problem into manageable parts. The functions used are `main` (orchestrates the workflow), `determine_question_type`, `extract_relevant_info`, `generate_answer`, `verify_answer` (each leveraging `call_llm` to interact with the Gemini LLM). The overall workflow is sequential, passing the output of each function as input to the next, with error handling at each stage."
  },
  {
    "iteration": 1,
    "strategy": "Exploitation",
    "accuracy": 1.0,
    "approach": "The script uses a chain-of-thought approach to answer questions by first determining the question type, then extracting relevant information, and finally generating the answer using the Gemini LLM. The problem is decomposed into three functions: `determine_question_type`, `extract_relevant_info`, and `generate_answer`, each acting as an agent with a specific role. The `call_llm` function is used to interface with the Gemini model, with the main function orchestrating the calls. The workflow is: main -> determine_question_type -> extract_relevant_info -> generate_answer, with each step passing information to the next."
  },
  {
    "iteration": 2,
    "strategy": "Exploitation",
    "accuracy": 1.0,
    "approach": "The script uses a chain-of-thought approach to answer questions by first determining the question type, then extracting relevant information, and finally generating the answer. Each step uses the `call_llm` function to interact with the Gemini LLM, acting as a specialized agent for question type classification, information extraction, and answer generation respectively. The `main` function orchestrates the workflow: `determine_question_type` identifies the question type, `extract_relevant_info` extracts needed information based on the question and its type, and `generate_answer` formulates the final answer. Error handling is incorporated at each step, returning an error message if a step fails."
  },
  {
    "iteration": 3,
    "strategy": "Exploitation",
    "accuracy": 0.6,
    "approach": "The script uses a decomposed approach to answer questions by first determining the question type, then extracting relevant information, and finally generating the answer, leveraging the Gemini LLM at each step. It uses a chain-of-thought prompting technique by providing examples to guide the LLM's responses for each subtask. The script employs different \"expert\" roles (question type classifier, information extractor, and answer generator) implemented via system instructions passed to the LLM. The functions used are `main` (orchestrates the process), `determine_question_type` (classifies question type), `extract_relevant_info` (extracts relevant information), `generate_answer` (generates the final answer), and `call_llm` (communicates with the Gemini API); with `main` calling the other functions sequentially to solve the task."
  },
  {
    "iteration": 4,
    "strategy": "Exploitation",
    "accuracy": 0.8,
    "approach": "The script uses a decompose-and-generate approach to answer questions by classifying the question type, extracting relevant information, and then generating the final answer using the Gemini LLM. It uses in-context learning with examples to guide the LLM in each step. There are three distinct agent roles: a question type classifier, an information extractor, and an answer generator.\n\nThe `main` function orchestrates the process by calling `determine_question_type`, `extract_relevant_info`, and `generate_answer` sequentially; these functions formulate specific prompts for the LLM. The `call_llm` function is used to interact with the Gemini LLM, sending prompts and receiving responses."
  },
  {
    "iteration": 5,
    "strategy": "Exploitation",
    "accuracy": 0.4,
    "approach": "The script uses a chain-of-thought approach to answer questions by first determining the question type, then extracting relevant information, and finally generating the answer, using the `gemini-2.0-flash` model. The problem is decomposed into three steps: question type determination, information extraction, and answer generation. The agents used are \"expert at classifying question types\", \"expert at extracting relevant information\", and \"expert at generating correct answers.\"\n\nThe main function orchestrates the process, calling `determine_question_type`, `extract_relevant_info`, and `generate_answer` sequentially. The `call_llm` function is used within the other functions to interact with the Gemini API. The workflow starts with the `main` function receiving a question, which is then passed to `determine_question_type` to classify the question. The question and its type are passed to `extract_relevant_info` which returns the relevant information from the passage. Finally, the question, its type, and the extracted information are passed to `generate_answer` which outputs the answer to the question."
  },
  {
    "iteration": 6,
    "strategy": "Exploitation",
    "accuracy": 0.5,
    "approach": "This script uses a decomposition approach to answer questions based on a given passage by determining the question type, extracting relevant information, and generating the final answer, leveraging LLMs at each step. It employs example-based prompting to guide the LLM. Three distinct agent roles are used, each corresponding to a dedicated function: question type classifier, information extractor, and answer generator. The functions used are `main`, `determine_question_type`, `extract_relevant_info`, `generate_answer`, and `call_llm`; the `main` function orchestrates the process by calling the question type, information extraction, and answer generation functions sequentially, and these functions use `call_llm` to interact with the Gemini model."
  },
  {
    "iteration": 7,
    "strategy": "Exploitation",
    "accuracy": 0.9,
    "approach": "The script uses a chain-of-thought approach to answer questions by first determining the question type, then extracting relevant information, and finally generating the answer. The problem is decomposed into these three distinct steps, each handled by a separate function with an LLM call. There are three agent roles within the script: Question Type Classifier, Information Extractor, and Answer Generator.\n\nThe `main` function orchestrates the process by calling `determine_question_type`, `extract_relevant_info`, and `generate_answer` sequentially. These functions use `call_llm` to interact with the Gemini LLM, passing prompts and system instructions to guide the LLM's responses."
  },
  {
    "iteration": 8,
    "strategy": "Exploitation",
    "accuracy": 0.6,
    "approach": "The script uses a decompose-and-generate approach to answer questions, leveraging the Gemini LLM. It first determines the question type, then extracts relevant information based on that type, and finally generates an answer using the extracted information. Three functions, `determine_question_type`, `extract_relevant_info`, and `generate_answer`, each act as specialized agents with their own system instructions to classify the question, extract relevant information from a passage, and formulate an answer, respectively. `main` orchestrates the process by calling these functions sequentially, while `call_llm` is used by each of these functions to prompt the LLM and get a response."
  },
  {
    "iteration": 9,
    "strategy": "Exploitation",
    "accuracy": 0.8,
    "approach": "This script uses a Chain-of-Thought approach to answer questions by decomposing the problem into three steps: determining the question type, extracting relevant information, and generating the answer. Each step uses a distinct LLM call with specific instructions to act as an expert in that area. The script uses the functions `determine_question_type`, `extract_relevant_info`, and `generate_answer` to classify the question, retrieve necessary data, and produce a final answer respectively, facilitated by the `call_llm` function. The overall workflow involves sequentially calling these functions, passing the output of one as input to the next, to arrive at the final answer, with error handling at each step."
  }
]

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 3 iterations):
        [
  {
    "iteration": 0,
    "issue": "The primary issue is the system's **failure to transition from identifying the required task to *actually executing* that task and generating a definitive answer.** The \"Verify that...\" pattern is a symptom of this core problem. It suggests the system is stuck in a planning/verification loop without proceeding to execution."
  },
  {
    "iteration": 1,
    "issue": "Since there are no error cases, there is no single most critical problem to fix."
  },
  {
    "iteration": 2,
    "issue": "The lack of error data makes it impossible to determine the most critical problem to fix. Further action is required to provide error cases."
  },
  {
    "iteration": 3,
    "issue": "The single most critical problem is **inaccurate and unreliable information extraction** from the provided text passages. The system frequently fails to identify and retrieve the specific numerical values, entities, and event details required to answer the questions correctly. This failure cascades into incorrect calculations and faulty reasoning, ultimately leading to wrong answers."
  },
  {
    "iteration": 4,
    "issue": "The primary issue is **inaccurate temporal reasoning and calculation, specifically the ability to precisely determine the difference in months between two dates mentioned in the passage.** The system is unable to reliably extract and compare dates, and then perform the necessary calculation to find the difference."
  },
  {
    "iteration": 5,
    "issue": "The most critical problem is the **inaccurate and indiscriminate information extraction process.** The system needs a much more sophisticated method of identifying the *specific* pieces of information required to answer the question, avoiding extraneous data. This requires improved semantic understanding of the question and the passage."
  },
  {
    "iteration": 6,
    "issue": "The primary issue is **inaccurate numerical reasoning and contextual understanding**, leading to failures in both identifying the correct numerical values to use in calculations and performing those calculations accurately. This further exacerbated by the models lack of ability to follow context for units."
  },
  {
    "iteration": 7,
    "issue": "The primary issue is the system's tendency to extract more information from the passage than is strictly necessary to answer the question, resulting in verbose and imprecise answers."
  },
  {
    "iteration": 8,
    "issue": "The primary issue is the system's inability to precisely interpret question intent and selectively extract the most relevant information from the passage, leading to over-inclusive or incorrectly combined answers. The system requires more robust contextual understanding to avoid including extraneous details and to perform calculations only when explicitly required and when the input data are logically connected."
  },
  {
    "iteration": 9,
    "issue": "The most critical problem is the system's **inability to accurately perform calculations and synthesize numerical information from the provided passages.** This manifests in errors related to summing scores, comparing crime rates, and extracting precise numerical quantities."
  }
]

        TARGETED IMPROVEMENTS:
        [
  "Refine Answer Generation:** Train the model to generate answers that are concise and directly address the question, avoiding unnecessary details or calculations unless explicitly requested.",
  "Increase training examples**: Increase the amount of training data for questions that have very specific answers.",
  "Constraint Filtering:** Develop a mechanism to filter extracted information based on constraints derived from the question, ensuring that only the most pertinent details are included in the answer.",
  "Intent-Based Extraction:** Implement a more sophisticated information extraction module that is guided by a deep understanding of the question's intent, employing techniques like semantic parsing or question answering models."
]
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity ✓
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity ✓
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity ✓
    4. All operations have O(1) average time complexity ✓
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        

            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        ```
# Dataset-Specific Research Log: Question Answering on Sports/Statistical Texts

This document serves as a running log of our research and findings specific to the task of question answering on a dataset consisting of short sports reports or statistical summaries followed by fact-based questions.

## 1. DATASET PATTERNS & CHARACTERISTICS

*   **Passage Structure:** Each example consists of a passage of text followed by a question, separated by "\n\nQUESTION: ". The passages describe sports events, statistical summaries, or other fact-based scenarios.
*   **Passage Density:** The passages are often dense with information, requiring precise identification of relevant facts amidst irrelevant details. This is evident in the crime rate passage, where numerous cities and rates are mentioned, increasing the chance of error.
*   **Question Types:**
    *   **Who:** Asking for a person who performed an action (e.g., scored a touchdown). Example: "Who kicked the field goal?"
    *   **How many:** Asking for a count of something (e.g., running backs). Example: "How many rushing touchdowns did he have?" The questions often use "How many..." or similar phrasing, but the answer might be a single number, a list of numbers, or a more complex calculation.
    *   **Calculation:** Asking for a result of a simple numerical operation (e.g., addition, subtraction, comparison). Example: "How many total yards did the team gain?"
    *   **Comparative Questions Requiring Numerical Extraction and Comparison:** Questions involving comparing two entities based on a numerical attribute mentioned in the passage (e.g., "Which star has a smaller mass..."). Example: "Which team had more passing yards, the Bears or the Packers?"
    *   **"Which Player" Questions Based on Specific Actions:** Asking "which player" performed a specific action during a game. Example: "Which player threw the touchdown pass?"
    *   **Date Related Questions:** Asking about dates of events
    *   **Value identification:** Questions that ask for exact quantity of something ("How many total points were scored...")
    *   **Reason Identification:** Questions that ask for the "why" behind a specific event in the text.
    *   **Temporal Reasoning & Numeric Anchoring:** A significant portion of questions requires temporal reasoning (e.g., "How many months did...") often involving calculations based on dates and durations mentioned within the passage. Passages frequently embed relevant numeric values (years, counts, scores) close to relevant entity mentions.
    *   **Event Sequencing and Ordering:** A considerable number of questions require determining the order of events described in the passage. These questions often present two events and ask which occurred earlier or later.
*   **Question Diversity:** Questions are diverse in terms of what information they require (dates, names, quantities, comparisons, reasons).
*   **Numerical Reasoning:** Many questions require numerical reasoning, such as addition, subtraction, comparison, or counting. Questions often require numerical reasoning or extraction of specific details from the passage. Examples include calculating differences ("How many yards was the difference...") or identifying specific values ("How many total points were scored..."). Many questions require numerical reasoning (addition, subtraction, comparison) based on explicitly mentioned values within the passage. The challenge lies in identifying *which* numbers are relevant and *how* they should be combined. Many questions require extracting and comparing numerical data embedded within the passage. The questions often ask for quantities, comparisons of quantities, or simple arithmetic (e.g., "How many points...", "Which cities had a rate lower than...").
*   **Multi-Step Reasoning:** Some questions implicitly require multiple steps to arrive at the correct answer. For example, identifying *all* cities with rates below a threshold involves scanning the entire passage and comparing each city's rate.
*   **Answer Types:** Answers are short, usually a person's name, a number, or a short phrase directly derived from the passage. Answers are generally not ambiguous. Answers are fact-based and require extracting specific pieces of information from the passage. The questions aren't open-ended or opinion-based.
*   **Passage Length Variability:** Passages vary significantly in length and complexity, from simple scenarios to entire game summaries. Longer passages and more complex questions (requiring multi-step reasoning or implicit relationship understanding) can lead to failures in either the extraction or generation stages.
*   **Domain Knowledge:** Basic understanding of sports (teams, scoring) is helpful but not strictly required. All information needed is in the passage. The dataset heavily features summaries of sports games (American football in the examples).
*   **Implicit Relationship Understanding:** Some questions require understanding implicit relationships or performing simple calculations (e.g., "How many years after...").
*   **Complex Passages with Multiple Facts:** Passages tend to contain a high density of factual information, requiring accurate parsing and filtering to identify the specific details relevant to the question.
*   **Temporal Reasoning Focus:** A notable portion of the questions require reasoning about time, specifically calculating the difference between dates or identifying chronological order of events described in the passage.
*   **Named Entity Dependency:** The questions heavily rely on correctly identifying and associating named entities (people, organizations, locations) mentioned within the passage. The correct answer often involves identifying relationships between these entities.
*   **Mixed Question Types:** The dataset contains a mix of question types, including temporal reasoning, entity relationship queries, and fact retrieval.
*   **Sports Game Summaries & Calculation/Comparison Questions:** The dataset heavily features summaries of sports games (American football in the examples). A common question type involves calculating or comparing numerical values (points, yards) mentioned within these summaries. The questions often require extracting *specific* numbers and performing a calculation.
*   **Varied Numerical Information:** The numerical information isn't always presented in the same way. For example, it might be explicitly stated ("3-yard touchdown") or implied by game scores ("7-0 lead").
*   **Passage Topic Variability:** The passages themselves vary in topic, spanning history, sports, and potentially other domains, requiring broad factual knowledge.
*   **Implicit Unit Understanding:** Many questions require implicit understanding of units (e.g., months, years, points). The model needs to not only find the correct numbers but also use the correct units and context. Questions frequently omit the units of measurement (e.g., "yards"), requiring the system to infer the correct units from the passage context. This can lead to errors when multiple numerical values with different units are present.
*   **Explicit References:** Many questions require pinpointing specific details directly mentioned in the provided passage. The questions don't often require complex inference or reasoning *beyond* directly stated facts.
*   **Granular Detail:** The questions often target very specific pieces of information, such as particular locations, quantities, or names, not broader summaries.
*   **Numerical Information:** A number of questions involve extracting numerical data from the passage, whether it be point scores, quantities of warships, or amounts of money.

## 2. EFFECTIVE TASK-SPECIFIC STRATEGIES

*   **Chain-of-Thought Decomposition:** Decomposing the problem into smaller, well-defined functions (`determine_question_type`, `extract_relevant_info`, `generate_answer`) is generally effective. This modularity likely helps in focusing the LLM's reasoning at each step.
*   **LLM-Driven Modularity:** Using the LLM for each stage (question type determination, information extraction, and answer generation), rather than hard-coded rules, provides the flexibility needed to handle the diversity of question types.
*   **Directly Tie Extraction to Question Type:** Tailor the information extraction process to the specific question type identified. If it's a comparison question, the extraction should be geared towards finding the specific entities and their attributes to be compared. If the question requires a reason, the extraction should focus on extracting reasons from the text. This improves efficiency and accuracy.
*   **Specialized Agents for CoT:** The Chain-of-Thought (CoT) approach, using specialized agents for question type classification, information extraction, and answer generation, is effective *when it works correctly*. This breaks down the complex task into smaller, manageable steps. However, reliability needs to be assessed with more data.
*   **Chain-of-Thought Prompting with Examples:** The CoT structure with explicit examples seems promising, likely helping ground the LLM in the specific task by guiding it towards mimicking the format and content of the examples.
*   **Decomposition Potential:** Decomposition into question type classification, information extraction, and answer generation *could* be effective *if* each component were highly accurate. The potential benefit is modularity and easier debugging/improvement of individual components.
*   **Example-Based Prompting Promise:** Example-based prompting shows promise as a way to guide the LLM, but the examples need to be carefully chosen to cover a range of question types and numerical reasoning scenarios present in the dataset.
*   **Chain-of-thought effectiveness:** Given the consistently high accuracy (Iteration 7: 0.90), the chain-of-thought approach, breaking down the task into question type classification, information extraction, and answer generation, is fundamentally sound *for this dataset*. The modularity appears to be effective. However, the low accuracy in Iteration 9 (0.80) suggests that the Chain-of-Thought approach, while conceptually sound, is insufficient on its own when the task involves numerical reasoning.
*   **Explicit Examples Effectiveness:** The use of explicit examples within the prompts for each stage (question type classification, information extraction, answer generation) helps guide the LLM towards the correct output format.
*   **Decompose-and-Generate (Inconclusive):** While conceptually sound, the decompose-and-generate approach is highly sensitive to the accuracy of each component. Errors in question type classification or information extraction cascade downstream, leading to incorrect answers. No specific strategies can be determined to be working well, as the accuracy is low in iteration 8. However, the decompose-and-generate approach could be promising if the individual components are improved.
*   **Augment with Numerical Reasoning Tools (Proposed):** Integrate external tools or modules specifically designed for numerical extraction, comparison, and calculation.

## 3. COMMON FAILURE MODES ON THIS DATASET

*   **Difficulty Locating Relevant Information:** Especially for longer passages, finding the specific sentence or phrase containing the answer can be challenging. Longer passages and more complex questions (requiring multi-step reasoning or implicit relationship understanding) can lead to failures in either the extraction or generation stages.
*   **Incorrect Entity Identification:** Misidentifying the entities (players, teams) referred to in the question within the passage.
*   **Numerical Calculation Errors:** Performing the wrong numerical calculation or misinterpreting the wording of the question leading to incorrect arithmetic. The system fails to perform simple arithmetic correctly. In the Giants points example (Iteration 9), the system could not accurately sum the points of the Giants to come up with the correct number of 3 points. This suggests a lack of robustness in the 'Generate Answer' step when calculations are required.
*   **Stuck in Verification Loop (Generic Answer):** The system gets stuck in a planning or verification stage instead of executing the task, producing canned responses like "Verify that it makes sense to..." or "Verify that the required information is known and possible". This indicates a breakdown between task identification and task execution. This was observed with a chain-of-thought approach using a `verify_answer` function. Example: Question asked for the number of touchdowns, but the system got stuck verifying the *possibility* of finding that information.
*   **Inability to Extract and Compare Numerical Values:** When presented with comparative questions, the system fails to extract the relevant numerical values from the passage and perform the necessary comparison, instead getting stuck on "verifying" that such a comparison is possible. Example: Question asking which team had more yards, and the system failed to extract the yardage numbers for each team. The system struggles to accurately identify and compare numerical values. In the crime rate example (Iteration 9), the system failed to correctly identify *all* cities with rates below 8, including New York City when it should only have included Los Angeles. This indicates a failure in precise extraction and a possible confusion with similar numerical values (11.69 of Houston).
*   **Failure to Identify Specific Actors in Complex Scenarios:** In questions asking about a specific player's action within a game summary, the system gets stuck on "verifying" the identification of the player instead of extracting the name associated with the event from the passage.
*   **Script Errors:** NameError: name 'question' is not defined. This occurred during script repair. The variable 'question' was not defined in the scope where it was being used (specifically in the `generate_answer` function). This highlights the importance of careful variable scoping and passing in modular code.
*   **Lack of Error Information:** The primary failure mode currently is the lack of error data. Without knowing *why* the system fails, it's impossible to pinpoint specific weaknesses. We don't know if it's failing to classify question types correctly, extract the right information, or generate the answer properly. This makes targeted improvements impossible.
*   **Potential sensitivity to passage and question complexity:** It's likely that longer passages and more complex questions (requiring multi-step reasoning or implicit relationship understanding) could lead to failures in either the extraction or generation stages.
*   **Incorrect Temporal Calculation:** Failing to accurately extract and process temporal information. Example: The system incorrectly calculated the number of months between February and December, latching onto the numbers 12 and 2 but failing to apply the correct logic and adding an extraneous "+ 1". The model incorrectly calculated the duration between February and December.
*   **Misinterpreting Point Differentials:** Failing to extract scores and calculate the difference. Example: The LLM extracted numbers relating to points but failed to identify the point differential correctly, indicating an error in understanding the context and intended calculation. For example, in the question about the Falcons' lead at halftime, the model failed to correctly determine the point difference based on the passage.
*   **Erroneous Event Sequencing:** Difficulty in identifying the order of events and their relative timing based on the passage. Example: The system struggles to correctly identify the sequence based on the passage, leading to the selection of the wrong event.
*   **Temporal Calculation Errors:** The primary failure mode (observed in iteration 4) is the inability to accurately calculate time differences. The model struggles to extract the correct dates, convert them to a usable format, and then perform the subtraction to determine the month difference. *Example: Incorrectly calculating the number of months between the PC release of a video game and a later preview at the Target Bullseye Lounge.*
*   **Entity Confusion:** The model incorrectly identifies the relevant entities or confuses their relationships. *Example: In iteration 4, the model identified the wrong person who King Tancred seized power from.* This likely stems from issues in the information extraction step.
*   **Poor Numerical Extraction & Calculation (Iteration 5):** The system struggles to extract the *correct* numbers required for calculations. It extracts irrelevant numbers or misinterprets the relationships between them. *Example 1*: Instead of identifying the specific TD and field goal yardages to subtract, the system just pulls out other numbers and performs an incorrect subtraction. *Example 3*: The system incorrectly sums unrelated yardage values from multiple plays instead of identifying the initial points scored.
*   **Inadequate Temporal Reasoning (Iteration 5):** The system seems to have a weak understanding of temporal relations expressed in the text. *Example 2*: It fails to correctly order the events related to the march on Tehran and the dynasty's corruption. This suggests the need for a deeper understanding of chronological cues and event dependencies.
*   **Indiscriminate Information Extraction (Iteration 5):** The root cause is the "expert at extracting relevant information" agent isn't precise enough. It extracts too much information, and the subsequent steps are unable to filter and use only the necessary parts.
*   **Inaccurate Numerical Reasoning (Iteration 6):** The model frequently fails to correctly identify the relevant numerical values and perform the required calculations (addition, subtraction, etc.).
*   **Contextual Misunderstanding of Units (Iteration 6):** The model struggles to consistently apply the correct units in its calculations and answers, demonstrating a lack of contextual awareness.
*   **Over-Extraction (Iteration 7 & 8):** The most prominent failure mode is the tendency to include *more* information than strictly necessary to answer the question. This suggests the "Information Extractor" agent is not precise enough in isolating only the required details. This can be due to prompts that are not specific enough or an inherent bias in the LLM to provide more context. For the sample question "Which areas did Louis IX give to Henry III?", the system included "Revenues of the Agenais and his domains" *in addition to* the correct areas (Limoges, Cahors, and P\u00e9rigueux). This indicates a need for stricter constraints on the information extraction step.
    *   **Over-Inclusive Extraction:** The `extract_relevant_info` function often extracts too much information, including irrelevant numerical values, as seen in the first error example where all of Gostkowski's field goal yardages were included instead of just the one requested. *Why:* The system lacks sufficient contextual understanding to filter out extraneous details based on the specific question. The system is including all mentions of relevant entities and failing to filter them based on their role in the text.
*   **Incorrect Arithmetic Operations (Iteration 8):** The system sometimes performs arithmetic when it's not required or combines numbers that shouldn't be combined, as shown in the second error example where the system incorrectly adds two quantities. *Why:* The system misinterprets the question's intent and applies arithmetic operations without proper validation of their relevance.
*   **Failure to Distinguish Different Roles/Attributes (Iteration 8):** The system fails to differentiate between different attributes of the same entity. For example, it might conflate different field goal distances, failing to specifically identify the longest.
*   **LLM Reliance on Explicit Instructions (Iteration 8):** The LLM relies heavily on the system instructions provided in the prompt. If the instructions are not sufficiently precise and comprehensive, the LLM may produce unpredictable or inaccurate results.

*   **Need for Improved Contextual Understanding (Iteration 8):** The low accuracy underscores the need for enhancing the system's contextual understanding capabilities. The system needs to be able to discern the relationships between entities, actions, and attributes within the passage to extract the correct information.
*   **Inaccurate Numerical Extraction & Comparison (Iteration 9):** The system struggles to accurately identify and compare numerical values. In the crime rate example, the system failed to correctly identify *all* cities with rates below 8, including New York City when it should only have included Los Angeles. This indicates a failure in precise extraction and a possible confusion with similar numerical values (11.69 of Houston).
*   **Numerical Arithmetic Errors (Iteration 9):** The system fails to perform simple arithmetic correctly. In the Giants points example, the system could not accurately sum the points of the Giants to come up with the correct number of 3 points. This suggests a lack of robustness in the 'Generate Answer' step when calculations are required.

## 4. EXPERIMENT LOG & FINDINGS

*   **2025-05-17 12:44:34:** Initial Dataset Analysis: Comprehensive breakdown of data characteristics, challenges, potential approaches, creative insights, and implementation recommendations. Key recommendations included: (1) Keyword Matching, (2) Sentence Similarity, (3) Information Extraction with Rules, (4) LLM-based Question Answering. Also included a sample prompt structure for text based techniques.
*   **2025-05-17 12:44:44:** SCRIPT ERROR ENCOUNTERED: NameError: name 'question' is not defined. This occurred during script repair in the `generate_answer` function. The variable 'question' was not properly passed or defined within that function's scope.
*   **Iteration 0 Findings:** Chain-of-thought decomposition into question type classification, information extraction, answer generation, and verification was *ineffective*. The sequential chain, as implemented, led to premature "verification" steps that prevented the system from generating concrete answers. The system exhibited an over-reliance on verification, hindering progress. Accuracy was very low.
*   **Iteration 1 Findings:** Chain-of-thought approach (determine type, extract info, generate answer) achieved 100% accuracy, suggesting it is well-suited for this dataset. The modularity likely helps in focusing the LLM's reasoning at each step. Removing the verification step did not negatively impact performance, suggesting it was unnecessary overhead. This confirms the hypothesis that a chain-of-thought approach, leveraging the Gemini LLM for question type determination, information extraction, and answer generation, is highly effective for this dataset.
*   **Iteration 2 Findings:** CoT with specialized agents *can* achieve perfect accuracy (1.00) on a small subset, suggesting the underlying strategy has potential. However, without error data, we cannot determine how robust this approach is. The 1.00 accuracy might be a fluke on a small, easy subset of the data. Reliability is unknown.
*   **Iteration 3 Findings:** The exploitation strategy using a decomposed approach and chain-of-thought prompting achieved only moderate accuracy (0.60). This suggests that while the overall structure is promising, the information extraction and reasoning capabilities need significant improvement. The hypothesis that a structured, multi-step approach would reliably solve these questions was not confirmed.
*   **Iteration 4 Findings:** The decompose-and-generate approach, as implemented, is not yet effective for this dataset. The accuracy of 0.80 indicates that at least one of the sub-tasks (question type classification, information extraction, answer generation) is introducing errors. The current in-context learning examples are insufficient to guide the LLM in performing the required temporal reasoning and complex entity relationship extraction. The LLM is either misinterpreting the examples or failing to generalize from them effectively.
*   **Iteration 5 Findings:** A naive chain-of-thought implementation, without more precise extraction and reasoning, performs poorly on this dataset. The accuracy was low, indicating significant issues with information extraction and temporal reasoning.
*   **Iteration 6 Findings:** The exploitation strategy, while aiming to leverage existing knowledge, isn't sufficient without improvements to numerical reasoning and contextual understanding. The current implementation of the decomposition approach is limited by the accuracy of its individual components, particularly in extracting and manipulating numerical information. Relying solely on the LLM for calculations, even with prompting, is unreliable for this dataset.
*   **Iteration 7 Findings:** The high accuracy (0.90) indicates that the overall strategy of question decomposition is effective for this type of question-answering task *on this specific dataset*. The isolated error mode (over-extraction) suggests that the primary area for improvement is refining the information extraction process, rather than fundamentally altering the overall approach. The base assumption, that the question can be successfully answered by classifying, extracting, and generating, has been validated.
*   **Iteration 8 Findings:** The decompose-and-generate approach needs significant improvement, as the accuracy is low. This highlights that while the overall structure may be promising, the information extraction, numerical reasoning, and contextual understanding need further enhancement.
*   **Iteration 9 Findings:** The Chain-of-Thought approach, while conceptually sound, is insufficient on its own when the task involves numerical reasoning. The decomposition into question type, extraction, and generation does not guarantee accuracy in numerical operations. The hypothesis that the LLM can reliably perform numerical comparisons and calculations within the Chain-of-Thought framework is rejected. Accuracy was 0.80.

## 5. NEXT RESEARCH DIRECTIONS

*   **Prioritize Error Logging and Analysis:** The *most critical* next step is to implement robust error logging that captures the following for each failure:
    *   The original question and passage.
    *   The predicted question type.
    *   The extracted information.
    *   The generated answer.
    *   The ground truth answer.
    *   The exact error message or exception that occurred.
*   **Analyze Error Data:** Once error data is available, analyze it to identify the most frequent failure modes (e.g., incorrect question type classification, poor information extraction for certain question types, hallucination during answer generation).
*   **Targeted Improvements:** Based on the error analysis, focus on improving the weakest components of the pipeline. This might involve:
    *   Providing more training examples for question type classification.
    *   Refining the information extraction prompts or logic.
    *   Improving the answer generation prompts to reduce hallucination.
*   **Introduce Complexity & Ambiguity:** Introduce more complex or ambiguous questions to test the limits of the current approach.
*   **Evaluate on Larger Dataset:** Evaluate the approach on a larger, more diverse dataset to ensure its robustness and generalizability.
*   **Prompt Engineering Exploration:** Explore the impact of different prompt engineering techniques for each step (determine type, extract info, generate answer) to potentially improve efficiency or reduce reliance on the LLM.
*   **Cost Metric Analysis:** Consider adding a cost metric (e.g., number of LLM calls) to evaluate the efficiency of the approach.
*   **Error Case Creation:** Create error cases in the dataset to evaluate the robustness of the approach. This could involve questions with missing information, ambiguous wording, or requiring deeper inference.
*   **Address Scope Issues:** Debug and resolve the scope issue that caused the NameError in the `generate_answer` function. Ensure that the `question` variable is properly passed and accessible within the function's scope.
*   **Prioritize Answer Generation:** Refocus the system on generating a *tentative* answer first, even if it's potentially incorrect. Subsequent steps can then refine or correct this answer. Shift the emphasis from verification to execution.
*   **Implement Numerical Reasoning Module:** For numerical comparison questions, incorporate a specific module designed for numerical extraction and comparison. This module should be triggered by the question type classifier and should handle the parsing and comparison of numbers found in the passage. This module should handle the parsing and comparison of numbers found in the passage.
*   **Test on Simpler Subsets:** Before scaling, test the adapted approach on smaller, simpler subsets of the data containing only one or two question types to isolate the effectiveness of the changes.
*   **Enhance Information Extraction Robustness:** Focus on improving the reliability of the `extract_relevant_info` function, with specific attention on numerical values and relationships, temporal phrases, and key entities. Incorporate techniques such as:
    *   **Fine-tuning a NER (Named Entity Recognition) model:** Train a model to identify specific entities (dates, locations, people, numerical values).
    *   **Regular Expression-based extraction:** Implement regular expressions to robustly extract numerical values and dates.
    *   **Contextual filtering:** Add logic to filter extracted information based on keywords in the question (e.g., if the question asks about "months," prioritize extracting date ranges).
*   **Improve Temporal Reasoning:** Implement specific modules or techniques to improve temporal reasoning. This could involve:
        *   Developing a robust date extraction component that can identify and normalize date mentions in the passage.
        *   Using a dedicated date calculation library or API to perform the month difference calculation.
        *   Providing more explicit examples of temporal reasoning in the in-context learning prompts.
    *   Explicitly demonstrate how to calculate time differences and order events in the chain-of-thought examples.
*   **Improve Entity Extraction and Linking:** Focus on improving the accuracy of the information extraction component, especially for named entities and their relationships. This could involve:
        *   Fine-tuning a named entity recognition (NER) model on a dataset relevant to the domain of the passages.
        *   Using a knowledge graph or entity linking service to resolve entity ambiguities.
        *   Adding more explicit examples of entity linking to the in-context learning prompts.
*   **Introduce Validation Steps:** Implement validation steps after information extraction to check the reasonableness and consistency of extracted values before passing them to the answer generation phase.
*   **Debug Sub-task Performance:** Break down the end-to-end accuracy by evaluating the performance of each sub-task independently (question type classification, information extraction, answer generation). This will help pinpoint the bottleneck in the pipeline.
*   **Increase In-Context Examples:** Experiment with increasing the number and diversity of in-context learning examples, especially for complex reasoning tasks like temporal calculation and entity relationship identification.
*   **Focus on Precise Information Extraction (Iteration 5):** The primary focus needs to be on improving the information extraction step. This likely requires a more sophisticated approach than a simple prompt. Consider these specific techniques:
    *   **Question-Aware Extraction:** Condition the information extraction on a *deeper* understanding of the question's requirements. The prompt for extraction should incorporate the question's intent (e.g., "Extract only the yards for touchdowns and field goals mentioned in the passage").
    *   **Named Entity Recognition (NER) with Type Constraints:** Use NER to identify numerical values, but *constrain* the entities based on the question type. For example, if the question asks about points, only consider NER results that are directly associated with scoring plays (touchdowns, field goals).
    *   **Relationship Extraction:** Implement a relationship extraction step to identify the *relationship* between the extracted entities and the question's targets. This could involve identifying the player, the type of play (run, pass, field goal), and the resulting score.

*   **Temporal Reasoning Module (Iteration 5):** For temporal reasoning questions, consider a dedicated module that explicitly identifies and orders events within the passage. This could involve techniques like temporal tagging or dependency parsing.
*   **Fine-tuning or Few-Shot Learning for Extraction (Iteration 5):** Fine-tune a model specifically for extracting relevant information from sports summaries or use a more robust few-shot learning strategy with more diverse examples to guide the extraction agent.
*   **Rethink Prompting (Iteration 5):** The current prompting strategy of labeling agents as "experts" may be insufficient. Experiment with more detailed and directive prompts that guide the extraction and reasoning processes more explicitly. Include more detailed examples in the prompt itself.
*   **Implement a robust numerical reasoning module (Iteration 6):** Integrate a dedicated module for performing calculations. This could involve:
    *   Explicitly identifying numerical values and their units during information extraction.
    *   Using a calculator or similar tool to ensure accurate computation.
    *   Validating the units of the final answer.
*   **Improve Date and Time Handling (Iteration 6):** Enhance the model's ability to parse and reason about dates and times. This could involve using a dedicated library for date calculations.
*   **Augment Prompting with More Diverse Examples (Iteration 6):** Provide a wider range of examples in the prompts, specifically focusing on questions that require numerical reasoning, unit conversions, and duration calculations. Examples should cover different question types (sum, difference, duration, etc.) and potential edge cases.
*   **Evaluate and Improve Information Extraction (Iteration 6):** Carefully examine the output of the information extraction stage. If the correct numbers aren't being extracted in the first place, no amount of calculation will fix the problem.
*   **Refine Information Extractor Prompt (Iteration 7 & 8):** Modify the prompt for the `extract_relevant_info` function to explicitly emphasize extracting ONLY the information required to directly answer the question. Add negative examples showing what *not* to include (e.g., examples of over-extraction). For example: "Extract ONLY the specific details mentioned in the passage that directly answer the question. Do not include any additional context or background information."
    *   **Role-Based Extraction:**  Instruct the LLM to explicitly identify the *role* of each numerical value in the context of the question (e.g., "distance of the first field goal," "distance of the longest field goal").
    *   **Contextual Similarity:** Train a smaller model (or use a few-shot prompt) to score the relevance of each sentence to the question, and then prioritize numerical values from the most relevant sentences.
*   **Post-Processing Pruning (Iteration 7):** Implement a post-processing step to prune the extracted information. This could involve a second LLM call or rule-based system to check if the extracted information contains unnecessary details and remove them. The prompt could be, "Given the question, '{question}', and the extracted information, '{extracted_info}', identify and remove any information that is not strictly required to answer the question. Return only the essential details."
*   **Evaluate Question Type Impact (Iteration 7):** Investigate whether certain question types are more prone to over-extraction. If so, tailor the extraction prompt based on the identified question type to improve precision.
*   **Focus Evaluation on Over-Extraction Cases (Iteration 7):** Create a specific evaluation dataset comprised of examples where over-extraction is likely, allowing for targeted testing of the improved information extraction prompts and post-processing techniques.
*   **Arithmetic Operation Validation (Iteration 8):** Before performing any arithmetic operation, add a step to explicitly validate the necessity and correctness of the operation based on the question type and extracted information. This could involve a separate LLM call to confirm the logic of the proposed calculation.
*   **Unit Awareness (Iteration 8):** Incorporate unit awareness into the system. Explicitly extract the units of measurement associated with each numerical value and ensure that only values with compatible units are combined in arithmetic operations. The question type determiner could be improved to extract what type of answer is expected, to guide the next steps.
*   **Improve Question Type Classifier (Iteration 8):** Enhance the question type classifier to better identify questions that require specific types of reasoning or information extraction, as well as the types of output.
    *   **Augment with Numerical Reasoning Tools:** Integrate external tools or modules specifically designed for numerical extraction, comparison, and calculation. This could involve:
        *   **Rule-based extraction:** Employ regular expressions or pattern matching to identify numerical values and associated entities (city names, team names, etc.).
        *   **Numerical comparison functions:** Implement functions to accurately compare extracted numerical values based on the question's requirements (e.g., "less than," "greater than," "sum").
    *   **Focus on Extraction Step Enhancement:** Concentrate on improving the accuracy of the information extraction step, particularly in identifying relevant numerical facts. This might involve:
        *   **Training data with numerical annotations:** Fine-tune the LLM on a dataset where numerical values and their relationships are explicitly annotated.
    *   **Test with Targeted Numerical Questions:** Create a more rigorous evaluation set comprised *exclusively* of questions requiring numerical reasoning. This will provide a more precise measure of the system's performance in this critical area.
```
        

            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        

            TOP PERFORMING APPROACHES TO BUILD UPON:
            
TOP PERFORMING APPROACH #1:
Iteration: 1
Accuracy: 1.00
Approach Summary: The script uses a chain-of-thought approach to answer questions by first determining the question type, then extracting relevant information, and finally generating the answer using the Gemini LLM. The problem is decomposed into three functions: `determine_question_type`, `extract_relevant_info`, and `generate_answer`, each acting as an agent with a specific role. The `call_llm` function is used to interface with the Gemini model, with the main function orchestrating the calls. The workflow is: main -> determine_question_type -> extract_relevant_info -> generate_answer, with each step passing information to the next.

FULL SCRIPT TO REFINE:
```python
import os
import re
import math

def main(question):
    """
    This script solves questions based on a given passage by:
    1. Determining the question type with examples.
    2. Extracting the relevant information with examples.
    3. Generating the answer with examples.

    The 'verify_answer' step has been removed to avoid getting stuck in a verification loop.
    """

    # Step 1: Determine the question type
    question_type = determine_question_type(question)
    if "Error" in question_type:
        return question_type  # Return error message

    # Step 2: Extract relevant information from the passage
    extracted_info = extract_relevant_info(question, question_type)
    if "Error" in extracted_info:
        return extracted_info

    # Step 3: Generate the answer
    generated_answer = generate_answer(extracted_info, question_type, question)
    if "Error" in generated_answer:
        return generated_answer

    return generated_answer # Directly return the generated answer

def determine_question_type(question):
    """Determine the type of the question (numerical, identification, etc.) with examples."""
    system_instruction = "You are an expert at classifying question types."
    prompt = f"""
    Determine the type of question given the following examples. Return the type only.

    Example 1:
    Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
    Type: Numerical

    Example 2:
    Question: Who caught the final touchdown of the game?
    Type: Identification

    Example 3:
    Question: Which star has a smaller mass, Nu Phoenicis or Gliese 915?
    Type: Comparative

    Question: {question}
    Type:
    """
    try:
        question_type = call_llm(prompt, system_instruction)
        if not question_type:
            return "Error: Could not determine question type"
        return question_type
    except Exception as e:
        return f"Error: {str(e)}"

def extract_relevant_info(question, question_type):
    """Extract relevant information from the passage with examples, tailored to question type."""
    system_instruction = "You are an expert at extracting relevant information."
    prompt = f"""
    Extract relevant information from the passage based on the given question type.
    Return the extracted information as a plain text summary.

    Example 1:
    Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
    Type: Numerical
    Extracted Info: Chris Johnson's first touchdown yards, Jason Hanson's first field goal yards.

    Example 2:
    Question: Who caught the final touchdown of the game?
    Type: Identification
    Extracted Info: Player who caught the final touchdown.

    Example 3:
    Question: Which star has a smaller mass, Nu Phoenicis or Gliese 915?
    Type: Comparative
    Extracted Info: Mass of Nu Phoenicis, Mass of Gliese 915.

    Question: {question}
    Type: {question_type}
    Extracted Info:
    """
    try:
        extracted_info = call_llm(prompt, system_instruction)
        if not extracted_info:
            return "Error: Could not extract information."
        return extracted_info
    except Exception as e:
        return f"Error: {str(e)}"

def generate_answer(extracted_info, question_type, question):
    """Generate the answer based on extracted information and question type with examples."""
    system_instruction = "You are an expert at generating correct answers."
    prompt = f"""
    Generate an answer to the question based on the extracted information.

    Example 1:
    Extracted Info: Chris Johnson's first touchdown yards = 40, Jason Hanson's first field goal yards = 30.
    Question Type: Numerical
    Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
    Answer: 40 + 30 = 70 yards

    Example 2:
    Extracted Info: Player who caught the final touchdown = Mark Clayton
    Question Type: Identification
    Question: Who caught the final touchdown of the game?
    Answer: Mark Clayton

    Example 3:
    Extracted Info: Mass of Nu Phoenicis = 1.2 solar masses, Mass of Gliese 915 = 0.85 solar masses.
    Question Type: Comparative
    Question: Which star has a smaller mass, Nu Phoenicis or Gliese 915?
    Answer: Gliese 915

    Extracted Info: {extracted_info}
    Question Type: {question_type}
    Question: {question}
    Answer:
    """
    try:
        answer = call_llm(prompt, system_instruction)
        if not answer:
            return "Error: Could not generate answer."
        return answer
    except Exception as e:
        return f"Error: {str(e)}"

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
```

TOP PERFORMING APPROACH #2:
Iteration: 2
Accuracy: 1.00
Approach Summary: The script uses a chain-of-thought approach to answer questions by first determining the question type, then extracting relevant information, and finally generating the answer. Each step uses the `call_llm` function to interact with the Gemini LLM, acting as a specialized agent for question type classification, information extraction, and answer generation respectively. The `main` function orchestrates the workflow: `determine_question_type` identifies the question type, `extract_relevant_info` extracts needed information based on the question and its type, and `generate_answer` formulates the final answer. Error handling is incorporated at each step, returning an error message if a step fails.

Key approach aspects (full code available for reference)

TOP PERFORMING APPROACH #3:
Iteration: 7
Accuracy: 0.90
Approach Summary: The script uses a chain-of-thought approach to answer questions by first determining the question type, then extracting relevant information, and finally generating the answer. The problem is decomposed into these three distinct steps, each handled by a separate function with an LLM call. There are three agent roles within the script: Question Type Classifier, Information Extractor, and Answer Generator.

The `main` function orchestrates the process by calling `determine_question_type`, `extract_relevant_info`, and `generate_answer` sequentially. These functions use `call_llm` to interact with the Gemini LLM, passing prompts and system instructions to guide the LLM's responses.

Key approach aspects (full code available for reference)

            
FULL SCRIPT TO REFINE:
```python
import os
import re
import math

def main(question):
    """
    This script solves questions based on a given passage by:
    1. Determining the question type with examples.
    2. Extracting the relevant information with examples.
    3. Generating the answer with examples.

    The 'verify_answer' step has been removed to avoid getting stuck in a verification loop.
    """

    # Step 1: Determine the question type
    question_type = determine_question_type(question)
    if "Error" in question_type:
        return question_type  # Return error message

    # Step 2: Extract relevant information from the passage
    extracted_info = extract_relevant_info(question, question_type)
    if "Error" in extracted_info:
        return extracted_info

    # Step 3: Generate the answer
    generated_answer = generate_answer(extracted_info, question_type, question)
    if "Error" in generated_answer:
        return generated_answer

    return generated_answer # Directly return the generated answer

def determine_question_type(question):
    """Determine the type of the question (numerical, identification, etc.) with examples."""
    system_instruction = "You are an expert at classifying question types."
    prompt = f"""
    Determine the type of question given the following examples. Return the type only.

    Example 1:
    Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
    Type: Numerical

    Example 2:
    Question: Who caught the final touchdown of the game?
    Type: Identification

    Example 3:
    Question: Which star has a smaller mass, Nu Phoenicis or Gliese 915?
    Type: Comparative

    Question: {question}
    Type:
    """
    try:
        question_type = call_llm(prompt, system_instruction)
        if not question_type:
            return "Error: Could not determine question type"
        return question_type
    except Exception as e:
        return f"Error: {str(e)}"

def extract_relevant_info(question, question_type):
    """Extract relevant information from the passage with examples, tailored to question type."""
    system_instruction = "You are an expert at extracting relevant information."
    prompt = f"""
    Extract relevant information from the passage based on the given question type.
    Return the extracted information as a plain text summary.

    Example 1:
    Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
    Type: Numerical
    Extracted Info: Chris Johnson's first touchdown yards, Jason Hanson's first field goal yards.

    Example 2:
    Question: Who caught the final touchdown of the game?
    Type: Identification
    Extracted Info: Player who caught the final touchdown.

    Example 3:
    Question: Which star has a smaller mass, Nu Phoenicis or Gliese 915?
    Type: Comparative
    Extracted Info: Mass of Nu Phoenicis, Mass of Gliese 915.

    Question: {question}
    Type: {question_type}
    Extracted Info:
    """
    try:
        extracted_info = call_llm(prompt, system_instruction)
        if not extracted_info:
            return "Error: Could not extract information."
        return extracted_info
    except Exception as e:
        return f"Error: {str(e)}"

def generate_answer(extracted_info, question_type, question):
    """Generate the answer based on extracted information and question type with examples."""
    system_instruction = "You are an expert at generating correct answers."
    prompt = f"""
    Generate an answer to the question based on the extracted information.

    Example 1:
    Extracted Info: Chris Johnson's first touchdown yards = 40, Jason Hanson's first field goal yards = 30.
    Question Type: Numerical
    Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
    Answer: 40 + 30 = 70 yards

    Example 2:
    Extracted Info: Player who caught the final touchdown = Mark Clayton
    Question Type: Identification
    Question: Who caught the final touchdown of the game?
    Answer: Mark Clayton

    Example 3:
    Extracted Info: Mass of Nu Phoenicis = 1.2 solar masses, Mass of Gliese 915 = 0.85 solar masses.
    Question Type: Comparative
    Question: Which star has a smaller mass, Nu Phoenicis or Gliese 915?
    Answer: Gliese 915

    Extracted Info: {extracted_info}
    Question Type: {question_type}
    Question: {question}
    Answer:
    """
    try:
        answer = call_llm(prompt, system_instruction)
        if not answer:
            return "Error: Could not generate answer."
        return answer
    except Exception as e:
        return f"Error: {str(e)}"

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
```

            PREVIOUSLY ATTEMPTED VARIATIONS:
            

            EXPLOITATION GUIDANCE:
            1. Review the error patterns, targeted improvements, and accumulated learnings carefully
            2. CRITICAL: Break down the problem into distinct reasoning steps before modifying code
            3. CRITICAL: Analyze the best scripts to identify which components are working well and which are failing. Focus your improvements on the weak points while preserving successful components.
            4. Maintain the core successful elements of the best approaches
            5. Consider how you can combine strengths from multiple top-performing approaches
            6. CRITICAL: Add EMBEDDED EXAMPLES to EVERY LLM prompt that illustrate:
               - Sample input that resembles the dataset
               - Step-by-step reasoning through the example
               - Properly formatted output
            7. Focus on fixing specific issues identified in previous error analyses. Create an explicit HYPOTHESIS for each targeted improvement, as well as a way to verify if it's successful.
            8. Enhance chain-of-thought reasoning and verification steps. Verification steps should be added to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
            9. Apply the key insights from ACCUMULATED LEARNINGS to enhance the approach
            10. Pay SPECIAL ATTENTION to the weaknesses and improvement suggestions from the capability assessment

            IMPROVEMENT STRATEGY:
            Analyze why the top approaches succeeded where others failed. Identify the key differentiators and strengthen them further.

            SYSTEMATIC ENHANCEMENT APPROACH:
            1. First, identify which specific function or component is underperforming based on error analysis
            2. Examine how error cases differ from successful cases
            3. For each identified weakness, implement a targeted enhancement
            4. Add additional verification steps around modified components
            5. Consider how components interact - ensure improvements don't break successful parts

            Consider enhancing the script with one or more of these patterns:
            - Repeated validation with feedback loops
            - Multi-perspective analysis with synthesis
            - Dynamic input-dependent routing
            - Hybrid approaches combining LLM with deterministic functions
            - Best-of-n solution generation and selection
            - ReAct pattern for interactive reasoning and action
            - If it is unknown how successful a processing state or part of the pipeline is, include verification steps to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
            - Answer checkers to validate the final answer against the problem statement. If the answer is incorrect, the checker can send the answer back to an earlier part of the system for refinement with feedback

            Here's how to call the Gemini API. Use this example without modification and don't invent configuration options:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

            Since this is an EXPLOITATION phase:
            - Build upon what's working well in the best approaches
            - Consider creative combinations of successful techniques from different scripts
            - Make TARGETED improvements to address specific error patterns
            - For EACH key LLM prompt, include a relevant example with:
              * Sample input similar to the dataset
              * Expected reasoning steps
              * Desired output format
            - Apply the knowledge from our accumulated learnings
            - Significantly enhance the script to address weaknesses identified in the capability assessment

            CRITICAL REQUIREMENTS:
            1. The script MUST properly handle all string literals - be extremely careful with quotes and triple quotes
            2. The script MUST NOT exceed 150 lines of code to prevent truncation
            3. Include detailed comments explaining your improvements
            4. EVERY SINGLE LLM PROMPT must include at least one embedded example showing:
               - Sample input with reasoning
               - Desired output format
            5. Make proper use of error handling
            6. Implement robust capabilities to address the specific weaknesses identified in the capability assessment
            7. Do NOT use json.loads() in the LLM calls to process input data. JSON formatting is good to use to structure information as inputs and outputs, but attempting to have functions process JSON data explicitly with strict built-in functionality is error prone due to formatting issues and additional text that appears as documentation, reasoning, or comments. When passing data into another LLM call, you can read it as plain text rather than trying to load it in strict json format, is the better approach.

            Return a COMPLETE, RUNNABLE Python script that:
            1. Has a main function that takes a question string as input and returns the answer string
            2. Makes multiple LLM calls for different reasoning steps
            3. Has proper error handling for API calls
            4. Includes embedded examples in EVERY LLM prompt
            5. Is COMPLETE - no missing code, no "..." placeholders
            6. Closes all string literals properly

            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            