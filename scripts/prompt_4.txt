
            You are developing a Python script to solve problems using LLM reasoning capabilities.
            You are in the EXPLORATION PHASE. You must generate a NEW approach that's different from previous approaches but informed by their successes and failures. With this approach, you will have a specific NEW HYPOTHESIS or variable you are trying to test. Your goal is to see if this new approach works, and you must add verification and validation steps to deduce if this new change is helpful. You may also test RADICAL NEW APPROACHES that are substantially different from previous approaches. 
            
            You should try NEW THINGS:
            
            Break down the problem into smaller pieces
            Think CREATIVELY about how to solve your problem if other approaches aren't working
            Transform data into different formats to see if it helps

            # YOUR TASK
            You are deeply familiar with prompting techniques and the agent works from the literature. 
            Your goal is to maximize the specified performance metrics by proposing interestingly new agents.
            Observe the past discovered agents and scripts carefully and think about what insights, lessons, or stepping stones can be learned from them.
            Be creative when thinking about the next interesting agent to try. You are encouraged to draw inspiration from related agent papers or academic papers from other research areas.
            Use the knowledge from the archive and inspiration from academic literature to propose the next interesting agentic system design.
            THINK OUTSIDE THE BOX.
            

            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "In which month of 2005 was the Lal Bahadur Shastri Dam, also known as the Almatti Dam, located on the Krishna River in North Karnataka, India, completed?",
    "answer": "July"
  },
  {
    "id": 1,
    "question": "In which year did Maharaj Kishan Bhan (an Indian pediatrician and clinical scientist) receive the Padma Bhushan for civil services?",
    "answer": "2013"
  },
  {
    "id": 2,
    "question": "Who is known as the first rock star of the Middle East?",
    "answer": "Lydia Canaan"
  }
]

            HISTORICAL CONTEXT:
            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 4
        - Current explore/exploit balance: 18/45
        - Best accuracy achieved: 0.67 (iteration 2)

        APPROACH HISTORY (last 4 iterations):
        [
  {
    "iteration": 0,
    "strategy": "baseline",
    "accuracy": 0.1,
    "approach": "Simple baseline script: Direct LLM call without sophisticated techniques"
  },
  {
    "iteration": 1,
    "strategy": "explore",
    "accuracy": 0.0,
    "approach": "This script answers questions using LLMs by decomposing the problem into entity/relationship extraction, search query generation, information retrieval, answer generation, and answer validation. Each step leverages the Gemini LLM with a specific system instruction to act as a specialized agent (e.g., information extractor, query generator). The script uses a few functions: `call_llm` for interacting with the Gemini model, `extract_entities_and_relationships` to identify key information, `generate_search_query` to create a search query, `retrieve_information` to simulate a search engine, `generate_answer` to formulate an answer, and `validate_answer` to check the answer's correctness. The overall workflow involves sequentially calling these functions, passing the output of one as input to the next, ultimately returning the validated answer or an error message."
  },
  {
    "iteration": 2,
    "strategy": "exploit",
    "accuracy": 0.6666666666666666,
    "approach": "The script employs a validation-driven problem-solving approach using the Gemini LLM, where a problem is first solved and then iteratively refined based on validation feedback. The problem is decomposed into solution generation and solution validation steps. There are two agent roles: a solver and a validator, each with specific system instructions. `call_llm` is used to interact with the LLM by passing prompts and instructions, while `solve_with_validation_loop` manages the iterative solving and validation process, and `main` is the entry point that calls `solve_with_validation_loop`. The overall workflow involves generating an initial solution with `call_llm` using the solver agent, validating it with `call_llm` using the validator agent, and refining the solution using `call_llm` with feedback until it's deemed valid or the maximum number of attempts is reached."
  },
  {
    "iteration": 3,
    "strategy": "explore",
    "accuracy": 0.3333333333333333,
    "approach": "The script uses LLM-based information retrieval and answer generation. It decomposes the problem into retrieving relevant context and then generating an answer using that context. The `retrieve_relevant_context` function acts as a search agent, generating search queries, simulating search, and validating the retrieved context for relevance. The `generate_answer_with_context` function acts as an expert at answering questions using the provided context.\n\nThe `call_llm` function is used to interact with the Gemini LLM, taking prompts and system instructions as input. The `retrieve_relevant_context` calls `call_llm` to generate search queries, simulate search, and validate context relevance; this function is then called by the `main` function, which then calls `generate_answer_with_context`, which then calls `call_llm` to create an answer using the retrieved context, finally returning it. The `main` function orchestrates the workflow by calling both functions sequentially."
  }
]

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 3 iterations):
        [
  {
    "iteration": 0,
    "issue": "The most critical problem is the system's reliance on an unreliable knowledge source which leads to the retrieval and provision of factually incorrect information. The lack of a verification mechanism exacerbates this issue, as the system blindly trusts the incorrect information."
  },
  {
    "iteration": 1,
    "issue": "The most critical problem is the **inaccurate and unreliable information extraction process, coupled with insufficient validation and error detection**. The system needs to be significantly improved in its ability to pinpoint the specific information required to answer the question correctly and verify that the retrieved information is accurate and relevant."
  },
  {
    "iteration": 2,
    "issue": "The primary issue is **inaccurate knowledge retrieval**. The system provides a definite answer that is factually incorrect, indicating a flaw in its information gathering or database. This highlights the need for improved source reliability and validation."
  },
  {
    "iteration": 3,
    "issue": "The primary issue is **inaccurate factual recall, particularly involving dates and specific names**. This leads to the system providing incorrect answers even when it understands the question's intent. The root cause could stem from flaws in the training data, the knowledge retrieval mechanism, or the final answer selection process."
  }
]

        TARGETED IMPROVEMENTS:
        [
  "Stage 2: Semantic Filtering:** Filter the retrieved information based on semantic relevance to the question. Use techniques like sentence similarity or knowledge graph traversal to identify the most relevant pieces of information.",
  "Improved Ranking Algorithm:** Experiment with different ranking algorithms to prioritize answers based on factors such as source credibility, frequency of mention, and relevance to the question's context. For example, prioritize information extracted from reputable encyclopedias or biographical databases.",
  "Stage 1: Focused Retrieval:** Retrieve a narrow set of potentially relevant information based on keywords and entity recognition.",
  "Confidence Scoring and Thresholding:** Develop a confidence scoring mechanism for potential answers and set a threshold for acceptance. Answers below the threshold should be rejected, and the system should indicate its uncertainty or seek clarification.",
  "Data Verification and Augmentation:** Implement automated and manual processes to verify and correct information in the knowledge base. Focus on fact-checking and updating numerical data (especially dates). Consider augmenting the knowledge base with more comprehensive biographical data, especially for prominent figures.",
  "Fact Verification Module:** Integrate a fact verification module that cross-references potential answers against multiple external sources to assess their accuracy and reliability.",
  "Stage 3: Verification and Validation:** Verify the accuracy and consistency of the filtered information by cross-referencing with other sources, applying logical rules, or using external validation tools."
]
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity ✓
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity ✓
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity ✓
    4. All operations have O(1) average time complexity ✓
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        

            PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS). YOUR APPROACH MUST BE SUBSTANTIVELY DIFFERENT THAN THESE:
            
PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS):

=== SCRIPT FROM ITERATION 3 (explore, ACCURACY: 0.33) ===
Approach: The script uses LLM-based information retrieval and answer generation. It decomposes the problem into retrieving relevant context and then generating an answer using that context. The `retrieve_relevant_context` function acts as a search agent, generating search queries, simulating search, and validating the retrieved context for relevance. The `generate_answer_with_context` function acts as an expert at answering questions using the provided context.

The `call_llm` function is used to interact with the Gemini LLM, taking prompts and system instructions as input. The `retrieve_relevant_context` calls `call_llm` to generate search queries, simulate search, and validate context relevance; this function is then called by the `main` function, which then calls `generate_answer_with_context`, which then calls `call_llm` to create an answer using the retrieved context, finally returning it. The `main` function orchestrates the workflow by calling both functions sequentially.

```python
import os
import re
import math # for react
from google import genai
from google.genai import types

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def retrieve_relevant_context(question, max_attempts=3):
    """
    Retrieve relevant context for the given question using LLM-based search query generation and a simulated search engine.
    This directly addresses the hallucination and inaccurate retrieval issues from previous iterations.
    Includes a validation loop to ensure the retrieved context is relevant.
    """
    system_instruction = "You are an expert at generating search queries to retrieve relevant information."

    for attempt in range(max_attempts):
        # Step 1: Generate search query with examples
        query_prompt = f"""
        Generate a concise search query to find relevant information for the given question.

        Example 1:
        Question: In which year was Jamini Roy (an Indian painter) awarded the Padma Bhushan by the Government of India?
        Search Query: "Jamini Roy Padma Bhushan award year"

        Example 2:
        Question: Which architect was tasked with finishing the chapel in the newly built Papal apartment when its construction remained incomplete after Pope Paul IV moved in, in October 1556?
        Search Query: "architect finishing Papal apartment Pope Paul IV 1556"

        Example 3:
        Question: In 1993, Vaughan Jones was elected to which academy?
        Search Query: "Vaughan Jones elected academy 1993"

        Question: {question}
        Search Query:
        """
        search_query = call_llm(query_prompt, system_instruction)

        # Step 2: Simulate search and retrieve context
        context = call_llm(f"Provide concise information about: {search_query}", "You are a helpful search engine.")

        # Step 3: Validate context relevance with examples
        validation_prompt = f"""
        Validate if the retrieved context is relevant to the question.
        If relevant, respond with "RELEVANT: [brief explanation]".
        If not relevant, respond with "IRRELEVANT: [detailed explanation]".

        Example 1:
        Question: In which year was Jamini Roy awarded the Padma Bhushan?
        Context: Jamini Roy received the Padma Bhushan in 1954.
        Validation: RELEVANT: The context directly answers the question about the award year.

        Example 2:
        Question: Which architect finished the Papal apartment chapel in 1556?
        Context: Pirro Ligorio completed the chapel in the Papal apartment in October 1556.
        Validation: RELEVANT: The context identifies the architect and the task.

        Question: {question}
        Context: {context}
        Validation:
        """
        validation_result = call_llm(validation_prompt, "You are an expert at determining the relevance of a text to a question.")

        if "RELEVANT:" in validation_result:
            return context
        else:
            print(f"Attempt {attempt + 1}: Retrieved context is irrelevant. Retrying...")

    return "No relevant context found." # Fallback after multiple attempts

def generate_answer_with_context(question, context):
    """
    Generate the answer using the retrieved context.
    This leverages the LLM's reasoning capabilities to synthesize an answer based on the context.
    """
    system_instruction = "You are an expert at answering questions based on provided context."

    prompt = f"""
    Answer the question using the provided context. If the context does not contain the answer, state "Answer not found in context."

    Example 1:
    Question: In which year was Jamini Roy awarded the Padma Bhushan?
    Context: Jamini Roy received the Padma Bhushan in 1954.
    Answer: 1954

    Example 2:
    Question: Which architect finished the Papal apartment chapel in 1556?
    Context: Pirro Ligorio completed the chapel in the Papal apartment in October 1556.
    Answer: Pirro Ligorio

    Question: {question}
    Context: {context}
    Answer:
    """
    return call_llm(prompt, system_instruction)

def main(question):
    """
    Main function: Orchestrates context retrieval and answer generation.
    """
    context = retrieve_relevant_context(question)
    if "No relevant context found" in context:
        return "Could not find the answer."

    answer = generate_answer_with_context(question, context)
    return answer
```

=== SCRIPT FROM ITERATION 2 (exploit, ACCURACY: 0.67) ===
Approach: The script employs a validation-driven problem-solving approach using the Gemini LLM, where a problem is first solved and then iteratively refined based on validation feedback. The problem is decomposed into solution generation and solution validation steps. There are two agent roles: a solver and a validator, each with specific system instructions. `call_llm` is used to interact with the LLM by passing prompts and instructions, while `solve_with_validation_loop` manages the iterative solving and validation process, and `main` is the entry point that calls `solve_with_validation_loop`. The overall workflow involves generating an initial solution with `call_llm` using the solver agent, validating it with `call_llm` using the validator agent, and refining the solution using `call_llm` with feedback until it's deemed valid or the maximum number of attempts is reached.

```python
import os
import re
import json
import math # for react
from google import genai
from google.genai import types

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop.
    This is based on successful patterns from previous iterations, particularly in Iteration 0,
    but enhanced with iterative refinement for better accuracy."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions. Focus on factual accuracy and completeness."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements, ensuring factual correctness and completeness."

    # Initial solution generation - Enhanced with multi-example prompting
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements. Focus on factually accurate and complete answers.

    Example 1:
    Problem: What is the name of the university where Ana Figueroa, a political activist and government official, studies and graduates from?
    Solution: University of Chile

    Example 2:
    Problem: Which genus was the ruby-throated bulbul moved to from *Turdus* before finally being classified in the genus *Rubigula*?
    Solution: Genus Pycnonotus

    Example 3:
    Problem: In what year did Etta Cone last visit Europe?
    Solution: 1938

    Problem:
    {problem}
    """

    solution = call_llm(solution_prompt, system_instruction_solver)

    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution - Enhanced with specific validation examples
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem. Ensure factual correctness and completeness.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues, including specific factual errors or omissions]".

        Example 1:
        Problem: What is the capital of France?
        Solution: Paris
        Validation: VALID: The capital of France is indeed Paris.

        Example 2:
        Problem: Who painted the Mona Lisa?
        Solution: Leonardo DaVinci
        Validation: VALID: The Mona Lisa was painted by Leonardo da Vinci.

        Example 3:
        Problem: What year did World War II begin?
        Solution: 1940
        Validation: INVALID: World War II began in 1939, not 1940.

        Problem:
        {problem}

        Proposed Solution:
        {solution}
        """

        validation_result = call_llm(validation_prompt, system_instruction_validator)

        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution

        # If invalid, refine the solution - Provides multi-example based feedback to ensure robust refinement
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed. Ensure that you only use information from the original problem in your response, and ensure that the response is factually correct and as complete as possible.

        Problem:
        {problem}

        Your previous solution:
        {solution}

        Validation feedback:
        {validation_result}

        Example of a corrected solution based on validation feedback:

        Problem: When did the Titanic sink?
        Your previous solution: April 1912
        Validation Feedback: INVALID: The Titanic sank on April 15, 1912, include the day.

        Corrected Solution: April 15, 1912

        Please provide a completely revised solution that addresses all the issues mentioned. Be as factual as possible. Do not attempt to create new information that is not present in the original response.
        """

        solution = call_llm(refined_prompt, system_instruction_solver)

    return solution

def main(question):
    """
    Main function that orchestrates the solution process using solve_with_validation_loop.
    This function now incorporates the iterative validation loop for enhanced accuracy.
    This is a hybrid approach combining elements from Iteration 0 (direct LLM call) with the idea
    of iterative refinement from later iterations.
    """
    answer = solve_with_validation_loop(question)
    return answer
```

=== SCRIPT FROM ITERATION 1 (explore, ACCURACY: 0.00) ===
Approach: This script answers questions using LLMs by decomposing the problem into entity/relationship extraction, search query generation, information retrieval, answer generation, and answer validation. Each step leverages the Gemini LLM with a specific system instruction to act as a specialized agent (e.g., information extractor, query generator). The script uses a few functions: `call_llm` for interacting with the Gemini model, `extract_entities_and_relationships` to identify key information, `generate_search_query` to create a search query, `retrieve_information` to simulate a search engine, `generate_answer` to formulate an answer, and `validate_answer` to check the answer's correctness. The overall workflow involves sequentially calling these functions, passing the output of one as input to the next, ultimately returning the validated answer or an error message.

```python
import os
from google import genai
from google.genai import types
import re

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def extract_entities_and_relationships(question):
    """Extract key entities and relationships from the question using LLM with multiple examples."""
    system_instruction = "You are an expert information extractor specializing in entities and relationships."

    prompt = f"""
    Extract the key entities and relationships from the following question. Provide the entities and relationships in plain text.

    Example 1:
    Question: What is the name of the man who purchased Belmont on St. Saviour's Hill, Jersey, UK, in 1822?
    Entities: man, Belmont, St. Saviour's Hill, Jersey, UK
    Relationships: purchased Belmont on St. Saviour's Hill in 1822

    Example 2:
    Question: How many corners did Barcelona take in the Champions League semi-final match between Barcelona and Milan on April 27, 2006?
    Entities: corners, Barcelona, Champions League semi-final, Milan, April 27, 2006
    Relationships: Barcelona took corners in match against Milan on April 27, 2006

    Example 3:
    Question: Specify the day, month, and year in which Activision Blizzard announced the upcoming establishment of a new esports division.
    Entities: day, month, year, Activision Blizzard, esports division
    Relationships: Activision Blizzard announced esports division

    Question: {question}
    Entities and Relationships:
    """
    return call_llm(prompt, system_instruction)

def generate_search_query(entities_and_relationships, question):
    """Generate a search query from extracted entities and relationships using LLM with multiple examples."""
    system_instruction = "You are an expert query generator."

    prompt = f"""
    Generate a search query from the extracted entities and relationships. The search query should be optimized for finding the answer to the question.

    Example 1:
    Question: What is the name of the man who purchased Belmont on St. Saviour's Hill, Jersey, UK, in 1822?
    Entities and Relationships: man, Belmont, St. Saviour's Hill, Jersey, UK; purchased Belmont on St. Saviour's Hill in 1822
    Search Query: "man purchased Belmont St. Saviour's Hill Jersey 1822"

    Example 2:
    Question: How many corners did Barcelona take in the Champions League semi-final match between Barcelona and Milan on April 27, 2006?
    Entities and Relationships: corners, Barcelona, Champions League semi-final, Milan, April 27, 2006; Barcelona took corners in match against Milan on April 27, 2006
    Search Query: "Barcelona Milan Champions League semi-final corners April 27 2006"

    Example 3:
    Question: Specify the day, month, and year in which Activision Blizzard announced the upcoming establishment of a new esports division.
    Entities and Relationships: day, month, year, Activision Blizzard, esports division; Activision Blizzard announced esports division
    Search Query: "Activision Blizzard esports division announcement date"

    Question: {question}
    Entities and Relationships: {entities_and_relationships}
    Search Query:
    """
    return call_llm(prompt, system_instruction)

def retrieve_information(search_query):
    """Simulate information retrieval from a search engine."""
    # In a real implementation, this would call an actual search API
    system_instruction = "You are a search engine that provides concise information."
    return call_llm(f"Provide information about: {search_query}", system_instruction)

def generate_answer(question, retrieved_information):
    """Generate the answer from the retrieved information using LLM with multiple examples."""
    system_instruction = "You are an expert answer generator."

    prompt = f"""
    Generate an answer to the question using the retrieved information.

    Example 1:
    Question: What is the name of the man who purchased Belmont on St. Saviour's Hill, Jersey, UK, in 1822?
    Retrieved Information: Sir Colin Halkett purchased Belmont on St. Saviour's Hill, Jersey, UK, in 1822.
    Answer: Sir Colin Halkett

    Example 2:
    Question: How many corners did Barcelona take in the Champions League semi-final match between Barcelona and Milan on April 27, 2006?
    Retrieved Information: Barcelona took 3 corners in the Champions League semi-final match between Barcelona and Milan on April 27, 2006.
    Answer: 3

    Example 3:
    Question: Specify the day, month, and year in which Activision Blizzard announced the upcoming establishment of a new esports division.
    Retrieved Information: Activision Blizzard announced the upcoming establishment of a new esports division on 21 of October of 2015.
    Answer: 21 of October of 2015

    Question: {question}
    Retrieved Information: {retrieved_information}
    Answer:
    """
    return call_llm(prompt, system_instruction)

def validate_answer(question, answer):
    """Validate the generated answer against the question using LLM with multiple examples."""
    system_instruction = "You are an expert answer validator."

    prompt = f"""
    Validate if the answer correctly answers the question. If there are issues respond with INVALID: [explain issues], else respond with VALID: [brief explanation]

    Example 1:
    Question: What is the name of the man who purchased Belmont on St. Saviour's Hill, Jersey, UK, in 1822?
    Answer: Sir Colin Halkett
    Validation: VALID: The answer provides the name of the man.

    Example 2:
    Question: How many corners did Barcelona take in the Champions League semi-final match between Barcelona and Milan on April 27, 2006?
    Answer: 3
    Validation: VALID: The answer provides the number of corners.

    Example 3:
    Question: Specify the day, month, and year in which Activision Blizzard announced the upcoming establishment of a new esports division.
    Answer: 21 of October of 2015
    Validation: VALID: The answer provides the day, month, and year.

    Question: {question}
    Answer: {answer}
    Validation:
    """
    return call_llm(prompt, system_instruction)

def main(question):
    """
    Solve a question using LLM by extracting entities and relationships, generating a search query,
    retrieving information, generating an answer, and validating the answer.
    """
    try:
        # Step 1: Extract entities and relationships
        entities_and_relationships = extract_entities_and_relationships(question)

        # Step 2: Generate a search query
        search_query = generate_search_query(entities_and_relationships, question)

        # Step 3: Retrieve information
        retrieved_information = retrieve_information(search_query)

        # Step 4: Generate an answer
        answer = generate_answer(question, retrieved_information)

        # Step 5: Validate the answer
        validation_result = validate_answer(question, answer)

        if "VALID:" in validation_result:
            return answer
        else:
            return f"Error: {validation_result}"

    except Exception as e:
        print(f"Error: {str(e)}")
        return f"Error: {str(e)}"
```

=== SCRIPT FROM ITERATION 0 (baseline, ACCURACY: 0.10) ===
Approach: Simple baseline script: Direct LLM call without sophisticated techniques

```python
import os
from google import genai
from google.genai import types

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response"""
    try:
        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question):
    """
    Baseline script: Simple direct LLM call without sophisticated techniques.
    This establishes the baseline performance capability for this dataset.
    """
    system_instruction = "You are a helpful assistant. Answer the question directly and concisely based on the information provided."

    # Simple, direct call to LLM
    answer = call_llm(question, system_instruction)

    return answer
    
```


            LEARNINGS FROM PREVIOUS ITERATIONS:
            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        ```
# Dataset-Specific Experiment Log: Question Answering

This document serves as a continuously updated log of patterns, strategies, and findings related to the question-answering task for this specific dataset. It prioritizes concrete, task-specific insights over general principles.

## 1. DATASET PATTERNS & CHARACTERISTICS

*   **Question Type Dominance:** The dataset predominantly features *Who* questions, seeking individuals or groups associated with specific events, creations, or awards. Example: "Who created Groove Coaster?". There is also at least one example of a "What" question focusing on a specific part of a person's name. Example: "What was the first name of Ralph E. Oesper?".
*   **Answer Type:** Answers are typically short-form, factual names of people, groups, or sometimes dates/numbers. They represent precise details directly related to the question.
*   **Knowledge Breadth:** Questions span a wide range of topics, requiring broad domain knowledge. Examples include music production (Groove Coaster), chemistry history (Ralph E. Oesper), oceanography (Jerlov Award), music band creation (Sho?), and TV series details ("El guardián invisible").
*   **Question Specificity:** Varies from very precise to allowing some interpretation.
*   **Structure and Format:** Questions are natural language sentences. Answers are simple noun phrases or names. Each entry has an ID field (string).
*   **Reasoning Type:** Primarily fact retrieval. Answers are facts needing extraction from a knowledge source.
*   **Entity and Relationship Focus:** Questions often contain specific entities (person, place, organization) and relationships (purchased, announced).
*   **Date Sensitivity:** Correctness is highly sensitive to dates; even slight variations are considered incorrect.
    *   *Example:* Needs to be able to distinguish between "October 20" vs. "21 of October".
    *   Questions often demand precise factual recall, including specific years ("In which year did..."), months ("In which month of 2005...").
*   **Need for Completeness:** Answers must include all parts requested in the question (e.g., day, month, and year when asked for).
*   **Complex Relationships:** Questions frequently involve identifying relationships between entities across different time periods or classification schemes (e.g., genus changes, visit dates). This requires the system to track and correlate information from potentially disparate sources.
*   **Implicit Assumptions:** Some questions rely on implicit assumptions or background knowledge that the LLM might not possess. For example, understanding the taxonomic hierarchy to follow genus changes.
*   **Comparative Reasoning:** The dataset requires the system to compare and contrast information (e.g., "moved *to* from *Turdus* *before* finally being classified"). This necessitates accurate tracking of changes in classification over time.
*   **Specific Factual Recall:** The questions demand precise factual recall, often involving dates, names, and associations (e.g., "In which year was X awarded Y?").
*   **Entity Recognition & Disambiguation:** Questions involve named entities (people, organizations, awards) that require the LLM to correctly identify and disambiguate them. The failure highlights the importance of accurately mapping entities and their properties.
*   **Complex Relational Queries:** The questions often require understanding the relationship between multiple entities (e.g., person, achievement, organization). This goes beyond simple fact retrieval and requires the LLM to understand and reason about the relationships between different pieces of information.
*   **Compound Information:** Questions often contain multiple pieces of information (e.g., name, title, location), requiring the system to integrate and filter information accurately. *Example:* "In which month of 2005 was the Lal Bahadur Shastri Dam, also known as the Almatti Dam, located on the Krishna River in North Karnataka, India, completed?"
*   **Ambiguity in "First" or "Pioneer" Questions:** Questions asking about "first" or "pioneer" individuals or events may have multiple valid answers or lack a definitive answer, leading to discrepancies between the expected answer and the system's response.

## 2. EFFECTIVE TASK-SPECIFIC STRATEGIES

*   **Ineffective:** Direct LLM question answering without knowledge retrieval or verification (Baseline Experiment). Accuracy was only 10%.
*   **Ineffective:** Simple chain-of-thought approach using specialized LLM agents for each step (extraction, query generation, retrieval, answer generation, validation). Accuracy was 0%.
*   **Potentially Effective:** The validation loop approach has promise, but it needs to be coupled with more robust information retrieval to be effective for this dataset. The idea of iterative refinement based on validation feedback is sound, but only if the initial information provided to the solver is accurate.
*   **Untested:** Knowledge Base Retrieval (using LLM to formulate queries for external knowledge bases)
*   **Untested:** Hybrid Approach (LLM for query rephrasing and search results informing answer generation)
*   **Untested:** Entity and Relation Extraction before Knowledge Retrieval (though initial experiments highlight the need for *precise* extraction).
*   **Untested:** Structured Query Generation (e.g., SPARQL)
*   **Untested:** Few-Shot Learning, Chain-of-Thought Prompting, Answer Verification Prompting, Specialized "Who" question prompts.
*   **No Successful Strategies (Iteration 3):** The accuracy of 0.33 in Iteration 3 indicates that the current LLM-based information retrieval and answer generation approach needs significant improvement.

## 3. COMMON FAILURE MODES ON THIS DATASET

*   **Factual Inaccuracy (Hallucination):** LLM provides incorrect facts not supported by evidence. Example: Incorrect details about "Barcelona corners" or the "Belmont purchaser" in the baseline experiment. This indicates the LLM is "hallucinating" facts. In Iteration 2, the LLM incorrectly stated "The Royal Society" instead of "American Academy of Arts and Science." This indicates the LLM's knowledge base or retrieval mechanism is unreliable for this specific information.
*   **Date Discrepancies:** Small differences in dates (e.g., "October 20" vs. "21 of October") are marked as incorrect, showing the need for high date precision.
*   **Incomplete Answers:** LLM fails to provide all parts of the answer requested by the question. Example: Providing only the month and year when the question asks for the day, month, and year.
*   **Ambiguity:** (Hypothesized from initial analysis) Some questions could be ambiguous if taken out of context.
*   **Multiple Valid Answers:** (Hypothesized from initial analysis) Some questions might have multiple correct answers, or answers that vary in specificity.
*   **Name Variations:** (Hypothesized from initial analysis) People's names can be written in different ways (e.g., "Robert" vs. "Bob").
*   **Cultural Differences:** (Hypothesized from initial analysis) Name formats differ across cultures. The dataset might contain names from various regions.
*   **Misspellings:** (Hypothesized from initial analysis) Questions might contain misspellings of names or terms, which could make retrieval difficult.
*   **Incorrect Entity/Relationship Extraction:** The system fails to accurately extract the precise entities and relationships needed to answer the question. For example, the system extracted "American University" which was not present in the gold answer, and was not relevant in Iteration 1.
*   **Inability to Discern Temporal Order:** The system struggles to correctly identify the sequence of events or changes over time. This is evident in the ruby-throated bulbul question, where the system failed to establish the order of genus classifications. The last visit to Europe question also shows this.
*   **Inaccurate Information Retrieval & Validation:** The system struggles to validate if information retrieved is in fact valid. This is demonstrated across all samples in Iteration 1 and Iteration 3.
*   **Lack of Reliable Source Attribution:** Since the responses aren't grounded in verifiable sources, it is difficult to determine the source of the error. This makes debugging and improving the system challenging.
*   **Incorrect Year Retrieval:** The system incorrectly retrieves the year in several questions, indicating a failure in the information retrieval or context validation steps. *Example:* For the question about Maharaj Kishan Bhan in Iteration 3, the system returned "1999" instead of "2013". This suggests a problem in filtering or prioritizing information within the retrieved context.
*   **Handling Ambiguity in "First" Questions:** The system struggled with questions seeking the "first" of something in Iteration 3. Instead of providing the exact expected answer ("Lydia Canaan"), it offered multiple possibilities or disclaimers about the lack of a single definitive answer. This indicates a failure in resolving ambiguity and selecting the most appropriate answer based on the context.
*   **Context Validation Inadequacy**: In Iteration 3, the LLM struggles to validate the context against the question and extracts the incorrect named entity, date, or other fact from the search results.
*   **Script Errors:** Script errors encountered during attempted repairs, indicating fragility of the codebase and the need for more robust error handling. Examples:
    *   `ERROR: Answer not found in context`
    *   `ERROR: Gemini API call failed with AttributeError: 'module' object has no attribute 'GenerativeModel'`
    *   `ERROR: Gemini API error and irrelevant context indicate script failure.`

## 4. EXPERIMENT LOG & FINDINGS

*   **Iteration 0 (Baseline):**
    *   **Approach:** Direct LLM call with a basic prompt.
    *   **Runtime:** Not explicitly recorded, but assumed to be fast due to direct prompting.
    *   **Accuracy:** 10%.
    *   **Key Findings:** Direct LLM prompting is insufficient. Requires knowledge retrieval and verification.
    *   **Error Analysis:** Factual inaccuracies and date discrepancies were the primary error types.

*   **Iteration 1:**
    *   **Approach:** Chain-of-thought approach using specialized LLM agents for each step (extraction, query generation, retrieval, answer generation, validation).
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** 0%.
    *   **Key Findings:** A simple chain-of-thought approach is not sufficient. Inaccuracies at the extraction stage propagate through the entire pipeline. The LLM struggles with understanding temporal information, tracking relationships across multiple sources, and performing accurate information validation.
    *   **Error Analysis:** Incorrect entity/relationship extraction, inability to discern temporal order, and inaccurate information retrieval and validation.

*   **Iteration 2:**
    *   **Approach:** Validation loop with specialized LLM agents. (Details of the individual components as in Iteration 1 were not re-stated but assumed).
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** Not explicitly recorded, but noted that validation alone was insufficient.
    *   **Key Findings:** The validation loop did not catch the factual inaccuracy in the example. This suggests the validator agent needs to be more rigorous in verifying factual claims, and needs access to reliable information to do so.
    *   **Error Analysis:** Incorrect Factual Recall. Validation alone is insufficient. Need for External Knowledge Injection. Lack of Reliable Source Attribution.

*   **Iteration 3:**
    *   **Approach:** LLM-based information retrieval and answer generation (details of the specific implementation not provided).
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** 0.33%.
    *   **Key Findings:** The current LLM-based information retrieval and answer generation approach, while conceptually sound, does not achieve satisfactory accuracy on this dataset. Contextual Understanding is Not Enough. The approach can be adapted by changing the prompt template, finetuning the LLM, or by giving it a validation step after information retrieval and before answer generation. More data alone may not be the solution. Prompt engineering or other improvements may be more beneficial.
    *   **Error Analysis:** Incorrect Year Retrieval. Handling Ambiguity in "First" Questions. Context Validation Inadequacy.

## 5. NEXT RESEARCH DIRECTIONS

*   **Implement Knowledge Retrieval:** Integrate a search engine or knowledge base to retrieve supporting information *before* answer generation. This is crucial for addressing factual inaccuracies.
*   **Implement Answer Verification:** Verify the LLM's answer against a reliable external source and correct it if discrepancies are found. This should reduce hallucinations.
*   **Date Normalization:** Standardize date formats in questions and retrieved information to enable accurate comparison.
*   **Prompt Engineering for Completeness:** Revise the prompt to ensure the model provides all parts of the answer or responds with an appropriate error message (e.g., "Unable to determine the day."). Example prompt update: "Question: {question}. Answer: If all requested parts of the answer (day, month, year) are known, provide them all. Otherwise, state 'Insufficient Information'."
*   **Explore Hybrid Approach:** Use LLM for query rephrasing to improve search engine results, and then use those results to generate an answer.
*   **Enhance Entity and Relationship Extraction:** Implement more robust methods for entity and relationship extraction, potentially using named entity recognition (NER) models finetuned on similar datasets. Focus on precise extraction of the specific entities and relationships relevant to the question.
*   **Incorporate Temporal Reasoning:** Add a dedicated temporal reasoning module to track changes over time. This could involve using specialized data structures or algorithms to represent temporal relationships and perform reasoning about event sequences.
*   **Improve Answer Validation:** Implement more rigorous answer validation techniques, such as cross-referencing information from multiple sources and using a separate validation model to assess the answer's correctness. Implement a system that assesses the quality of the extracted entities *before* query generation.
*   **Iterative Refinement with Feedback:** Create a feedback loop where incorrect answers are analyzed to identify the specific errors made by each component in the pipeline. Use this feedback to iteratively refine the system and improve its performance.
*   **Test Structured Query Generation:** Convert the natural language question into a structured query (e.g., SPARQL) to retrieve information from knowledge graphs like Wikidata.
*   **Investigate Few-Shot Learning:** Provide the LLM with examples of question-answer pairs to improve performance.
*   **Evaluate Chain-of-Thought Prompting:** Prompt the LLM to explain its reasoning process step-by-step (though a simple version of this failed).
*   **Develop Answer Verification Prompting:** Use a separate prompt to ask the LLM to verify the answer's accuracy.
*   **Create Specialized "Who" Question Prompts:** Given the dominance of "Who" questions, design highly optimized prompts.
*   **Track Latency:** Measure and log the runtime (latency) of each experiment, to understand the performance impact of different strategies.
*   **Analyze Failure Cases:** Perform detailed analysis of failure cases to identify patterns and refine strategies.
*   **Implement Fact-Checking Mechanism:** Integrate a mechanism to fact-check the LLM's answers against a reliable knowledge base (e.g., Wikipedia, a curated database). The validator agent should use this to verify the LLM's claims.
*   **Implement Source Tracking:** Modify the `call_llm` function to include source tracking. This will allow the system to track where the information comes from and identify potential sources of error.
*   **RAG Implementation**: Explore Retrieval-Augmented Generation (RAG) to provide the LLM with relevant context from external sources during the solution generation phase. This will help to ground the LLM's answers in verifiable evidence. This should be implemented as part of the `call_llm` function.
*   **Improve Context Validation:** Implement a more robust context validation mechanism in the `retrieve_relevant_context` function. This could involve techniques like verifying the presence of key entities (names, dates) and assessing the overall relevance of the retrieved context to the specific question.
*   **Fine-tune Answer Selection Logic:** Refine the answer selection logic in the `generate_answer_with_context` function to prioritize precise factual matches and resolve ambiguity in "first" or "pioneer" questions. Explore techniques like confidence scoring or rule-based filtering to select the most appropriate answer.
*   **Prompt Engineering for Date Retrieval:** Optimize the prompts used for generating search queries and validating context to emphasize the importance of accurate date retrieval. Include explicit instructions to prioritize sources that provide specific dates.
*   **Prompt Engineering for validation:** Give the LLM the ability to "check its work" by validating its initial answer against the original question and the retrieved context. For example, "Does this answer the question completely? Are all the details consistent with the provided context?"
*   **Implement robust error handling and logging** to better understand and recover from script errors.
```
        

            CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        

            EXPLORATION GUIDANCE:
            1. Review the historical approaches, error patterns, and accumulated learnings carefully
            2. Review the FULL CODE of previous scripts to understand what has already been tried
            3. Design a new approach that is DISTINCTLY DIFFERENT from previous attempts. This approach should have a specific NEW HYPOTHESIS or variable you are trying to test. 
            4. CRITICAL: Include EMBEDDED EXAMPLES directly within your LLM prompts
            5. For each key function, show a complete worked example, or include multiple examples, including:
               - Input example that resembles the dataset
               - Step-by-step reasoning through the example
               - Properly formatted output
            6. Apply the insights from the ACCUMULATED LEARNINGS section to avoid repeating past mistakes
            7. Pay SPECIAL ATTENTION to the weaknesses and improvement suggestions from the capability assessment
            8. Consider implementing one or more of these LLM usage patterns:
               - Repeated validation with feedback loops
               - Multi-perspective analysis with synthesis
               - Dynamic input-dependent routing with an orchestrator
               - Hybrid approaches combining LLM with deterministic functions
               - Best-of-n solution generation and selection
               - ReAct pattern for interactive reasoning and action
               - If it is unknown how successful a processing state or part of the pipeline is, include verification steps to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
               - Answer checkers to validate the final answer against the problem statement. If the answer is incorrect, the checker can send the answer back to an earlier part of the system for for refinement with feedback

            Here's how to call the Gemini API. Use this example without modification and don't invent configuration options:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

            Since this is an EXPLORATION phase:
            - Try a fundamentally different approach to reasoning about the problem. Test a NEW HYPOTHESIS or variable, and add verification steps to deduce if this new change is helpful.
            - THIS IS KEY: Break down the problem into new, distinct reasoning steps based on past performance before you start coding
            - For EACH key LLM prompt, include a relevant example with:
              * Sample input similar to the dataset
              * Expected reasoning steps
              * Desired output format
            - Apply a verifier call to different parts of the pipeline in order to understand what parts of the pipeline of calls is successful and where the system is breaking
            - Pay special attention to addressing the primary issues from previous iterations
            - Ensure your new approach addresses the weaknesses identified in the capability assessment

            CRITICAL REQUIREMENTS:
            1. The script MUST properly handle all string literals - be extremely careful with quotes and triple quotes
            2. The script MUST NOT exceed 150 lines of code to prevent truncation
            3. Include detailed comments explaining your reasoning approach
            4. EVERY SINGLE LLM PROMPT must include at least one embedded example showing:
               - Sample input with reasoning
               - Desired output format
            5. Make proper use of error handling
            6. Implement robust capabilities to address the specific weaknesses identified in the capability assessment
            7. Do NOT use json.loads() in the LLM calls to process input data. JSON formatting is good to use to structure information as inputs and outputs, but attempting to have functions process JSON data explicitly with strict built-in functionality is error prone due to formatting issues and additional text that appears as documentation, reasoning, or comments. When passing data into another LLM call, you can read it as plain text rather than trying to load it in strict json format, is the better approach.

            Return a COMPLETE, RUNNABLE Python script that:
            1. Has a main function that takes a question string as input and returns the answer string
            2. Makes multiple LLM calls for different reasoning steps
            3. Has proper error handling for API calls
            4. Includes embedded examples in EVERY LLM prompt
            5. Is COMPLETE - no missing code, no "..." placeholders
            6. Closes all string literals properly

            This should be FUNDAMENTALLY DIFFERENT from all previous approaches. Do not reuse the same overall structure.

            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            