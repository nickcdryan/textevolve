
            You are performing SURGICAL REFINEMENT of the single best-performing script.
            Your goal is to identify specific weaknesses in this script and make targeted improvements while preserving its strengths.

            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "Multi-hop reasoning task:\n\nQuestion: What group did Carlene LeFevre and Rich LeFevre form in Brooklyn, New York City?\n\nSupporting Documents:\n=== Document 1: Downtown Brooklyn Cultural District ===\nThe Brooklyn Cultural District (formerly known as the BAM-Downtown Brooklyn Cultural District) is a $100 million development project that focuses on the arts, public spaces and affordable housing in Fort Greene, Brooklyn, New York. The project reflected the joint efforts of New York City's Economic Development Corporation, the Department of Cultural Affairs, the Department of Housing Preservation and Development, the Department of City Planning, and the Downtown Brooklyn Partnership to continue to develop the Brooklyn neighborhood area. Joining the area's longtime institutional stakeholders (BAM, the Brooklyn Museum and the Brooklyn Public Library) are new homes for Mark Morris Dance Group, Theatre for a New Audience (TFANA), UrbanGlass and BRIC Arts and the BAM's Fisher Building. \n\n=== Document 2: Carlene LeFevre ===\nCarlene LeFevre is a competitive eater from Henderson, Nevada. She and her husband, Rich LeFevre, are said to form the \"First Family of Competitive Eating\" in spite of having normal weights and ages around 60, and are both top ranked members of the International Federation of Competitive Eating. The childless couple has combined to take two of the top seven places in Nathan's Hot Dog Eating Contest in 2003, 2004, and 2005. She is nicknamed \"The Madam of Etiquette\" for her relative degree of decorum while consuming mass quantities of food quickly. Her trademark technique is called the \"Carlene Pop,\" in which she bounces up and down while eating to get the food to settle. \n\n=== Document 3: Brooklyn Grange ===\nBrooklyn Grange is a 2.5-acre organic urban rooftop farm in New York City, growing high quality vegetables and honey for local restaurants, markets, and community-supported agriculture. The farms span across two rooftops, one on a 43,000 sq. ft. building straddling Astoria and Long Island City, and the other atop the Brooklyn Navy Yard \u2013 the world\u2019s largest rooftop soil farm. Together, they produce over 40,000\u00a0lbs. of organically-grown vegetables each year. The Grange also operates New York City\u2019s largest apiary, with over thirty naturally-managed honey beehives, which yields approximately 1,500 pounds of honey annually. It was started in the spring of 2010 by transplanted Wisconsinite Ben Flanner, now President and Head Farmer, with the help of Anastasia Plakias, current Vice President, and Gwen Schantz, current Chief Operating Officer. The group took out loans, contributed their own money and found community investors to fund the project. The Brooklyn Navy Yard farm was financed in part by at $592,730 grant from the NYCDEP's Green Infrastructure Grant Program. In addition to growing and distributing local vegetables and herbs, Brooklyn Grange provides urban farming and green roof consulting and installation services to clients worldwide and partner with numerous non-profit organizations throughout New York to promote healthy and strong local communities. \n\n=== Document 4: Rich LeFevre ===\nRich LeFevre (nickname \"The Locust\") is a competitive eater from Henderson, Nevada. Rich and his wife, Carlene LeFevre, are said to form the \"First Family of Competitive Eating\" in spite of having normal weights and ages around 60, and are both top ranked members of the International Federation of Competitive Eating. The childless couple has combined to take two of the top seven places in Nathan's Hot Dog Eating Contest in 2003, 2004, and 2005. He competed at Wing Bowl XIV in Philadelphia, Pennsylvania in which he placed second behind Joey Chestnut, another IFOCE champion. \n\n=== Document 5: List of bus routes in Brooklyn ===\nThe Metropolitan Transportation Authority (MTA) operates a number of bus routes in Brooklyn, New York, United States; one minor route is privately operated under a city franchise. Many of them are the direct descendants of streetcar lines (see list of streetcar lines in Brooklyn); the ones that started out as bus routes were almost all operated by the Brooklyn Bus Corporation, a subsidiary of the Brooklyn\u2013Manhattan Transit Corporation, until the New York City Board of Transportation took over on June 5, 1940. Of the 55 local Brooklyn routes operated by the New York City Transit Authority, roughly 35 are the direct descendants of one or more streetcar lines, and most of the others were introduced in full or in part as new bus routes by the 1930s. Only the eastern section of the B82 (then the B50), the B83, and the B84 were created by New York City Transit from scratch, in 1978, 1966, and 2013, respectively. \n\n=== Document 6: Sports in New York (state) ===\nNew York has two Major League Baseball teams, the New York Yankees (based in the Bronx) and the New York Mets (based in Queens). New York is home to three National Hockey League franchises: the New York Rangers in Manhattan, the New York Islanders in Brooklyn and the Buffalo Sabres in Buffalo. New York has two National Basketball Association teams, the New York Knicks in Manhattan, and the Brooklyn Nets in Brooklyn. New York has one Major League Soccer team: New York City FC. Although the New York Red Bulls represent the New York metropolitan area they play in Red Bull Arena, located in Harrison, New Jersey. \n\n=== Document 7: Queens ===\nQueens is the easternmost and largest in area of the five boroughs of New York City. It is geographically adjacent to the borough of Brooklyn at the southwestern end of Long Island, and to Nassau County farther east on Long Island; in addition, Queens shares water borders with the boroughs of Manhattan and the Bronx. Coterminous with Queens County since 1899, the borough of Queens is the second-largest in population (after Brooklyn), with a census-estimated 2,333,054 residents in 2016, approximately 48% of them foreign-born. Queens County also is the second-most populous county in the U.S. state of New York, behind the neighboring borough of Brooklyn, which is coterminous with Kings County. Queens is the fourth-most densely populated county among New York City's boroughs, as well as in the United States. If each of New York City's boroughs were an independent city, Queens also would be the nation's fourth most populous, after Los Angeles, Chicago, and Brooklyn. Queens is the most ethnically diverse urban area in the world. \n\n=== Document 8: Brooklyn Heights ===\nBrooklyn Heights is an affluent residential neighborhood within the New York City borough of Brooklyn. Originally referred to as Brooklyn Village, it has been a prominent area of Brooklyn since 1834. The neighborhood is noted for its low-rise architecture and its many brownstone rowhouses, most of them built prior to the Civil War. It also has an abundance of notable churches and other religious institutions. Brooklyn's first art gallery, the Brooklyn Arts Gallery, was opened in Brooklyn Heights in 1958. In 1965, a large part of Brooklyn Heights was protected from unchecked development by the creation of the Brooklyn Heights Historic District, the first such district in New York City. The district was added to the National Register of Historic Places in 1966. \n\n=== Document 9: Fort Greene, Brooklyn ===\nFort Greene is a neighborhood in the New York City borough of Brooklyn. Part of Brooklyn Community Board 2 and served by the New York City Police Department's 88th Precinct, Fort Greene is listed on the New York State Registry and on the National Register of Historic Places, and is a New York City\u2013designated Historic District. It is located in northwest Brooklyn in the area known as South Brooklyn, just across from Lower Manhattan and north of Prospect Park. \n\n=== Document 10: Nathan's Hot Dog Eating Contest ===\nThe Nathan's Hot Dog Eating Contest is an annual American hot dog competitive eating competition. It is held each year on Independence Day at Nathan's Famous Corporation's original, and best-known restaurant at the corner of Surf and Stillwell Avenues in Coney Island, a neighborhood of Brooklyn, New York City. \n\n\nProvide your answer based on the information in the supporting documents.",
    "answer": "the \"First Family of Competitive Eating\""
  },
  {
    "id": 1,
    "question": "Multi-hop reasoning task:\n\nQuestion: Micha\u00ebl Llodra of France, called \"the best volleyer on tour\", defeated Juan Mart\u00edn del Potro a professional of what nationality?\n\nSupporting Documents:\n=== Document 1: Adrian Mannarino ===\nAdrian Mannarino (born 29 June 1988) is a French professional tennis player who is currently ranked world No. 31 in men's singles by the Association of Tennis Professionals (ATP). He has a career-high singles ranking of world No. 27 (July 2015) and was the singles runner-up in three ATP World Tour tournaments - Auckland, Bogot\u00e1 and Antalya. Mannarino has achieved victories over Stan Wawrinka, Milos Raonic, Juan Mart\u00edn del Potro, Gilles Simon, Juan M\u00f3naco, Philipp Kohlschreiber and Ga\u00ebl Monfils. \n\n=== Document 2: 2010 US Open \u2013 Men's Singles ===\nJuan Mart\u00edn del Potro was the defending champion, but chose not to participate this year, after undergoing a wrist operation in May and only starting to practice again in August. Del Potro was the third man in the Open Era not to defend his US Open title, after Ken Rosewall in 1971 (due to conflicts between the World Championship Tennis (WCT) and the International Lawn Tennis Federation (ILTF)), and Pete Sampras in 2003 (who unofficially retired after the 2002 final). For the first time in U.S. Open history, no American player was seeded in the top 8, this was reflected in the result. \n\n=== Document 3: Juan Mart\u00edn del Potro ===\nJuan Mart\u00edn del Potro (] , born 23 September 1988), also known as Delpo is an Argentinian professional tennis player who is currently ranked world No. 24 in men's singles by the Association of Tennis Professionals (ATP). His biggest achievement has been winning the 2009 US Open, defeating Rafael Nadal in the semifinal and 5-time defending champion Roger Federer in the final. He was the first to defeat both Federer and Nadal during the same major and was the only man outside the Big Four to win a major between the 2005 French Open and the 2013 US Open, a span of 35 tournaments. He is also the second Argentine and the fifth-youngest man to win the US Open in the Open Era. Other career highlights include winning the bronze medal in men's singles at the 2012 London Olympics and the silver medal at the 2016 Rio Olympics and being part of his country's successful Davis Cup team; but his career has also been hampered by a succession of wrist injuries. \n\n=== Document 4: 2009 US Open \u2013 Men's Singles ===\nRoger Federer was the five-time defending champion, but was defeated by Juan Mart\u00edn del Potro in the final, 3\u20136, 7\u20136, 4\u20136, 7\u20136, 6\u20132. This was del Potro's first major title. \n\n=== Document 5: Juan Mart\u00edn del Potro career statistics ===\nThis is a list of the main career statistics of Argentine professional tennis player, Juan Mart\u00edn del Potro. To date, Del Potro has won 19 ATP singles titles, including one Grand Slam singles title at the 2009 US Open. He was also the runner-up at the 2009 ATP World Tour Finals, a semi-finalist at the 2009 French Open and 2013 Wimbledon Championships, a quarterfinalist at the Australian Open in 2009 and 2012, a bronze medalist at the 2012 London Olympics, and a silver medalist at the 2016 Rio Olympics. On January 11, 2010, Del Potro achieved a career high singles ranking of world No. 4 for the first time. \n\n=== Document 6: 2017 Laver Cup ===\nOn 24 August 2016, Roger Federer and Rafael Nadal were the first of six players to confirm their participation for team Europe. On 15 May 2017, more than eight months later, Milos Raonic was the first of six players to confirm his participation for the World team. By 24 August 2017, all six players from each team had been chosen: Roger Federer, Rafael Nadal, Alexander Zverev, Marin \u010cili\u0107, Dominic Thiem, and Tom\u00e1\u0161 Berdych for team Europe, and Milos Raonic, John Isner, Jack Sock, Sam Querrey, Juan Mart\u00edn del Potro, and Denis Shapovalov for team World. Shortly afterwards Raonic withdrew and was replaced by Nick Kyrgios. Later Frances Tiafoe took the place of del Potro who had also withdrawn. \n\n=== Document 7: 2016 Davis Cup ===\nThe 2016 Davis Cup was the 105th edition of the Davis Cup, a tournament between national teams in men's tennis. It was sponsored by BNP Paribas. From this season's tournament the deciding set of each match would be settled by a tiebreak at 6 games all rather than playing an advantage set until a player or a team were two games clear. Argentina won their first Davis Cup title, after 4 runner-up finishes, defeating Croatia in the final. Federico Delbonis defeated Ivo Karlovi\u0107 in the final match to give Argentina its first Davis Cup title, after a huge comeback from Juan Mart\u00edn del Potro against Marin \u010cili\u0107 in the fourth match. \n\n=== Document 8: 2009 Heineken Open ===\nThe 2009 Heineken Open is a tennis tournament played on outdoor hard courts. It is the 34th edition of the Heineken Open, and part of the ATP World Tour 250 series of the 2009 ATP Tour. It took place at the ASB Tennis Centre in Auckland, New Zealand, from 12 January through 17 January 2008. First-seeded Juan Mart\u00edn del Potro won the singles title. \n\n=== Document 9: Micha\u00ebl Llodra ===\nMicha\u00ebl Llodra (] ; born 18 May 1980) is a French former professional tennis player. He is a successful doubles player with three Grand Slam championships and an Olympic silver medal, and has also had success in singles, winning five career titles and gaining victories over Novak Djokovic, Juan Mart\u00edn del Potro, Tom\u00e1\u0161 Berdych, Robin S\u00f6derling, Jo-Wilfried Tsonga, Nikolay Davydenko, Janko Tipsarevi\u0107 and John Isner. Llodra has been called \"the best volleyer on tour\". \n\n=== Document 10: 2013 ABN AMRO World Tennis Tournament ===\nThe 2013 ABN AMRO World Tennis Tournament (or Rotterdam Open) was a men's tennis tournament played on indoor hard courts. It took place at the Ahoy Rotterdam arena in the Dutch city of Rotterdam, between 11 and 17 February 2013. It was the 41st edition of the Rotterdam Open, whose official name is the ABN AMRO World Tennis Tournament. The competition was part of the ATP World Tour 500 series of the 2013 ATP World Tour. Second-seeded Juan Mart\u00edn del Potro won the singles title. \n\n\nProvide your answer based on the information in the supporting documents.",
    "answer": "Argentinian"
  },
  {
    "id": 2,
    "question": "Multi-hop reasoning task:\n\nQuestion: What company produced the 1978 movie based on a book written by a radio playwright and children's book author born in 1900?\n\nSupporting Documents:\n=== Document 1: An Na ===\nAn Na (born 1972) is a South Korea-born American children's book author. Starting her career as a middle school English and History teacher, Na turned to writing novels after taking a young adult literature class while enrolled in an M.F.A. program at Vermont College of Fine Arts. She gained success with her very first novel \"A Step From Heaven\", published by Front Street Press in 2001, which won the annual Michael L. Printz Award from the American Library Association recognizing the year's \"best book written for teens, based entirely on its literary merit\". It was also a finalist for the National Book Award, Young People's Literature, and later found its way onto numerous \"best book\" lists. Na still makes frequent visits to middle schools to talk about her works and encourages young Asian-American students to become artists and harness their creativity. She cites Frank McCourt's \"Angela's Ashes\" and Sandra Cisneros's \"The House on Mango Street\" among the influences on her writing and also admires the work of Madeleine L'Engle and of her first writing teacher, Jacqueline Woodson. She divides her time between Oakland, California and Warren, Vermont. \n\n=== Document 2: Charles Tazewell ===\nCharles Tazewell (June 2, 1900 \u2013 June 26, 1972) was a radio playwright and children's book author, whose work has been adapted multiple times for film. \n\n=== Document 3: Hank Zipzer's Christmas Catastrophe ===\nHank Zipzer's Christmas Catastrophe is a 2016 stand alone British Christmas movie based on the Hank Zipzer series of books by Henry Winkler and Lin Oliver and the TV series airing on CBBC. The film will be airing on CBBC on 12 December 2016. It is written by Joe Williams and is directed by Matt Bloom. The film is produced by Kindle Entertainment in association with Walker Productions and DHX Media with support from Screen Yorkshire\u2019s Yorkshire Content Fund. It is the fourth movie based on a CBBC programme after \"\", \"Shaun the Sheep Movie\" and \"\". It is the second movie based on a CBBC show, which has not been released in cinemas and only shown on TV after \"\" \n\n=== Document 4: Kraft Suspense Theatre ===\nThe Kraft Suspense Theatre is an American television anthology series that was produced and broadcast from 1963 to 1965 on NBC. Sponsored by Kraft Foods, it was seen three weeks out of every four and was pre-empted for Perry Como's \"Kraft Music Hall\" specials once monthly. Como's production company, Roncom Films, also produced \"Kraft Suspense Theatre.\" (The company name, \"Roncom Films\" stood for \"RONnie COMo,\" Perry's son, who was in his early twenties when this series premiered). Writer, editor, critic and radio playwright Anthony Boucher served as consultant on the series. \n\n=== Document 5: The Small One ===\nThe Small One is a 1978 American animated featurette produced by Walt Disney Productions and released theatrically by Buena Vista Distribution on December 16, 1978 with a Christmas 1978 re-issue of \"Pinocchio\". The story is based on a children's book of the same name by Charles Tazewell and was an experiment for the new generation of Disney animators including Don Bluth, Richard Rich, Henry Selick, Gary Goldman and John Pomeroy. \n\n=== Document 6: The Face on the Milk Carton (film) ===\nThe Face on the Milk Carton is a 1995 made for television movie based on the book written by Caroline B. Cooney. The movie stars Kellie Martin as Jennifer Sands/Janie Jessmon, a 16-year-old girl who finds her face on the back of a milk carton and puts the pieces of her past together. \n\n=== Document 7: Pichilemu Blues ===\nPichilemu Blues is a 1993 book written by Chilean politician Esteban Valenzuela. A movie based on the book was also released, starring Peggy Cordero, Ximena Nogueira and Evaristo Acevedo. \n\n=== Document 8: G\u00e1bor N\u00f3gr\u00e1di ===\nG\u00e1bor N\u00f3gr\u00e1di (born June 22, 1947, Ny\u00edregyh\u00e1za) is a Hungarian book author, screenwriter, playwright, essayist, publicist and poet who is best known for his children's novels such as the \"Pigeon granny\" and \"The story of\" \"Pie (\"original title PetePite\")\", a book which won the 2002 Children's Book of the Year award, was on the IBBY Honor List (International Board for Young People) and was ranked among the 100 most popular books in Hungary in the 2005 'Big Book' competition. \n\n=== Document 9: Randy Romero ===\nRandy Paul Romero (born December 22, 1957 in Erath, Louisiana) is a Hall of Fame jockey in the sport of Thoroughbred horse racing, Born into a family involved with horses, his father Lloyd J. Romero was a Louisiana state trooper who trained American Quarter Horses and later, after a drunk driver crashed into his police car and permanently disabled him, he began training Thoroughbreds for flat racing. The 1978 movie \"Casey's Shadow\" is based on Lloyd Romero and his family. He was elected into the Thoroughbred Racing Hall of Fame May 27, 2010. \n\n=== Document 10: Wilbooks ===\nWilbooks is a children\u2019s book educational publishing company based in West Chester, Pennsylvania. The company was founded by children\u2019s book author Bruce Larkin in 1996. The company publishes fiction, non-fiction, humor, and poetry books geared towards children from Pre-kindergarten to third grade. Wilbooks publishes leveled, educational books with a focus on teaching children how to read. In 2009 Wilbooks (through Bruce Larkin) donated over 500,000 books to schools, teachers, and literacy organizations throughout the United States. \n\n\nProvide your answer based on the information in the supporting documents.",
    "answer": "Walt Disney Productions"
  }
]

            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 4
        - Current explore/exploit balance: 80/10
        - Best accuracy achieved: 0.80 (iteration 0)

        APPROACH HISTORY (last 4 iterations):
        [
  {
    "iteration": 0,
    "strategy": "baseline",
    "accuracy": 0.8,
    "approach": "Simple baseline script: Direct LLM call without sophisticated techniques"
  },
  {
    "iteration": 1,
    "strategy": "explore",
    "accuracy": 0.0,
    "approach": "The script addresses multi-hop reasoning by first summarizing each document and then reasoning across the summaries to answer a question. It employs a verification step to ensure the summaries retain relevant information, retrying summarization if the verification fails. The `main` function orchestrates the process, calling `summarize_document_with_verification` for each document and then `reason_across_summaries` to generate the final answer. `call_llm` is the core function that communicates with the Gemini LLM, while `summarize_document_with_verification` summarizes individual documents and verifies the summaries, and `reason_across_summaries` answers the final question based on the generated summaries."
  },
  {
    "iteration": 2,
    "strategy": "explore",
    "accuracy": 0.0,
    "approach": "The script implements a knowledge graph approach to answer questions using supporting documents by leveraging the Gemini LLM. It decomposes the problem into knowledge extraction and reasoning, assigning the LLM the roles of knowledge extraction expert and question answering system. The `extract_knowledge` function extracts entities and relationships, validates the format of the extracted knowledge, and returns the knowledge in JSON format, calling `call_llm` to generate and validate the knowledge, while `reason_over_knowledge` generates the final answer based on the extracted knowledge, using `call_llm` to generate the answer. The `main` function orchestrates the process by calling the other functions and iterating through the supporting documents to generate an answer."
  },
  {
    "iteration": 3,
    "strategy": "explore",
    "accuracy": 0.0,
    "approach": "The script uses a focused retrieval and reasoning approach with chain of thought to answer questions based on provided documents. It decomposes the problem into three steps: identifying relevant documents, extracting relevant snippets from those documents, and then answering the question based on the snippets. There are three agent roles that are each specialized in their respective steps.\n\nThe functions used are `identify_relevant_documents`, `extract_relevant_snippets`, `answer_question`, `call_llm`, and `main`. The `main` function orchestrates the overall workflow by first calling `identify_relevant_documents` to select the most pertinent documents, then `extract_relevant_snippets` to pull out key information, and finally uses `answer_question` to formulate a response based on the extracted snippets. Each of these functions makes calls to `call_llm` to interface with the LLM using prompts and system instructions."
  }
]

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 3 iterations):
        [
  {
    "iteration": 0,
    "issue": "The primary issue is the system's **inability to synthesize extracted information to provide precise and concise answers** that directly address the core question, even when relevant information is present in the documents."
  },
  {
    "iteration": 1,
    "issue": "The most critical problem is the `test_script_1.py` file calling the `main` function incorrectly. It is calling `answer = module.main(question)` when it should be calling `answer = module.main(question, supporting_documents)`."
  },
  {
    "iteration": 2,
    "issue": "The single most critical problem is the incorrect invocation of the `main()` function within the `test_script_2.py` script.  The `supporting_documents` argument is not being passed, leading to a `TypeError`."
  },
  {
    "iteration": 3,
    "issue": "The most critical problem is the incorrect function call in `test_script_3.py`. The `main` function requires both `question` and `supporting_documents` as arguments, but the script is only passing the `question`."
  }
]

        TARGETED IMPROVEMENTS:
        []
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity ✓
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity ✓
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity ✓
    4. All operations have O(1) average time complexity ✓
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```\n\n```python\ndef solve_with_meta_programming(question):
            """
            Advanced: Script generates and executes its own code/prompts dynamically.
            The script acts as its own programmer and prompt engineer.
            """

            # Step 1: Analyze what approach is needed
            strategy_prompt = f"""
            For this problem: {question}

            What's the best approach?
            A) Generate Python code to calculate/process something
            B) Generate specialized LLM prompts for analysis  
            C) Use a hybrid approach with both code and LLM calls

            Explain your choice and what specific code or prompts I should generate.
            """


                analysis_system_prompt = """ 
                You are a problem analysis expert. You are a master of problem analysis and can 
                determine the best approach to solve a problem, understanding the strenghts and 
                weaknesses of LLMs for problem solving, when to delegate a more specific or problem 
                or subproblem to an additional LLM call, and when to write code to solve a problem.
            """
            strategy = call_llm(strategy_prompt, analysis_system_prompt)

            # Step 2: Generate and execute based on strategy
            if "###CODE_ONLY###" in strategy.lower():
                # Generate code dynamically
                code_gen_prompt = f"""
                Problem: {question}
                Strategy: {strategy}

                Write Python code to solve this problem. Include print statements for output.
                Return ONLY the Python code:
                """

                generated_code = call_llm(code_gen_prompt, "You are a Python programmer.")

                # Clean up code if wrapped in markdown
                import re
                code_match = re.search(r'```python\s*\n(.*?)\n```', generated_code, re.DOTALL)
                if code_match:
                    clean_code = code_match.group(1).strip()
                else:
                    clean_code = generated_code.strip()

                # Execute the generated code
                execution_result = execute_code(clean_code)

                # Interpret the execution result
                interpretation_prompt = f"""
                Original problem: {question}
                Generated code: {clean_code}
                Execution result: {execution_result}

                What is the final answer based on these results?
                """

                final_answer = call_llm(interpretation_prompt, "You are a solution interpreter.")
                return final_answer

            elif "###PROMPT_ONLY###" in strategy.lower():
                # Generate specialized prompts dynamically
                prompt_design = f"""
                For this problem: {question}
                Strategy: {strategy}

                Design the most effective prompt to solve this problem:
                """

                specialized_prompt = call_llm(prompt_design, "You are a prompt engineer.")

                # Use the generated prompt
                solution = call_llm(specialized_prompt, "You are an expert problem solver.")
                return solution

            else:  # Hybrid approach
                # Chain code and LLM calls dynamically
                current_result = question

                for step in range(3):
                    # Decide what to do at this step
                    step_decision = call_llm(f"""
                    Step {step + 1} of hybrid approach.
                    Current state: {current_result}

                    What should I do next?
                    - Generate and execute code
                    - Make an LLM analysis call
                    - Provide final answer

                    Choose one and explain exactly what to do.
                    """, "You are a workflow coordinator.")

                    if "final answer" in step_decision.lower():
                        return current_result
                    elif "code" in step_decision.lower():
                        # Generate code for this step
                        step_code_prompt = f"""
                        Based on this decision: {step_decision}
                        Current data: {current_result}

                        Write Python code to process this. Return only the code:
                        """
                        step_code = call_llm(step_code_prompt, "You are a Python programmer.")
                        code_result = execute_code(step_code)
                        current_result = f"Previous: {current_result}\nCode result: {code_result}"
                    else:
                        # Make LLM call for this step  
                        step_analysis = call_llm(f"Analyze this data: {current_result}\nBased on: {step_decision}", "You are an analyst.")
                        current_result = f"Previous: {current_result}\nAnalysis: {step_analysis}"

                return current_result\n```\n\n```python\ndef self_modifying_solver(problem):
            """
            A solver that rewrites its own approach based on intermediate results.
            Advanced meta-programming where the script evolves its strategy.
            """

            strategy = "direct_analysis"
            attempts = 0
            max_attempts = 3

            while attempts < max_attempts:
                attempts += 1

                if strategy == "direct_analysis":
                    # Try direct LLM analysis
                    result = call_llm(f"Solve this problem: {problem}", "You are an expert problem solver.")

                    # Evaluate if this worked
                    evaluation_prompt = f"""
                    Problem: {problem}
                    My attempt: {result}

                    Did this solve the problem correctly? If not, what approach should I try next?
                    Options: computational_approach, step_by_step_breakdown, code_generation
                    """

                    evaluation = call_llm(evaluation_prompt, "You are a solution evaluator.")

                    if "correct" in evaluation.lower() or "solved" in evaluation.lower():
                        return result
                    elif "computational" in evaluation.lower():
                        strategy = "computational_approach"
                    elif "step_by_step" in evaluation.lower():
                        strategy = "step_by_step_breakdown"  
                    else:
                        strategy = "code_generation"

                elif strategy == "computational_approach":
                    # Generate and execute computational code
                    comp_prompt = f"""
                    Problem: {problem}

                    Write Python code to solve this computationally. Include:
                    - Extract relevant numbers or data
                    - Perform calculations
                    - Print results clearly

                    Return only the Python code:
                    """

                    comp_code = call_llm(comp_prompt, "You are a computational programmer.")
                    comp_result = execute_code(comp_code)

                    # Interpret computational result
                    interpretation = call_llm(f"Problem: {problem}\nComputation result: {comp_result}\nFinal answer:", "You are an interpreter.")
                    return interpretation

                elif strategy == "step_by_step_breakdown":
                    # Generate step-by-step solution code
                    breakdown_prompt = f"""
                    Problem: {problem}

                    Write Python code that breaks this problem into steps and solves it methodically:
                    """

                    breakdown_code = call_llm(breakdown_prompt, "You are a systematic programmer.")
                    breakdown_result = execute_code(breakdown_code)

                    # Build final solution based on breakdown
                    final_solution = call_llm(f"Problem: {problem}\nStep-by-step result: {breakdown_result}\nFinal answer:", "You are a problem solver.")
                    return final_solution

                else:  # code_generation strategy
                    # Generate completely custom code for this problem
                    custom_prompt = f"""
                    Problem: {problem}

                    Write custom Python code specifically designed to solve this exact problem type:
                    """

                    custom_code = call_llm(custom_prompt, "You are a custom code generator.")
                    custom_result = execute_code(custom_code)

                    return f"Custom solution result: {custom_result}"

            return "Could not solve after multiple strategy attempts"\n```\n\n```python\ndef adaptive_chain_solver(question):
            """
            Chains multiple code generations and LLM calls adaptively.
            Each step decides what the next step should be.
            """

            current_data = question
            step_count = 0
            max_steps = 5

            while step_count < max_steps:
                step_count += 1

                # Decide what to do at this step
                decision_prompt = f"""
                Step {step_count}: Working with: {current_data}

                What should I do next to solve this problem?
                A) Generate and execute Python code to process/calculate something
                B) Generate a specialized LLM prompt for analysis
                C) I have enough information - provide final answer

                Choose A, B, or C and explain exactly what to do:
                """

                decision = call_llm(decision_prompt, "You are an adaptive workflow coordinator.")

                if "C)" in decision or "final answer" in decision.lower():
                    # Generate final answer
                    final_prompt = f"""
                    Original question: {question}
                    Current data/results: {current_data}

                    Based on all the processing done, what is the final answer?
                    """
                    return call_llm(final_prompt, "You are a solution synthesizer.")

                elif "A)" in decision or "code" in decision.lower():
                    # Generate and execute code
                    code_prompt = f"""
                    Current data: {current_data}
                    Decision: {decision}

                    Write Python code to process this data as suggested. Return only the code:
                    """

                    code = call_llm(code_prompt, "You are a Python programmer.")

                    # Execute and update current data
                    code_result = execute_code(code)
                    current_data = f"Step {step_count} result: {code_result}"

                else:  # Generate specialized LLM prompt
                    # Create specialized prompt
                    prompt_design = f"""
                    Current data: {current_data}
                    Decision: {decision}

                    Design a specialized prompt for this analysis:
                    """

                    specialized_prompt = call_llm(prompt_design, "You are a prompt engineer.")

                    # Use the specialized prompt
                    analysis_result = call_llm(specialized_prompt, "You are a specialized analyst.")
                    current_data = f"Step {step_count} analysis: {analysis_result}"

            return f"Final result after {max_steps} steps: {current_data}"\n```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        

            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        Okay, here's the updated and synthesized version of our learnings, focused on the specifics of this multi-hop reasoning dataset and task, organized into the requested sections:

**1. DATASET PATTERNS & CHARACTERISTICS**

*   **Explicit Multi-Hop Reasoning:** Questions are explicitly framed as multi-hop reasoning tasks, demanding information linkage across multiple documents. Single document retrieval is insufficient.
    *   *Example:* Questions require linking information about a person's career, location, and related events found in separate documents.
*   **Diverse Subject Matter:** Questions span various domains including history, geography, entertainment, and current events.
    *   *Example:* Questions range from historical figures to film trivia to geographical locations of organizations.
*   **Concise Factual Answers:** Answers are generally concise named entities, dates, or short phrases, directly responding to the question.
    *   *Example:* "1984", "Los Angeles", "Emilio Estevez".
*   **Answer Source Constraint:** Answers are *explicitly* within the provided supporting documents. The model should *not* require external knowledge. The core challenge is information retrieval and synthesis from provided texts.
*   **Information Overload:** "Supporting Documents" contain significant irrelevant information (noise). Effective models must filter and focus on relevant passages.
*   **Ambiguity:** Terms and entities can be ambiguous. Context is crucial for disambiguation.
*   **Synonymy and Paraphrasing:** Concepts are expressed differently in questions and supporting documents, requiring understanding of synonyms and paraphrases.
*   **Reasoning Depth Variation:** The number of inference "hops" to answer questions varies.
*   **Edge Cases Exist:**
    *   **Missing Information:** Documents *may not* contain the complete answer, even if relevant.
    *   **Contradictory Information:** Documents might contain conflicting information, requiring a resolution strategy.
    *   **Coreference Resolution:** Pronoun references must be resolved (e.g., "He" refers to whom?).
*   **Complex Multi-Hop Reasoning (Reinforced):** The dataset heavily relies on complex multi-hop reasoning. Answering a question often requires connecting information from multiple documents, sometimes in subtle ways.
    *   *Example:* Connecting "Emilio Estevez starred in Nightmares" with another document mentioning a film released in the same year.
*   **Information Synthesis Required (Reinforced):** Correct answers require synthesizing information rather than directly quoting a single document.
    *   *Example:* Combining facts, dates, names, and contexts to produce a derived answer.
*   **Real-World Knowledge Assumptions (Identified):** The questions often implicitly assume some basic real-world knowledge not explicitly in the documents.
    *   *Example:* Needing common-sense reasoning to understand the question's context even with supporting documents.
*   **Contextual Clues in Document Titles:** The document titles often provide crucial context for understanding the content of the document and its relevance to the question (e.g., "Oasis discography", "St. John's College, Belize").
*   **Varied Document Content:** The supporting documents encompass a wide range of formats, including discographies, lists of band members, historical context, and descriptions of albums and tours. This variety requires flexible information extraction.
*   **Entity Relationships Critical:** Questions often hinge on identifying relationships between entities mentioned across different documents.
    *   *Example:* Determining if a person played a specific instrument on a specific song.
*   **Passage Complexity Varies:** Supporting documents (passages of text) exhibit varying lengths and complexity. This requires robust handling of both concise and verbose information sources.
*   **Complex Question Structure (Reinforced):** Questions are complex, often embedding multiple entities and relationships. They often require identifying named entities and linking them across documents based on contextual clues.
*   **Contextual Document Relevance (Reinforced):** Documents are not always directly relevant to the question. Some documents provide background information or tangential details that could be confusing to the retrieval process.

**2. EFFECTIVE TASK-SPECIFIC STRATEGIES**

*   *(Initial State: No strategies have been proven effective yet as the baseline.)*
*   *(Update: No strategies have been proven effective. Experiment 1 resulted in accuracy 0.00 due to a critical error in the test script.)*
*   *(Update: No strategies have been proven effective. Experiment 2 resulted in accuracy 0.00 due to a critical error in the test script.)*
*   *(Update: No strategies have been proven effective. Experiment 3 resulted in accuracy 0.00 due to a critical error in the test script.)*

**3. COMMON FAILURE MODES ON THIS DATASET**

*   **Information Overload:** The LLM struggles to sift through the volume of information provided in the supporting documents.
*   **Inability to Connect Disparate Facts:** The system fails when the answer requires linking information that isn't explicitly connected in the documents.
    *   *Example:* Failing to connect Emilio Estevez and Nightmares with another document mentioning a different film released in the same year.
*   **Poor Summarization and Extraction:** The system struggles to extract the specific requested detail, returning more general information.
    *   *Example:* In the "Eric S. Pistorius" example, failing to extract the *concept* of his work, instead returning a more general description of his specializations as an attorney.
*   **Lack of Temporal Reasoning:** Weakness in temporal reasoning; the system can't easily determine which events occurred in the same year.
    *   *Example:* Failure involving Emilio Estevez demonstrates a weakness in temporal reasoning; the system can't easily determine which events occurred in the same year without more sophisticated processing.
*   **Basic Information Extraction is Not Enough:** Simply extracting facts from documents is insufficient. The system must be able to reason *with* those facts.
*   **Incorrect Function Call (CRITICAL - Repeated):** Critical failure due to incorrect function call in the test script, preventing the LLM from accessing the supporting documents. This has occurred in Experiment 1, Experiment 2, *and Experiment 3*.
*   **Script Integration Failure:** Failure to correctly integrate supporting documents into the script's `main()` function prevents the LLM from even parsing or processing the document set. The `TypeError` indicates a fundamental flaw in how the script receives input data.
*   **Reliance on Imperfect Retrieval (Confirmed):** Because supporting documents are not passed to the function, retrieval becomes random and ineffective. The questions require information from the supporting documents, thus failure is guaranteed.

**4. EXPERIMENT LOG & FINDINGS**

*   **Experiment 0: Baseline LLM Call**
    *   *Description:* Direct call to the LLM with the question and supporting documents.
    *   *Accuracy:* 80%
    *   *Findings:* Baseline performance indicates that the task complexity exceeds the capabilities of a simple LLM call without additional reasoning or information retrieval techniques. Highlights the need for a more structured approach to reasoning, potentially involving intermediate steps to identify relevant entities, relationships, and temporal information.
    *   *Failure Mode Examples:*
        *   Inability to connect disparate facts across documents.
        *   Poor summarization and extraction of specific details.
        *   Lack of temporal reasoning.
*   **Experiment 1: Summarization and Reasoning Pipeline**
    *   *Description:* Attempted to use `summarize_document_with_verification` to summarize supporting documents, then `reason_across_summaries` to answer the question.
    *   *Accuracy:* 0.00
    *   *Findings:* The experiment failed due to a critical error in the test script (`test_script_1.py`). The `main` function was called with only the question, omitting the `supporting_documents` argument. This prevented the model from accessing the necessary information to answer the questions, leading to zero accuracy. This highlights the importance of rigorous testing and validation to ensure the correct flow of information.
    *   *Error Logs:*
        ```
        === SCRIPT ERROR ENCOUNTERED [2025-06-01 07:02:35] ===
        Error detected during script repair (attempt 1): main() missing 1 required positional argument: 'supporting_documents'
        Traceback (most recent call last):
          File "/home/runner/workspace/scripts/test_script_1.py", line 261, in <module>
            answer = module.main(question)
                     ^^^^^^^^^^^^^^^^^^^^^
        TypeError: main() missing 1 required positional argument: 'supporting_documents'
        === END SCRIPT ERROR ===
        ```
        *(Similar error logs for attempt 2 and 3)*
    *   *Root Cause Identified:* Incorrect function call `module.main(question)` instead of `module.main(question, supporting_documents)`.
*   **Experiment 2: Knowledge Graph Extraction and Reasoning**
    *   *Description:* Attempted to use a knowledge graph extraction approach followed by reasoning over the graph to answer the question.
    *   *Accuracy:* 0.00
    *   *Findings:* The experiment failed due to a critical error in the test script (`test_script_2.py`). The `main` function was called with only the question, omitting the `supporting_documents` argument. This prevented the model from accessing the necessary information to answer the questions, leading to zero accuracy. The architectural design itself (knowledge extraction followed by reasoning) remains a potentially promising direction, but is untested.
    *   *Error Logs:*
        ```
        === SCRIPT ERROR ENCOUNTERED [2025-06-01 07:03:31] ===
        Error detected during script repair (attempt 1): main() missing 1 required positional argument: 'supporting_documents'
        Traceback (most recent call last):
          File "/home/runner/workspace/scripts/test_script_2.py", line 261, in <module>
            answer = module.main(question)
                     ^^^^^^^^^^^^^^^^^^^^^
        TypeError: main() missing 1 required positional argument: 'supporting_documents'
        === END SCRIPT ERROR ===
        ```
        *(Similar error logs for attempt 2 and 3)*
    *   *Root Cause Identified:* Incorrect function call `module.main(question)` instead of `module.main(question, supporting_documents)`. This is a *repeat* of the error from Experiment 1.
*   **Experiment 3: [Experiment Title from Iteration 3]**
    *   *Description:* [Description of experiment from Iteration 3 - FILL THIS IN]
    *   *Accuracy:* 0.00
    *   *Findings:* The experiment failed due to a critical error in the test script (`test_script_3.py`). The `main` function was called with only the question, omitting the `supporting_documents` argument. This prevented the model from accessing the necessary information to answer the questions, leading to zero accuracy. The hypothesis that the script could answer multi-hop questions based on document retrieval and reasoning is rejected. This is solely due to an error preventing the supporting documents from being passed to the script, rather than the script's underlying logic.
    *   *Error Logs:*
        ```
        === SCRIPT ERROR ENCOUNTERED [2025-06-01 07:04:45] ===
        Error detected during script repair (attempt 1): main() missing 1 required positional argument: 'supporting_documents'
        Traceback (most recent call last):
          File "/home/runner/workspace/scripts/test_script_3.py", line 261, in <module>
            answer = module.main(question)
                     ^^^^^^^^^^^^^^^^^^^^^
        TypeError: main() missing 1 required positional argument: 'supporting_documents'
        === END SCRIPT ERROR ===
        ```
        *(Similar error logs for attempt 2 and 3)*
    *   *Root Cause Identified:* Incorrect function call `module.main(question)` instead of `module.main(question, supporting_documents)`. This is a *repeat* of the errors from Experiments 1 and 2. The importance of meticulous attention to detail in script implementation is confirmed. A simple error in function calls can render an entire system useless.

**5. NEXT RESEARCH DIRECTIONS**

*   **Correct the Function Call (CRITICAL - RED ALERT - URGENT):** Immediately fix the function call in `test_script_1.py`, `test_script_2.py`, *and* `test_script_3.py` to `answer = module.main(question, supporting_documents)`. This is the most critical step to enable proper testing and move forward. The *same* error has blocked progress in *three* consecutive experiments. This is an unacceptable level of oversight.
*   **Establish Rigorous Testing Protocol (MANDATORY):** Implement a *mandatory* and more rigorous testing protocol *before* launching experiments. This *must* include unit tests to verify that function calls are correct and that all required arguments are passed. This protocol should be documented and followed meticulously. No exceptions.
*   **Verify Input Handling (CRITICAL):** Once the function call is corrected, add input validation within the `main` function to ensure that the `supporting_documents` are received and properly formatted before proceeding with summarization and reasoning. This validation should include checks for:
    *   Presence of the `supporting_documents` argument.
    *   Correct data type of `supporting_documents` (e.g., list of strings).
    *   Non-empty `supporting_documents` list.
*   **Implement Comprehensive Logging (NEW):** Implement comprehensive logging within the `main` function to track the values of all input arguments. This will help diagnose future errors related to incorrect input data.
*   **Evaluate Retrieval Effectiveness:** After correcting the input, evaluate the performance of the document retrieval component (where applicable in the script). Analyze cases where the correct documents were not retrieved, and explore strategies to improve retrieval accuracy (e.g., using more sophisticated retrieval algorithms, fine-tuning the LLM for document relevance scoring). This becomes meaningful *only* after the function call is fixed.
*   **Evaluate Summarization Quality:** After correcting the input in Experiment 1, analyze the summaries produced by `summarize_document_with_verification`. Determine if the verification process effectively retains relevant information from the original documents. If the summaries are consistently poor, refine the summarization prompts and verification criteria. Consider metrics to quantify the information retained in the summaries.
*   **Analyze Knowledge Graph Quality:** After correcting the input in Experiment 2, analyze the extracted knowledge graph. Assess the quality of the nodes (entities) and edges (relationships) extracted from the supporting documents. Refine the knowledge extraction prompts to improve the accuracy and completeness of the graph.
*   **Analyze Reasoning Chain:** Inspect the reasoning steps performed by `reason_across_summaries` (Experiment 1) or the reasoning performed on the Knowledge Graph (Experiment 2). Identify any logical gaps or incorrect inferences made by the model. Experiment with different prompting strategies and reasoning frameworks to improve the accuracy of the final answer. Focus on the ability of the reasoning chain to synthesize information from multiple summaries/graph nodes.
*   **Implement Document Ranking/Filtering:** Implement a mechanism to rank or filter documents based on their relevance to the question *before* feeding them to the LLM.
    *   *Potential Techniques:* Keyword matching, semantic similarity, named entity recognition.
*   **Introduce a Chain-of-Thought Prompting:** Structure the LLM prompt to encourage chain-of-thought reasoning.
    *   *Example Prompt Structure:* Ask the LLM to first identify relevant entities, then identify relevant relationships between those entities, and finally answer the question based on those relationships.
*   **Fine-tune LLM (if feasible):** If resources allow, consider fine-tuning the LLM on a subset of the dataset to improve its ability to perform multi-hop reasoning and information synthesis. This would require a carefully constructed training set with questions and corresponding "reasoning paths".
*   **Incorporate External Knowledge (Cautiously):** Consider incorporating external knowledge sources (e.g., a knowledge graph or a database of facts) to augment the information provided in the documents. However, be careful to avoid introducing irrelevant or contradictory information.
*   **Implement a Temporal Reasoning Module:** Specifically address the temporal reasoning challenges by incorporating a module that can reason about dates, time intervals, and the order of events. This module could be a rule-based system or a machine learning model trained on temporal reasoning tasks.
*   **Re-evaluate Baseline (After Fix):** Once the function call error is resolved, *immediately* re-run the baseline LLM call (Experiment 0) with the corrected script. This will provide a more accurate baseline for comparison.
*   **Root Cause Analysis (for Repeated Errors):** Conduct a thorough root cause analysis to determine *why* the same function call error occurred in three consecutive experiments. Identify the flaws in our development and testing processes that allowed this to happen. Implement corrective actions to prevent similar errors in the future. The priority is to discover why unit tests were not in place to catch these errors.
        

            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        

            BEST SCRIPT TO REFINE:
            Iteration: 0
            Accuracy: 0.80
            Approach Summary: Simple baseline script: Direct LLM call without sophisticated techniques

            CURRENT BEST SCRIPT CODE:
            ```python
            import os
from google import genai
from google.genai import types

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response"""
    try:
        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question):
    """
    Baseline script: Simple direct LLM call without sophisticated techniques.
    This establishes the baseline performance capability for this dataset.
    """
    system_instruction = "You are a helpful assistant. Answer the question directly and concisely based on the information provided."

    # Simple, direct call to LLM
    answer = call_llm(question, system_instruction)

    return answer
    
            ```

            SPECIFIC SUCCESS CASES (what the script does well):
            [
  {
    "question": "Multi-hop reasoning task:\n\nQuestion: What animated movie, starring Danny Devito, featured music written and produced by Kool Kojak?\n\nSupporting Documents:\n=== Document 1: Best Friend's Brother ===\n\"Best Friend's Brother\" is a song performed by American pop recording artist Victoria Justice, billed as Cast of \"Victorious\" featuring Victoria Justice. It was produced by Kool Kojak, who also co-wrote the song with Savan Kotecha and Victoria Justice, for \"\" (2011), the soundtrack to the Nickelodeon television series, \"Victorious\". It was released as the album's third single on May 20, 2011 through Columbia Records in association with Nickelodeon. Musically, the song runs through an electropop oriented dance beat with teen pop lyrics, and the lyrics speak of a girl's crush on her best friend's brother. \n\n=== Document 2: Curmudgeons (film) ===\nCurmudgeons is a 2016 American comedy short film directed, produced by, and starring Danny DeVito. It is written and co-produced by Joshua Conkel. \n\n=== Document 3: Blow (Kesha song) ===\n\"Blow\" is a song by American singer and songwriter Kesha from her first extended play (EP), \"Cannibal\" (2010). The song was released on February 8, 2011. It was written by Kesha, along with Klas \u00c5hlund, Lukasz Gottwald, Allan Grigg, Benjamin Levin and Max Martin, with production done by Dr. Luke, Max Martin, Benny Blanco and Kool Kojak. According to Kesha the song's lyrics are representative of herself and her fans. \"Blow\" is dominantly an electropop and dance-pop song and is described as a party anthem as it portrays a simple message of having a desire to have a good time at a club. \n\n=== Document 4: Brian R. Etting ===\nBrian R. Etting is an American producer, director, and screenwriter known for producing \"Broken\", \"Funny or Die\", \"A Good Old Fashioned Orgy\", and Relative Strangers starring Danny DeVito. He also executive produced \"Drunk History: Douglass & Lincoln\" which won Best Short Film at the 2010 Sundance Film Festival. Etting also owns his own production company with Josh Etting called Garlin Pictures. \n\n=== Document 5: Va Va Voom ===\n\"Va Va Voom\" is a song by Trinidadian recording artist Nicki Minaj from the deluxe version of her second studio album, \"\". It was released on September 12, 2012 by Young Money, Cash Money, and Universal Republic as the fifth single from the album. The song was written by Minaj, Lukasz Gottwald, Allan Grigg, Max Martin, and Henry Walter, and it was produced by Dr. Luke, Kool Kojak, and Cirkut. Being released as the fifth single, it was sent to UK radio stations on September 15, 2012 and later sent to Top 40 mainstream radio on October 23, 2012. It was planned to serve as the lead single, but its release was postponed at the last minute in favor of \"Starships\"; it was later released as a promotion for the album's reissue \"\". \n\n=== Document 6: Kool Kojak ===\nAllan P. Grigg, better known by his stage name Kool Kojak and stylized as \"KoOoLkOjAk\", is an American musician, songwriter, record producer, film director, and artist notable for co-writing and co-producing Flo Rida's #1 Billboard hit single \"Right Round\", Nicki Minaj's hit single \"Va Va Voom\" , and Ke$ha's top 10 single \"Blow\". Kool Kojak has written and produced for artists such as Sean Paul, Yelle, Waka Flocka Flame, Travis Barker, Dr. Seuss's The Lorax, Britney Spears, Jesse and Joy, Andy Milonakis, Icona Pop, N.A.S.A., Dirt Nasty, Lordz of Brooklyn, Ursula 1000, and Warren G. Kool Kojak was a featured producer on the Simon Cowell TV program X Factor and has appeared as himself on the Nickelodeon show \"Victorious\". He has won two ASCAP Pop Awards and one ASCAP Urban Award, a WormTown Sound Award, and has been awarded the Key to the City of Worcester, Massachusetts. \n\n=== Document 7: The Lorax (film) ===\nThe Lorax (also known as Dr. Seuss' The Lorax) is a 2012 American 3D computer-animated musical fantasy\u2013comedy film produced by Illumination Entertainment and based on Dr. Seuss's children's book of the same name. The film was released by Universal Pictures on March 2, 2012, on the 108th birthday of Dr. Seuss. The second film adaptation of the book (following the 1972 animated television special), the film builds on the book by expanding the story of Ted, the previously unnamed boy who visits the Once-ler. The cast includes Danny DeVito as the Lorax, Ed Helms as the Once-ler, and Zac Efron as Ted. New characters introduced in the film are Audrey (voiced by Taylor Swift), Aloysius O'Hare (Rob Riggle), Mrs. Wiggins, Ted's mother (Jenny Slate), and Grammy Norma (Betty White). \n\n=== Document 8: Rock Me (One Direction song) ===\n\"Rock Me\" is a song by English-Irish boy band One Direction from their second studio album, \"Take Me Home\" (2012). It was written by Peter Svensson, Sam Hollander, Lukasz Gottwald, Henry Walter, Breanna Smith, and Allan Grigg, with production handled by Dr. Luke, Circut and Kool Kojak. Created in one day, Grigg carried out the mid-tempo beat, Hollander conceptualised the title and the pop rock melody \"just came\". Its clapping riff has been noted as similar to that of the Queen 1977 single \"We Will Rock You\". \n\n=== Document 9: Victorious: Music from the Hit TV Show ===\nVictorious: the debut soundtrack for the Nickelodeon TV series \"Victorious\". The majority of the album was sung by the lead actress of the television series, Victoria Justice, with the \"Victorious\" cast being listed beside her. Some of the other singers on the album feature Ariana Grande, Elizabeth Gillies, Miranda Cosgrove, Matt Bennett, Daniella Monet and Avan Jogia. The majority of the album was written by Michael Corcoran, Dan Schneider, Savan Kotecha, Kool Kojak and CJ Abraham with Victoria Justice involved in the composition of \"Best Friend's Brother\" and Leon Thomas III on \"Song 2 You\". \n\n=== Document 10: Give the Drummer Some ===\nGive the Drummer Some is the first solo record by Blink-182 drummer Travis Barker. Barker had earlier announced that the album would be slated for a September 2010 release, but was later pushed back, with the album being released on March 15, 2011. The album, released under Interscope Records, was produced by the drummer himself, alongside The Neptunes, RZA, Kool Kojak, Chuck Inglish, Transplants, Kid Cudi, EDIT, Corey Taylor and Steve Aoki. The album debuted at number nine on the US \"Billboard\" 200 chart, with first-week sales of 28,000 copies in the United States. \n\n\nProvide your answer based on the information in the supporting documents.",
    "system_answer": "The animated movie starring Danny DeVito, which featured music written and produced by Kool Kojak, is The Lorax (also known as Dr. Seuss' The Lorax).",
    "golden_answer": "The Lorax",
    "explanation": "The system answer includes \"The Lorax\" which is the golden answer. While the system answer provides additional information, it still contains the correct answer."
  },
  {
    "question": "Multi-hop reasoning task:\n\nQuestion: Out of the actors who have played the role of Luc Deveraux in the Universal Soldier franchise, which actor has also starred in the movies Holby City, Doctor Strange, the Bourne Ultimatum and Zero Dark Thirty?\n\nSupporting Documents:\n=== Document 1: Nick Jordan (character) ===\nNick Jordan is a fictional character from the BBC medical dramas \"Casualty\" and \"Holby City\", portrayed by actor Michael French. Jordan first appeared in two episodes of \"Casualty\" in 1998, before becoming a main character in spin-off show \"Holby City\" from its 1999 conception, in the role of Cardiothoracic Surgical Registrar. He departed from the show in its second series, returning for a 2005 Christmas crossover special between the two series, styled \"Casualty@Holby City\". He returned again to \"Holby City\" in 2006, taking on the role of General Surgical Consultant, departing a few months later in order to pursue a transfer back to cardiothoracics. In 2008, he rejoined the cast of \"Casualty\", becoming Clinical Lead of the show's Emergency Department. French left his role as Nick Jordan in February 2013, four weeks after his return. \n\n=== Document 2: Universal Soldier (franchise) ===\nThe Universal Soldier franchise is a series of science fiction action films. The franchise began in 1992 with \"Universal Soldier\" and as of 2012 comprises six entries (some of which are now considered non-canon). The films centered on the character of Luc Deveraux (played first by Jean-Claude Van Damme and then by Matt Battaglia) until \"\", which focuses on a new protagonist named John (played by Scott Adkins). \n\n=== Document 3: Adrian Fletcher (character) ===\nAdrian \"Fletch\" Fletcher is a fictional character from the BBC medical dramas \"Casualty\" and \"Holby City\", portrayed by actor Alex Walkinshaw. He first appeared in the twenty-sixth series episode \"Zero Sum Game\", broadcast on 7 July 2012. Fletch was a Staff Nurse in Holby City Hospital's emergency department upon his arrival, but was promoted to Senior Staff Nurse in 2013. On 1 April 2014, Walkinshaw announced his departure from \"Casualty\", but revealed that he would be reprising his role as the ward manager of the fictitious AAU ward in spin-off show \"Holby City\". Fletch departed \"Casualty\" on 29 June 2014 and made his debut on \"Holby City\" on 12 August 2014, over six weeks later. Walkinshaw reprised his role in \"Casualty\" for the 30th anniversary episode \"Too Old for This Shift\", which aired on 27 August 2016. \n\n=== Document 4: Universal Soldier (1992 film) ===\nUniversal Soldier is a 1992 American military science fiction action film directed by Roland Emmerich, produced by Mario Kassar and Allen Shapiro, and written by Richard Rothstein and Dean Devlin. The film tells the story of Luc Deveraux, a former US Army soldier who was killed in Vietnam War in 1969, and returned to life following a secret military project called the \"Universal Soldier\" program. However, he finds out about his past even although his memory was erased, and escapes alongside a young TV journalist. Along the way, they have to deal with the return of his archenemy, Sgt. Andrew Scott, who had lost his sanity in the Vietnam War, and became a psychotic megalomaniac, intent on killing him and leading the Universal Soldiers. \"Universal Soldier\" was released by TriStar Pictures on July 10, 1992. The film grossed $36 million worldwide against its budget of $23 million. It spawned a series of films, including several rather poorly received direct-to-TV films: \"\", which has since been removed from the series canon, followed by \"\" and \"\". \n\n=== Document 5: Universal Soldier: Regeneration ===\nUniversal Soldier: Regeneration (also known in some countries as Universal Soldier: A New Beginning) is a 2009 American sci-fi action film directed and edited by John Hyams (the son of director Peter Hyams, who previously worked with Jean-Claude Van Damme on three films, 1994's \"Timecop\", 1995's \"Sudden Death\" and 2013's \"Enemies Closer\"; in this film Peter is the director of photography). The film stars Jean-Claude Van Damme and Dolph Lundgren, who both reprise their roles from the first film. It is the third theatrical installment in the \"Universal Soldier series\". The film is a direct sequel to the original \"Universal Soldier\" from 1992, unrelated to the two \"Universal Soldier\" television sequels that were produced in 1998 and completely ignores the events from the 1999 theatrical sequel \"\". \n\n=== Document 6: Luc Deveraux ===\nLuc Deveraux is a fictional character and the protagonist of the \"Universal Soldier\" film series. He is most famously portrayed by Belgian actor and martial artist Jean-Claude Van Damme. Van Damme portrays Luc in the 1992 film \"Universal Soldier\" and its sequels \"\" (1999), \"\" (2009), and \"\" (2012); he is portrayed by Matt Battaglia in the direct-to-video sequels \"\" (1998) and \"\" (1998). \n\n=== Document 7: Jayne Grayson ===\nJayne Grayson is a fictional character in the BBC medical drama \"Holby City\", portrayed by actress Stella Gonet. The character first appeared on-screen on 10 July 2007 in episode \"Under the Radar\" - series 9, episode 39 of the programme. Her role in the show was that of Chief Executive Officer of the Holby City Hospital Primary Care Trust, making her the only regular character who is not a medic by profession. Gonet formerly appeared as a doctor in \"Holby City\"<nowiki>'</nowiki>s sister show \"Casualty\", and has since appeared in crossover episodes of the drama, this time as Jayne Grayson. Her storylines in \"Holby City\" have revolved around issues of hospital bureaucracy, as well as her husband's affair with her colleague Connie Beauchamp. A two-part episode which saw Jayne fight the hospital's Board of Directors and the British government over the separation surgery of the conjoined twin daughters of illegal Korean immigrants proved a critical success, and was positively received by many tabloid TV critics. \n\n=== Document 8: List of accolades received by Zero Dark Thirty ===\n\"Zero Dark Thirty\" is a 2012 American action thriller directed and co-produced by Kathryn Bigelow with screenplay by Mark Boal. The film was released in the United States on December 19, 2012, with a limited release at five theaters in Los Angeles and New York City. It made $124,848 in its limited release weekend, making it one of the biggest limited mid-week openings ever. As of March 6, 2013, \"Zero Dark Thirty\" has grossed a worldwide total of $106.8 million. \"Zero Dark Thirty\" also received a high critical acclaim, accumulating an approval rating of 93% on the review aggregator site Rotten Tomatoes. \n\n=== Document 9: Scott Adkins ===\nScott Edward Adkins (born 17 June 1976) is an English actor and martial artist who is best known for playing Russian prison fighter Yuri Boyka in the 2006 film \"\" and its following two sequels: \"\" (2010) and \"\" (2016) and Casey Bowman in Ninja and its sequel . He is also known for playing Bradley Hume in \"Holby City\", Lucian in \"Doctor Strange\", Kiley in \"The Bourne Ultimatum\" and John in \"Zero Dark Thirty\". Adkins has also appeared in \"EastEnders\", \"Hollyoaks\", \"Doctors\" as well as starred in many direct-to-video films. \n\n=== Document 10: Connie Beauchamp ===\nConstance \"Connie\" Beauchamp is a fictional character from the BBC medical dramas \"Holby City\" and \"Casualty\", portrayed by actress Amanda Mealing. She first appeared in the series six, episode 35, \"In at the Deep End\", broadcast on 1 June 2004, and appeared in \"Holby City's\" sister show \"Casualty\" multiple times, having already appeared in crossover \"Casualty@Holby City\" episodes. Mealing continued her role as Connie until the thirteenth series of \"Holby City\", departing in the 28 December 2010 episode \"Snow Queens\". Connie's role in \"Holby City\" was that of Clinical Lead of Cardiothoracic Surgery in Darwin, and Joint Director of Surgery. \n\n\nProvide your answer based on the information in the supporting documents.",
    "system_answer": "Scott Adkins has starred in Holby City, Doctor Strange, The Bourne Ultimatum, and Zero Dark Thirty and has played the role of John in the Universal Soldier franchise (Document 2, Document 9). Jean-Claude Van Damme and Matt Battaglia have played the role of Luc Deveraux in the Universal Soldier franchise (Document 2, Document 6). Therefore, the answer is Scott Adkins.",
    "golden_answer": "Scott Adkins",
    "explanation": "The system answer correctly identifies Scott Adkins as the answer, despite including additional reasoning."
  },
  {
    "question": "Multi-hop reasoning task:\n\nQuestion: Tommy's Honour was a drama film that included the actor who found success with what 2016 BBC miniseries?\n\nSupporting Documents:\n=== Document 1: Tommy's Honour ===\nTommy's Honour is a 2016 historical drama film depicting the lives and careers of, and the complex relationship between, the pioneering Scottish golfing champions Old Tom Morris and his son Young Tom Morris. The film is directed by Jason Connery, and the father and son are portrayed by Peter Mullan and Jack Lowden. The film won Best Feature Film at the 2016 British Academy Scotland Awards. \n\n=== Document 2: H\u00e9l\u00e8ne Kuragina ===\nPrincess Yelena \"H\u00e9l\u00e8ne\" Vasilyevna Kuragina (Russian: \u0415\u043b\u0435\u043d\u0430 \"\u042d\u043b\u0435\u0301\u043d\" \u0412\u0430\u0441\u0438\u0301\u043b\u044c\u0435\u0432\u043d\u0430 \u041a\u0443\u0440\u0430\u0301\u0433\u0438\u043d\u0430 ) is a fictional character in Leo Tolstoy's novel \"War and Peace\" and its various cinematic adaptations. She is played by Anita Ekberg in the 1956 film, by Amber Gray in \"Natasha, Pierre & The Great Comet of 1812\", and by Tuppence Middleton in the 2016 BBC miniseries. \n\n=== Document 3: Kevin McKidd ===\nKevin McKidd (born 9 August 1973) is a Scottish-American television and film actor, director, and occasional singer. Before playing the role of Owen Hunt in \"Grey's Anatomy\", for which he is perhaps most widely known, McKidd starred as Dan Vasser in the NBC Series \"Journeyman\" (2007), Tommy in Danny Boyle's \"Trainspotting\" (1996), Count Vronsky in the BBC miniseries \"Anna Karenina\" (2000), and Lucius Vorenus in the historical drama series \"Rome\" (2005\u20132007). He also provides the voice of John \"Soap\" MacTavish in the video games \"\" and \"\". He also played Poseidon in the film \"\". \n\n=== Document 4: Ken Stott ===\nKenneth Campbell \"Ken\" Stott (born 19 October 1954) is a Scottish stage, television and film actor who won the Laurence Olivier Award for Best Actor in a Supporting Role in 1995 in the play \"Broken Glass\" at Royal National Theatre. He is more recently known for his role as the dwarf Balin in \"The Hobbit\" film trilogy (2012\u20132014), and as Ian Garrett in the 2014 BBC TV mini-series \"The Missing\" starring alongside James Nesbitt. His many notable roles in UK television include the role of Edward 'Eddie' McKenna in the Scottish BBC miniseries \"Takin' Over The Asylum\" (1994) co-starring with a young David Tennant, the title character DI John Rebus in the crime fiction-mystery series \"Rebus\" (2000\u20132007) and also as DCI Red Metcalfe in \"Messiah\" (2001\u20132008). \n\n=== Document 5: Elisabeth Moss ===\nElisabeth Singleton Moss (born July 24, 1982) is an American film, stage, and television actor. She is known for her roles as Zoey Bartlet, the youngest daughter of President Josiah Bartlet, on the NBC television series \"The West Wing\" (1999\u20132006); Peggy Olson, secretary-turned-copywriter, on the AMC series \"Mad Men\" (2007\u20132015), which earned her six Emmy Awards nominations and a Golden Globe nomination; Det. Robin Griffin in the BBC miniseries \"Top of the Lake\" (2013, 2017), which won her a Golden Globe for Best Actress in a Miniseries or TV Film; and Offred on the Hulu series \"The Handmaid's Tale\", for which she won the Primetime Emmy Award for Outstanding Lead Actress in a Drama Series and the Primetime Emmy Award for Outstanding Drama Series, as producer. \n\n=== Document 6: Tina Heath ===\nTina Heath is a British actress and former television presenter. Her first TV appearance came in 1969, when she appeared in \"Broaden Your Mind\" on BBC Two alongside Graeme Garden and Tim Brooke-Taylor. A one-off appearance in \"Z-Cars\" followed in 1970. In 1973, she played the title role in the popular children's television serial \"Lizzie Dripping\" after first playing the character in an episode of \"Jackanory Playhouse\" in 1972; her character was supposed to be 12 years old, but in fact Heath was already 20 at the time. She also played, in that same year's BBC miniseries production of \"Jane Eyre\" (1973), the character of Helen Burns, the fourteen-year-old boarding-school girl who is cruelly birched by Miss Scatcherd and who befriends the ten-year-old Jane when Jane is a newcomer to Lowood Institute. Other TV appearances included a role in the BBC's \"Play Of The Month: The Linden Tree\" by J.B. Priestley in 1974; \"Churchill's People\" in 1975; Muriel Spark's \"The Girls Of Slender Means\"; and The Sweeney in 1976. \n\n=== Document 7: Jack Lowden ===\nJack Andrew Lowden (born 2 June 1990) is a Scottish stage, television, and film actor. Following a highly successful and award-winning four-year stage career, his first major international onscreen success was in the 2016 BBC miniseries \"War & Peace\", which led to starring roles in feature films. \n\n=== Document 8: Kate Buffery ===\nKatharine Winifred Buffery (born 23 July 1957) is an English actress. She is known for her numerous roles on British television, including the ITV drama series \"Wish Me Luck\" (1988-1990), BBC miniseries \"Close Relations\" (1998), Channel 5 legal drama \"Wing and a Prayer\" (1997-1999) and the ITV police drama \"Trial and Retribution\" (1997-2002). Her stage work includes the 1983 original West End production of \"Daisy Pulls it Off\", which earned her an Olivier Award nomination. \n\n=== Document 9: Cultural depictions of William III of England ===\nWilliam III of England has been played on screen by Bernard Lee in the 1937 film \"The Black Tulip\", based on the novel by Alexandre Dumas, p\u00e8re, Henry Daniell in the 1945 film \"Captain Kidd\", Olaf Hytten in the 1952 film \"Against All Flags\", Alan Rowe in the 1969 BBC drama series \"The First Churchills\", Laurence Olivier in the 1986 NBC TV mini-series \"Peter the Great\", Thom Hoffman in the 1992 film \"Orlando\", based on the novel by Virginia Woolf, Corin Redgrave in the 1995 film \"England, My England\", the story of the composer Henry Purcell, Jochum ten Haaf in the 2003 BBC miniseries \"\", Bernard Hill in the 2005 film \"The League of Gentlemen's Apocalypse\", Russell Pate in the 2008 BBC film \"King Billy Above All\", Egbert-Jan Weber in the 2015 film \"Michiel de Ruyter\", George Webster in \"Versailles\" (2015) and Carl Prekopp in the 2015 premiere of the play \"Queen Anne\". \n\n=== Document 10: The Day of the Triffids (2009 TV miniseries) ===\nThe Day of the Triffids is a BBC miniseries adaptation of John Wyndham's novel of the same name. The novel had previously been adapted in 1962 as a theatrical film and by the BBC in a 1981 series. \n\n\nProvide your answer based on the information in the supporting documents.",
    "system_answer": "War & Peace.",
    "golden_answer": "War & Peace",
    "explanation": "Both answers are identical, conveying the same information: \"War & Peace\"."
  }
]

            SPECIFIC FAILURE CASES (what needs improvement):
            [
  {
    "question": "Multi-hop reasoning task:\n\nQuestion: In which film did Emilio Estevez star in in the same year as Nightmares\n\nSupporting Documents:\n=== Document 1: Rated X (film) ===\nRated X is a 2000 American television film starring brothers Charlie Sheen and Emilio Estevez, with the latter also directing. Based on the nonfiction book \"X-Rated\" by David McCumber, the film chronicles the story of the Mitchell brothers, Jim and Artie Mitchell, who were pioneers in the pornography and strip club businesses in San Francisco in the 1970s and 1980s. The film focuses on the making of their most profitable film, \"Behind the Green Door\". It also portrays Artie's descent into drug addiction and increasingly erratic behavior, culminating in his murder at Jim's hands. \n\n=== Document 2: The Way (2010 film) ===\nThe Way is a 2010 American drama film directed, produced and written by Emilio Estevez, starring his father Martin Sheen, Deborah Kara Unger, James Nesbitt, Yorick van Wageningen, and Ren\u00e9e Estevez. \n\n=== Document 3: Wisdom (film) ===\nWisdom is a 1986 American romantic crime film written and directed by its star Emilio Estevez in his filmmaking debut. The film also stars Demi Moore, along with Tom Skerritt and Veronica Cartwright (both of \"Alien\" fame) as Estevez's parents. The end credits song is \"Home Again\" by Oingo Boingo and the score by Danny Elfman. \n\n=== Document 4: Culture Clash in AmeriCCa ===\nCulture Clash in AmeriCCa is a 2005 documentary film directed by Emilio Estevez. It is an anthology of fun and thought-provoking skits and monologues portraying diverse American immigrants. Emlio Estevez doesn't appear in this documentary film. Inspiration came from thousands of interviews conducted nationwide during a period of 20 years, by Culture Clash. \n\n=== Document 5: Emilio Estevez ===\nEmilio Estevez ( ; born May 12, 1962) is an American actor, director, and writer. He started his career as an actor and is well known for being a member of the acting Brat Pack of the 1980s, starring in \"The Breakfast Club\", \"St. Elmo's Fire\", and also acting in the 1983 hit movie \"The Outsiders\". He is also known for \"Repo Man\", \"The Mighty Ducks\" and its sequels, \"Stakeout\", \"Maximum Overdrive\", \"Bobby\" (which he also wrote and directed), and his performances in Western films such as \"Young Guns\" and its sequel. \n\n=== Document 6: Nightmares (1983 film) ===\nNightmares is a 1983 American horror anthology film directed by Joseph Sargent, and starring Emilio Estevez, Lance Henriksen, Cristina Raines, Veronica Cartwright, and Richard Masur. The film is made up of four short films based on urban legends; the first concerns a woman who encounters a killer in the backseat of her car; the second concerns a video game-addicted teenager who is consumed by his game; the third focuses on a fallen priest who is stalked by a pickup truck from hell; and the last follows a suburban family battling a giant rat in their home. \n\n=== Document 7: Dollar for the Dead ===\nDollar for the Dead is a 1998 TNT western television film. Film directed and written by Gene Quintano and starring Emilio Estevez. It is the third western film which Estevez stars. Film also stars William Forsythe, Joaquim de Almeida, Jonathan Banks, Ed Lauter and Howie Long. Actor Jordi Moll\u00e0 nominated for Fotogramas de Plata award. \n\n=== Document 8: Here Not There ===\nHere Not There is the second full length studio album by Canadian singer-songwriter Jane Child. It was released in 1993 on Warner Bros. Records (see 1993 in music). Due to a strange single choice (the label opted for the title track instead of \"Do Whatcha Do\", which would have been in keeping with her style established on \"Don't Wanna Fall In Love\", her hit from 4 years previously), it was less successful. It also saw her undergoing a stylistic change; while her debut was mostly synthesized dance-pop with R&B undercurrents, \"Here Not There\" saw her edging away and mixing new jack swing rhythms with almost hard rock elements. The year prior to the release of the album, she contributed the song \"Mona Lisa Smiles\" to the movie \"Freejack\", which starred Emilio Estevez. \n\n=== Document 9: Joe Estevez ===\nJoseph \"Joe\" Estevez (born February 13, 1946) is an American actor, director and producer. He is the younger brother of actor Martin Sheen and the uncle of Emilio Estevez, Charlie Sheen, Ren\u00e9e Estevez and Ramon Estevez. \n\n=== Document 10: Bobby (2006 film) ===\nBobby is a 2006 American drama film written and directed by Emilio Estevez, and starring an ensemble cast featuring Harry Belafonte, Joy Bryant, Nick Cannon, Laurence Fishburne, Spencer Garrett, Helen Hunt, Anthony Hopkins, Ashton Kutcher, Shia LaBeouf, Lindsay Lohan, William H. Macy, Demi Moore, Martin Sheen, Christian Slater, Sharon Stone, Elijah Wood and Estevez himself. The screenplay is a fictionalized account of the hours leading up to the June 5, 1968, shooting of U.S. Senator Robert F. Kennedy in the kitchen of the Ambassador Hotel in Los Angeles following his win of the 1968 Democratic Party presidential primary in California. \n\n\nProvide your answer based on the information in the supporting documents.",
    "system_answer": "Nightmares.",
    "golden_answer": "The Outsiders",
    "explanation": "The answers \"Nightmares\" and \"The Outsiders\" are completely different and do not convey the same information."
  },
  {
    "question": "Multi-hop reasoning task:\n\nQuestion: What was the concept of the business Eric S .Pistorius worked for after being an attorney?\n\nSupporting Documents:\n=== Document 1: Commissioner v. Flowers ===\nCommissioner v. Flowers, 326 U.S. 465 (1946), was a Federal income tax case before the Supreme Court of the United States. The Court held that in order to deduct the expense of traveling under \u00a7162, the expense must be incurred while away from home, and must be a reasonable expense necessary or appropriate to the development and pursuit of a trade or business. In this case, the attorney in question could only deduct traveling expenses from her gross income when the railroad's business forced attorney to travel and live temporarily at some place other than the railroad's principal place of business. Where attorney preferred for personal reasons to live in a different state from the location of his employer's principal office, and his duties required frequent trips to that office, the evidence sustained Tax Court's finding that the necessary relation between expenses of such trips and the railroad's business was lacking. \n\n=== Document 2: Circuit court ===\nCircuit court is the name of court systems in several common law jurisdictions. The core concept of circuit courts requires judges to travel to different locales in order to ensure wide visibility and understanding of cases in a region. More generally, some modern circuit courts may also refer to a court which merely holds trials for cases of multiple locations in some rotation. \n\n=== Document 3: Eliot Spitzer ===\nEliot Laurence Spitzer (born June 10, 1959) is a former American Democratic politician and attorney who served as the 54th governor of New York, from January 1, 2007, until his resignation in disgrace fourteen months later on March 17, 2008. Prior to being elected governor of New York, he was elected to two four-year terms as the Attorney General of New York, from 1999 to 2006. Prior to becoming attorney general, Spitzer worked for six years as a prosecutor with the office of the Manhattan district attorney and also worked as an attorney in private practice with several New York law firms. \n\n=== Document 4: North Carolina Attorney General ===\nThe Attorney General of North Carolina is the elected head of the state's Department of Justice. The North Carolina constitution, in Article III Section 7, provides for the election of the Attorney General. http://www.ncga.state.nc.us/Legislation/constitution/article3.html By statute, Attorney General's duties include providing legal representation and advice to all state agencies. The parameters of that duty have been the subject of some debate, when, for example, United States Attorney General Eric Holder suggested that state Attorneys General should not squander their state's resources in defense of laws they know to be unconstitutional. By statute, in defense of the public interest, the Attorney General may initiate legal action or intervene in proceedings before any courts, regulatory officers, agencies or bodies \u2014 either state or federal \u2014 on behalf of the state's agencies and citizens. http://www.ncga.state.nc.us/EnactedLegislation/Statutes/PDF/ByChapter/Chapter_114.pdf The Attorney General also renders legal opinions, either formally or informally, upon all questions of law submitted by the General Assembly, the Governor or any other state officer. Attorney General opinions may be viewed online. http://www.ncdoj.gov/About-DOJ/Legal-Services/Legal-Opinions.aspx \n\n=== Document 5: Lloyd Kenyon, 1st Baron Kenyon ===\nLloyd Kenyon, 1st Baron Kenyon {'1': \", '2': \", '3': \", '4': \"} (5 October 1732 \u2013 4 April 1802) was a British politician and barrister, who served as Attorney General, Master of the Rolls and Lord Chief Justice. Born to a country gentleman, he was initially educated in Hanmer before moving to Ruthin School aged 12. Rather than going to university he instead worked as a clerk to an attorney, joining the Middle Temple in 1750 and being called to the Bar in 1756. Initially almost unemployed due to the lack of education and contacts which a university education would have provided, his business increased thanks to his friendships with John Dunning, who, overwhelmed with cases, allowed Kenyon to work many, and Lord Thurlow who secured for him the Chief Justiceship of Chester in 1780. He was returned as the Member of Parliament (MP) for Hindon the same year, serving repeatedly as Attorney General under William Pitt the Younger. He effectively sacrificed his political career in 1784 to challenge the ballot of Charles James Fox, and was rewarded with a baronetcy; from then on he did not speak in the House of Commons, despite remaining an MP. \n\n=== Document 6: Eric S. Pistorius ===\nEric S. Pistorius (born 1956), is a Circuit court Judge of the Seventh Circuit of Illinois, residing from Jerseyville, Illinois. He used to be an attorney at law for his law firm and specialized in the areas of: personal injury, litigation, criminal defense, and collections. \n\n=== Document 7: Peter Deegan ===\nPeter Deegan is an American attorney who has been confirmed to serve as the next United States Attorney for the United States District Court for the Northern District of Iowa. He previously served as an assistant U.S. attorney and the chief of the criminal division in the Northern District of Iowa. Deegan was an assistant U.S. attorney in the United States District Court for the Eastern District of Michigan from 2004 to 2006. He has prosecuted a number of federal offenses, including complex white collar and business crime. Earlier in his career, Deegan was an associate attorney at Murphy Smith and Polk in Chicago, where he was active in labor and employment litigation. After being nominated to become a U.S. Attorney by President Donald Trump, Deegan was confirmed unanimously by the United States Senate on September 14, 2017. \n\n=== Document 8: Loretta Lynch ===\nLoretta Elizabeth Lynch (born May 21, 1959) is an American attorney who served as the 83rd Attorney General of the United States, appointed by President Barack Obama in 2015 to succeed Eric Holder. Previously, she held the position for United States Attorney for the Eastern District of New York under both the Clinton (1999\u20132001) and Obama administrations (2010\u201315). As U.S. Attorney, Lynch oversaw federal prosecutions in Brooklyn, Queens, Staten Island and Long Island. \n\n=== Document 9: Bryan Schroder ===\nBryan Schroder is an American attorney who is the Acting United States Attorney for the United States District Court for the District of Alaska. He is President Donald Trump's nominee to become the U.S. Attorney for the District of Alaska. Schroder has worked at the U.S. Attorney's Office in the District of Alaska since 2005. Before becoming the Acting U.S. Attorney, he was the first assistant U.S. attorney, chief of the criminal division, anti-terrorism prosecutor, and district ethics adviser. He has prosecuted cases involving violent crimes, drug distribution, gun crimes, fraud, tax evasion, environmental crimes, and fisheries and wildlife offenses. \n\n=== Document 10: Kings County District Attorney ===\nThe Kings County District Attorney is the elected district attorney for Kings County, coterminous with the Borough of Brooklyn, in New York City. The office is responsible for the prosecution of violations of the Laws of New York. (Violations of federal law are prosecuted by the United States Attorney for the Eastern District of New York). The current district attorney is Acting District Attorney Eric Gonzalez. \n\n\nProvide your answer based on the information in the supporting documents.",
    "system_answer": "Eric S. Pistorius worked as an attorney at law for his law firm and specialized in the areas of: personal injury, litigation, criminal defense, and collections.",
    "golden_answer": "to ensure wide visibility and understanding of cases in a region",
    "explanation": "The golden answer discusses ensuring visibility and understanding of cases, while the system answer describes the legal specializations of Eric S. Pistorius. These are completely different topics."
  }
]

            REFINEMENT ANALYSIS GUIDANCE:
            1. IDENTIFY THE CORE STRENGTH:
               - What specific technique or approach makes this script successful?
               - Which components are working well and must be preserved?
               - What is the script's main competitive advantage?

            2. PINPOINT SPECIFIC WEAKNESSES:
               - Where exactly do the failures occur in the processing pipeline?
               - What specific patterns cause the script to fail?
               - Are failures due to information extraction, reasoning, formatting, or verification?
               - Can you identify the exact function or step where problems arise?

            3. FORM A SPECIFIC HYPOTHESIS:
               - What is the ONE most critical weakness to address?
               - What specific change would most likely improve performance?
               - How can you fix this weakness without breaking the existing strengths?
               - What verification can you add to test if your fix works?

            4. SURGICAL IMPROVEMENT STRATEGY:
               - Make the MINIMUM changes necessary to address the identified weakness
               - Preserve all successful components and logic
               - Add targeted verification for the specific area being improved
               - Enhance error handling for the identified failure mode
               - Add debugging output to verify the fix is working

            SPECIFIC REFINEMENT TECHNIQUES:
            - If failures are in information extraction: Improve prompts, add verification, better parsing
            - If failures are in reasoning: Add chain-of-thought, verification loops, multi-step reasoning
            - If failures are in formatting: Add output validation, format checking, retry logic
            - If failures are inconsistent: Add confidence scoring, multiple attempts, consensus approaches

            CRITICAL REFINEMENT REQUIREMENTS:
            1. Preserve the core successful approach - don't change what's working
            2. Make targeted, minimal changes focused on the specific weakness identified
            3. Add verification steps specifically for the area being improved
            4. Include debugging output to verify improvements are working
            5. EVERY LLM PROMPT must include embedded examples
            6. Test your hypothesis with additional verification

            Here's how to call the Gemini API. Use this example without modification:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

            REFINEMENT IMPLEMENTATION:
            State Your Hypothesis: Clearly comment what specific weakness you're addressing and how
            Preserve Strengths: Keep all successful components intact
            Targeted Fix: Implement the minimal change needed to address the weakness
            Add Verification: Include checks to ensure your fix is working
            Debug Output: Add print statements to track the improvement

            Return a COMPLETE, RUNNABLE Python script that:
            1. Preserves the successful core approach of the original script
            2. Makes targeted improvements to address the specific identified weakness
            3. Includes a clear comment stating your improvement hypothesis
            4. Adds verification specifically for the improved component
            5. Includes embedded examples in EVERY LLM prompt
            6. Is COMPLETE - no missing code, no "..." placeholders
            7. Closes all string literals properly

            REFINEMENT HYPOTHESIS: [State your specific hypothesis about what to improve and why in a comment]

            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            MAKE SURGICAL IMPROVEMENTS WHILE PRESERVING THE SCRIPT'S CORE STRENGTHS!
            