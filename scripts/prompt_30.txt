
            You are developing a Python script to solve problems using LLM reasoning capabilities.
            You are in the EXPLORATION PHASE. You must generate a NEW approach that's different from previous approaches but informed by their successes and failures. With this approach, you will have a specific NEW HYPOTHESIS or variable you are trying to test. Your goal is to see if this new approach works, and you must add verification and validation steps to deduce if this new change is helpful. You may also test RADICAL NEW APPROACHES that are substantially different from previous approaches. 
            
            You should try NEW THINGS:
            
            Break down the problem into smaller pieces
            Think CREATIVELY about how to solve your problem if other approaches aren't working
            Transform data into different formats to see if it helps

            # YOUR TASK
            You are deeply familiar with prompting techniques and the agent works from the literature. 
            Your goal is to maximize the specified performance metrics by proposing interestingly new agents.
            Observe the past discovered agents and scripts carefully and think about what insights, lessons, or stepping stones can be learned from them.
            Be creative when thinking about the next interesting agent to try. You are encouraged to draw inspiration from related agent papers or academic papers from other research areas.
            Use the knowledge from the archive and inspiration from academic literature to propose the next interesting agentic system design.
            THINK OUTSIDE THE BOX.
            

            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "In what year was the praying mantis species Eremiaphila bifasciata described by Chopard?",
    "answer": "1940"
  },
  {
    "id": 1,
    "question": "What's the secret identity of the third Doll Man?",
    "answer": "Dane Maxwell"
  },
  {
    "id": 2,
    "question": "On what day, month, and year was David Sweet, Canadian politician, born?",
    "answer": "June 24, 1957"
  }
]

            HISTORICAL CONTEXT:
            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 30
        - Current explore/exploit balance: 65/35
        - Best accuracy achieved: 0.67 (iteration 22)

        APPROACH HISTORY (last 10 iterations):
        [
  {
    "iteration": 20,
    "strategy": "Exploration",
    "accuracy": 0.0,
    "approach": "This script addresses factual questions using an LLM-driven approach that includes question transformation, knowledge base retrieval (simulated), and a verification loop. The problem is decomposed into transforming the initial question into a more specific query, simulating a knowledge base search based on the transformed query, and then verifying if the retrieved information accurately answers the original question. Three agents are involved: a question transformation expert, a knowledge base, and an expert validator. The functions used are `call_llm` to interact with the LLM with `main` calling `call_llm` three times sequentially for question transformation, knowledge base retrieval, and verification. The overall workflow transforms the question, retrieves information, and validates the answer."
  },
  {
    "iteration": 21,
    "strategy": "Exploitation",
    "accuracy": 0.0,
    "approach": "The script implements a retrieval-augmented generation approach to answer questions using an LLM. It decomposes the problem into information extraction, search query generation, search (simulated with an LLM), answer extraction with confidence scoring, and answer validation. The agent roles are information extractor, search query generator, search engine, answer extraction expert, and answer validator. The main function orchestrates the process, calling `extract_information` to get entities and constraints, `generate_search_query` to create a search query, `call_llm` to simulate search, `extract_answer` to find the answer and confidence, and `validate_answer` to check the correctness of the response, returning the extracted answer if valid, otherwise an error message."
  },
  {
    "iteration": 22,
    "strategy": "Exploration",
    "accuracy": 0.6666666666666666,
    "approach": "The script uses an LLM to fact-check by simulating multiple search engines with different biases, extracting answers from each, reconciling the answers, and then validating the final answer. The problem is decomposed into simulating search, extracting answers, reconciling those answers, and then validating the reconciled answer. Different system instructions assign the LLM roles of search engine, answer extractor, reconciler, and validator.\n\n*   `call_llm` is the base function that calls the LLM with a prompt and system instruction.\n*   `simulate_search` generates search results for a given query using a simulated search engine.\n*   `extract_answer` extracts answers from the simulated search results.\n*   `reconcile_answers` combines the answers from the search engines to produce one answer.\n*   `validate_answer` validates the reconciled answer to determine if it is correct.\n*   `main` orchestrates the process, calling the search, extraction, reconcile, and validation functions.\n\nThe overall workflow involves simulating multiple searches, extracting answers from each search, reconciling them into a single answer, validating the answer, and returning the reconciled answer if it is valid."
  },
  {
    "iteration": 23,
    "strategy": "Exploitation",
    "accuracy": 0.0,
    "approach": "The script uses an LLM-driven approach to answer questions by simulating multiple search engines and reconciling their answers. The problem is decomposed into simulating search results, extracting answers from those results, reconciling the different answers into one, and validating the final answer. Several agent roles are defined: a search engine simulator, an answer extractor, an answer reconciler, and a validator.\n\nThe functions used are `call_llm`, `simulate_search`, `extract_answer`, `reconcile_answers`, `validate_answer`, and `main`. `simulate_search` uses `call_llm` to simulate search engine results, then `extract_answer` uses `call_llm` to get an answer from each search result. `reconcile_answers` then takes all the answers and the original question and uses `call_llm` to create one answer. Lastly, `validate_answer` validates the reconciled answer.\n\nThe overall workflow involves simulating searches with multiple engines using `simulate_search`, extracting answers from each search result with `extract_answer`, reconciling the extracted answers into a single answer using `reconcile_answers`, validating the final answer using `validate_answer`, and then returning the validated answer or an error message."
  },
  {
    "iteration": 24,
    "strategy": "Exploration",
    "accuracy": 0.6666666666666666,
    "approach": "The script implements a QA system using two simulated knowledge bases (KB1 and KB2) and LLM-driven answer extraction and validation, using the `gemini-2.0-flash` model. The problem is decomposed into knowledge retrieval, answer extraction, and answer validation, with KB1 having higher authority in cases of conflict. The agent roles involved are a knowledge base simulator, an answer extractor, and a validator.\n\nThe functions used are:\n1.  `call_llm`: Used to interface with the LLM for all tasks.\n2.  `simulate_knowledge_base`: Simulates retrieving information from two knowledge bases, KB1 and KB2.\n3.  `extract_answer`: Extracts the answer from the knowledge base results, giving precedence to KB1.\n4.  `validate_answer`: Validates the extracted answer against the knowledge bases, also prioritizing KB1.\n5.  `main`: Orchestrates the entire QA process.\n\nThe workflow starts with `main` calling `simulate_knowledge_base` twice to get results from KB1 and KB2, then calls `extract_answer` to extract an answer from these results, and then uses `validate_answer` to validate the extracted answer."
  },
  {
    "iteration": 25,
    "strategy": "Exploration",
    "accuracy": 0.3333333333333333,
    "approach": "The script employs a multi-agent system to answer questions by simulating multiple search engines and reconciling their results, ultimately validating the final answer. It uses the `call_llm` function to interact with the Gemini LLM, assigning specific roles to different agents via system instructions. The problem is decomposed into simulating search (`simulate_search`), extracting answers (`extract_answer`), reconciling answers (`reconcile_answers`), and verifying sources (`source_verifier`). The `main` function orchestrates the workflow, calling `simulate_search` to get results, `extract_answer` to extract the answers from the search results, `reconcile_answers` to get a final answer, and `source_verifier` to validate the answer."
  },
  {
    "iteration": 26,
    "strategy": "Exploitation",
    "accuracy": 0.0,
    "approach": "The script employs multiple LLM-based techniques, including simulating multiple search engines and answer reconciliation, to improve fact-checking. The problem is decomposed into simulating search results, extracting answers, reconciling them, and finally validating the reconciled answer. Several agent roles are defined through system instructions (search engine, answer extraction expert, reconciliation expert, validator). Other functions used include `call_llm`, which interfaces with the Gemini model. The script uses the following functions: `call_llm` is used to interact with the Gemini model, `simulate_search` simulates search engine results, `extract_answer` extracts the answer from the search results, `reconcile_answers` reconciles the answers from different search engines, and `validate_answer` validates the reconciled answer. The overall workflow involves simulating multiple searches, extracting answers from each, reconciling those answers, and validating the final reconciled answer."
  },
  {
    "iteration": 27,
    "strategy": "Exploration",
    "accuracy": 0.0,
    "approach": "This script uses multiple LLM-based agents to answer questions with a layered validation approach. The problem is decomposed into search, answer extraction, and validation steps. The agents include a search engine simulator, an answer extractor, a fact validator, a temporal validator, and a source reliability validator.\n\nThe core functions are `call_llm`, `simulate_search`, `extract_answer`, `fact_validator`, `temporal_validator`, `source_reliability_validator`, and `main`. `main` calls `simulate_search` to get results, then `extract_answer` to get an answer and its source. Finally, it calls `fact_validator`, `temporal_validator`, and `source_reliability_validator` to validate the answer.\n\nThe workflow involves simulating a search, extracting an answer, and then validating the answer using multiple specialized validators focusing on factuality, temporal consistency, and source reliability, returning the validated answer or error message."
  },
  {
    "iteration": 28,
    "strategy": "Exploration",
    "accuracy": 0.0,
    "approach": "This script employs a two-stage retrieval and focused summarization approach, enhanced with a self-reflection mechanism, to answer questions using the Gemini LLM. It decomposes the problem into three distinct stages: initial information retrieval, focused summarization, and self-reflection validation. The script defines the functions `initial_info_retrieval`, `focused_summarization`, and `self_reflection_validation`, each acting as an agent with a specific role (information retrieval, summarization, and validation, respectively), while `call_llm` sends the prompt to the LLM and retrieves a response, and `main` orchestrates the entire process. The `main` function first calls `initial_info_retrieval` to get background information, then passes this and the original question to `focused_summarization`, and finally uses `self_reflection_validation` to validate and answer the question."
  },
  {
    "iteration": 29,
    "strategy": "Exploration",
    "accuracy": 0.3333333333333333,
    "approach": "The script uses a multi-agent system with a \"Knowledge Navigator\" and a \"Fact Checker\" to answer questions accurately. The Knowledge Navigator iteratively refines search queries and extracts candidate answers and sources, while the Fact Checker validates the answer. The core functions are `call_llm` to interact with the LLM, `knowledge_navigator` to find a candidate answer, and `fact_checker` to validate the answer. The `main` function orchestrates the process by calling `knowledge_navigator` to obtain a candidate answer and source, then calls `fact_checker` to validate and return the final answer."
  }
]

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 3 iterations):
        [
  {
    "iteration": 20,
    "issue": "The single most critical problem is **incorrect answer selection due to a failure to adequately rank or filter potential answers based on contextual information and temporal constraints**. The system is retrieving *some* relevant information but failing to pinpoint the *most accurate* answer within that set."
  },
  {
    "iteration": 21,
    "issue": "The most critical problem is the **overly trusting nature of the solution verification process**, which leads to the validation of incorrect information extracted from search results. This is coupled with a **lack of critical assessment of the search results**."
  },
  {
    "iteration": 22,
    "issue": "The primary issue is **failure in reliable fact retrieval and validation**. The system extracts information from a source but fails to verify its correctness before presenting it as the final answer. This highlights a critical weakness in the system's ability to ground its reasoning in accurate, verifiable facts."
  },
  {
    "iteration": 23,
    "issue": "The most critical problem is **lack of a robust fact-verification mechanism**. The system retrieves or generates an answer without sufficiently checking its accuracy against multiple reliable sources or applying reasonableness checks."
  },
  {
    "iteration": 24,
    "issue": "The most critical problem is **inaccurate fact retrieval.** The system retrieves and presents information that is factually incorrect with respect to the question asked. This is evidenced by the incorrect spouse name in the error case."
  },
  {
    "iteration": 25,
    "issue": "The most critical problem is the **lack of capability to verify and refine answers against common knowledge** or to perform simple logical deductions to arrive at the golden answer if it is not directly stated in the provided source. The current system focuses heavily on retrieving and presenting information directly from the source without critical evaluation or synthesis. Furthermore, there seems to be an issue defining what constitutes semantic equivalence with respect to numerical gold answers when the system response contains supporting details."
  },
  {
    "iteration": 26,
    "issue": "The most critical problem is **inaccurate information retrieval and integration, particularly related to dates, compounded by a lack of reliable source prioritization**. The system needs a more robust mechanism for validating and prioritizing information from different sources before using it to generate answers."
  },
  {
    "iteration": 27,
    "issue": "The most critical problem is the **lack of a robust information verification process**. The system needs to go beyond simply retrieving information from a source and implement strategies for validating its accuracy, such as cross-referencing multiple sources, applying logical constraints, or using common sense reasoning."
  },
  {
    "iteration": 28,
    "issue": "The single most critical problem is the inability to reliably retrieve and validate information required to answer questions. The vague error message \"Could not be validated\" masks the underlying cause, be it a failure in searching, parsing, or verifying the extracted information."
  },
  {
    "iteration": 29,
    "issue": "The single most critical problem is the **failure of the validation component to accurately assess the correctness of candidate answers.** It's reporting \"VALID\" even when the retrieved information is incorrect, indicating a fundamental flaw in its logic and/or the data it uses to perform the validation."
  }
]

        TARGETED IMPROVEMENTS:
        [
  "After generating an initial answer, query multiple knowledge sources (e.g., Wikidata, DBpedia, a custom knowledge base) for the same information.",
  "Semantic similarity between the answer and the question.",
  "The presence of relevant keywords and entities.",
  "Score each source based on its reliability (e.g., based on the number of citations or the reputation of the source).",
  "The fulfillment of any explicit or implicit constraints (e.g., temporal constraints, specific dates).",
  "Compare the answers from different sources. If there is significant disagreement, flag the answer as potentially incorrect and try to find a more reliable answer.",
  "For questions involving numerical values, identify reasonable ranges for the answer. For example, the depth of a lake is unlikely to be more than a few hundred meters.",
  "Use structured data more effectively:** If the underlying knowledge base contains structured data (e.g., tables, knowledge graphs), leverage this structure to improve the accuracy of knowledge retrieval. This involves designing queries that target specific fields and relationships within the structured data.",
  "Reject answers that fall outside of these ranges and try to find a more reasonable answer.",
  "The source reliability for the candidate answer."
]
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity ✓
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity ✓
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity ✓
    4. All operations have O(1) average time complexity ✓
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        

            PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS). YOUR APPROACH MUST BE SUBSTANTIVELY DIFFERENT THAN THESE:
            
PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS):

=== SCRIPT FROM ITERATION 29 (Exploration, ACCURACY: 0.33) ===
Approach: The script uses a multi-agent system with a "Knowledge Navigator" and a "Fact Checker" to answer questions accurately. The Knowledge Navigator iteratively refines search queries and extracts candidate answers and sources, while the Fact Checker validates the answer. The core functions are `call_llm` to interact with the LLM, `knowledge_navigator` to find a candidate answer, and `fact_checker` to validate the answer. The `main` function orchestrates the process by calling `knowledge_navigator` to obtain a candidate answer and source, then calls `fact_checker` to validate and return the final answer.

```python
import os
import re
import math

# Hypothesis: Implement a multi-agent system with a "Knowledge Navigator" that uses iterative refinement and source validation to address the problem of factual accuracy.
# The Knowledge Navigator will:
# 1. Formulate initial search queries
# 2. Evaluate initial search results for relevance and source credibility
# 3. Refine search queries based on initial results and source credibility
# 4. Extract a candidate answer and source
# 5. Validate the candidate answer against external knowledge and internal consistency
# 6. Use a second "Fact Checker" agent to confirm or deny findings.

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def knowledge_navigator(question, max_iterations=3):
    """Navigate knowledge sources to find a reliable answer."""
    system_instruction = "You are a Knowledge Navigator, tasked with finding accurate information from multiple sources. You will analyze, refine, and validate information."

    search_query = question  # Initial search query
    candidate_answer = "No answer found." # initialize the candidate answer
    candidate_source = "None" # initialize the candidate source

    for i in range(max_iterations):
        # Step 1: Simulate search and extract potential answers

        search_results = call_llm(f"Search for: {search_query}", system_instruction="You are a search engine simulator. Provide concise, fact-based answers with source URLs.")

        # Step 2: Evaluate relevance and source credibility
        evaluation_prompt = f"""
        Evaluate these search results for relevance and source credibility.
        Question: {question}
        Search Results: {search_results}

        Example 1:
        Question: What is the capital of Australia?
        Search Results: Canberra is the capital of Australia. Source: wikipedia.org
        Relevance: Very Relevant
        Credibility: High

        Example 2:
        Question: What is the capital of Australia?
        Search Results: A blog post about visiting Sydney, Australia. Source: travelblog.com
        Relevance: Not Relevant
        Credibility: Low

        Relevance:
        Credibility:
        """

        evaluation = call_llm(evaluation_prompt, system_instruction="You are an expert at judging the relevancy and credibility of sources.")

        # Step 3: Extract potential answer

        extract_prompt = f"""
        From these search results, extract a concise answer and source.
        Question: {question}
        Search Results: {search_results}

        Example 1:
        Question: What is the capital of Australia?
        Search Results: Canberra is the capital of Australia. Source: wikipedia.org
        Answer: Canberra, Source: wikipedia.org

        Example 2:
        Question: Babymetal's song "Road of Resistance" charted at what number...?
        Search Results: Road of Resistance peaked at number 22 on the Billboard... Source: billboard.com
        Answer: 22, Source: billboard.com

        Answer:
        """
        extracted_answer = call_llm(extract_prompt, system_instruction="You are an expert answer extractor, focus on accuracy and succinctness.")

        if "Source:" in extracted_answer:
            candidate_answer = extracted_answer.split("Source:")[0].strip()
            candidate_source = extracted_answer.split("Source:")[1].strip()

        # Step 4: Refine search query (only if needed)
        if "Not Relevant" in evaluation:
            search_query = call_llm(f"Refine the search query for: {question}", system_instruction="You are an expert query refiner, use all known info to make queries more specific.")

    return candidate_answer, candidate_source

def fact_checker(question, answer, source):
    """Verify the answer with an external source."""
    system_instruction = "You are a Fact Checker, verifying information against reliable external sources."
    validation_prompt = f"""
    Verify this answer against a reliable external source.
    Question: {question}
    Answer: {answer}
    Source: {source}

    Example 1:
    Question: What is the capital of Australia?
    Answer: Canberra
    Source: wikipedia.org
    Validation: VALID

    Example 2:
    Question: Babymetal's song "Road of Resistance" charted at what number...?
    Answer: 22
    Source: billboard.com
    Validation: VALID

    Validation:
    """

    validation_result = call_llm(validation_prompt, system_instruction)
    return validation_result

def main(question):
    """Solve questions using a Knowledge Navigator and Fact Checker."""
    try:
        # Step 1: Knowledge Navigation

        candidate_answer, candidate_source = knowledge_navigator(question)
        print(f"Candidate answer is {candidate_answer}") # debug
        print(f"Candidate source is {candidate_source}") #debug
        # Step 2: Fact Checking

        validation_result = fact_checker(question, candidate_answer, candidate_source)
        print(f"Validation result is {validation_result}") #debug
        if "VALID" in validation_result:
            return candidate_answer
        else:
            return "Could not be validated."

    except Exception as e:
        return f"Error: {str(e)}"
```

=== SCRIPT FROM ITERATION 28 (Exploration, ACCURACY: 0.00) ===
Approach: This script employs a two-stage retrieval and focused summarization approach, enhanced with a self-reflection mechanism, to answer questions using the Gemini LLM. It decomposes the problem into three distinct stages: initial information retrieval, focused summarization, and self-reflection validation. The script defines the functions `initial_info_retrieval`, `focused_summarization`, and `self_reflection_validation`, each acting as an agent with a specific role (information retrieval, summarization, and validation, respectively), while `call_llm` sends the prompt to the LLM and retrieves a response, and `main` orchestrates the entire process. The `main` function first calls `initial_info_retrieval` to get background information, then passes this and the original question to `focused_summarization`, and finally uses `self_reflection_validation` to validate and answer the question.

```python
import os
import re
import math

# Hypothesis: Implementing a two-stage retrieval and focused summarization approach with self-reflection.
# The first stage retrieves broad information, then the second stage summarizes with respect to the question.
# A self-reflection mechanism validates the answer.

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response."""
    try:
        from google import genai
        from google.genai import types

        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def initial_info_retrieval(question):
    """Retrieve initial information related to the question."""
    system_instruction = "You are an information retrieval expert. Find relevant background information."
    prompt = f"""
    Provide background information that could be useful in answering the question.

    Example:
    Question: What is the capital of Australia?
    Background Information: Australia is a country in the southern hemisphere. Its largest city is Sydney, but its capital is Canberra.

    Question: {question}
    Background Information:
    """
    return call_llm(prompt, system_instruction)

def focused_summarization(question, background_info):
    """Summarize the background information, focusing on the question."""
    system_instruction = "You are a summarization expert, focusing on answering the given question."
    prompt = f"""
    Summarize the background information with respect to the specific question.

    Example:
    Question: What is the capital of Australia?
    Background Information: Australia is a country in the southern hemisphere. Its largest city is Sydney, but its capital is Canberra.
    Focused Summary: The capital of Australia is Canberra.

    Question: {question}
    Background Information: {background_info}
    Focused Summary:
    """
    return call_llm(prompt, system_instruction)

def self_reflection_validation(question, summarized_info):
    """Validate the summarized information and answer the question."""
    system_instruction = "You are a validation expert, verifying the accuracy of the information and answering the question."
    prompt = f"""
    Validate the summarized information and answer the question concisely. Determine if it answers the question accurately, and provide a concise answer. If validation fails respond with 'Could not be validated.'

    Example:
    Question: What is the capital of Australia?
    Summarized Information: The capital of Australia is Canberra.
    Validation and Answer: Canberra

    Question: {question}
    Summarized Information: {summarized_info}
    Validation and Answer:
    """
    validation_result = call_llm(prompt, system_instruction)
    return validation_result

def main(question):
    """Solve questions using two-stage retrieval and self-reflection."""
    try:
        # Initial Information Retrieval
        background_info = initial_info_retrieval(question)

        # Focused Summarization
        summarized_info = focused_summarization(question, background_info)

        # Self-Reflection Validation and Answer
        validation_result = self_reflection_validation(question, summarized_info)

        return validation_result

    except Exception as e:
        return f"Error: {str(e)}"
```

=== SCRIPT FROM ITERATION 27 (Exploration, ACCURACY: 0.00) ===
Approach: This script uses multiple LLM-based agents to answer questions with a layered validation approach. The problem is decomposed into search, answer extraction, and validation steps. The agents include a search engine simulator, an answer extractor, a fact validator, a temporal validator, and a source reliability validator.

The core functions are `call_llm`, `simulate_search`, `extract_answer`, `fact_validator`, `temporal_validator`, `source_reliability_validator`, and `main`. `main` calls `simulate_search` to get results, then `extract_answer` to get an answer and its source. Finally, it calls `fact_validator`, `temporal_validator`, and `source_reliability_validator` to validate the answer.

The workflow involves simulating a search, extracting an answer, and then validating the answer using multiple specialized validators focusing on factuality, temporal consistency, and source reliability, returning the validated answer or error message.

```python
import os
import re
import math

# Hypothesis: Leveraging a layered validation approach with specific validation agents to address fact retrieval and date handling weaknesses.
# This script introduces a new layered validation system with specialized validator agents for fact-checking, temporal consistency, and source reliability.

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def simulate_search(query, engine_id):
    """Simulate different search engines."""
    system_instruction = f"You are a simulated search engine with ID {engine_id} providing factual and CONCISE information. You MUST provide a verifiable source URL at the end of your answer. Be concise."
    prompt = f"""
    Simulate search results for the query: '{query}'.

    Example 1 (Engine ID: 1, Source: Wikipedia):
    Query: capital of Australia
    Search Results: Canberra is the capital of Australia. Source: en.wikipedia.org/wiki/Canberra

    Example 2 (Engine ID: 2, Source: Britannica):
    Query: capital of Australia
    Search Results: Australia's capital is Canberra, located in the Australian Capital Territory. Source: britannica.com/place/Canberra

    Query: {query}
    Search Results:
    """
    return call_llm(prompt, system_instruction)

def extract_answer(question, search_results):
    """Extract potential answers from search results."""
    system_instruction = "You are an answer extraction expert, focusing on precision. You MUST extract the concise answer and the source URL from the provided search results."
    prompt = f"""
    Extract the concise answer and its source from the search results.

    Example:
    Question: What is the capital of Australia?
    Search Results: Canberra is the capital of Australia. Source: en.wikipedia.org/wiki/Canberra
    Answer: Canberra Source: en.wikipedia.org/wiki/Canberra

    Question: {question}
    Search Results: {search_results}
    Answer:
    """
    return call_llm(prompt, system_instruction)

def fact_validator(question, answer):
    """Validates the answer for factual correctness."""
    system_instruction = "You are a strict fact validator. Determine if the provided answer is factually correct based on your knowledge. If not, provide corrected answer."
    prompt = f"""
    Question: {question}
    Answer: {answer}
    Is this answer factually correct? If not, provide a corrected answer.

    Example:
    Question: What is the capital of France?
    Answer: London
    Validation: INCORRECT. The capital of France is Paris.

    Question: {question}
    Answer: {answer}
    Validation:
    """
    return call_llm(prompt, system_instruction)

def temporal_validator(question, answer):
    """Validates the answer for temporal consistency."""
    system_instruction = "You are an expert in temporal validation. Ensure the answer is temporally consistent with the question and the context. Provide feedback if dates don't align."
    prompt = f"""
    Question: {question}
    Answer: {answer}
    Is this answer temporally consistent? If not, provide the reasoning and a corrected date if available.

    Example:
    Question: What year did World War II begin?
    Answer: 1930
    Temporal Validation: INCORRECT. World War II began in 1939.

    Question: {question}
    Answer: {answer}
    Temporal Validation:
    """
    return call_llm(prompt, system_instruction)

def source_reliability_validator(question, answer):
    """Validates the reliability of the source."""
    system_instruction = "You are an expert at source validation. Assess if the provided source is reliable and trustworthy."
    prompt = f"""
    Question: {question}
    Answer: {answer}
    Assess the reliability of the source. State whether is it deemed reliable or not and why.

    Example:
    Question: What is the capital of Australia?
    Answer: Canberra Source: en.wikipedia.org/wiki/Canberra
    Source Validation: RELIABLE. Wikipedia is a generally reliable source of information.

    Question: {question}
    Answer: {answer}
    Source Validation:
    """
    return call_llm(prompt, system_instruction)

def main(question):
    """Solve questions using multiple search engines and layered validation."""
    search_results = simulate_search(question, 1)
    answer_with_source = extract_answer(question, search_results)

    fact_validation = fact_validator(question, answer_with_source)
    temporal_validation = temporal_validator(question, answer_with_source)
    source_validation = source_reliability_validator(question, answer_with_source)

    if "INCORRECT" in fact_validation:
        return fact_validation
    if "INCORRECT" in temporal_validation:
        return temporal_validation
    if "NOT RELIABLE" in source_validation:
        return source_validation
    else:
        return answer_with_source
```

=== SCRIPT FROM ITERATION 26 (Exploitation, ACCURACY: 0.00) ===
Approach: The script employs multiple LLM-based techniques, including simulating multiple search engines and answer reconciliation, to improve fact-checking. The problem is decomposed into simulating search results, extracting answers, reconciling them, and finally validating the reconciled answer. Several agent roles are defined through system instructions (search engine, answer extraction expert, reconciliation expert, validator). Other functions used include `call_llm`, which interfaces with the Gemini model. The script uses the following functions: `call_llm` is used to interact with the Gemini model, `simulate_search` simulates search engine results, `extract_answer` extracts the answer from the search results, `reconcile_answers` reconciles the answers from different search engines, and `validate_answer` validates the reconciled answer. The overall workflow involves simulating multiple searches, extracting answers from each, reconciling those answers, and validating the final reconciled answer.

```python
import os
import re
import math

# This script improves fact-checking by enhancing search simulation, answer reconciliation, and validation.
# It builds upon the success of multiple simulated search engines (Approach #1, Iteration 22).
# To address weaknesses, it integrates source reliability and numerical range validation.

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response."""
    try:
        from google import genai
        from google.genai import types

        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                config=types.GenerateContentConfig(system_instruction=system_instruction),
                contents=prompt
            )
        else:
            response = client.models.generate_content(model="gemini-2.0-flash", contents=prompt)

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def simulate_search(query, engine_id):
    """Simulate different search engines with source reliability."""
    system_instruction = f"You are a simulated search engine with ID {engine_id} providing factual and CONCISE information. Include a reliability score (1-10, 10=highest) for each piece of information. Your search results might be slightly biased or incomplete."
    prompt = f"""
    Simulate search results for the query: '{query}'. Include source reliability.

    Example 1 (Engine ID: 1):
    Query: capital of Australia
    Search Results: Canberra is the capital of Australia. (Reliability: 9)

    Example 2 (Engine ID: 2):
    Query: capital of Australia
    Search Results: Australia's capital is Canberra, located in the Australian Capital Territory. (Reliability: 8)

    Query: {query}
    Search Results:
    """
    return call_llm(prompt, system_instruction)

def extract_answer(question, search_results):
    """Extract potential answers and reliability from search results."""
    system_instruction = "You are an answer extraction expert, focusing on precision and source reliability."
    prompt = f"""
    Extract the concise answer and its reliability score from the search results.

    Example:
    Question: What is the capital of Australia?
    Search Results: Canberra is the capital of Australia. (Reliability: 9)
    Answer: Canberra (Reliability: 9)

    Question: {question}
    Search Results: {search_results}
    Answer:
    """
    return call_llm(prompt, system_instruction)

def reconcile_answers(question, answers):
    """Reconcile answers from different engines, considering reliability."""
    system_instruction = "You are an expert at reconciling conflicting answers, considering source reliability, and determining the most accurate answer."
    all_answers = "\n".join([f"Engine {i+1}: {answer}" for i, answer in enumerate(answers)])
    prompt = f"""
    Reconcile these answers from different sources, considering reliability, to answer the question.

    Example:
    Question: What is the capital of Australia?
    Engine 1: Canberra (Reliability: 9)
    Engine 2: Canberra is the capital city. (Reliability: 8)
    Reconciled Answer: Canberra

    Question: {question}
    {all_answers}
    Reconciled Answer:
    """
    return call_llm(prompt, system_instruction)

def validate_answer(question, reconciled_answer):
    """Validate the reconciled answer, including numerical range checks where applicable."""
    system_instruction = "You are a strict validator, focusing on factual correctness and reasonable numerical ranges."
    prompt = f"""
    Validate if the reconciled answer is correct for the question. Consider numerical ranges for reasonableness.

    Example 1:
    Question: What is the capital of Australia?
    Answer: Canberra
    Validation: VALID - Canberra is the capital of Australia.

    Example 2:
    Question: How many years was the Legacy of Walt Disney museum open at Disneyland, CA?
    Answer: 3
    Validation: VALID - 3 years is a reasonable timeframe for a museum exhibit.

    Question: {question}
    Answer: {reconciled_answer}
    Validation:
    """
    validation_result = call_llm(prompt, system_instruction)
    return validation_result

def main(question):
    """Solve questions using multiple search engines, answer reconciliation, and validation."""
    num_engines = 3
    answers = []

    # Simulate search with multiple engines
    for i in range(num_engines):
        search_results = simulate_search(question, i+1)
        answer = extract_answer(question, search_results)
        answers.append(answer)

    # Reconcile answers
    reconciled_answer = reconcile_answers(question, answers)

    # Validate answer
    validation_result = validate_answer(question, reconciled_answer)

    if "VALID" in validation_result:
        return reconciled_answer
    else:
        return "Could not be validated."
```

=== SCRIPT FROM ITERATION 25 (Exploration, ACCURACY: 0.33) ===
Approach: The script employs a multi-agent system to answer questions by simulating multiple search engines and reconciling their results, ultimately validating the final answer. It uses the `call_llm` function to interact with the Gemini LLM, assigning specific roles to different agents via system instructions. The problem is decomposed into simulating search (`simulate_search`), extracting answers (`extract_answer`), reconciling answers (`reconcile_answers`), and verifying sources (`source_verifier`). The `main` function orchestrates the workflow, calling `simulate_search` to get results, `extract_answer` to extract the answers from the search results, `reconcile_answers` to get a final answer, and `source_verifier` to validate the answer.

```python
import os
import re
import math

# Hypothesis: Enhancing Fact-Checking with Multi-Agent Collaboration and Explicit Source Identification
# We will use multi-agent collaboration with explicit source identification for enhanced fact-checking and answer extraction.
# Specifically, we will introduce a "Source Verifier" agent that cross-references information from multiple sources
# and prioritizes answers based on source credibility. This approach aims to improve answer extraction and validation.

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def simulate_search(query, engine_id):
    """Simulate different search engines."""
    system_instruction = f"You are a simulated search engine with ID {engine_id} providing factual and CONCISE information. You MUST provide a verifiable source URL at the end of your answer, or respond with 'No Results'. Be concise."
    prompt = f"""
    Simulate search results for the query: '{query}'.

    Example 1 (Engine ID: 1, Source: Wikipedia):
    Query: capital of Australia
    Search Results: Canberra is the capital of Australia. Source: en.wikipedia.org/wiki/Canberra

    Example 2 (Engine ID: 2, Source: Britannica):
    Query: capital of Australia
    Search Results: Australia's capital is Canberra, located in the Australian Capital Territory. Source: britannica.com/place/Canberra

    Example 3 (Engine ID: 3, No Results):
    Query: life expectancy of a hamster on Mars
    Search Results: No Results

    Query: {query}
    Search Results:
    """
    return call_llm(prompt, system_instruction)

def extract_answer(question, search_results):
    """Extract potential answers from search results."""
    system_instruction = "You are an answer extraction expert, focusing on precision. You MUST extract the concise answer and the source URL from the provided search results."
    prompt = f"""
    Extract the concise answer and its source from the search results.

    Example:
    Question: What is the capital of Australia?
    Search Results: Canberra is the capital of Australia. Source: en.wikipedia.org/wiki/Canberra
    Answer: Canberra, Source: en.wikipedia.org/wiki/Canberra

    Question: {question}
    Search Results: {search_results}
    Answer:
    """
    return call_llm(prompt, system_instruction)

def reconcile_answers(question, answers):
    """Reconcile answers from different engines."""
    system_instruction = "You are an expert at reconciling conflicting answers from different sources and determining the most accurate answer. You MUST provide a final answer and the source URL."
    all_answers = "\n".join([f"Engine {i+1}: {answer}" for i, answer in enumerate(answers)])
    prompt = f"""
    Reconcile these answers from different sources to answer the question.

    Example:
    Question: What is the capital of Australia?
    Engine 1: Canberra, Source: en.wikipedia.org/wiki/Canberra
    Engine 2: Canberra is the capital city, Source: britannica.com/place/Canberra
    Reconciled Answer: Canberra, Source: en.wikipedia.org/wiki/Canberra

    Question: {question}
    {all_answers}
    Reconciled Answer:
    """
    return call_llm(prompt, system_instruction)

def source_verifier(question, reconciled_answer):
    """Validate the reconciled answer."""
    system_instruction = "You are a strict validator, focusing on factual correctness and source credibility. The source should have a verifiable link. You must respond with 'VALID: [answer] [source]' or 'INVALID: [reason]'."
    prompt = f"""
    Validate if the reconciled answer is correct for the question and check the source's credibility.
    
    Example:
    Question: What is the capital of Australia?
    Answer: Canberra, Source: en.wikipedia.org/wiki/Canberra
    Validation: VALID: Canberra, Source: en.wikipedia.org/wiki/Canberra

    Question: {question}
    Answer: {reconciled_answer}
    Validation:
    """
    validation_result = call_llm(prompt, system_instruction)
    return validation_result

def main(question):
    """Solve questions using multiple search engines and answer reconciliation."""
    num_engines = 3
    answers = []

    # Simulate search with multiple engines
    for i in range(num_engines):
        search_results = simulate_search(question, i+1)
        answer = extract_answer(question, search_results)
        answers.append(answer)

    # Reconcile answers
    reconciled_answer = reconcile_answers(question, answers)

    # Validate answer
    validation_result = source_verifier(question, reconciled_answer)

    if "VALID" in validation_result:
        return validation_result.split("VALID: ")[1]
    else:
        return "Could not be validated."
```


            LEARNINGS FROM PREVIOUS ITERATIONS:
            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        Okay, here's the updated and synthesized research log, incorporating the new learnings from Iteration 29, focusing on maintaining a comprehensive and actionable record of our work with this specific dataset while staying within the token limit. I've condensed redundant sections and prioritized concrete insights.

## 1. DATASET PATTERNS & CHARACTERISTICS

*   **Factual and Specific Questions:** Questions are factual and seek specific information across a wide range of topics. Require recalling specific factual information, sometimes involving multiple parts (first, middle, last names).
*   **Emphasis on Proper Nouns and Named Entities:** Questions frequently contain proper nouns, specific dates, and locations, making them amenable to information retrieval. Answers are often short phrases. Need to handle generational titles correctly (e.g., "James Vernon the Younger"). Many questions center around entities like people, places, creative works, organizations (universities), and biological classifications (genus, species). The questions often directly ask *for the name of* these things. Examples: "Who was the first cardiologist in Kashmir?" "Who murdered the supervillain Monsieur Mallah?". Entity Recognition and Disambiguation are Key.
*   **Temporal Specificity (CRITICAL):** Questions often require precise temporal information (month, year, range of years), with many questions requiring events within a specific year or even date (e.g., "February 27, 2022"). The dataset is brittle with even small discrepancies leading to incorrect responses. Temporal Reasoning required. Questions frequently involve ordinal numbers (e.g., 21st) or specific years, which should be critical signals. **Date-centric questions are prevalent, requiring precise dates (day, month, year). Synonymity (different date formats) and ambiguity are present.** Example: Kunming Metro's Line 6 opening date requires "23 September 2020". **Small differences (e.g., 1940 vs. 1941, June vs. October) are critical.**
*   **Extraction of Specific Dates/Names From Historical Contexts:** The questions often require extracting specific dates or names from historical contexts ("month, day, and year of philosopher Edmund Burke's last day in office").
*   **"Which" Questions Targeting Specific Individuals/Entities with Constraints:** Several questions begin with "Which" and seek a specific individual or entity associated with a particular role or accomplishment, often including specific constraints related to entities, timeframes, or categories (e.g., "Which architect was tasked with finishing the chapel...?"). The constraints are crucial to identifying the correct answer.
*   **"Specific Instance" Questions:** The questions frequently ask for the specific instance of a general category ("Which *specific* ocean was the asteroid 224 Oceana named after?"). This requires high precision.
*   **Complex Noun Phrases:** The questions often include complex noun phrases that require accurate parsing and understanding (e.g., "Aomori University Men’s Rhythmic Gymnastics Team").
*   **Question Types:** Primarily fact-retrieval questions asking for specific details about people, places, or things ("Who...", "What...", "Name of...", "Which...").
*   **Need for Precise Factual Recall:** Questions require precise factual recall, often involving dates, numbers, or names (e.g., "Champions League semi-final match between Barcelona and Milan on April 27, 2006", "What month and year did Apple release the Hey Siri feature?"). The Babymetal question requires understanding the song title, the specific chart, and the date.
*   **Contextual Specificity:** Questions often contain specific contextual details to narrow the scope of the answer. Complex event queries require integrating information from multiple sources (e.g., sports matches, product announcements). Questions rely on specific contextual information such as research paper titles, authors, and publication years.
*   **Information Extraction Task:** The dataset tests the ability to extract *very specific* pieces of information from potentially larger contexts. It's not simply about broad topic understanding. **Many questions require answers in a highly structured format (e.g., "day, month, year").**
*   **Definitive Answers Expected:** The questions expect precise answers (specific dates, names), not general descriptions or related concepts.
*   **Complex Factual Questions:** The dataset contains complex factual questions that often require multi-hop reasoning or accessing multiple pieces of information. Questions often involve multiple entities or relationships (e.g., "the younger daughter of Mehbooba Mufti"). This requires the system to correctly identify and relate these elements to retrieve the final answer.
*   **Multi-part Answers Common:** Several questions require multi-part answers, such as specifying both the award and the category (e.g., "Which award and in which category..."). This necessitates the system to identify and extract multiple pieces of information that are linked together.
*   **Entity Specificity:** Many questions revolve around specific entities like people (e.g., "Anita Sandoval"), projects ("Project Firebreak"), awards ("ISCB Accomplishment by a Senior Scientist Award"), and products ("Horizon Zero Dawn"). The correctness depends heavily on identifying these entities accurately. **Questions often involve specific entities (e.g., a particular praying mantis species, a specific politician).**
*   **Indirect Relationships:** Questions often involve indirect relationships. The answer isn't directly stated but needs to be inferred from the provided information.
*   **Broad Subject Matter:** The questions are varied in subject matter, ranging from history and geography to academia and linguistics, politics, zoology, and performing arts. This requires a broad knowledge base and adaptable information retrieval.
*   **Fact-Seeking and Precise Information:** The questions are fact-seeking, requiring precise information retrieval (dates, ages, publication details).
*   **Name Disambiguation:** Many questions revolve around specific individuals, requiring accurate identification and disambiguation (e.g., Satyanarayan Gangaram Pitroda). The system needs to differentiate between similar names. Questions involving people's names (spouse of Silas A. Holcomb) require disambiguation.
*   **Complex Attribute Retrieval:** The task involves retrieving specific attributes (e.g., award received, position held, professorship duration) associated with entities.
*   **Fact-based, Detail-Oriented Questions:** The questions require precise factual recall, often related to dates, names, or specific versions/patches. Even slight inaccuracies are penalized.
*   **Subject Matter Specificity:** The questions span diverse domains (history, video games, academia), requiring a broad base of knowledge or the ability to quickly acquire it. The "Terraria" example highlights the need for specialized knowledge.
*   **Implicit Context:** Some questions require understanding implicit context or relationships. Simply extracting keywords is insufficient.
*   **Numerical Data:** The dataset contains fact-based questions requiring precise answers, often involving numerical data (e.g., resolution in pixels). Numerical Precision Matters.
*   **Niche Knowledge Domain:** The questions cover very specific areas (video game mechanics, advanced mathematics, historical events in specific fields).
*   **Complex Sentence Structure:** The questions can have complex sentence structures involving conditions, mathematical notation and specific terminology.
*   **Potential for Contradictory Information:** Search results might contain contradictory information (e.g., stating "no survivors" versus a specific name).
*   **Communist Party Emphasis:** Several questions involve the Communist party, which might indicate a bias or specific focus within the dataset.
*   **Unit Awareness:** The questions explicitly specify the units required in the answer (e.g., meters). The system must not only find the correct numerical value but also ensure it is expressed in the correct unit.
*   **Numerical Reasoning Required:** Several questions require numerical reasoning or calculation based on provided context (e.g., "How many years was the Legacy of Walt Disney museum open..."). The system needs to extract relevant dates/numbers and perform calculations.
*   **Implied Answers/Common Knowledge:** Many questions expect the model to leverage common knowledge or perform basic inference to arrive at the answer. The answer isn't always explicitly stated in the source (e.g., how mayors are elected).
*   **Diverse Information Types:** The questions target a wide range of information types, including dates, numbers, names of instruments, and potentially less common information (e.g., Kunming Metro's Line 6 opening date). This requires the system to be adaptable to different types of facts.

## 2. EFFECTIVE TASK-SPECIFIC STRATEGIES

*   **Current Accuracy Very Low:** With a maximum accuracy of 0.67 (Iteration 22) no current strategies can be considered effectively "working" consistently. The underlying framework shows potential but requires significant refinement.
*   **RAG Framework Insufficient:** The initial hypothesis that a basic RAG framework with Gemini is sufficient for this task is rejected.
*   **Fact Verification Approach Needs Active Information Seeking**: The fact verification approach with multi-source integration shows promise, since it decomposes the problem into generating multiple search queries, simulating context retrieval from these queries while verifying their relevance, extracting answers from each context, synthesizing a final answer, and validating that final answer. However, this strategy needs an active information-seeking loop.
*   Decomposing the task into sub-tasks such as information extraction, search query generation, answer extraction, and validation seems like a reasonable approach and may provide a good starting point if further refined.
*   **Concept Expansion is Insufficient:** The approach's reliance on concept expansion alone isn't sufficient to guarantee accurate answer extraction.
*   **NONE:** The accuracy in most experiments is 0.00, suggesting no part of the current strategy is effective.
*   **Knowledge Base Selection:** The knowledge base selection *seems* to be functioning reasonably well (but needs further validation).
*   **Decomposition Potential:** Using LLMs to decompose the problem into KB selection, query generation, answer extraction, and fact verification shows potential as a framework.
*   **Decomposition Isn't Always Helpful:** Decomposing the question into sub-questions without a robust mechanism for ensuring that the sub-questions are helpful and comprehensively answered can lead to failures. LLM confidence scores are not reliable.
*   **Simulated Search Diversity (Potentially):** The core idea of using multiple simulated search engines with different biases has the *potential* to be a strength. However, it's only effective if the simulated biases are truly diverse and relevant to the questions asked.
*   **Source Verification Alone is Insufficient:** Merely verifying sources isn't enough. The system needs the ability to synthesize information, perform logical deductions, and leverage common knowledge to arrive at the correct answer.
*   **"Exploitation" strategy failure:** Focusing on building upon previous successes resulted in 0.00 accuracy, suggesting that current methods are not robust enough or not generalizable enough.
*   **(Iteration 27, 28, 29):** No consistently working strategies were observed.

## 3. COMMON FAILURE MODES ON THIS DATASET

*   **Incorrect Entity Association:** The system retrieves information related to the correct event but attributes it to the wrong entity. Example: Identifying a different winner of the Eddington Medal (George W. Wetherill instead of Leon Mestel). Entity Confusion is a key failure mode. *In Iteration 24, the system retrieved *a* spouse's name but not the *correct* spouse's name (Martha Alice Brinson) for the Holcomb example.* The LLM hallucinates a plausible but incorrect answer.
*   **Incorrect Fact Retrieval:** The system struggles to retrieve the correct facts from sources, leading to mismatches between the expected and actual answers (Gliese 146, Karl Polanyi examples). The primary failure mode (Iteration 28) is the inability to reliably retrieve the correct information, leading to "Could not be validated" errors.
*   **Inability to Extract Precise Information:** The system often fails to extract the *exact* piece of information required to answer the question.
*   **Insufficient Answer Granularity:** The system often returns answers that are correct but lack the precision required. E.g., answering "2023" when the expected answer was "Sep 07, 2023."
*   **Inability to Perform Temporal Reasoning:** The system fails when a question requires comparing or understanding dates. Example: The Edmund Burke question.
*   **Lack of Granularity and Specificity:** The system provides overly general answers when specific details are needed. Example: The Oceana question where "the ocean" was returned instead of "Pacific Ocean."
*   **Incorrect Granularity:** The system sometimes provides answers that are either too broad or too narrow.
*   **Inference Limitations:** Questions requiring inference based on the knowledge graph structure consistently fail.
*   **Validation Sensitivity INSUFFICIENT (CRITICAL):** Validation often fails to catch incorrect answers, especially on dates. The `fact_checker` fails to recognize discrepancies in dates or years, even when the retrieved answer is numerically close to the correct answer (e.g., validating "1941" when the expected answer is "1940").
*   **Lack of Contextual Understanding During Validation:** The validator likely struggles to relate the retrieved information to the specific constraints in the question. Contextual misinterpretation occurs in event queries. Exact Match Validation Failure: The validation stage requires an exact match, leading to failures even with near-correct answers. The validator needs to be robust to handle different date formats, synonymity, and numeric tolerance. **Erroneous validation of alternative facts occurs.**
*   **Dependency on Perfect Information Retrieval:** The system's reliance on simulated retrieval means a slightly off search query can lead to no relevant information.
*   **Inability to resolve conflicting information.** The system needs a more robust method for selecting the correct answer when multiple possibilities are presented in the search context.
*   **Failure to identify the correct entity in search results.** The system struggles with identifying the correct entities when multiple entities might fit the search query. The system doesn't appear to be able to accurately resolve ambiguous search results.
*   **Poor Simulated Search Results:** The low accuracy reinforces the possibility that the simulated search results are not accurate or relevant enough, hindering both information extraction and validation.
*   **Inaccurate Entity Resolution:** The "James Vernon the Younger" example highlights the need to handle generational titles correctly.
*   **Knowledge Graph Misidentification/Hallucination:** The "ISCB Accomplishment by a Senior Scientist Award" example showcases a failure to retrieve the correct information from the knowledge source.
*   **Information Extraction Bottleneck:** The primary failure mode is the inability of the system to extract the correct answer from the provided context. The system struggles to translate expanded concepts into precise, factual answers. **The system struggles to extract and present answers in the precise format requested by the question (e.g., "day, month, year").**
*   **Insufficient Context Detection Failure:** The agent struggles when the initially retrieved context (simulated or otherwise) lacks the information required to answer the question.
*   **Passive Behavior Regarding Missing Information:** The system's most significant failure mode is its inability to actively address information gaps.
*   **Lack of Numerical Precision:** The James Vernon example demonstrates a failure in matching numerical answers.
*   **Inability to handle ambiguity and semantic equivalence.**
*   **Poor Query Generation:** The generated search queries consistently fail to retrieve the necessary information.
*   **Incorrect Entity Attribution:** The system incorrectly attributes information to the wrong entity. This highlights a failure in entity recognition and linking.
*   **Temporal Reasoning Errors:** The system struggles with temporal reasoning, such as determining the start and end years of a professorship.
*   **Inability to Handle Partial Information:** Even when the system extracts *some* relevant information, it fails if it cannot find *all* the required pieces.
*   **Ineffective Search Query Formulation:** The core failure stems from the inability to generate search queries that retrieve relevant information.
*   **Lack of Domain Specificity in Search:** The search queries appear to lack the domain-specific nuance required.
*   **Failure to Handle Implicit Context:** The Valiant example illustrates that the search strategy cannot infer the necessary contextual information to formulate an effective search query.
*   **Answer Extraction Failure:** A common failure is returning "No Answer" even when related concepts are identified. This points to a weakness in the answer extraction stage.
*   **Lack of Information Retrieval Linkage:** Even when expanded concepts are relevant, the system fails to connect them to the information needed to answer the question.
*   **Incorrect Precision:** The system struggles with providing answers that match the precise detail required by the dataset (e.g., resolution in pixels).
*   **Inability to Extract Specific Temporal Data:** The system failed to extract the exact date of the Hubble Telescope incident, instead providing general information about anomalies. This often happens if the context is murky or if multiple dates are present.
*   **Insufficient Fact Verification:** The system doesn't effectively cross-reference information from multiple sources to validate the accuracy of its answers.
*   **Inability to extract exact dates/years:** The primary failure is the inability to pinpoint the specific year or date from the simulated search results. The LLM often provides a range or an approximation (e.g., "likely 2005 or later") instead of the exact answer.
*   **Insufficient precision in information extraction:** Even when relevant information is present in the "retrieved" text, the system fails to extract the specific answer.
*   **Reliance on Simulated Results:** Since the search results are simulated, any errors in the simulation are passed down the chain.
*   **Incorrect Fact Retrieval and Ranking:** LLM retrieves/ranks incorrect facts. Information retrieval and fact prioritization need improvement.
*   **Inability to Find Relevant Facts:** LLM fails to find the correct answer, indicating limitations in search strategy or knowledge base.
*   **Synthesis of Incorrect Information:** The synthesis stage does not correct misinformation provided in the extracted facts.
*   **Lack of Robust Verification:** Fact verification fails, as incorrect answers are confidently presented.
*   **Contextual Misdirection (Location):** The system incorrectly identified "Winterthur" as the location. The system needs improved filtering of relevant vs. irrelevant location information.
*   **Numerical Extraction Failure (Version Number):** The system failed to extract the version number "1.1" related to the Terraria patch.
*   **General Numerical Extraction:** The system demonstrates a weakness in reliably extracting specific numerical values from text.
*   **Unit Conversion and Precision:** The system failed to provide the correct wingspan for *Eugnosta misella* because it either retrieved the wrong information or failed to convert the units correctly.
*   **Incomplete Sub-question Answering:** The system failed to answer the IJCAI award question because the LLM agent was unable to provide the sub-questions with adequate information to retrieve the answer.
*   **Inability to Disambiguate Related but Incorrect Information:** The "Avatar" question demonstrates the system's failure to differentiate between related but incorrect information.
*   **Insufficient Error Handling when Search Fails:** In the "college" question, when the search fails to return the correct answer, the system defaults to "I cannot answer the question."
*   **Failure to Retrieve Precise Numerical Data:** The system struggles to accurately retrieve and report numerical values. Numerical Tolerance is needed.
*   **Insufficient Information Retrieval for Niche Topics:** The system fails when questions require information from specialized or less readily available sources.
*   **Inability to Verify Semantic Equivalence with Numerical Values:** The "explanation" field in the failed examples indicates a specific problem with semantic equivalence in the context of numerical values.
*   **"No Answer" Response:** The system sometimes defaults to "No answer" even when a correct answer exists within its knowledge base (or could be found). This indicates an issue with confidence or a failure to properly search and retrieve relevant information.
*   **Numerical Discrepancy Failure:** Providing incorrect numerical answers when the correct answer is a specific year (e.g., answering "1996" when the correct answer is "1994").
*   **Lack of Verifiable Source:** The system relies on the LLM's generated search results which are simulated. The LLM doesn't check against any authoritative source, and trusts it's own synthetic output.
*   **Lack of Disambiguation:** The system doesn't adequately differentiate between similar entities or contexts.
*   **Failure to perform numerical reasoning:** The model fails to extract the relevant years and compute the difference in the "Legacy of Walt Disney museum" question. It extracts information but doesn't perform the required calculation.
*   **Reliance on direct extraction, not inference:** The model struggles with questions that require inference or the application of common knowledge. In the "Toronto mayors" example, the model correctly identifies that the source doesn't explicitly state who elected the mayors, but it fails to infer that mayors are typically elected by "the public".
*   **Inability to Verify and Refine Answers:** The system cannot critically evaluate or synthesize information to arrive at the correct answer. It retrieves and presents information directly from the source, even if that information is insufficient or indirectly relevant.
*   **Conflicting Dates and Poor Reconciliation:** The system struggled with questions where different simulated search results provided conflicting dates, failing to reconcile them correctly, as demonstrated in the Popoff example. It defaulted to listing all possibilities rather than determining the correct one.
*   **Incorrect Date Extraction:** Even when sources agreed, the system sometimes extracted the wrong date or a date associated with a related but incorrect event. The KÃ¶ln Frankfurter StraÃŸe station example clearly shows the system extracted a different date than the expected one, indicating failures in the extraction process itself.
*   **No Source Reliability Prioritization:** All sources appeared to be treated equally, even though some sources are inherently more reliable than others. This led to the model being unable to discern correct information when conflicting information appeared.
*   **Date Extraction/Handling Errors:** The system exhibits difficulties extracting and providing accurate dates, as seen in the Mario-Rafael Ionian example. This suggests issues with the temporal validator or the answer extraction process when dealing with dates.
*   **Lack of Cross-Referencing:** The system isn't validating information across multiple sources.
*   **Parsing and Extraction Failure:** Even if the retrieval component finds relevant documents, the focused summarization and subsequent extraction are likely failing to extract the correct entities (date, number, instrument) from the text. This is evident because the validation consistently fails.

## 4. EXPERIMENT LOG & FINDINGS

*   **Experiment 0-28:** *\[Previous Experiment Log entries, see original document for full list. These have been condensed to save space, but the core findings remain represented in the sections above.]*
*   **Experiment 29:** Strategy: Using a "Knowledge Navigator" and "Fact Checker" to improve accuracy. Result: 0.33 accuracy. Finding: The hypothesis of using a "Knowledge Navigator" and "Fact Checker" to improve accuracy is not supported by these results. The validation component's failure undermines the entire approach. The iterative refinement process is not leading to accurate answers.

## 5. NEXT RESEARCH DIRECTIONS

*   **Improve Information Retrieval Accuracy (CRITICAL):** The immediate and primary focus should be on improving the accuracy of the `initial_info_retrieval` function. The retrieval stage is a significant bottleneck. This might involve:
    *   **Query Expansion:** Use LLM to generate multiple search queries from a single question.
    *   **Contextualized Search:** Incorporate the question context into the search query to refine the search results.
    *   **Specialized Search Engines:** Explore using specialized search engines or APIs relevant to specific question types (e.g., a music database for the Babymetal question).
    *   **Fine-tuning embedding model:** If using embeddings, fine-tune the model on question/answer pairs from this dataset.
*   **Implement Entity Disambiguation (CRITICAL):** Integrate an entity disambiguation module into the retrieval process. This module should identify the specific entity being queried and filter the knowledge base results accordingly. **Improve initial search queries to include details that help to clarify the entity that is the subject of the question.**
*   **Enhance Answer Extraction with Cross-Validation (CRITICAL):** Implement a more robust answer extraction mechanism that considers multiple sources and cross-validates information *before* presenting a final answer. This should include explicitly comparing information from different sources and resolving discrepancies.
*   **Implement External Validation (CRITICAL):** Augment the `validate_answer` function to compare the LLM's answer with a reliable external knowledge base (e.g., Wikidata, a fact-checked database) to verify its accuracy *after* reconciliation. This is crucial for addressing the lack of verifiable sources.
*   **Refine Temporal Validation (CRITICAL):** Improve the temporal validator to handle date extraction and comparison more accurately. This may involve using more advanced date parsing techniques or incorporating external knowledge sources about date formats and conventions. Specifically, make sure the validator can handle different date formats and can identify conflicting dates from different sources.
*   **Implement Flexible Answer Validation:** Revise the `self_reflection_validation` function to allow for some degree of flexibility in matching the expected answer. This may involve:
    *   **Date Normalization:** Implement date normalization to handle different date formats.
    *   **Synonym Handling:** Use a synonym database or LLM to identify equivalent answers.
    *   **Numeric Tolerance:** Allow for small numerical differences (e.g., "approximately 22" should match "22").
*   **Focus Summarization on Entity Extraction:** Modify the `focused_summarization` function to specifically extract the entity (date, number, name) required to answer the question. Use a more robust method to parse the document, with clear entity definitions.
*   **Add intermediate logging:** Add logging to inspect the outputs of each stage (`initial_info_retrieval`, `focused_summarization`, `self_reflection_validation`) to better understand where the failures are occurring and why. Specifically log the retrieved documents or snippets, the extracted entities, and the validation logic.
*   **Strengthen Source Reliability Validation:** Enhance the source reliability validator to more accurately assess the trustworthiness of sources.
*   **Implement Numerical Reasoning Module:** Introduce a module specifically designed for numerical reasoning, including the ability to extract numerical data, perform calculations, and compare results.
*   **Incorporate Common Knowledge Base:** Integrate a common knowledge base that the system can access to answer questions requiring general knowledge or typical assumptions (e.g., mayors are elected by the public).
*   **Introduce an Inference/Deduction Step:** Add a step where the system attempts to infer the answer based on the extracted information and the common knowledge base. This could involve simple logical deductions.
*   **Improve Search Simulation Fidelity:** Enhance the `simulate_search` function to generate more realistic and diverse search results.
*   **Focus on Numerical Answer Accuracy:** Implement specific error handling for questions requiring numerical answers. If the reconciled answer is a number, add a step to double-check its validity against a range of possible values or known dates related to the question's topic. **The high error rate with year-based answers (Iteration 22) requires immediate attention.**
*   **Implement a Verification Loop:** Consider a feedback loop where the validators can trigger a re-search if the initial answer is deemed unreliable. This would allow the system to iteratively refine its answer until it meets the validation criteria.
*   **Revamp the Validation Logic:**
    *   Implement stricter numerical and temporal comparison in the `fact_checker`. For date-related questions, use date parsing libraries to compare dates directly rather than relying on string matching.
    *   Add a negative constraint capability: ensure extracted answers *do not* contain contradictory information.
*   **Improve Answer Formatting:**
    *   Add a post-processing step to the `fact_checker` that formats the answer according to the question's requirements. Use regular expressions or dedicated parsing libraries to extract and reformat the answer.
        

            CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        

            EXPLORATION GUIDANCE:
            1. Review the historical approaches, error patterns, and accumulated learnings carefully
            2. Review the FULL CODE of previous scripts to understand what has already been tried
            3. Design a new approach that is DISTINCTLY DIFFERENT from previous attempts. This approach should have a specific NEW HYPOTHESIS or variable you are trying to test. 
            4. CRITICAL: Include EMBEDDED EXAMPLES directly within your LLM prompts
            5. For each key function, show a complete worked example, or include multiple examples, including:
               - Input example that resembles the dataset
               - Step-by-step reasoning through the example
               - Properly formatted output
            6. Apply the insights from the ACCUMULATED LEARNINGS section to avoid repeating past mistakes
            7. Pay SPECIAL ATTENTION to the weaknesses and improvement suggestions from the capability assessment
            8. Consider implementing one or more of these LLM usage patterns:
               - Repeated validation with feedback loops
               - Multi-perspective analysis with synthesis
               - Dynamic input-dependent routing with an orchestrator
               - Hybrid approaches combining LLM with deterministic functions
               - Best-of-n solution generation and selection
               - ReAct pattern for interactive reasoning and action
               - If it is unknown how successful a processing state or part of the pipeline is, include verification steps to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
               - Answer checkers to validate the final answer against the problem statement. If the answer is incorrect, the checker can send the answer back to an earlier part of the system for for refinement with feedback

            Here's how to call the Gemini API. Use this example without modification and don't invent configuration options:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

            Since this is an EXPLORATION phase:
            - Try a fundamentally different approach to reasoning about the problem. Test a NEW HYPOTHESIS or variable, and add verification steps to deduce if this new change is helpful.
            - THIS IS KEY: Break down the problem into new, distinct reasoning steps based on past performance before you start coding
            - For EACH key LLM prompt, include a relevant example with:
              * Sample input similar to the dataset
              * Expected reasoning steps
              * Desired output format
            - Apply a verifier call to different parts of the pipeline in order to understand what parts of the pipeline of calls is successful and where the system is breaking
            - Pay special attention to addressing the primary issues from previous iterations
            - Ensure your new approach addresses the weaknesses identified in the capability assessment

            CRITICAL REQUIREMENTS:
            1. The script MUST properly handle all string literals - be extremely careful with quotes and triple quotes
            2. The script MUST NOT exceed 150 lines of code to prevent truncation
            3. Include detailed comments explaining your reasoning approach
            4. EVERY SINGLE LLM PROMPT must include at least one embedded example showing:
               - Sample input with reasoning
               - Desired output format
            5. Make proper use of error handling
            6. Implement robust capabilities to address the specific weaknesses identified in the capability assessment
            7. Do NOT use json.loads() in the LLM calls to process input data. JSON formatting is good to use to structure information as inputs and outputs, but attempting to have functions process JSON data explicitly with strict built-in functionality is error prone due to formatting issues and additional text that appears as documentation, reasoning, or comments. When passing data into another LLM call, you can read it as plain text rather than trying to load it in strict json format, is the better approach.

            Return a COMPLETE, RUNNABLE Python script that:
            1. Has a main function that takes a question string as input and returns the answer string
            2. Makes multiple LLM calls for different reasoning steps
            3. Has proper error handling for API calls
            4. Includes embedded examples in EVERY LLM prompt
            5. Is COMPLETE - no missing code, no "..." placeholders
            6. Closes all string literals properly

            This should be FUNDAMENTALLY DIFFERENT from all previous approaches. Do not reuse the same overall structure.

            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            