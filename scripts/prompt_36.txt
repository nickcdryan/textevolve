
            You are developing a Python script to solve problems using LLM reasoning capabilities.
            You are in the EXPLORATION PHASE. You must generate a NEW approach that's different from previous approaches but informed by their successes and failures. With this approach, you will have a specific NEW HYPOTHESIS or variable you are trying to test. Your goal is to see if this new approach works, and you must add verification and validation steps to deduce if this new change is helpful. You may also test RADICAL NEW APPROACHES that are substantially different from previous approaches. 
            
            You should try NEW THINGS:
            
            Break down the problem into smaller pieces
            Think CREATIVELY about how to solve your problem if other approaches aren't working
            Transform data into different formats to see if it helps

            # YOUR TASK
            You are deeply familiar with prompting techniques and the agent works from the literature. 
            Your goal is to maximize the specified performance metrics by proposing interestingly new agents.
            Observe the past discovered agents and scripts carefully and think about what insights, lessons, or stepping stones can be learned from them.
            Be creative when thinking about the next interesting agent to try. You are encouraged to draw inspiration from related agent papers or academic papers from other research areas.
            Use the knowledge from the archive and inspiration from academic literature to propose the next interesting agentic system design.
            THINK OUTSIDE THE BOX.
            

            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "Grid Transformation Task\n\n=== TRAINING EXAMPLES ===\n\nExample 1:\nInput Grid:\n[\n  [6, 6, 0]\n  [0, 6, 6]\n  [0, 0, 6]\n]\n\nOutput Grid:\n[\n  [6, 6, 0, 6, 6, 0, 6, 6, 0, 6, 6, 0]\n  [0, 6, 6, 0, 6, 6, 0, 6, 6, 0, 6, 6]\n  [0, 0, 6, 0, 0, 6, 0, 0, 6, 0, 0, 6]\n  [6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n]\nExample 2:\nInput Grid:\n[\n  [0, 4, 0]\n  [0, 4, 4]\n  [4, 0, 0]\n]\n\nOutput Grid:\n[\n  [0, 4, 0, 0, 4, 0, 0, 4, 0, 0, 4, 0, 0, 0, 0]\n  [0, 4, 4, 0, 4, 4, 0, 4, 4, 0, 4, 4, 0, 0, 0]\n  [4, 0, 0, 4, 0, 0, 4, 0, 0, 4, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n]\nExample 3:\nInput Grid:\n[\n  [3, 0, 3]\n  [3, 0, 3]\n  [0, 3, 3]\n]\n\nOutput Grid:\n[\n  [3, 0, 3, 3, 0, 3, 3, 0, 3]\n  [3, 0, 3, 3, 0, 3, 3, 0, 3]\n  [0, 3, 3, 0, 3, 3, 0, 3, 3]\n  [3, 0, 3, 3, 0, 3, 3, 0, 3]\n  [3, 0, 3, 3, 0, 3, 3, 0, 3]\n  [0, 3, 3, 0, 3, 3, 0, 3, 3]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0]\n]\nExample 4:\nInput Grid:\n[\n  [2, 0, 2]\n  [0, 2, 0]\n  [0, 0, 0]\n]\n\nOutput Grid:\n[\n  [2, 0, 2, 2, 0, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 2, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n]\n\n=== TEST INPUT ===\n[\n  [0, 0, 8]\n  [0, 8, 0]\n  [0, 0, 0]\n]\n\nTransform the test input according to the pattern shown in the training examples.",
    "answer": "[[0,0,8,0,0,8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,8,0,0,8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]"
  },
  {
    "id": 1,
    "question": "Grid Transformation Task\n\n=== TRAINING EXAMPLES ===\n\nExample 1:\nInput Grid:\n[\n  [0, 0, 0, 0, 1, 1, 4, 0, 2, 0, 0, 0, 0, 2, 0, 5]\n  [0, 0, 0, 3, 5, 0, 0, 0, 9, 9, 8, 0, 4, 0, 5, 8]\n  [1, 0, 8, 2, 8, 0, 0, 6, 0, 8, 5, 0, 0, 0, 8, 0]\n  [0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0]\n  [0, 0, 1, 2, 2, 2, 0, 0, 1, 9, 5, 0, 0, 2, 0, 4]\n  [0, 4, 0, 2, 2, 2, 0, 2, 0, 0, 7, 0, 0, 0, 0, 0]\n  [3, 0, 6, 2, 2, 2, 0, 0, 0, 3, 5, 0, 7, 0, 0, 0]\n  [7, 0, 4, 6, 0, 0, 4, 7, 7, 3, 0, 2, 0, 0, 7, 1]\n  [0, 7, 0, 0, 0, 0, 0, 9, 7, 7, 0, 0, 0, 8, 5, 2]\n  [1, 5, 6, 4, 9, 3, 0, 3, 0, 0, 0, 0, 0, 9, 4, 6]\n  [0, 2, 4, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 6, 0, 0]\n  [0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4]\n  [0, 0, 6, 0, 0, 0, 0, 0, 6, 0, 0, 2, 0, 0, 0, 0]\n  [0, 3, 0, 0, 7, 0, 2, 0, 7, 9, 0, 0, 0, 0, 0, 0]\n  [0, 0, 5, 0, 7, 0, 0, 0, 0, 0, 0, 0, 6, 5, 3, 0]\n  [1, 0, 0, 9, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 9, 0]\n]\n\nOutput Grid:\n[\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n]\nExample 2:\nInput Grid:\n[\n  [0, 0, 7, 0, 0, 6, 0, 6, 0, 0, 0, 7, 3, 0, 0, 0]\n  [0, 0, 3, 0, 0, 1, 0, 0, 8, 0, 0, 2, 0, 0, 0, 0]\n  [0, 0, 0, 0, 3, 9, 0, 0, 0, 0, 0, 0, 0, 8, 0, 8]\n  [2, 2, 0, 2, 9, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0]\n  [0, 5, 2, 0, 0, 7, 0, 6, 0, 0, 0, 3, 0, 0, 1, 0]\n  [4, 4, 0, 3, 9, 0, 0, 0, 0, 7, 0, 2, 0, 0, 0, 0]\n  [8, 0, 0, 0, 0, 6, 0, 0, 0, 8, 0, 0, 3, 0, 0, 0]\n  [0, 9, 0, 0, 0, 4, 8, 0, 0, 0, 7, 0, 0, 0, 0, 0]\n  [0, 0, 9, 5, 0, 0, 0, 0, 4, 6, 0, 1, 4, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 8, 0, 5, 9, 4]\n  [0, 9, 3, 9, 0, 3, 0, 0, 5, 6, 7, 0, 5, 0, 0, 0]\n  [0, 0, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 7, 0, 0]\n  [0, 4, 6, 6, 6, 6, 6, 6, 6, 0, 0, 4, 4, 6, 0, 2]\n  [0, 5, 0, 0, 0, 0, 4, 5, 3, 0, 8, 0, 0, 0, 6, 9]\n  [0, 0, 9, 7, 5, 0, 0, 0, 0, 0, 0, 0, 1, 0, 7, 1]\n  [0, 8, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 3, 8, 7, 0]\n]\n\nOutput Grid:\n[\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n]\nExample 3:\nInput Grid:\n[\n  [3, 0, 0, 0, 0, 0, 6, 2, 0, 0, 0, 5, 0, 0, 0, 3]\n  [0, 7, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 5, 0]\n  [0, 0, 0, 0, 0, 8, 8, 0, 7, 7, 7, 0, 0, 0, 0, 4]\n  [0, 2, 0, 0, 0, 0, 0, 0, 7, 7, 7, 0, 2, 0, 5, 0]\n  [0, 8, 0, 0, 9, 6, 1, 7, 7, 7, 7, 0, 0, 0, 0, 0]\n  [5, 0, 0, 0, 0, 3, 6, 0, 6, 0, 0, 3, 3, 0, 0, 0]\n  [0, 4, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0]\n  [9, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 8, 0, 0, 0, 0]\n  [0, 0, 3, 0, 0, 0, 0, 6, 0, 9, 0, 0, 0, 0, 0, 0]\n  [9, 0, 0, 0, 1, 0, 0, 3, 0, 8, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 7, 0]\n  [0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 5, 0, 0]\n  [4, 0, 0, 1, 7, 0, 3, 0, 0, 7, 5, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 1, 7, 2, 0, 0, 5, 0, 0, 1, 0, 4]\n  [0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0]\n  [0, 2, 0, 0, 0, 7, 9, 0, 0, 0, 5, 0, 2, 0, 3, 0]\n]\n\nOutput Grid:\n[\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n]\n\n=== TEST INPUT ===\n[\n  [0, 0, 1, 7, 3, 0, 0, 0, 0, 0, 1, 2, 0, 4, 7, 0]\n  [0, 0, 0, 3, 0, 0, 6, 8, 0, 0, 0, 0, 0, 0, 0, 0]\n  [6, 0, 0, 8, 0, 1, 0, 0, 1, 0, 0, 0, 7, 0, 4, 8]\n  [0, 3, 8, 0, 0, 0, 3, 0, 8, 0, 0, 0, 0, 0, 0, 0]\n  [5, 0, 0, 0, 1, 0, 0, 8, 0, 0, 3, 8, 0, 0, 5, 0]\n  [0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 3, 7, 0, 0, 0, 0]\n  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 5, 0, 7]\n  [0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 2, 7, 0, 7, 0, 0]\n  [9, 4, 0, 2, 1, 0, 0, 0, 0, 0, 7, 0, 0, 0, 9, 0]\n  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5]\n  [0, 8, 9, 4, 0, 5, 5, 5, 5, 5, 5, 3, 0, 0, 0, 0]\n  [0, 0, 3, 0, 6, 5, 5, 5, 5, 5, 5, 0, 1, 4, 0, 0]\n  [9, 5, 2, 0, 0, 5, 1, 3, 0, 0, 6, 2, 0, 0, 1, 5]\n  [0, 7, 0, 0, 0, 0, 1, 6, 0, 7, 0, 3, 0, 6, 0, 0]\n  [0, 0, 9, 0, 0, 3, 7, 7, 0, 6, 0, 0, 8, 0, 0, 0]\n  [5, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 9]\n]\n\nTransform the test input according to the pattern shown in the training examples.",
    "answer": "[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,5,5,5,5,5,5,0,0,0,0,0],[0,0,0,0,0,5,5,5,5,5,5,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]"
  },
  {
    "id": 2,
    "question": "Grid Transformation Task\n\n=== TRAINING EXAMPLES ===\n\nExample 1:\nInput Grid:\n[\n  [3, 3, 0]\n  [7, 4, 0]\n  [0, 0, 4]\n]\n\nOutput Grid:\n[\n  [3, 3, 3, 3, 3, 3, 0, 0, 0]\n  [3, 3, 3, 3, 3, 3, 0, 0, 0]\n  [3, 3, 3, 3, 3, 3, 0, 0, 0]\n  [7, 7, 7, 4, 4, 4, 0, 0, 0]\n  [7, 7, 7, 4, 4, 4, 0, 0, 0]\n  [7, 7, 7, 4, 4, 4, 0, 0, 0]\n  [0, 0, 0, 0, 0, 0, 4, 4, 4]\n  [0, 0, 0, 0, 0, 0, 4, 4, 4]\n  [0, 0, 0, 0, 0, 0, 4, 4, 4]\n]\nExample 2:\nInput Grid:\n[\n  [3, 0, 2]\n  [0, 2, 2]\n  [0, 0, 3]\n]\n\nOutput Grid:\n[\n  [3, 3, 3, 0, 0, 0, 2, 2, 2]\n  [3, 3, 3, 0, 0, 0, 2, 2, 2]\n  [3, 3, 3, 0, 0, 0, 2, 2, 2]\n  [0, 0, 0, 2, 2, 2, 2, 2, 2]\n  [0, 0, 0, 2, 2, 2, 2, 2, 2]\n  [0, 0, 0, 2, 2, 2, 2, 2, 2]\n  [0, 0, 0, 0, 0, 0, 3, 3, 3]\n  [0, 0, 0, 0, 0, 0, 3, 3, 3]\n  [0, 0, 0, 0, 0, 0, 3, 3, 3]\n]\n\n=== TEST INPUT ===\n[\n  [0, 1, 0]\n  [0, 0, 6]\n  [6, 1, 0]\n]\n\nTransform the test input according to the pattern shown in the training examples.",
    "answer": "[[0,0,0,1,1,1,0,0,0],[0,0,0,1,1,1,0,0,0],[0,0,0,1,1,1,0,0,0],[0,0,0,0,0,0,6,6,6],[0,0,0,0,0,0,6,6,6],[0,0,0,0,0,0,6,6,6],[6,6,6,1,1,1,0,0,0],[6,6,6,1,1,1,0,0,0],[6,6,6,1,1,1,0,0,0]]"
  }
]

            HISTORICAL CONTEXT:
            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 36
        - Current explore/exploit balance: 70/30
        - Best accuracy achieved: 0.67 (iteration 34)

        APPROACH HISTORY (last 10 iterations):
        [
  {
    "iteration": 26,
    "strategy": "Exploration",
    "accuracy": 0.0,
    "approach": "The script uses an iterative refinement approach with structural similarity analysis to solve grid transformation problems. It decomposes the problem into discovering transformation rules and iteratively refining/applying them. Two agent roles are implicitly defined: one for rule discovery and another for iterative refinement, both leveraging LLMs. The function `discover_transformation_rules` generates initial rules, `iteratively_refine_and_apply` refines these rules by comparing the transformed grid with the training output, and `call_llm` interfaces with the Gemini LLM. The overall workflow involves discovering initial rules, then iteratively applying and refining these rules using structural similarity feedback to generate the final transformed grid."
  },
  {
    "iteration": 27,
    "strategy": "Exploration",
    "accuracy": 0.0,
    "approach": "The script uses a meta-reasoning agent to dynamically select a strategy for solving grid transformation problems. The problem is decomposed into strategy selection and strategy application. Two agent roles are involved: a meta-reasoning agent for strategy selection and a strategy execution agent. Other functions used are `call_llm` to interact with the LLM and `solve_grid_transformation` to orchestrate the process.\n\nThe workflow is as follows: `main` calls `solve_grid_transformation`, which calls `select_strategy` to choose a strategy using `call_llm`, and then calls `apply_chosen_strategy` to apply the selected strategy, again using `call_llm`, and returns the transformed grid."
  },
  {
    "iteration": 28,
    "strategy": "Exploration",
    "accuracy": 0.3333333333333333,
    "approach": "The script solves grid transformation problems using a structured approach with the LLM acting as an expert in region identification, pattern inference, and template completion. It decomposes the problem into three main steps: identifying key regions in the grid, inferring the swapping pattern between these regions, and completing a grid template based on the pattern. The LLM is prompted with specific system instructions for each step to guide its reasoning. The `solve_grid_transformation` function orchestrates the process, calling `identify_key_regions`, `infer_swapping_pattern`, and `complete_template` sequentially, with each function using `call_llm` to interact with the LLM. `call_llm` handles the API call to the LLM and returns the text response."
  },
  {
    "iteration": 29,
    "strategy": "Exploitation",
    "accuracy": 0.0,
    "approach": "The script solves grid transformation problems using an LLM with multi-example prompting and verification. The problem is decomposed into analyzing visual features and applying the transformation. The LLM acts as a visual feature analyst and transformation applier. The functions used are `solve_grid_transformation`, `analyze_visual_features`, `apply_transformation`, and `call_llm`. The workflow involves `solve_grid_transformation` calling `analyze_visual_features` to get a transformation description, then `apply_transformation` to generate the final grid, using `call_llm` to interface with the LLM."
  },
  {
    "iteration": 30,
    "strategy": "Exploitation",
    "accuracy": 0.3333333333333333,
    "approach": "This script solves grid transformation problems using an LLM with multi-example prompting and verification. The problem is decomposed into analyzing visual features and applying transformations, handled by `analyze_visual_features` and `apply_transformation`, respectively. The LLM acts as an expert in both feature analysis and transformation application.\n\n*   **`solve_grid_transformation`**: Orchestrates the solution by calling `analyze_visual_features` and `apply_transformation`.\n*   **`analyze_visual_features`**: Analyzes the grid and returns the transformation description using `call_llm`, including a verification step for the transformation.\n*   **`apply_transformation`**: Applies the transformation to the input grid using `call_llm`.\n*   **`call_llm`**: A utility function to interact with the Gemini LLM API.\n\nThe overall workflow involves analyzing visual features, verifying the transformation description, and then applying the transformation to generate the final output grid."
  },
  {
    "iteration": 31,
    "strategy": "Exploration",
    "accuracy": 0.3333333333333333,
    "approach": "The script addresses grid transformation problems by employing an LLM to identify visual anchors, infer transformation rules based on these anchors, and then apply these rules to generate a transformed grid. The problem is decomposed into three main steps: identifying anchors, inferring rules, and applying the transformation, each handled by a dedicated function. There are no distinct agent roles, instead the functions `identify_visual_anchors`, `infer_transformation_rules`, and `apply_transformation` sequentially use the `call_llm` function with specific system instructions and prompts to leverage the LLM's reasoning capabilities at each step, with `solve_grid_transformation` orchestrating the overall process. The overall workflow is `solve_grid_transformation` -> (`identify_visual_anchors` -> `call_llm`), (`infer_transformation_rules` -> `call_llm`), (`apply_transformation` -> `call_llm`)."
  },
  {
    "iteration": 32,
    "strategy": "Exploration",
    "accuracy": 0.3333333333333333,
    "approach": "The script solves grid transformation problems by first analyzing local patterns using an LLM with a system instruction to act as a pattern recognition expert. The identified patterns are then used by another LLM, instructed to apply transformation rules, to transform a test input grid based on the local patterns. The problem is decomposed into local pattern analysis and transformation application, with `analyze_local_patterns` identifying patterns and `apply_transformation` generating the transformed grid. The `call_llm` function is used to interface with the Gemini model, using the system instruction when provided, and `solve_grid_transformation` orchestrates the process."
  },
  {
    "iteration": 33,
    "strategy": "Exploration",
    "accuracy": 0.0,
    "approach": "The script uses LLMs to solve grid transformation problems by employing a template matching and refinement approach. The problem is decomposed into matching the input grid to a similar training example and then refining the transformation based on the matched example's context. Two agent roles are defined: one for matching grids and another for refining transformations. The function `solve_grid_transformation` orchestrates the process, calling `match_template` to find a relevant training example and `refine_transformation` to generate the final solution. The `call_llm` function is used to interact with the Gemini LLM, and the `main` function is used to initiate and manage the entire process."
  },
  {
    "iteration": 34,
    "strategy": "Exploitation",
    "accuracy": 0.6666666666666666,
    "approach": "The script uses an LLM to solve grid transformation problems by decomposing the task into identifying the transformation type, analyzing visual features, and applying the transformation. Three agent roles are defined: \"expert in identifying transformation types\", \"expert at analyzing visual features\", and \"expert at applying transformations.\" The `solve_grid_transformation` function orchestrates the process, calling `identify_transformation_type`, `analyze_visual_features`, and `apply_transformation` sequentially. Each of these functions uses `call_llm` to interact with the Gemini model. The `main` function serves as the entry point and calls `solve_grid_transformation` to obtain the final answer."
  },
  {
    "iteration": 35,
    "strategy": "Exploitation",
    "accuracy": 0.0,
    "approach": "The script solves grid transformation problems using a chain-of-thought approach, breaking down the problem into identifying the transformation type, analyzing visual features, and then applying the transformation. It uses the `call_llm` function to interact with the Gemini model in each of these steps, leveraging system instructions to define expert roles. The main function `solve_grid_transformation` orchestrates the process, calling `identify_transformation_type`, `analyze_visual_features`, and `apply_transformation` in sequence and then validating the final result with `is_valid_grid`."
  }
]

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 3 iterations):
        [
  {
    "iteration": 26,
    "issue": "The most critical problem is the system's inconsistency in generating the final transformed grid after reasoning about the patterns. The system seems to stop at the descriptive stage, implying a disconnect between understanding the transformation logic and actually implementing it."
  },
  {
    "iteration": 27,
    "issue": "The single most critical problem is **inaccurate pattern interpretation and subsequent rule extraction**. The system frequently identifies patterns that are only partially correct or misinterprets the nature of the transformation (e.g., reflection, rotation, propagation). This leads to the derivation of incorrect rules that fail to generalize to the test input. The analysis of example 0 shows that even relatively simple corner rotation is improperly executed."
  },
  {
    "iteration": 28,
    "issue": "The most critical problem is the **failure to abstract and generalize patterns from the training examples to the test input.** The system relies too heavily on memorizing specific details of the training data rather than learning the underlying transformation logic. This manifests as incorrect row/column transformations, misapplication of patterns, and dimension mismatches."
  },
  {
    "iteration": 29,
    "issue": "The primary issue is the **misinterpretation of the task as a matrix summarization/reduction problem** instead of a grid transformation task. The system is consistently producing a small matrix as output, ignoring the grid structure and specific element-wise transformations demonstrated in the training examples."
  },
  {
    "iteration": 30,
    "issue": "The primary issue is the system's inadequate capacity to learn and apply complex patterns involving spatial relationships and element transformations in grid-based problems. The system cannot accurately deduce the underlying rules governing the transformations in the training examples and struggles to translate it into an accurate grid transformation."
  },
  {
    "iteration": 31,
    "issue": "The primary issue is **faulty pattern recognition and application**. The system struggles to correctly identify the rules governing the transformations between input and output grids in the training examples, and therefore incorrectly modifies the test input."
  },
  {
    "iteration": 32,
    "issue": "The primary issue is the **inability to accurately and completely apply learned transformation rules to new input grids**, leading to deviations from the golden answer. The system appears to over-generalize or incompletely understand the precise transformations required."
  },
  {
    "iteration": 33,
    "issue": "The primary issue is the system's flawed pattern recognition and generalization capabilities. It cannot accurately infer complex transformation rules based on the provided training examples and apply them to the test input."
  },
  {
    "iteration": 34,
    "issue": "The most critical problem is the system's failure to generalize grid transformation patterns when presented with test inputs of different dimensions than those in the training data. This suggests a deficiency in understanding spatial relationships and scaling transformations based on demonstrated patterns. The system tries to place the numbers at incorrect locations, leading to complete failure of the task."
  },
  {
    "iteration": 35,
    "issue": "The primary issue is the inability to translate the inferred pattern into a correctly formatted output grid. While the system might be inferring *some* aspect of the pattern, it's clearly failing to generate the final output in the expected format (specifically, the dimensions and population of the new grid)."
  }
]

        TARGETED IMPROVEMENTS:
        [
  "Data Augmentation:** Augment the training data with variations of the existing examples, such as rotated, flipped, or scaled versions of the grids. This could help the system learn to generalize the transformations to different grid sizes and arrangements.",
  "Spatial Reasoning Module:** Implement a dedicated module for spatial reasoning, capable of analyzing grid layouts, identifying shapes, and understanding spatial relationships between elements.",
  "Introduce Print Statements:** Add print statements that allows intermediate outputs to be viewed. This will allow a developer to step-by-step reproduce why the system went wrong.",
  "More Careful Analysis of Training Cases:** The system appears to be applying a range of transformations, but some of them are wrong. It is incorrectly mapping from column 6 in the training examples to column 9 in the test example, and the propagation rules are also inaccurate.",
  "Intermediate Representation Visualization:** Introduce intermediate representations (e.g., heatmaps showing the frequency of value occurrences at different positions, or dependency graphs highlighting relationships between grid cells).",
  "Add Error Logging:** Log any errors or inconsistencies detected during the rule verification process. This will help track the frequency and types of errors that occur.",
  "Incorporate spatial reasoning techniques:** Explore methods for incorporating spatial reasoning into the system, such as using convolutional neural networks or graph neural networks to model the relationships between different cells in the grid.",
  "Introduce a more robust pattern recognition and abstraction module:** This module should be able to identify and extract the core transformation rules from the training examples, representing them in a more abstract and generalizable form. For instance, using a convolutional neural network to extract common patterns, or a graphical representation of patterns.",
  "Pattern Encoding:** Explore techniques for encoding patterns in a more structured and expressive way, allowing for more complex transformations to be represented. Consider using graph representations or symbolic representations.",
  "Augmented Training Data:** Increase the diversity and complexity of training examples to expose the system to a wider range of patterns and transformations."
]
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity ✓
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity ✓
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity ✓
    4. All operations have O(1) average time complexity ✓
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        

            PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS). YOUR APPROACH MUST BE SUBSTANTIVELY DIFFERENT THAN THESE:
            
PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS):

=== SCRIPT FROM ITERATION 35 (Exploitation, ACCURACY: 0.00) ===
Approach: The script solves grid transformation problems using a chain-of-thought approach, breaking down the problem into identifying the transformation type, analyzing visual features, and then applying the transformation. It uses the `call_llm` function to interact with the Gemini model in each of these steps, leveraging system instructions to define expert roles. The main function `solve_grid_transformation` orchestrates the process, calling `identify_transformation_type`, `analyze_visual_features`, and `apply_transformation` in sequence and then validating the final result with `is_valid_grid`.

```python
import os
import re
import math

# This script improves grid transformation by adding multi-example prompting,
# a new intermediate step (identify_transformation_type), and a stricter validation process.
# It also adds a more explicit output format verification step to address the output size issue.

def solve_grid_transformation(question, max_attempts=3):
    """Solves grid transformation problems by analyzing features, identifying type, & applying transformations."""
    transformation_type_result = identify_transformation_type(question)
    if not transformation_type_result["is_valid"]:
        return f"Error: Could not identify transformation type."

    feature_analysis_result = analyze_visual_features(question, transformation_type_result["transformation_type"])
    if not feature_analysis_result["is_valid"]:
        return f"Error: Could not analyze visual features."

    transformed_grid = apply_transformation(question, feature_analysis_result["transformation_description"])
    # Add validation
    if not is_valid_grid(transformed_grid):
      return "Error: Invalid output grid format."
    return transformed_grid

def is_valid_grid(grid_string):
  """Verifies that the grid string represents a 2D array (nested list of numbers)."""
  try:
    # Check if the string starts and ends with brackets
    if not (grid_string.startswith("[[") and grid_string.endswith("]]")):
        return False

    # Check if each row in the grid is a list of numbers
    rows = grid_string.replace("[[", "").replace("]]", "").split("],[")
    for row in rows:
        numbers = row.split(",")
        for number in numbers:
            try:
                int(number.strip())  # Check if it is an integer value
            except ValueError:
                return False
    return True
  except:
    return False

def identify_transformation_type(question):
    """Identifies the type of transformation (e.g., mirroring, rotation, value replacement)."""
    system_instruction = "You are an expert in identifying transformation types in grid patterns."
    prompt = f"""
    Given the following grid transformation problem, identify the *type* of transformation being applied.

    Example 1:
    Input Grid: [[1, 2], [3, 4]]
    Output Grid: [[4, 3], [2, 1]]
    Transformation Type: Mirroring

    Reasoning: The output grid is a mirror image of the input grid.

    Example 2:
    Input Grid: [[1, 2], [3, 4]]
    Output Grid: [[3, 4], [1, 2]]
    Transformation Type: Row Swapping

    Reasoning: The rows of the input grid have been swapped to create the output grid.

    Problem: {question}
    Transformation Type:
    """
    transformation_type = call_llm(prompt, system_instruction)
    return {"is_valid": True, "transformation_type": transformation_type}

def analyze_visual_features(question, transformation_type):
    """Analyzes visual features of the grid transformation problem."""
    system_instruction = "You are an expert at analyzing visual features in grid transformations."
    prompt = f"""
    Given the following grid transformation problem (of type: {transformation_type}), analyze the examples and describe the transformation.

    Example 1:
    === TRAINING EXAMPLES ===
    Input Grid: [[0, 0, 0], [1, 1, 1], [0, 0, 0]]
    Output Grid: [[1, 1, 1], [0, 0, 0], [1, 1, 1]]
    Transformation Description: The transformation involves swapping the rows with '1' with adjacent rows.

    Reasoning:
    1. The row containing '1' values in the input is moved to the top in the output
    2. The row containing '1' values from input is moved to the bottom in output, with other rows padded.

    Problem: {question}
    Transformation Description:
    """
    transformation_description = call_llm(prompt, system_instruction)
    return {"is_valid": True, "transformation_description": transformation_description}

def apply_transformation(question, transformation_description):
    """Applies the transformation to the test input grid."""
    system_instruction = "You are an expert at applying transformations to grids based on a feature description."
    prompt = f"""
    Given the following problem and transformation description, apply the transformation to the test input.

    Problem: {question}
    Transformation Description: {transformation_description}

    Example:
    Problem: Input Grid: [[1, 2], [3, 4]] Output Grid: [[4, 3], [2, 1]]. Test Input: [[5, 6], [7, 8]]
    Transformation Description: Mirror the grid horizontally and vertically.

    Output Grid: [[8, 7], [6, 5]]

    Generate the output grid. The output grid should be a nested list of numbers like this: [[1, 2], [3, 4]].
    """
    output_grid = call_llm(prompt, system_instruction)
    return output_grid

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question):
    """Main function to solve the grid transformation task."""
    try:
        answer = solve_grid_transformation(question)
        return answer
    except Exception as e:
        return f"Error in main function: {str(e)}"
```

=== SCRIPT FROM ITERATION 34 (Exploitation, ACCURACY: 0.67) ===
Approach: The script uses an LLM to solve grid transformation problems by decomposing the task into identifying the transformation type, analyzing visual features, and applying the transformation. Three agent roles are defined: "expert in identifying transformation types", "expert at analyzing visual features", and "expert at applying transformations." The `solve_grid_transformation` function orchestrates the process, calling `identify_transformation_type`, `analyze_visual_features`, and `apply_transformation` sequentially. Each of these functions uses `call_llm` to interact with the Gemini model. The `main` function serves as the entry point and calls `solve_grid_transformation` to obtain the final answer.

```python
import os
import re
import math

# This script improves grid transformation by adding multi-example prompting,
# a new intermediate step (identify_transformation_type), and a stricter validation process.

def solve_grid_transformation(question, max_attempts=3):
    """Solves grid transformation problems by analyzing features, identifying type, & applying transformations."""
    transformation_type_result = identify_transformation_type(question)
    if not transformation_type_result["is_valid"]:
        return f"Error: Could not identify transformation type."

    feature_analysis_result = analyze_visual_features(question, transformation_type_result["transformation_type"])
    if not feature_analysis_result["is_valid"]:
        return f"Error: Could not analyze visual features."

    transformed_grid = apply_transformation(question, feature_analysis_result["transformation_description"])
    return transformed_grid

def identify_transformation_type(question):
    """Identifies the type of transformation (e.g., mirroring, rotation, value replacement)."""
    system_instruction = "You are an expert in identifying transformation types in grid patterns."
    prompt = f"""
    Given the following grid transformation problem, identify the *type* of transformation being applied.

    Example 1:
    Input Grid: [[1, 2], [3, 4]]
    Output Grid: [[4, 3], [2, 1]]
    Transformation Type: Mirroring

    Example 2:
    Input Grid: [[1, 2], [3, 4]]
    Output Grid: [[3, 4], [1, 2]]
    Transformation Type: Row Swapping

    Problem: {question}
    Transformation Type:
    """
    transformation_type = call_llm(prompt, system_instruction)
    return {"is_valid": True, "transformation_type": transformation_type}

def analyze_visual_features(question, transformation_type):
    """Analyzes visual features of the grid transformation problem."""
    system_instruction = "You are an expert at analyzing visual features in grid transformations."
    prompt = f"""
    Given the following grid transformation problem (of type: {transformation_type}), analyze the examples and describe the transformation.
    Example 1:
    === TRAINING EXAMPLES ===
    Input Grid: [[0, 0, 0], [1, 1, 1], [0, 0, 0]]
    Output Grid: [[1, 1, 1], [0, 0, 0], [1, 1, 1]]
    Transformation Description: The transformation involves swapping the rows with '1' with adjacent rows.
    Problem: {question}
    Transformation Description:
    """
    transformation_description = call_llm(prompt, system_instruction)
    return {"is_valid": True, "transformation_description": transformation_description}

def apply_transformation(question, transformation_description):
    """Applies the transformation to the test input grid."""
    system_instruction = "You are an expert at applying transformations to grids based on a feature description."
    prompt = f"""
    Given the following problem and transformation description, apply the transformation to the test input.
    Problem: {question}
    Transformation Description: {transformation_description}
    Generate the output grid. Example: The output grid should be a nested list of numbers like this: [[1, 2], [3, 4]].
    """
    output_grid = call_llm(prompt, system_instruction)
    return output_grid

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM."""
    try:
        from google import genai
        from google.genai import types
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                config=types.GenerateContentConfig(system_instruction=system_instruction),
                contents=prompt)
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash", contents=prompt)
        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question):
    """Main function to solve the grid transformation task."""
    try:
        answer = solve_grid_transformation(question)
        return answer
    except Exception as e:
        return f"Error in main function: {str(e)}"
```

=== SCRIPT FROM ITERATION 33 (Exploration, ACCURACY: 0.00) ===
Approach: The script uses LLMs to solve grid transformation problems by employing a template matching and refinement approach. The problem is decomposed into matching the input grid to a similar training example and then refining the transformation based on the matched example's context. Two agent roles are defined: one for matching grids and another for refining transformations. The function `solve_grid_transformation` orchestrates the process, calling `match_template` to find a relevant training example and `refine_transformation` to generate the final solution. The `call_llm` function is used to interact with the Gemini LLM, and the `main` function is used to initiate and manage the entire process.

```python
import os
import re
import math

# EXPLORATION: LLM-Guided Template Matching with Iterative Contextual Refinement
# HYPOTHESIS: We can improve grid transformation by having the LLM identify similar training grids, then use these matched training templates for localized context refinement of the target input grid transformations.

def solve_grid_transformation(question, max_attempts=3):
    """Solves grid transformation problems by matching templates and refining transformation based on context."""

    # Step 1: Match template
    template_matching_result = match_template(question)
    if not template_matching_result["is_valid"]:
        return f"Error: Could not match template. {template_matching_result['error']}"

    training_example = template_matching_result["training_example"]

    # Step 2: Refine transformation
    refined_transformation = refine_transformation(question, training_example)

    return refined_transformation

def match_template(question):
    """Matches the input grid to the most similar training grid."""
    system_instruction = "You are an expert at matching grid transformation input grids to the most similar training example input grids."

    prompt = f"""
    Given the following grid transformation problem, analyze the test input grid and identify the most similar input grid from the training examples. Return the entire matching training example (input and output grids).

    Example:
    Problem:
    === TRAINING EXAMPLES ===
    Input Grid:
    [[0, 0, 0],
     [1, 1, 1],
     [0, 0, 0]]
    Output Grid:
    [[2, 2, 2],
     [1, 1, 1],
     [2, 2, 2]]
    === TEST INPUT ===
    [[0, 0, 0],
     [5, 5, 5],
     [0, 0, 0]]
    Most similar Training Example:
    Input Grid:
    [[0, 0, 0],
     [1, 1, 1],
     [0, 0, 0]]
    Output Grid:
    [[2, 2, 2],
     [1, 1, 1],
     [2, 2, 2]]
    

    Problem:
    {question}
    Most similar Training Example:
    """

    training_example = call_llm(prompt, system_instruction)

    # Validation: Check if a training example was returned
    if training_example and training_example.strip():
        return {"is_valid": True, "training_example": training_example, "error": None}
    else:
        return {"is_valid": False, "training_example": None, "error": "Failed to identify similar training example."}

def refine_transformation(question, training_example):
    """Refines the transformation based on localized context using the matched training example."""
    system_instruction = "You are an expert at refining grid transformations based on context from matched training examples."

    prompt = f"""
    Given the following grid transformation problem and the most similar training example, refine the transformation based on localized context.

    Example:
    Problem:
    === TRAINING EXAMPLES ===
    Input Grid:
    [[0, 0, 0],
     [1, 1, 1],
     [0, 0, 0]]
    Output Grid:
    [[2, 2, 2],
     [1, 1, 1],
     [2, 2, 2]]
    === TEST INPUT ===
    [[0, 0, 0],
     [5, 5, 5],
     [0, 0, 0]]
    Most similar Training Example:
    Input Grid:
    [[0, 0, 0],
     [1, 1, 1],
     [0, 0, 0]]
    Output Grid:
    [[2, 2, 2],
     [1, 1, 1],
     [2, 2, 2]]
    Refined Transformation:
    [[2, 2, 2],
     [5, 5, 5],
     [2, 2, 2]]

    Problem:
    {question}
    Most similar Training Example: {training_example}
    Refined Transformation:
    """

    refined_transformation = call_llm(prompt, system_instruction)
    return refined_transformation

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question):
    """Main function to solve the grid transformation task."""
    try:
        answer = solve_grid_transformation(question)
        return answer
    except Exception as e:
        return f"Error in main function: {str(e)}"
```

=== SCRIPT FROM ITERATION 32 (Exploration, ACCURACY: 0.33) ===
Approach: The script solves grid transformation problems by first analyzing local patterns using an LLM with a system instruction to act as a pattern recognition expert. The identified patterns are then used by another LLM, instructed to apply transformation rules, to transform a test input grid based on the local patterns. The problem is decomposed into local pattern analysis and transformation application, with `analyze_local_patterns` identifying patterns and `apply_transformation` generating the transformed grid. The `call_llm` function is used to interface with the Gemini model, using the system instruction when provided, and `solve_grid_transformation` orchestrates the process.

```python
import os
import re
import math

# EXPLORATION: Grid Transformation using Local Pattern Analysis with Adaptive Context and Iterative Refinement
# HYPOTHESIS: We can improve grid transformation accuracy by analyzing local patterns around each cell and then iteratively refining the transformation based on those local patterns. The approach attempts to model what is "nearby" the target cell and adjust accordingly.

def solve_grid_transformation(question, max_attempts=3):
    """Solves grid transformation problems by analyzing local patterns and iteratively refining the transformation."""
    try:
        # Step 1: Analyze local patterns around each cell in the input grid.
        local_patterns_result = analyze_local_patterns(question)
        if not local_patterns_result["is_valid"]:
            return f"Error: Could not analyze local patterns. {local_patterns_result['error']}"
        local_patterns = local_patterns_result["patterns"]

        # Step 2: Apply transformation based on local patterns and original values
        transformed_grid = apply_transformation(question, local_patterns)
        return transformed_grid

    except Exception as e:
        return f"Error in solve_grid_transformation: {str(e)}"

def analyze_local_patterns(question):
    """Analyzes local patterns around each cell in the input grid using a LLM for pattern recognition."""
    system_instruction = "You are an expert at identifying local patterns in grid transformation problems."

    prompt = f"""
    Given the following grid transformation problem, analyze the training examples and identify local patterns around each cell in the grid. Focus on the immediate neighborhood of each cell.

    Example 1:
    Problem:
    === TRAINING EXAMPLES ===
    Input Grid:
    [[0, 0, 0],
     [1, 1, 1],
     [0, 0, 0]]
    Output Grid:
    [[2, 2, 2],
     [1, 1, 1],
     [2, 2, 2]]
    Local Patterns:
    - A '0' surrounded by other '0's becomes a '2'.
    - A '1' remains a '1'.

    Problem:
    {question}
    Local Patterns:
    """

    patterns = call_llm(prompt, system_instruction)

    # Validation: Check if patterns are not empty and a sensible statement
    if patterns and patterns.strip():
        return {"is_valid": True, "patterns": patterns, "error": None}
    else:
        return {"is_valid": False, "patterns": None, "error": "Failed to identify local patterns."}

def apply_transformation(question, local_patterns):
    """Applies the transformation rules to the test input grid."""
    system_instruction = "You are an expert at applying transformation rules to grids based on local patterns."

    prompt = f"""
    Given the following grid transformation problem and the local patterns, apply the patterns to the test input grid. Focus the transformation on the information captured by the 'Local Patterns' portion of the prompt.

    Example:
    Problem:
    === TRAINING EXAMPLES ===
    Input Grid:
    [[0, 0, 0],
     [1, 1, 1],
     [0, 0, 0]]
    Output Grid:
    [[2, 2, 2],
     [1, 1, 1],
     [2, 2, 2]]
    Local Patterns:
    - A '0' surrounded by other '0's becomes a '2'.
    - A '1' remains a '1'.
    Test Input:
    [[5, 5, 5],
     [6, 6, 6],
     [7, 7, 7]]
    Completed Grid:
    [[2, 2, 2],
     [6, 6, 6],
     [2, 2, 2]]

    Problem:
    {question}
    Local Patterns: {local_patterns}
    Completed Grid:
    """

    completed_grid = call_llm(prompt, system_instruction)
    return completed_grid

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question):
    """Main function to solve the grid transformation task."""
    try:
        answer = solve_grid_transformation(question)
        return answer
    except Exception as e:
        return f"Error in main function: {str(e)}"
```

=== SCRIPT FROM ITERATION 31 (Exploration, ACCURACY: 0.33) ===
Approach: The script addresses grid transformation problems by employing an LLM to identify visual anchors, infer transformation rules based on these anchors, and then apply these rules to generate a transformed grid. The problem is decomposed into three main steps: identifying anchors, inferring rules, and applying the transformation, each handled by a dedicated function. There are no distinct agent roles, instead the functions `identify_visual_anchors`, `infer_transformation_rules`, and `apply_transformation` sequentially use the `call_llm` function with specific system instructions and prompts to leverage the LLM's reasoning capabilities at each step, with `solve_grid_transformation` orchestrating the overall process. The overall workflow is `solve_grid_transformation` -> (`identify_visual_anchors` -> `call_llm`), (`infer_transformation_rules` -> `call_llm`), (`apply_transformation` -> `call_llm`).

```python
import os
import re
import math

# EXPLORATION: Visual Anchor Identification and Transformation Rule Application with Adaptive Example Selection and Validation
# HYPOTHESIS: The LLM can better generalize grid transformations by first identifying "visual anchors" (stable elements) in the grid,
# then inferring transformation rules relative to those anchors. The approach also uses an adaptive example selection strategy,
# choosing relevant training examples based on similarity to the input grid, and includes detailed validation steps to ensure the LLM
# is producing valid outputs. We will validate different parts of the pipeline to identify which part is failing.

def solve_grid_transformation(question, max_attempts=3):
    """Solves grid transformation problems by identifying visual anchors and applying transformation rules."""

    # Step 1: Identify Visual Anchors
    anchor_identification_result = identify_visual_anchors(question)
    if not anchor_identification_result["is_valid"]:
        return f"Error: Could not identify visual anchors. {anchor_identification_result['error']}"
    anchors = anchor_identification_result["anchors"]

    # Step 2: Infer Transformation Rules
    rule_inference_result = infer_transformation_rules(question, anchors)
    if not rule_inference_result["is_valid"]:
        return f"Error: Could not infer transformation rules. {rule_inference_result['error']}"
    rules = rule_inference_result["rules"]

    # Step 3: Apply Transformation
    transformed_grid = apply_transformation(question, anchors, rules)
    return transformed_grid

def identify_visual_anchors(question):
    """Identifies visual anchors (stable elements) in the grid."""
    system_instruction = "You are an expert at identifying visual anchors in grid transformation problems. Visual anchors are stable elements or regions that remain unchanged or predictably change during the transformation."

    prompt = f"""
    Given the following grid transformation problem, analyze the training examples and identify visual anchors within the grid.

    Example:
    Problem:
    === TRAINING EXAMPLES ===
    Input Grid:
    [[0, 0, 0],
     [1, 1, 1],
     [0, 0, 0]]
    Output Grid:
    [[2, 2, 2],
     [1, 1, 1],
     [2, 2, 2]]
    Visual Anchors: The row containing all 1s remains unchanged.

    Problem:
    {question}
    Visual Anchors:
    """

    anchors = call_llm(prompt, system_instruction)

    # Validation: Ensure that *something* was output
    if anchors and anchors.strip():
        return {"is_valid": True, "anchors": anchors, "error": None}
    else:
        return {"is_valid": False, "anchors": None, "error": "Failed to identify visual anchors."}

def infer_transformation_rules(question, anchors):
    """Infers transformation rules relative to the identified visual anchors."""
    system_instruction = "You are an expert at inferring transformation rules in grid-based problems, relative to visual anchors."

    prompt = f"""
    Given the following grid transformation problem and identified visual anchors, infer the transformation rules that describe how other elements change relative to these anchors.

    Example:
    Problem:
    === TRAINING EXAMPLES ===
    Input Grid:
    [[0, 0, 0],
     [1, 1, 1],
     [0, 0, 0]]
    Output Grid:
    [[2, 2, 2],
     [1, 1, 1],
     [2, 2, 2]]
    Visual Anchors: The row containing all 1s remains unchanged.
    Transformation Rules: Rows above and below the anchor row are transformed to rows of 2s.

    Problem:
    {question}
    Visual Anchors: {anchors}
    Transformation Rules:
    """

    rules = call_llm(prompt, system_instruction)

    # Validation: Check if rules are not empty and a sensible statement
    if rules and rules.strip():
        return {"is_valid": True, "rules": rules, "error": None}
    else:
        return {"is_valid": False, "rules": None, "error": "Failed to infer transformation rules."}

def apply_transformation(question, anchors, rules):
    """Applies the transformation rules to the test input grid."""
    system_instruction = "You are an expert at applying transformation rules to grids based on visual anchors and described transformations. You must respond with a valid grid, which is a list of lists."

    prompt = f"""
    Given the following grid transformation problem, identified visual anchors, and inferred transformation rules, apply the rules to the test input grid.

    Example:
    Problem:
    === TRAINING EXAMPLES ===
    Input Grid:
    [[0, 0, 0],
     [1, 1, 1],
     [0, 0, 0]]
    Output Grid:
    [[2, 2, 2],
     [1, 1, 1],
     [2, 2, 2]]
    Visual Anchors: The row containing all 1s remains unchanged.
    Transformation Rules: Rows above and below the anchor row are transformed to rows of 2s.
    Test Input:
    [[5, 5, 5],
     [6, 6, 6],
     [7, 7, 7]]
    Completed Grid:
    [[2, 2, 2],
     [6, 6, 6],
     [2, 2, 2]]

    Problem:
    {question}
    Visual Anchors: {anchors}
    Transformation Rules: {rules}
    Completed Grid:
    """

    completed_grid = call_llm(prompt, system_instruction)
    return completed_grid

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question):
    """Main function to solve the grid transformation task."""
    try:
        answer = solve_grid_transformation(question)
        return answer
    except Exception as e:
        return f"Error in main function: {str(e)}"
```


            LEARNINGS FROM PREVIOUS ITERATIONS:
            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        Okay, I've integrated the new learnings from Iteration 35 into the existing knowledge base, while carefully managing the document length to stay within the token limit. I've focused on condensing redundant information and prioritizing concrete, task-specific insights.

**1. DATASET PATTERNS & CHARACTERISTICS**

*   **Question Format:** "Grid Transformation Tasks" with "TRAINING EXAMPLES" (input/output grid pairs labeled "Input Grid," "Output Grid") and a "TEST INPUT" grid. Objective: generate the "OUTPUT GRID" for the test input, following patterns from the training examples.
*   **Consistent Structure:** Header, training examples, test input, transformation instruction.
*   **Grid Representation:** Nested lists of integers (e.g., `[[1, 2], [3, 4]]`). Dimensions vary (e.g., 10x10, 17x17). Grids are typically small matrices of integers, often with a background value (e.g., 0) and a few other distinct values that participate in transformations. Input/Output grid dimensions may vary between training and test grids. Training examples define constraints for output grid size.
*   **Mixed Element Types:** Grids contain zeros and other numerical values.
*   **Transformation Logic:** Identifying transformation logic is the core challenge, often involving spatial manipulations (e.g., shifts, expansions, alternating patterns, boundary adherence, region swapping). Rules are often *localized* or dependent on subgrid characteristics. New elements can be placed at new locations based on spatial relationships; transformations are not simple element-wise operations, involving replacing numbers based on their position relative to other numbers or patterns, expansion, replication, and *selective* substitution based on contextual patterns. A cell's new value may depend on multiple other cells (local context), with potentially asymmetric transformations. Transformations involve replication or rearrangement of elements within the grid.
*   **Abstracted Transformations:** Transformations are implicit and must be inferred. Identifying content-based and spatial patterns is critical.
*   **Implicit Rules:** Transformation logic is *never* explicitly stated.
*   **Few-Shot Learning Format:** Questions are presented in a few-shot format.
*   **Varying Grid Sizes:** Transformations might be size-dependent; output grid size may differ from input grid. LLM struggles with size differences. (e.g., 2x2, 3x3, 5x5). Test grid dimensions may differ from training. Correctly inferring output grid dimensions is crucial.
*   **Multiple Possible Rules:** Different transformations might yield similar training results but diverge on the test data.
*   **Value Encoding:** Values (e.g., 0, 1, 2, 3, 4, 8, 9) have semantic meaning related to the transformation. Examples often introduce *new* values not present in the input grid.
*   **Element Distribution:** Performance impacted by different element distributions. Sparse (mostly zeros) vs. dense patterns exist.
*   **Limited Training Examples:** Limited examples (typically 1-3) make generalization challenging.
*   **Zero Prevalence:** Many grids contain significant zeros. Non-zero values often represent distinct visual elements.
*   **Limited Integer Values:** Grids contain a limited set of integer values (e.g., 0, 1, 2, 3, 4, 6, 8).
*   **Varied Transformation Rules:** Diverse rules ranging from neighbor-based replacement to tiling/replication patterns, rotations/reflections, or more complex pattern propagations. Pattern complexity varies significantly; some are simple row/column shifts/substitutions, while others involve non-linear transformations or combinations of changes in subgrids.
*   **Combined transformations:** Struggles with multiple rules within a single grid, combining copying and fill values, or performing different transformations simultaneously.
*   **Implicit spatial relationships:** Struggles to discern spatial aspects and accurately capture how the grid modifies elements and positions.
*   **Element-wise transformations:** Training examples demonstrate element-wise transformations based on visual features and patterns within the grid. Specific numbers might trigger changes in neighboring cells, or overall arrangements of numbers could imply a transformation.
*   **Small Integers:** Grids contain small integers, and tasks often involve identifying relationships between different numbers within the grid (e.g., 5s causing modifications to 0s or 1s).
*   **Consistent Training Example Structure:** Questions are presented in a consistent format: a problem description, training examples, and test input, but extracting transformation rules remains complex.
*   **Output grid restrictions:** Output grids have specific dimensions and populations of new elements.

**2. EFFECTIVE TASK-SPECIFIC STRATEGIES**

*   **No consistently effective strategy:** Given consistently low accuracy, no single strategy has emerged as reliably effective (Iterations 22-35: Accuracy <= 0.33).
*   **LLM-based Visual Feature Analysis (Potentially Useful):** Using an LLM to analyze visual features and infer transformation rules is a promising approach, but generalization remains a major challenge. Decomposing the problem into visual feature analysis and transformation application is potentially sound, *if* the model can accurately perform the feature analysis.
*   **Two-Step LLM Approach (Potentially Useful):** The two-step LLM approach (analyze then apply) shows some promise, but is still insufficient for reliable generalization.
*   **Chain-of-Thought with Specialized Agents (Inconsistent):** The chain-of-thought approach, with specialized expert agents for visual feature analysis and transformation application, shows promise but suffers from inconsistent performance due to the LLM's issues with generalization. Role separation may not be effective.
*   **Decomposition (Helpful):** Breaking down the problem into analyzing visual features and applying the transformation simplifies the task. Decomposition into identifying transformation type, analyzing visual features, and applying the transformation appears insufficient to handle variations in grid sizes between training and test examples.
*   **Chain-of-Thought with Multi-Example Prompts (Helpful):** Chain-of-thought prompting with multi-example prompts has been helpful in guiding the LLM to recognize patterns. Multi-example prompting aids in establishing the underlying transformation rule.
*   **Analogical Reasoning (Potentially Useful):** The analogical reasoning approach demonstrates potential in pattern recognition, but has not achieved high accuracy.
*   **Proper API Configuration (Critical):** API configuration is paramount for any LLM-based strategy to function. The `GOOGLE_API_KEY` environment variable must be correctly set, and the chosen LLM model (e.g., 'gemini-pro') must be accessible. LLM access failure due to API configuration issues (Iteration 12).
*   **LLM-guided recursive subdivision (Potentially Useful):** If transformations are locally consistent, then LLM-guided recursive subdivision *might* have potential.
*   **Coordinate-based transformation rules generated from an LLM (Potentially Useful):** At a basic level the LLM is able to correctly identify locations of numbers.
*   **Code Generation approach:** The LLM is successful at generating Python code to manipulate the grid data structures.
*   **Neighbor detection:** The model attempts to look at neighbors and perform transformations based on neighbor values. Pattern recognition considering neighboring cells is a good start.
*   **Region Identification (Potentially Useful):** The general idea of region identification might be useful.

**3. COMMON FAILURE MODES ON THIS DATASET**

*   **Incorrect Output Format:** The most prominent failure is generating an incorrectly formatted output grid, resulting in "Invalid output grid format" errors. (Iteration 35).
*   **Dimensionality Mismatch:** The system fails when the test input grid has different dimensions than the training examples, or when the *output* grid has dimensions inconsistent with the transformation implied by training examples. The core issue lies in the inability to extrapolate or scale the learned transformation rules to grids of different sizes. The LLM hallucinates incorrect placement of elements.
*   **Pattern Generalization:** The primary failure is the inability to generalize from training examples to the test input. The model seems to overfit the training examples, memorizing specific transformations rather than inferring the underlying logic. Even when training examples appear similar, the LLM fails to apply the learned transformation correctly, indicating a lack of robust pattern understanding (e.g., failing to place '2's correctly in positions where input has '1's, Iteration 33).
*   **Incorrect Rule Abstraction/Incomplete Transformations:** Inability to accurately infer the transformation rule from limited examples and then implement it. Often *describes* the rules but fails to *implement* them. The system often applies a partial transformation, failing to extrapolate identified patterns to the entire grid.
*   **Descriptive vs. Implementational Gap:** The agent can often *describe* the transformation rules but fails to *implement* them to produce the final numerical grid.
*   **Halting at Structural Similarity:** Gets stuck in a loop of assessing and refining structural similarity, never producing the concrete numerical output.
*   **Incorrect JSON Formatting:** Frequently produces invalid JSON (backticks, quotation marks, extra nesting, leading/trailing whitespace).
*   **Inability to Extrapolate Complex Transformations:** Struggles to generalize from training examples, especially with complex spatial reasoning.
*   **Incorrect Content Transformation:** Fails to accurately identify and apply correct number transformations.
*   **Numerical Mapping Errors:** Fails to consistently map numbers correctly, memorizing specific number-to-number transformations instead of extracting underlying logic. (Iteration 33: failing to correlate output '2's to original input values).
*   **Spatial Configuration Misinterpretation:** Struggles to discern the spatial aspects of the transformations.
*   **Output Grid Dimension Errors:** Generates output grids with incorrect dimensions.
*   **Null Value Handling:** Encounters `None` values during numerical comparisons.
*   **Unexpected Input Values**: Inconsistencies in input data format not properly validated.
*   **Empty Output Grid:** Returns an empty list `[[]]`.
*   **Inability to Generate Valid Output:** Fails to capture underlying patterns and apply them to the test input.
*   **Complex Reasoning:** Struggles with questions requiring complex spatial or value dependency reasoning.
*   **Over-Reliance on Memorization:** Memorizes training examples rather than generalizing transformation logic.
*   **Inability to Abstract Complex Rules:** Requires abstraction of non-linear relationships and contextual dependencies.
*   **Incorrect Pattern Generalization:** Fails to correctly identify the underlying transformation patterns.
*   **Incorrect Application:** Even when the LLM correctly identifies the transformation, it struggles to apply it to the test input.
*   **Lack of Spatial Precision:** Struggles with precise spatial relationships, failing to place transformed elements correctly.
*   **Shape and Dimensionality Errors:** Generated output grids often have incorrect shapes or dimensions.
*   **Output Format Mismatch:** Output grid does not match the expected size or shape.
*   **Incorrect Value Mapping:** Fails to map values correctly.
*   **Incorrect Element Replacement:** Identifies correct structure but uses wrong numbers.
*   **Value Errors:** Generates grids containing numbers not present in the target grid.
*   **Code Generation Errors:** Outputs Python code rather than the grid itself.
*   **Ambiguity:** Transformations are implicit and can be interpreted in multiple ways.
*   **Complexity:** Transformations involve combinations of replication, shifting, value changes, etc.
*   **Inability to Extract Accurate Transformation Rules:** Consistently fails to extract accurate and generalizable rules.
*   **Fragility of Pattern Recognition:** Pattern recognition is fragile and easily disrupted by small variations.
*   **Lack of Rule Validation:** Rule validation is not robust enough. The verification loop, as implemented, is not effective at detecting and correcting errors in the LLM's reasoning or transformation application.
*   **Localized Contextual Analysis Insufficient:** Struggles to generalize local rules across the entire grid.
*   **Oversimplification of Transformations:** Tends to oversimplify transformation rules.
*   **Complex Rule Interpretation:** Struggles with multiple intertwined rules.
*   **Incomplete Generalization:** Fails to accurately generalize rules based on limited examples.
*   **In-place vs. New Object Confusion:** Confused with modifying the input grid vs. creating a new output grid.
*   **Incorrect Mirroring Logic:** Flawed mirroring logic leads to incorrect placements.
*   **Positional Transformation Neglect:** Unable to accurately capture how elements and positions change.
*   **Difficulty with complex value dependencies:** Struggles when the transformation relies on complex relationships between values.
*   **Misinterpretation of spatial relationships:** Incorrectly interprets how objects and values are spatially related.
*   **Misinterpretation of Visual Features:** `analyze_visual_features` prone to misinterpreting key features.
*   **Inconsistent Transformation Application:** Inconsistent application of identified rules.
*   **Ambiguous Transformations:** Training examples might have multiple interpretations.
*   **Error in output format**: Correct reasoning, but incorrect grid size or text output instead of code.
*   **Dimensionality and Element Distribution:** Fails with different dimensions or element distributions. The LLM doesn't accurately predict output grid size based on training examples (Iteration 33).
*   **Error in Transformation:** LLM produces an error rather than a valid transformation.
*   **Overfitting to Superficial Patterns:** Overfits to simple patterns (Iteration 13).
*   **Inability to Generalize Complex Rules:** Struggles with complex rules involving relationships between elements or regions (Iteration 13).
*   **Output validation inadequate:** Relying on an LLM for output validation may be flawed.
*   **Incorrect Pattern Identification:** LLM fails to correctly identify the transformation pattern.
*   **Inability to Handle Number Transformations:** Struggles with generalizing number transformations.
*   **Code Generation Errors:** Generated code contains logical errors or fails to translate the pattern.
*   **Ignoring Existing Grid Values:** Struggles with transformations requiring *both* copying and fill values.
*   **Context-Switching Errors:** Struggles to perform different transformations in the same grid.
*   **No Fallback Mechanisms:** Lacks robust error handling or fallback mechanisms.
*   **Lack of Output Constraints:** Not constrained to produce valid numerical grids.
*   **Incorrect coordinate application:** Coordinate transformation misapplied.
*   **Combined Analytical and Application Complexity:** Requires perfect output format which is difficult to guarantee.
*   **Misinterpretation of Patterns:** Model incorrectly identified a pattern.
*   **Incorrect Transformation Implementation:** Implements incorrect code.
*   **Limitation of Simple CoT:** LLM unable to correctly interpret the problem transformation.
*   **Incorrect Corner Rotation:** Model identifies clockwise corner rotation but applies it incorrectly.
    *   **Misinterpreting Non-Zero Value Patterns:** Model struggles to decipher patterns in grids with non-zero values.
    *   **Faulty Reflection Logic:** Meta-reasoning agent fails to identify proper reflection.
*   **Incorrect Region Identification/Transformation:** The LLM fails to identify key regions and the correct swapping pattern, leading to incorrect transformations of rows and columns. The LLM misinterprets the relevant features in the grids, failing to identify consistent regions and corresponding transformation rules between input and output grids.
*   **Matrix summarization/reduction instead of transformation:** The system consistently failed by outputting small, summarized matrices (e.g., 2x2) instead of transforming the full input grid based on the provided examples.
*   **Ignoring the input grid's dimensions:** The system failed to preserve the size/dimensions of the input grid during transformation.
*   **Incorrect Anchor Identification:** The LLM fails to accurately identify stable "visual anchors" or misinterprets their role in the transformation. If the "anchor" is incorrectly perceived as a corner of a pattern, the subsequent transformation will be misaligned in the test grid.
*   **Faulty Rule Inference:** Even with correct anchors, the LLM struggles to infer the transformation rule, leading to incorrect replacement logic.
*   **Overgeneralization:** The system can overgeneralize patterns, leading to incorrect transformations (e.g., transforming the tenth row incorrectly in error example one, assuming numbers should become '2's).
*   **Limited Contextual Understanding:** The system fails to capture the full range of contextual dependencies for substitutions. In training examples, substitutions are subtle and depend heavily on recognizing which neighbors are present/absent (e.g., error example 2, incorrect pattern after row 4).
*   **Incomplete Rule Application:** The system often applies a partial transformation, identifying some patterns but failing to extrapolate them to the entire grid (rows 3-5 of first error example).

**4. EXPERIMENT LOG & FINDINGS**

*   **Iteration 0:** Direct pattern matching is ineffective (Accuracy 0.00).
*   **Iteration 1:** Explicit rule extraction and validation are insufficient (Accuracy 0.00).
*   **Iteration 2:** Localized contextual analysis is insufficient (Accuracy 0.00).
*   **Iteration 3:** Breaking down grid transformation into individual element transformations is insufficient (Accuracy 0.00).
*   **Iteration 4:** Extracting and applying rules with validation does not lead to performance (Accuracy 0.00)
*   **Iteration 5:** Row and column analysis does not simplify pattern recognition (Accuracy 0.00).
*   **Iteration 6:** Hierarchical decomposition does not address the underlying inability to generalize (Accuracy 0.00).
*   **Iteration 7:** Describing transformations in terms of visual features partially improves generalization (Accuracy 0.33).
*   **Iteration 8:** Analogical reasoning via multiple LLM calls was not validated (Accuracy 0.00).
*   **Iteration 9:** Relying solely on LLM's ability to directly transform grid based on multi-example prompts is insufficient (Accuracy 0.00).
*   **Iteration 10:** Multi-example prompting improves performance but is insufficient for reliable generalization (Accuracy 0.33).
*   **Iteration 11:** Exploitation of the current approach did not yield significant improvement (Accuracy 0.33).
*   **Iteration 12:** LLM access failure due to API configuration issues (Accuracy 0.00). Proper API configuration is paramount.
*   **Iteration 13:** Detailed examples and validation loop do not improve visual feature analysis and transformation application (Accuracy 0.00).
*   **Iteration 14:** LLM cannot directly generate the output grid by learning a transformation function represented implicitly in the examples (Accuracy 0.00).
*   **Iteration 15:** Multi-example prompting and implementing explicit output checks did not improve generalization (Accuracy 0.00).
*   **Iteration 16:** Better validation loops do not lead to higher generalization (Accuracy 0.00).
*   **Iteration 17:** LLM cannot generate and follow a natural language transformation script effectively (Accuracy: 0.33).
*   **Iteration 18:** A multi-agent iterative refinement strategy cannot improve the LLM's ability to generalize grid transformation patterns (Accuracy: 0.00)
*   **Iteration 19:** LLM-Orchestrated Recursive Subdivision and Transformation fails (Accuracy: 0.00). Encountered `None` values.
*   **Iteration 20:** Prompting coordinate-based rules with contextual awareness only partially supports the hypothesis (Accuracy: 0.33).
*   **Iteration 21:** Region-based transformation with rule selection via LLM is not effective (Accuracy 0.33).
*   **Iteration 22:** Knowledge Graph approach does not improve generalization or spatial reasoning (Accuracy: 0.33).
*   **Iteration 23:** LLM to analyze visual features and generate the output grid, is unsuccessful. Consistently fails to produce the required nested list structure, leading to 0.00 accuracy. *SCRIPT ERROR ENCOUNTERED*.
*   **Iteration 24:** Contextual grid completion with value propagation using a pattern-based template and LLM verification failed (Accuracy: 0.00).
*   **Iteration 25:** Adding more examples and a more robust validation loop with formatting enforcement did not improve generalization. (Accuracy: 0.00).
*   **Iteration 26:** Structural similarity analysis combined with iterative refinement failed (Accuracy: 0.00).
*   **Iteration 27:** Meta-reasoning approach to strategy selection resulted in 0.00 accuracy, indicating the strategy selection and application process is ineffective.
*   **Iteration 28:** Structured approach with region identification, pattern inference, and template completion failed to achieve acceptable accuracy (0.33).
*   **Iteration 29:** Multi-example prompting failed (Accuracy 0.00).
*   **Iteration 30:** Multi-example prompting and verification loop failed (Accuracy 0.33).
*   **Iteration 31:** Visual anchor identification and transformation rule application using an LLM achieved poor performance (Accuracy 0.00).
*   **Iteration 32:** Local pattern analysis with iterative refinement achieved 33% accuracy. Models struggle to robustly and accurately identify and apply the relevant transformation rules; adaptive context adjustment doesn't help.
*   **Iteration 33:** Template matching with contextual refinement failed (Accuracy 0.00). Demonstrates a failure to understand underlying patterns; LLM struggled with both template matching and refining transformations based on context.
    *   **Iteration 34:** Three-agent role assignment approach failed to improve accuracy. Decomposing the task into identifying transformation type, analyzing visual features, and applying the transformation appears insufficient to handle variations in grid sizes between training and test examples. Demonstrates that the LLM hallucinates elements, dimensions of training and test grids differ.
*   **Iteration 35:** Exploitation strategy failed (Accuracy: 0.00). System failed to generate a correctly formatted output grid, returning "Invalid output grid format" errors. Failure occurs because the system struggles with inferring the correct dimensions of the output grid and accurately populating it, even for simple replication tasks. *SCRIPT ERROR ENCOUNTERED*.

**5. NEXT RESEARCH DIRECTIONS**

*   **Dimensionality Awareness and Enforcement:** Explicitly add a module to the script that checks the output grid's dimensions and content *against the constraints implied by the input grids in the training examples*. Refine the prompting strategy to specifically request the LLM to output the grid dimensions *before* generating the full grid, allowing for independent verification. Prompt strategies should encourage reasoning about the *relationship* between input and output grid dimensions *before* generating the transformed grid.
*   **Robust Output Validation:** Implement a more robust validation function (`is_valid_grid`) that checks not just format, but also dimensional consistency with training examples and element values.
*   **Size Invariance:** Implement a mechanism to normalize grid sizes during the feature analysis stage. This could involve scaling or padding the input grids to match a common size before applying the transformation.
*   **Explicit Spatial Relationship Modeling:** Augment the feature analysis to explicitly capture spatial relationships between elements in the grid. For example, model relative positions and distances between key elements, rather than absolute coordinates.
*   **Transformation Parameterization:** Instead of relying solely on visual feature analysis, try to parameterize the transformation itself. For example, represent rotations as angles, scaling as ratios, and translations as vectors. The LLM can then predict these parameters based on the training examples and apply them to the test input.
*   **Implement a robust validation step:** Validate the dimensions of the generated grid before outputting, padding or cropping the grid if necessary.
*   **Focused Training Examples:** Design a training set that specifically addresses the common transformation operations (replication, translation, value mapping). The training examples should isolate each type of transformation to help the LLM learn them individually.
*   **Explicit Transformation Rules:** Prompt the LLM to first explicitly state the transformation rule it identifies in the training examples, and then apply that rule to the test input. This will provide a more interpretable solution and allow for easier debugging.
*   **Dimensionality Handling:** Include pre-processing steps to normalize or standardize grid sizes, or to explicitly encode the grid dimensions into the LLM prompt.
*   **Iterative Refinement:** Instead of a single refinement step, implement an iterative refinement loop where the LLM refines its solution in multiple passes, receiving feedback on each pass.
*   **Expand Contextual Analysis:** Increase the scope of the "local pattern analysis" to consider a larger neighborhood around each cell. Explore techniques to weigh the importance of different neighbors (e.g., cells immediately adjacent vs. cells further away).
*   **Explicit Rule Extraction:** Instead of relying solely on LLMs to implicitly learn the rules, consider a mechanism to explicitly extract transformation rules from the training examples. This could involve identifying recurring patterns in the input and output grids and formalizing them as rules.
*   **Fine-grained Conditionals:** Incorporate more fine-grained conditional logic into the transformation application process. This could involve using the extracted rules to create conditional statements that determine how each cell should be transformed based on its local context.
*   **Curriculum Learning:** Train the model on simpler examples first, gradually increasing the complexity. This can help the model learn the basic transformations before tackling more complex scenarios.
*   **Data Augmentation:** Create synthetic training examples by applying known transformations to existing examples. This can help increase the diversity of the training data and improve the model's generalization ability.
*   **Improve Pattern Abstraction:** Prompt the LLM to describe the *transformation logic* in a general way (e.g., "rows 1-3 are copied to rows 5-7"). Increase the number and diversity of training examples to expose the LLM to a wider range of possible transformations. Focus on improved pattern recognition. Experiment with methods that explicitly encourage the LLM to identify and describe the transformation rules in a structured way. Prompt the LLM to output a formal representation of the transformation rules.
*   **Verification and Validation:** Implement additional validation steps to verify intermediate results and completed grids. The LLM should be asked to justify its choices. Incorporate external knowledge or reasoning to validate the transformation. Explore techniques that incorporate external knowledge or reasoning to validate the transformation. Implement a more robust verification strategy.
*   **Region Definition:** Experiment with different methods for region definitions - perhaps using coordinates or other explicit instructions.
*   **Enforce Output Generation:** Modify the prompt to *explicitly* demand the full transformed grid as the final answer. Add constraints to ensure the LLM outputs a numerical grid.
*   **Rule Extraction Focus:** Shift the focus from code generation to more robust rule extraction. Prompt the LLM to *describe* the transformation rule in detail *before* generating code.
*   **Separate Reasoning and Execution:** Separate the tasks of describing the transformation rules and applying them. Have the LLM output a set of transformation rules in a structured format (e.g., JSON), then use a deterministic function to apply these rules to the test grid.
*   **Test-Driven Development (TDD) Approach:** Implement a TDD-like workflow. Generate a proposed output, then compare to the gold standard using a similarity metric, and use this feedback to refine the code *automatically*.
*   **Introduce a "Completion" Condition:** Add a mechanism to detect when further iterations are unlikely to improve the result. If similarity score changes are below a threshold, force the final grid to be output.
*   **Symbolic Reasoning:** Explore incorporating symbolic reasoning techniques or external tools to help the LLM identify and represent the transformation rules more formally.
*   **Decomposition of Sub-Tasks:** Explicitly decompose the process of rule extraction, rule validation, and code implementation.
*   **Few-shot examples:** Investigate providing the model with more varied few-shot examples to improve the extraction of the transformation rules. Carefully curate the training set to include examples that highlight different types of transformations and spatial relationships.
*   **Focus on JSON Formatting:** Implement a strict post-processing step that validates the JSON output and corrects common formatting errors.
*   **Simplify the Transformation Task:** Break the task down into smaller, more manageable steps. For instance, identify the type of transformation occurring (e.g., value replacement, pattern replication) and then apply specific functions to implement that transformation.
*   **Improve Template Identification and Encoding:** Investigate how to better identify and encode the template or pattern present in the training examples.
*   **Consider a Hybrid Approach:** Explore combining the LLM with a more traditional algorithm.
*   **Address Output Size Variability:** Explicitly handle cases where the output grid size is different from the input grid size.
*   **Explore data preprocessing techniques.** Investigate methods to simplify the grid representations or highlight relevant visual features to improve the LLM's ability to analyze the patterns.
*   **Improve Pattern Generalization:** Implement a mechanism to explicitly encourage the LLM to focus on relationships *between* elements and regions in the input/output grids.
*   **Decompose Numerical Mapping:** Instead of letting the LLM directly predict numerical substitutions, force it to extract rules for *how* numbers change.
*   **Augment Training Examples:** Provide more varied training examples.
*   **Explicit Spatial Reasoning:** Improve the prompts to encourage the LLM to explicitly describe spatial relationships.
*   **Reinforce Spatial Reasoning:** Provide the LLM with more focused training examples that emphasize spatial relationships.
*   **Coordinate Transformation:** Consider more direct coordinate transformations to supplement the knowledge graph approach.
*   **Iterative KG Refinement:** Implement a feedback loop where the LLM can iteratively refine the knowledge graph based on errors in the transformation.
*   **Focus on Debugging and Correctness:** Prioritize debugging the core logic responsible for grid manipulation and output generation.
*   **Robust Input Validation:** Thoroughly validate the input grid to ensure it contains only expected numerical values and has the correct dimensions.
*   **Null/None Handling:** Add explicit checks for `None` values *before* any numerical comparison or operation.
*   **LLM Output Validation and Constraints:** Constrain the LLM to produce valid numerical outputs that conform to the grid's expected data type.
*   **Implement Error Recovery:** Incorporate mechanisms to detect and recover from errors during the LLM-based transformation process.
*   **Prioritize LLM Access and Error Handling:** Verify LLM access and improve error handling within the `call_llm` function.
*   **Enhanced Feature Analysis:** The `analyze_visual_features` function needs significant improvement.
*   **Refine Transformation Application:** Enhance the `apply_transformation` function to accurately apply the inferred transformations to the test input grid.
*   **Implement Robust Output Validation:** Implement a comprehensive validation step to check the dimensions, value ranges, and overall structure of the output grid.
*   **Implement a More Robust Rule Extraction Mechanism:** Develop a mechanism that can identify and formalize the transformation rules in a more abstract and general way.
*   **Explore Different Model Architectures:** Evaluate the performance of other model architectures.
*   **Incorporate a More Fine-Grained Validation Process:** Implement a validation process that checks the individual steps of the transformation, rather than just the final result.
*   **Enhanced Example Descriptions:** Provide more structured information to the LLM, emphasizing key elements like grid dimensions and relationships between input and output.
*   **Transformation Validation:** Implement a more robust validation step for the transformation descriptions generated by the LLM.
*   **Refine Output Formatting:** Implement stricter output format validation.
*   **Enhanced Feature Analysis:** Prompt the LLM to explicitly identify the *type* of transformation (e.g., "maximum value in a subgrid," "rotation," "reflection," "number replacement based on neighbor values") before attempting to describe it in detail.
*   **Targeted Examples:** Carefully select training examples that represent a wider variety of transformations and edge cases to improve pattern generalization.
*   **Code Generation Fine-Tuning:** Encourage the LLM to generate a *validated* code implementation of the transformation rules.
*   **Explicit Spatial Reasoning:** If spatial relationships are involved, provide the LLM with explicit spatial reasoning tools.
*   **Improved Output Validation:** Implement a more robust validation function that can evaluate the *logic* of the transformation in the output grid.
*   **Introduce Explicit Rule Extraction:** Focus on methods that first extract explicit transformation rules from the training examples and then apply those rules to the test input.
*   **Decompose the Transformation Process:** Decompose the transformation process into smaller, more manageable steps.
*   **Increase Training Data Diversity:** Supplement the training data with more diverse examples.
*   **Explore Hybrid Approaches:** Investigate hybrid approaches that combine the LLM's reasoning abilities with more traditional algorithms.
*   **Implement Validation Techniques:** Develop more robust validation techniques that can detect and correct errors in the transformed grid.
*   **Introduce Verification Mechanisms:** Implement more robust verification mechanisms to validate the transformation description before applying it to the test input.
*   **Fine-tune LLM Prompts:** Carefully refine the prompts used for `call_llm` to provide more context and guidance to the LLM.
*   **Train for positional reasoning**: It's not enough to know what values to change. The system must reason about *where* to change them.
*   **Rethink the LLM Agent Roles:** Re-evaluate the roles of the LLM agents.
*   **Incorporate Validation Steps:** Add validation steps to ensure the generated output grid adheres to patterns observed in the training examples.
*   **Focus on Rule Decomposition:** Explicitly decompose the transformation rule into smaller, more manageable sub-rules.
*   **Implement a More Structured Validation Process:** Develop a more rigorous validation process that checks for specific aspects of the transformation, such as element counts and row/column patterns.
*   **Explore Explicit Coordinate-Based Rules:** Shift the representation of rules to be more explicit about coordinates.
*   **Generate More Diverse Training Data:** Consider augmenting the training dataset with examples that cover a wider range of transformation types and complexities.
*   **Add unit tests:** Add unit tests for `transform_grid` function.
*   **Enhanced Rule Extraction:** Refine the `extract_transformation_rule` agent to focus explicitly on identifying the source locations of numbers to be replicated. Implement a mechanism to distinguish between "source" values and "filler" values.
*   **Value-Specific Propagation:** Modify the `refine_transformation_rule` and `apply_transformation` agents to ensure that the correct values are being propagated based on their source locations.
*   **Hybrid Approach:** Test a combination of explicit rule-based transformations (hard-coded logic for common patterns) with the LLM-based agents.
*   **Context Aware Prompting:** Change the prompting strategy to explicitly instruct agents to be aware of the grid context during transformations.
*   **Simplify Transformation Logic:** Start with very basic transformation patterns and gradually increase complexity.
*   **Output format:** The prompt needs to be much more specific to encourage the output to be a valid grid, not just a description of a grid.
*   **Implement a more robust method for pattern identification, potentially incorporating techniques like convolution filters or feature extraction methods to discern relevant features.**
*   **Refine the agent's ability to accurately translate identified patterns into executable rules.**
*   **Test a wider range of strategies beyond simple rotation or reflection.**
*   **Implement unit tests for each sub-module, especially focusing on the strategy application to isolate failure points.**
*   **Refine prompting for element-wise transformations:** Modify the `analyze_visual_features` prompt to explicitly request an element-wise transformation rule.
*   **Ensure output grid size consistency:** Constrain the `apply_transformation` function to *always* produce an output grid with the *exact* same dimensions as the input grid.
*   **Explicitly describe cell relationships in prompts:** Instead of general visual feature analysis, prompt the LLM to specifically analyze how the value of a cell (and its surrounding cells) in the Input Grid affects the corresponding cell in the Output Grid.
*   **Incorporate a "reasoning trace" in the prompt:** Ask the LLM to provide a step-by-step explanation for how each cell in the test input grid is transformed.
*   **Consider few-shot prompting with "before-and-after" cell examples:** Include in the prompt, specific examples of how individual cells (with their surrounding context) change from the Input Grid to the Output Grid.
*   **Verify the output grid's validity**: Implement a checker function to ensure that the output grid is a valid grid by verifying that it is a 2D array.
*   **Focus on improved pattern recognition:** Experiment with methods that explicitly encourage the LLM to identify and describe the transformation rules in a structured way.
*   **Consider breaking down the transformation process into smaller, more manageable steps:** Instead of having the LLM directly generate the output grid, explore approaches that involve intermediate steps, such as identifying key features, applying local transformations, and then assembling the final grid.
*   **Improve Anchor Identification Robustness:** Explore techniques to make anchor identification more robust. This could involve providing more explicit instructions to the LLM about what constitutes a good anchor or using a different method for anchor selection altogether (e.g., a rule-based approach).
*   **Enhance Rule Inference with Contextual Information:** Provide the LLM with more contextual information during rule inference. This could include explicitly stating the relationships between elements in the grid (e.g., "element A is above element B") or providing additional training examples that highlight different aspects of the transformation rule.
*   **Implement a Validation Mechanism:** Implement a validation mechanism to check the plausibility of the inferred transformation rules. This could involve applying the rules to the training examples and comparing the results to the known output grids, or using a different LLM to evaluate the quality of the rules.
*   **Explore Alternative Transformation Approaches:** If LLMs struggle with this task, consider exploring alternative approaches such as image processing techniques.
*   **Augment Training Data:** Consider augmenting the training data with more diverse examples to improve the LLM's generalization ability. The augmented examples should expose the LLM to
        

            CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        

            EXPLORATION GUIDANCE:
            1. Review the historical approaches, error patterns, and accumulated learnings carefully
            2. Review the FULL CODE of previous scripts to understand what has already been tried
            3. Design a new approach that is DISTINCTLY DIFFERENT from previous attempts. This approach should have a specific NEW HYPOTHESIS or variable you are trying to test. 
            4. CRITICAL: Include EMBEDDED EXAMPLES directly within your LLM prompts
            5. For each key function, show a complete worked example, or include multiple examples, including:
               - Input example that resembles the dataset
               - Step-by-step reasoning through the example
               - Properly formatted output
            6. Apply the insights from the ACCUMULATED LEARNINGS section to avoid repeating past mistakes
            7. Pay SPECIAL ATTENTION to the weaknesses and improvement suggestions from the capability assessment
            8. Consider implementing one or more of these LLM usage patterns:
               - Repeated validation with feedback loops
               - Multi-perspective analysis with synthesis
               - Dynamic input-dependent routing with an orchestrator
               - Hybrid approaches combining LLM with deterministic functions
               - Best-of-n solution generation and selection
               - ReAct pattern for interactive reasoning and action
               - If it is unknown how successful a processing state or part of the pipeline is, include verification steps to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
               - Answer checkers to validate the final answer against the problem statement. If the answer is incorrect, the checker can send the answer back to an earlier part of the system for for refinement with feedback

            Here's how to call the Gemini API. Use this example without modification and don't invent configuration options:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

            Since this is an EXPLORATION phase:
            - Try a fundamentally different approach to reasoning about the problem. Test a NEW HYPOTHESIS or variable, and add verification steps to deduce if this new change is helpful.
            - THIS IS KEY: Break down the problem into new, distinct reasoning steps based on past performance before you start coding
            - For EACH key LLM prompt, include a relevant example with:
              * Sample input similar to the dataset
              * Expected reasoning steps
              * Desired output format
            - Apply a verifier call to different parts of the pipeline in order to understand what parts of the pipeline of calls is successful and where the system is breaking
            - Pay special attention to addressing the primary issues from previous iterations
            - Ensure your new approach addresses the weaknesses identified in the capability assessment

            CRITICAL REQUIREMENTS:
            1. The script MUST properly handle all string literals - be extremely careful with quotes and triple quotes
            2. The script MUST NOT exceed 150 lines of code to prevent truncation
            3. Include detailed comments explaining your reasoning approach
            4. EVERY SINGLE LLM PROMPT must include at least one embedded example showing:
               - Sample input with reasoning
               - Desired output format
            5. Make proper use of error handling
            6. Implement robust capabilities to address the specific weaknesses identified in the capability assessment
            7. Do NOT use json.loads() in the LLM calls to process input data. JSON formatting is good to use to structure information as inputs and outputs, but attempting to have functions process JSON data explicitly with strict built-in functionality is error prone due to formatting issues and additional text that appears as documentation, reasoning, or comments. When passing data into another LLM call, you can read it as plain text rather than trying to load it in strict json format, is the better approach.

            Return a COMPLETE, RUNNABLE Python script that:
            1. Has a main function that takes a question string as input and returns the answer string
            2. Makes multiple LLM calls for different reasoning steps
            3. Has proper error handling for API calls
            4. Includes embedded examples in EVERY LLM prompt
            5. Is COMPLETE - no missing code, no "..." placeholders
            6. Closes all string literals properly

            This should be FUNDAMENTALLY DIFFERENT from all previous approaches. Do not reuse the same overall structure.

            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            