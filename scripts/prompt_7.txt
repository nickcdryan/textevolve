
            You are developing a Python script to solve problems using LLM reasoning capabilities.
            You are in the EXPLORATION PHASE. You must generate a NEW approach that's different from previous approaches but informed by their successes and failures. With this approach, you will have a specific NEW HYPOTHESIS or variable you are trying to test. Your goal is to see if this new approach works, and you must add verification and validation steps to deduce if this new change is helpful. You may also test RADICAL NEW APPROACHES that are substantially different from previous approaches. 
            
            You should try NEW THINGS:
            
            Break down the problem into smaller pieces
            Think CREATIVELY about how to solve your problem if other approaches aren't working
            Transform data into different formats to see if it helps

            # YOUR TASK
            You are deeply familiar with prompting techniques and the agent works from the literature. 
            Your goal is to maximize the specified performance metrics by proposing interestingly new agents.
            Observe the past discovered agents and scripts carefully and think about what insights, lessons, or stepping stones can be learned from them.
            Be creative when thinking about the next interesting agent to try. You are encouraged to draw inspiration from related agent papers or academic papers from other research areas.
            Use the knowledge from the archive and inspiration from academic literature to propose the next interesting agentic system design.
            THINK OUTSIDE THE BOX.
            

            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "PASSAGE: Coming off their divisional home win over the Chiefs, the Chargers flew to Heinz Field for a Week 11 duel with the Pittsburgh Steelers.  In the first quarter, the 'Bolts struck first as RB LaDainian Tomlinson got a 3-yard TD run.  In the second quarter, the Steelers slowly responded as LB James Harrison forced a fumble from QB Philip Rivers in his own endzone, which was picked up by OT Marcus McNeill, who got tackled by Harrison in the endzone for a safety.  Pittsburgh would end the half as kicker Jeff Reed got a 21-yard field goal. In the third quarter, the Steelers took the lead as Reed made a 22-yard field goal.  In the fourth quarter, San Diego regained the lead as kicker Nate Kaeding made a 22-yard field goal.  However, Pittsburgh got the lead as Reed nailed a 32-yard field goal. This marked the first time in NFL history that an NFL game ended with a final score of 11-10.\n\nQUESTION: How many total points were scored in the game?",
    "answer": "21"
  },
  {
    "id": 1,
    "question": "PASSAGE: Hoping to rebound from their loss to the Saints the Seahawks played on home ground where they played their former division rival, the Kansas City Chiefs. In the first quarter, the Seahawks trailed early with QB Matt Cassel getting a 7-yard TD pass to WR Dwayne Bowe. They responded after FS Earl Thomas returned a blocked punt 10 yards for a touchdown. They fell behind as Shaun Smith got a 1-yard TD run, followed by Cassel finding Bowe again on a 36-yard TD pass. The Seahawks cut the lead when kicker Olindo Mare got a 43-yard field goal, followed by QB Matt Hasselbeck getting a 13-yard TD pass to TE Chris Baker. The struggled further with RB Jamaal Charles getting a 3-yard TD run, followed by Cassel throwing to Bowe on a 9-yard TD pass. The Seahawks responded as Hasselbeck completed an 87-yard TD pass to WR Ben Obomanu, but the Chiefs increased their lead as Cassel got a 6-yard TD pass to TE Tony Moeaki.\n\nQUESTION: Which player had the first touchdown run of the game?",
    "answer": "Shaun Smith"
  },
  {
    "id": 2,
    "question": "PASSAGE: In mid-October 1524, Francis himself crossed the Alps and advanced on Milan at the head of an army numbering more than 40,000. Bourbon and d'Avalos, their troops not yet recovered from the campaign in Provence, were in no position to offer serious resistance. The French army moved in several columns, brushing aside Imperial attempts to hold its advance, but failed to bring the main body of Imperial troops to battle. Nevertheless, Charles de Lannoy, who had concentrated some 16,000 men to resist the 33,000 French troops closing on Milan, decided that the city could not be defended and withdrew to Lodi on 26 October. Having entered Milan and installed Louis II de la Tr\u00e9moille as the governor, Francis  advanced on Pavia, where Antonio de Leyva remained with a sizable Imperial garrison.\n\nQUESTION: How many more men did the French troops 33,000 have over Charles de Lannoy 16,000 men?",
    "answer": "17000"
  }
]

            HISTORICAL CONTEXT:
            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 7
        - Current explore/exploit balance: 60/20
        - Best accuracy achieved: 1.00 (iteration 2)

        APPROACH HISTORY (last 7 iterations):
        [
  {
    "iteration": 0,
    "strategy": "baseline",
    "accuracy": 0.8,
    "approach": "Simple baseline script: Direct LLM call without sophisticated techniques"
  },
  {
    "iteration": 1,
    "strategy": "explore",
    "accuracy": 0.6666666666666666,
    "approach": "The script implements a multi-stage LLM-driven approach to answer a question based on a given passage. It decomposes the problem into keyword identification and passage simplification, information extraction, and answer verification stages. Three LLM agent roles are employed: a passage simplifier, an information extraction expert, and an answer checker. The `call_llm` function is used to interact with the Gemini model. The `main` function orchestrates the workflow: `call_llm` is used to extract keywords and simplify the passage, then used to extract the answer, and finally used again to verify the extracted answer's correctness."
  },
  {
    "iteration": 2,
    "strategy": "exploit",
    "accuracy": 1.0,
    "approach": "The script uses a hybrid approach, combining direct LLM calls with multi-stage analysis for question answering. It decomposes the problem into keyword identification and passage simplification, information extraction with examples, and verification. There are two agent roles: Passage Simplifier, Information Extractor and Answer Verifier and Corrector.\n\nThe main function orchestrates the process, first using `call_llm` with a \"Passage Simplifier\" to simplify the input passage. It then calls `call_llm` again with an \"Information Extractor\" to extract a concise answer. Finally, it calls `call_llm` a third time with an \"Answer Verifier and Corrector\" to verify the extracted answer and correct it if needed. The `call_llm` function is a wrapper for interacting with the Gemini LLM, taking a prompt and optional system instruction as input."
  },
  {
    "iteration": 3,
    "strategy": "explore",
    "accuracy": 0.5,
    "approach": "The script implements a question-decomposition and answer-synthesis approach using the Gemini LLM to enhance reasoning. The main function, `main`, decomposes the original question into sub-questions, answers each sub-question individually, and then synthesizes these answers into a final comprehensive answer. It uses the `call_llm` function to interact with the Gemini API, utilizing different system instructions for question decomposition, answering sub-questions, and synthesizing information. The functions `call_llm` sends requests to the Gemini model, and the `main` function orchestrates the decomposition, answering, and synthesis steps to arrive at the final answer."
  },
  {
    "iteration": 4,
    "strategy": "explore",
    "accuracy": 0.9,
    "approach": "The script employs a fact-verification with self-correction approach using the Gemini LLM to answer questions. It decomposes the problem into three stages: initial answer generation, fact verification/self-correction, and final output validation for conciseness. Three agent roles are employed: a precise information retriever, a fact-checker/self-correction expert, and a validator for concise answers.\n\nThe functions used are `call_llm` for querying the LLM, which is called within `main` to generate the initial answer, verify the answer, and validate the final output. The `main` function orchestrates the overall workflow, passing prompts to `call_llm` and processing the responses to refine the answer. The overall workflow involves generating an initial answer, verifying its correctness, correcting if necessary, and then validating/shortening the final answer."
  },
  {
    "iteration": 5,
    "strategy": "explore",
    "accuracy": 0.6,
    "approach": "The script implements a knowledge retrieval and answer generation approach using an LLM in two steps. First, the `call_llm` function is used with the LLM to generate a search query based on the input question. The `perform_search` function simulates a web search using the generated query. Finally, the `call_llm` function is again used with the LLM to synthesize the search results with the original question to generate a final answer. The `main` function orchestrates the workflow by calling the `call_llm` function twice, once for query generation and once for answer synthesis, and `perform_search` to simulate searching."
  },
  {
    "iteration": 6,
    "strategy": "exploit",
    "accuracy": 0.9,
    "approach": "The script employs a multi-stage, LLM-driven approach combining fact verification, self-correction, and relevant information extraction to answer questions. It decomposes the problem into initial answer generation, fact verification, keyword identification for sentence extraction, and final validation. The LLM acts in different roles throughout the process, including information retriever, fact-checker, keyword extractor, and final validator.\n\nThe functions used are `call_llm` (for prompting the LLM) and `main` (orchestrating the entire process). The `main` function calls `call_llm` with specific prompts designed for each stage: initial answer generation, verification, keyword extraction, and final validation. The output of one `call_llm` invocation often serves as the input for the next, creating a chain of reasoning.\n\nThe overall workflow begins with generating an initial answer using `call_llm`, verifying it, extracting a relevant sentence from the context, and then using the extracted sentence and corrected answer within a final prompt to produce the final answer with `call_llm`."
  }
]

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 3 iterations):
        [
  {
    "iteration": 0,
    "issue": "The most critical problem is the system's difficulty in isolating the *precise* answer element. It often provides overly descriptive responses, including context and redundant information, which leads to mismatches with the expected golden answers that prioritize conciseness."
  },
  {
    "iteration": 1,
    "issue": "The system misidentified at least one of the numerical values (longest touchdown pass or longest field goal) needed to perform the subtraction in error case 0, leading to an incorrect calculation. The system does not seem to be choosing the correct numbers based on contextual understanding."
  },
  {
    "iteration": 2,
    "issue": "The single most critical problem to address, based on the successful cases and general observations is:\n\n**Improving the system's reasoning capabilities to handle more complex questions and requiring deeper contextual understanding.**"
  },
  {
    "iteration": 3,
    "issue": "The primary issue is **inaccurate information extraction, specifically for dates and numerical values**. This often leads to miscalculations and flawed reasoning, even when the underlying logic is sound."
  },
  {
    "iteration": 4,
    "issue": "The single most critical problem is the system's failure to perform complex numerical reasoning, including identifying all relevant numerical information and performing the required arithmetic to arrive at the correct answer."
  },
  {
    "iteration": 5,
    "issue": "The most critical problem is the system's **inability to perform precise, focused reasoning based on the specific wording of the question and the context of the provided passage.** It often provides more information than required, or focuses on irrelevant details, indicating a lack of clear understanding of the question's intent."
  },
  {
    "iteration": 6,
    "issue": "The most critical problem is the system's inability to integrate external knowledge (like Boris Yeltsin's birth year) with information extracted from the passage to answer the question. The system's information extraction and reasoning capabilities are too narrowly focused on direct factual recall and simple arithmetic using only text within the provided passage."
  }
]

        TARGETED IMPROVEMENTS:
        [
  "Use coreference resolution to accurately track entities and their relationships throughout the passage.",
  "Explore using external tools like calculators or symbolic math solvers to improve accuracy.",
  "Add print statements and intermediate outputs throughout the code** to aid in debugging and tracking the system's reasoning process. This will help to identify where the system is going wrong and why.",
  "Implement a dedicated module for performing numerical calculations.",
  "Train the model on a larger dataset of question-answer pairs that require complex reasoning and inference.",
  "Add unit tests to ensure the numerical reasoning module performs correctly.",
  "Implement a post-processing step to verify that the answer meets the specific requirements of the question.** This step could involve checking the answer for unnecessary details or incorrect calculations.",
  "Train a separate model specifically for extracting date information.",
  "Fine-tune the model with question-answer pairs specifically designed to test its understanding of numerical questions and precise reasoning.** These pairs should include questions that require the system to differentiate between related but distinct concepts.",
  "Implement a pre-processing step to analyze the question and identify the specific information being requested.** This step could involve techniques such as keyword extraction or dependency parsing. This should guide the information extraction from the passage."
]
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity ✓
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity ✓
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity ✓
    4. All operations have O(1) average time complexity ✓
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```\n\n```python\ndef solve_with_meta_programming(question):
            """
            Advanced: Script generates and executes its own code/prompts dynamically.
            The script acts as its own programmer and prompt engineer.
            """

            # Step 1: Analyze what approach is needed
            strategy_prompt = f"""
            For this problem: {question}

            What's the best approach?
            A) Generate Python code to calculate/process something
            B) Generate specialized LLM prompts for analysis  
            C) Use a hybrid approach with both code and LLM calls

            Explain your choice and what specific code or prompts I should generate.
            """


                analysis_system_prompt = """ 
                You are a problem analysis expert. You are a master of problem analysis and can 
                determine the best approach to solve a problem, understanding the strenghts and 
                weaknesses of LLMs for problem solving, when to delegate a more specific or problem 
                or subproblem to an additional LLM call, and when to write code to solve a problem.
            """
            strategy = call_llm(strategy_prompt, analysis_system_prompt)

            # Step 2: Generate and execute based on strategy
            if "###CODE_ONLY###" in strategy.lower():
                # Generate code dynamically
                code_gen_prompt = f"""
                Problem: {question}
                Strategy: {strategy}

                Write Python code to solve this problem. Include print statements for output.
                Return ONLY the Python code:
                """

                generated_code = call_llm(code_gen_prompt, "You are a Python programmer.")

                # Clean up code if wrapped in markdown
                import re
                code_match = re.search(r'```python\s*\n(.*?)\n```', generated_code, re.DOTALL)
                if code_match:
                    clean_code = code_match.group(1).strip()
                else:
                    clean_code = generated_code.strip()

                # Execute the generated code
                execution_result = execute_code(clean_code)

                # Interpret the execution result
                interpretation_prompt = f"""
                Original problem: {question}
                Generated code: {clean_code}
                Execution result: {execution_result}

                What is the final answer based on these results?
                """

                final_answer = call_llm(interpretation_prompt, "You are a solution interpreter.")
                return final_answer

            elif "###PROMPT_ONLY###" in strategy.lower():
                # Generate specialized prompts dynamically
                prompt_design = f"""
                For this problem: {question}
                Strategy: {strategy}

                Design the most effective prompt to solve this problem:
                """

                specialized_prompt = call_llm(prompt_design, "You are a prompt engineer.")

                # Use the generated prompt
                solution = call_llm(specialized_prompt, "You are an expert problem solver.")
                return solution

            else:  # Hybrid approach
                # Chain code and LLM calls dynamically
                current_result = question

                for step in range(3):
                    # Decide what to do at this step
                    step_decision = call_llm(f"""
                    Step {step + 1} of hybrid approach.
                    Current state: {current_result}

                    What should I do next?
                    - Generate and execute code
                    - Make an LLM analysis call
                    - Provide final answer

                    Choose one and explain exactly what to do.
                    """, "You are a workflow coordinator.")

                    if "final answer" in step_decision.lower():
                        return current_result
                    elif "code" in step_decision.lower():
                        # Generate code for this step
                        step_code_prompt = f"""
                        Based on this decision: {step_decision}
                        Current data: {current_result}

                        Write Python code to process this. Return only the code:
                        """
                        step_code = call_llm(step_code_prompt, "You are a Python programmer.")
                        code_result = execute_code(step_code)
                        current_result = f"Previous: {current_result}\nCode result: {code_result}"
                    else:
                        # Make LLM call for this step  
                        step_analysis = call_llm(f"Analyze this data: {current_result}\nBased on: {step_decision}", "You are an analyst.")
                        current_result = f"Previous: {current_result}\nAnalysis: {step_analysis}"

                return current_result\n```\n\n```python\ndef self_modifying_solver(problem):
            """
            A solver that rewrites its own approach based on intermediate results.
            Advanced meta-programming where the script evolves its strategy.
            """

            strategy = "direct_analysis"
            attempts = 0
            max_attempts = 3

            while attempts < max_attempts:
                attempts += 1

                if strategy == "direct_analysis":
                    # Try direct LLM analysis
                    result = call_llm(f"Solve this problem: {problem}", "You are an expert problem solver.")

                    # Evaluate if this worked
                    evaluation_prompt = f"""
                    Problem: {problem}
                    My attempt: {result}

                    Did this solve the problem correctly? If not, what approach should I try next?
                    Options: computational_approach, step_by_step_breakdown, code_generation
                    """

                    evaluation = call_llm(evaluation_prompt, "You are a solution evaluator.")

                    if "correct" in evaluation.lower() or "solved" in evaluation.lower():
                        return result
                    elif "computational" in evaluation.lower():
                        strategy = "computational_approach"
                    elif "step_by_step" in evaluation.lower():
                        strategy = "step_by_step_breakdown"  
                    else:
                        strategy = "code_generation"

                elif strategy == "computational_approach":
                    # Generate and execute computational code
                    comp_prompt = f"""
                    Problem: {problem}

                    Write Python code to solve this computationally. Include:
                    - Extract relevant numbers or data
                    - Perform calculations
                    - Print results clearly

                    Return only the Python code:
                    """

                    comp_code = call_llm(comp_prompt, "You are a computational programmer.")
                    comp_result = execute_code(comp_code)

                    # Interpret computational result
                    interpretation = call_llm(f"Problem: {problem}\nComputation result: {comp_result}\nFinal answer:", "You are an interpreter.")
                    return interpretation

                elif strategy == "step_by_step_breakdown":
                    # Generate step-by-step solution code
                    breakdown_prompt = f"""
                    Problem: {problem}

                    Write Python code that breaks this problem into steps and solves it methodically:
                    """

                    breakdown_code = call_llm(breakdown_prompt, "You are a systematic programmer.")
                    breakdown_result = execute_code(breakdown_code)

                    # Build final solution based on breakdown
                    final_solution = call_llm(f"Problem: {problem}\nStep-by-step result: {breakdown_result}\nFinal answer:", "You are a problem solver.")
                    return final_solution

                else:  # code_generation strategy
                    # Generate completely custom code for this problem
                    custom_prompt = f"""
                    Problem: {problem}

                    Write custom Python code specifically designed to solve this exact problem type:
                    """

                    custom_code = call_llm(custom_prompt, "You are a custom code generator.")
                    custom_result = execute_code(custom_code)

                    return f"Custom solution result: {custom_result}"

            return "Could not solve after multiple strategy attempts"\n```\n\n```python\ndef adaptive_chain_solver(question):
            """
            Chains multiple code generations and LLM calls adaptively.
            Each step decides what the next step should be.
            """

            current_data = question
            step_count = 0
            max_steps = 5

            while step_count < max_steps:
                step_count += 1

                # Decide what to do at this step
                decision_prompt = f"""
                Step {step_count}: Working with: {current_data}

                What should I do next to solve this problem?
                A) Generate and execute Python code to process/calculate something
                B) Generate a specialized LLM prompt for analysis
                C) I have enough information - provide final answer

                Choose A, B, or C and explain exactly what to do:
                """

                decision = call_llm(decision_prompt, "You are an adaptive workflow coordinator.")

                if "C)" in decision or "final answer" in decision.lower():
                    # Generate final answer
                    final_prompt = f"""
                    Original question: {question}
                    Current data/results: {current_data}

                    Based on all the processing done, what is the final answer?
                    """
                    return call_llm(final_prompt, "You are a solution synthesizer.")

                elif "A)" in decision or "code" in decision.lower():
                    # Generate and execute code
                    code_prompt = f"""
                    Current data: {current_data}
                    Decision: {decision}

                    Write Python code to process this data as suggested. Return only the code:
                    """

                    code = call_llm(code_prompt, "You are a Python programmer.")

                    # Execute and update current data
                    code_result = execute_code(code)
                    current_data = f"Step {step_count} result: {code_result}"

                else:  # Generate specialized LLM prompt
                    # Create specialized prompt
                    prompt_design = f"""
                    Current data: {current_data}
                    Decision: {decision}

                    Design a specialized prompt for this analysis:
                    """

                    specialized_prompt = call_llm(prompt_design, "You are a prompt engineer.")

                    # Use the specialized prompt
                    analysis_result = call_llm(specialized_prompt, "You are a specialized analyst.")
                    current_data = f"Step {step_count} analysis: {analysis_result}"

            return f"Final result after {max_steps} steps: {current_data}"\n```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        

            PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS). YOUR APPROACH MUST BE SUBSTANTIVELY DIFFERENT THAN THESE:
            
PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS):

=== SCRIPT FROM ITERATION 6 (exploit, ACCURACY: 0.90) ===
Approach: The script employs a multi-stage, LLM-driven approach combining fact verification, self-correction, and relevant information extraction to answer questions. It decomposes the problem into initial answer generation, fact verification, keyword identification for sentence extraction, and final validation. The LLM acts in different roles throughout the process, including information retriever, fact-checker, keyword extractor, and final validator.

The functions used are `call_llm` (for prompting the LLM) and `main` (orchestrating the entire process). The `main` function calls `call_llm` with specific prompts designed for each stage: initial answer generation, verification, keyword extraction, and final validation. The output of one `call_llm` invocation often serves as the input for the next, creating a chain of reasoning.

The overall workflow begins with generating an initial answer using `call_llm`, verifying it, extracting a relevant sentence from the context, and then using the extracted sentence and corrected answer within a final prompt to produce the final answer with `call_llm`.

```python
import os
import re
import math
from google import genai
from google.genai import types

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question):
    """
    Hybrid approach: Combines elements from Iteration 2 (multi-stage with simplification, extraction, verification),
    Iteration 4 (fact verification and self-correction), and Iteration 0 (direct approach).
    This approach leverages the strengths of each while addressing their weaknesses.
    """

    # --- 1. Initial Answer Generation with Contextualized Prompt (from Iteration 4 & 0) ---
    initial_prompt = f"""
    Provide a concise answer to the question based on the provided text. Focus on extracting the most relevant entity or value.

    Example 1:
    Question: Which player kicked the only field goal of the game?
    Answer: Josh Scobee

    Example 2:
    Question: Which star has a smaller mass, Nu Phoenicis or Gliese 915?
    Answer: Gliese 915

    Example 3:
    Question: How many yards longer was the longest touchdown pass than the longest field goal?
    Answer: 32

    Question: {question}
    Answer:
    """
    try:
        initial_answer = call_llm(initial_prompt, "You are a precise information retriever. Focus on brevity.")
        initial_answer = initial_answer.strip()
    except Exception as e:
        print(f"Error generating initial answer: {e}")
        return "Error generating initial answer."

    # --- 2. Fact Verification and Self-Correction with Specific Examples (from Iteration 4) ---
    verification_prompt = f"""
    Analyze the answer for factual correctness based on the original question. If the answer is incorrect, provide a corrected answer using the available information.
    If the answer is already concise and accurate, just repeat it.

    Example 1:
    Question: Which player kicked the only field goal of the game?
    Proposed Answer: Tom Brady
    Corrected Answer: Josh Scobee

    Example 2:
    Question: Which star has a smaller mass, Nu Phoenicis or Gliese 915?
    Proposed Answer: Gliese 915
    Corrected Answer: Gliese 915

    Example 3:
    Question: How many yards longer was the longest touchdown pass than the longest field goal?
    Proposed Answer: 30
    Corrected Answer: 32

    Question: {question}
    Proposed Answer: {initial_answer}
    Corrected Answer:
    """
    try:
        verification_response = call_llm(verification_prompt, "You are a fact-checker and self-correction expert. If the answer is correct as is, repeat the exact answer given to you.")
        corrected_answer = verification_response.strip()
    except Exception as e:
        print(f"Error during fact verification: {e}")
        corrected_answer = initial_answer # Fallback in case verification fails

    # --- 3. Keyword Identification and Passage Simplification (from Iteration 2, enhanced with examples) ---
    keywords_prompt = f"""
    Identify the key entities and concepts in the question. Use these keywords to find and extract the most relevant sentence from the original text.

    Example 1:
    Question: Which player kicked the only field goal of the game?
    Passage: Game SummaryComing off their Thanksgiving road win over the Falcons, the Colts went home for a Week 13 AFC South rematch with the Jacksonville Jaguars.  In the first quarter, Indianapolis scored first with QB Peyton Manning completing a 5-yard TD pass to TE Dallas Clark, along with a 48-yard TD pass to WR Reggie Wayne.  In the second quarter, the Jaguars got on the board with RB Maurice Jones-Drew getting a 2-yard TD run. Afterwards, the Colts replied with Manning and Clark hooking up with each other again on a 14-yard TD pass. In the third quarter, Jacksonville tried to come back as QB David Garrard completed a 2-yard TD pass to TE Mercedes Lewis for the only score of the period. In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal. However, the Colts responded with Manning completing a 1-yard TD pass to RB Luke Lawton. Afterwards, Jacksonville tried to come back as Garrard completed a 17-yard TD pass to WR Dennis Northcutt (along with getting the 2-point conversion run). Indianapolis' defense managed to seal the deal. With their season-sweep over the Jaguars, the Colts improved to 10-2. During the game, the Colts gave Garrard his first interception of the year, courtesy of Safety Antoine Bethea.

    Keywords: player, field goal
    Relevant Sentence: In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.

    Example 2:
    Question: Which star has a smaller mass, Nu Phoenicis or Gliese 915?
    Passage: Nu Phoenicis is a yellow-white main sequence star of spectral type F9V and magnitude 4.96. Lying some 49 light years distant, it is around 1.2 times as massive as our sun, and likely to be surrounded by a disk of dust. It is the closest star in the constellation that is visible with the unaided eye. Gliese 915 is a white dwarf only 26 light years away. It is of magnitude 13.05, too faint to be seen with the naked eye. White dwarfs are extremely dense stars compacted into a volume the size of the Earth. With around 85% of the mass of the Sun, Gliese 915 has a surface gravity of 108.39 ± 0.01 (2.45 · 108) centimetre·second−2, or approximately 250,000 of Earths gravity.

    Keywords: star, mass, Nu Phoenicis, Gliese 915
    Relevant Sentence: Gliese 915 has around 85% of the mass of the Sun.

    Example 3:
    Question: How many yards longer was the longest touchdown pass than the longest field goal?
    Passage: Hoping to rebound from their first loss of the season, the Broncos returned home for an AFC West divisional rematch with the Kansas City Chiefs. After Peyton Manning became the NFL's all-time leader in regular season passing yardage, the game turned sour for the Broncos. Following a Manning interception, the Chiefs capitalized, with a 4-yard touchdown run by running back Charcandrick West. The Broncos' offense went three-and-out on their next two possessions, and the Chiefs increased their lead to 10-0, with a 48-yard field goal by placekicker Cairo Santos. The Chiefs increased their lead to 19-0 at halftime, with three more field goals by Santos &#8212 from 49, 34 and 33 yards out. By halftime, Manning had thrown three interceptions and the Broncos' offense had earned only one first down. The Broncos went three-and-out on their first possession of the second half, and a 50-yarder field goal by Santos increased the Chiefs' lead to 22-0. After Manning threw his fourth interception of the game on the Broncos' next possession, he was pulled and replaced by backup quarterback Brock Osweiler for the remainder of the game. Osweiler drove the Broncos' into the red zone early in the fourth quarter, but was intercepted by Chiefs' safety Eric Berry. Two plays later, the Chiefs increased their lead to 29-0, when quarterback Alex Smith connected with West on an 80-yard touchdown pass. The Broncos' finally got on the scoreboard with 5:31 remaining in the game, with running back Ronnie Hillman rushing for a 1-yard touchdown (two-point conversion attempt unsuccessful), followed by a 7-yard touchdown pass from Osweiler to wide receiver Andre Caldwell, but the Chiefs' lead was too much for the Broncos to overcome. Peyton Manning finished the day with the first 0.0 passer rating of his career.

    Keywords: yards, longest touchdown pass, longest field goal
    Relevant Sentence: The Chiefs increased their lead to 29-0, when quarterback Alex Smith connected with West on an 80-yard touchdown pass. The Chiefs increased their lead to 10-0, with a 48-yard field goal by placekicker Cairo Santos.

    Question: {question}
    Passage: {corrected_answer}
    Keywords:
    Relevant Sentence:
    """

    try:
        keywords_and_sentence = call_llm(keywords_prompt, "You are an expert at extracting keywords and relevant sentences.")
        relevant_sentence = keywords_and_sentence.split("Relevant Sentence:")[1].strip()
    except:
        relevant_sentence = corrected_answer  # If keyword extraction fails, use the corrected answer

    # --- 4. Final Validation with Relevant Sentence (from Iteration 2 and 4, combined) ---
    final_validation_prompt = f"""
    Validate that the {corrected_answer} accurately answers the {question} based on the {relevant_sentence}. If it isn't, provide the correct answer.

    Example 1:
    Question: Which player kicked the only field goal of the game?
    Corrected Answer: Josh Scobee
    Relevant Sentence: In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.
    Final Answer: Josh Scobee

    Example 2:
    Question: Which star has a smaller mass, Nu Phoenicis or Gliese 915?
    Corrected Answer: Gliese 915
    Relevant Sentence: Gliese 915 has around 85% of the mass of the Sun.
    Final Answer: Gliese 915

    Example 3:
    Question: How many yards longer was the longest touchdown pass than the longest field goal?
    Corrected Answer: 32
    Relevant Sentence: The Chiefs increased their lead to 29-0, when quarterback Alex Smith connected with West on an 80-yard touchdown pass. The Chiefs increased their lead to 10-0, with a 48-yard field goal by placekicker Cairo Santos.
    Final Answer: 32

    Question: {question}
    Corrected Answer: {corrected_answer}
    Relevant Sentence: {relevant_sentence}
    Final Answer:
    """

    try:
        final_answer = call_llm(final_validation_prompt, "You are a final validator. Ensure conciseness and accuracy.").strip()
        return final_answer
    except Exception as e:
        print(f"Error during final validation: {e}")
        return corrected_answer # As a final safety
```

=== SCRIPT FROM ITERATION 5 (explore, ACCURACY: 0.60) ===
Approach: The script implements a knowledge retrieval and answer generation approach using an LLM in two steps. First, the `call_llm` function is used with the LLM to generate a search query based on the input question. The `perform_search` function simulates a web search using the generated query. Finally, the `call_llm` function is again used with the LLM to synthesize the search results with the original question to generate a final answer. The `main` function orchestrates the workflow by calling the `call_llm` function twice, once for query generation and once for answer synthesis, and `perform_search` to simulate searching.

```python
import os
import re
import math
from google import genai
from google.genai import types

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question):
    """
    EXPLORATION: This script implements a **knowledge retrieval and answer generation** approach.
    It leverages the LLM to identify key concepts, uses those to reformulate a search query,
    and then synthesizes the search results with the original question to generate a final answer.

    Hypothesis: Integrating external knowledge retrieval into the QA process will significantly improve
    accuracy by providing the LLM with additional context and factual information, particularly for questions
    requiring numerical or temporal reasoning. This approach will be better for complex questions than previous methods.

    Key Differences from Previous Approaches:
    1. Explicit knowledge retrieval step using LLM-driven query reformulation
    2. Integration of search results directly into the answer generation prompt.

    Improvements Addressed:
    - Information Synthesis Failure: This framework forces the LLM to use external retrieved knowledge and integrate it when forming a final answer.
    - Complex Reasoning: Knowledge retrieval step uses an external knowledge base for more information, which will help for more complex questions.
    - Inaccurate Numerical/Temporal extraction: The retrieval process may help clarify or verify key numbers or dates.
    """

    # Step 1: Identify key concepts and generate a search query
    query_prompt = f"""
    Identify the key concepts in the question and generate a search query that could retrieve relevant information from the web.

    Example 1:
    Question: How many yards longer was the longest touchdown pass than the longest field goal?
    Key Concepts: longest touchdown pass, longest field goal, yards, difference
    Search Query: longest touchdown pass vs longest field goal yards

    Example 2:
    Question: Which player kicked the only field goal of the game?
    Key Concepts: player, field goal
    Search Query: player kicked field goal

    Question: {question}
    Key Concepts:
    Search Query:
    """

    try:
        query_response = call_llm(query_prompt, "You are an expert at generating search queries.").strip()
        search_query = query_response.split("Search Query:")[-1].strip()
    except Exception as e:
        print(f"Error generating search query: {e}")
        return "Error generating search query."

    # Step 2: Simulate a web search (replace with actual API call in a real implementation)
    # In a real implementation, this would call a search API and retrieve actual results
    def perform_search(query):
        """Simulate a web search. Returns a canned response for demonstration purposes."""
        if "longest touchdown pass vs longest field goal yards" in query:
            return "Search results: Longest touchdown pass was 80 yards, longest field goal was 48 yards."
        elif "player kicked field goal" in query:
            return "Search results: Josh Scobee kicked a 47-yard field goal."
        else:
            return "Search results: No relevant information found."

    search_results = perform_search(search_query)

    # Step 3: Synthesize search results with the original question to generate a final answer
    synthesis_prompt = f"""
    Synthesize the search results with the original question to generate a final answer.

    Original Question: {question}
    Search Results: {search_results}

    Example 1:
    Original Question: How many yards longer was the longest touchdown pass than the longest field goal?
    Search Results: Search results: Longest touchdown pass was 80 yards, longest field goal was 48 yards.
    Final Answer: 32

    Example 2:
    Original Question: Which player kicked the only field goal of the game?
    Search Results: Search results: Josh Scobee kicked a 47-yard field goal.
    Final Answer: Josh Scobee

    Final Answer:
    """

    try:
        final_answer = call_llm(synthesis_prompt, "You are an expert at synthesizing information.").strip()
        return final_answer
    except Exception as e:
        print(f"Error synthesizing final answer: {e}")
        return "Error synthesizing final answer."
```

=== SCRIPT FROM ITERATION 4 (explore, ACCURACY: 0.90) ===
Approach: The script employs a fact-verification with self-correction approach using the Gemini LLM to answer questions. It decomposes the problem into three stages: initial answer generation, fact verification/self-correction, and final output validation for conciseness. Three agent roles are employed: a precise information retriever, a fact-checker/self-correction expert, and a validator for concise answers.

The functions used are `call_llm` for querying the LLM, which is called within `main` to generate the initial answer, verify the answer, and validate the final output. The `main` function orchestrates the overall workflow, passing prompts to `call_llm` and processing the responses to refine the answer. The overall workflow involves generating an initial answer, verifying its correctness, correcting if necessary, and then validating/shortening the final answer.

```python
import os
import re
import math
from google import genai
from google.genai import types

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question):
    """
    This script uses a fact-verification with self-correction approach.
    HYPOTHESIS: Implement a robust fact-verification stage using external knowledge and leverage self-correction capabilities to improve accuracy.
    This approach uses a main LLM call for generating an initial answer, then uses a separate LLM call to verify facts in that answer against the passage and a broader knowledge set.
    """

    # Step 1: Initial Answer Generation
    initial_prompt = f"""
    Provide a concise answer to the question based on the provided text.

    Example 1:
    Question: Which player kicked the only field goal of the game?
    Answer: Josh Scobee

    Example 2:
    Question: Which star has a smaller mass, Nu Phoenicis or Gliese 915?
    Answer: Gliese 915

    Question: {question}
    Answer:
    """
    try:
        initial_answer = call_llm(initial_prompt, "You are a precise information retriever.")
        initial_answer = initial_answer.strip()
    except Exception as e:
        print(f"Error generating initial answer: {e}")
        return "Error generating initial answer."

    # Step 2: Fact Verification and Self-Correction
    verification_prompt = f"""
    Analyze the answer for factual correctness against the original question and a broader knowledge base.
    Identify any inaccuracies or inconsistencies. If the answer is incorrect, provide a corrected answer using the available information.

    Example 1:
    Question: Which player kicked the only field goal of the game?
    Proposed Answer: Tom Brady
    Analysis: Tom Brady is a quarterback, not a kicker. A more likely answer based on the context is a kicker like Josh Scobee. The passage will also be searched to verify this information.
    Corrected Answer: Josh Scobee

    Example 2:
    Question: Which star has a smaller mass, Nu Phoenicis or Gliese 915?
    Proposed Answer: Gliese 915
    Analysis: The answer is correct. Gliese 915 is a white dwarf and known to have less mass than Nu Phoenicis based on general astronomical knowledge.
    Corrected Answer: Gliese 915

    Question: {question}
    Proposed Answer: {initial_answer}
    Analysis:
    """
    try:
        verification_response = call_llm(verification_prompt, "You are a fact-checker and self-correction expert.")
        # Attempt to extract the corrected answer; if impossible, stick with the initial answer
        if "Corrected Answer:" in verification_response:
          corrected_answer = verification_response.split("Corrected Answer:")[-1].strip()
        else:
          corrected_answer = initial_answer
    except Exception as e:
        print(f"Error during fact verification: {e}")
        corrected_answer = initial_answer # Fallback in case verification fails

    # Step 3: Final Output Validation (Ensure it's concise)
    final_validation_prompt = f"""
    Validate if the final answer is concise and accurately answers the question. Return the answer or a more concise version.

    Example 1:
    Question: Which player kicked the only field goal of the game?
    Proposed Answer: Josh Scobee was the player
    Final Answer: Josh Scobee

    Example 2:
    Question: Which star has a smaller mass, Nu Phoenicis or Gliese 915?
    Proposed Answer: Gliese 915, the white dwarf star.
    Final Answer: Gliese 915

    Question: {question}
    Proposed Answer: {corrected_answer}
    Final Answer:
    """

    try:
        final_answer = call_llm(final_validation_prompt, "You are a validator for concise answers.")
        final_answer = final_answer.strip()
        return final_answer
    except Exception as e:
        print(f"Error validating final answer: {e}")
        return corrected_answer # As a final safety
```

=== SCRIPT FROM ITERATION 3 (explore, ACCURACY: 0.50) ===
Approach: The script implements a question-decomposition and answer-synthesis approach using the Gemini LLM to enhance reasoning. The main function, `main`, decomposes the original question into sub-questions, answers each sub-question individually, and then synthesizes these answers into a final comprehensive answer. It uses the `call_llm` function to interact with the Gemini API, utilizing different system instructions for question decomposition, answering sub-questions, and synthesizing information. The functions `call_llm` sends requests to the Gemini model, and the `main` function orchestrates the decomposition, answering, and synthesis steps to arrive at the final answer.

```python
import os
import re
import math
from google import genai
from google.genai import types

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question):
    """
    EXPLORATION: This script uses a question-decomposition and answer-synthesis approach
    to enhance reasoning and accuracy. This is a new approach compared to previous iterations.
    This approach focuses on breaking the original question into a series of sub-questions
    which allow the LLM to address specific elements. Finally, after the LLM has answered each
    sub-question, it synthesizes the sub-answers into a final answer. This is fundamentally different
    than the three-stage approach of the past few successful iterations.
    """

    # Step 1: Decompose the original question into sub-questions
    decomposition_prompt = f"""
    Decompose the original question into a series of simpler, more specific sub-questions that, when answered,
    will provide all the information needed to answer the original question.

    Example 1:
    Original Question: How many yards longer was the longest touchdown pass than the longest field goal?
    Sub-questions:
    1. What was the length of the longest touchdown pass?
    2. What was the length of the longest field goal?
    3. What is the difference between the length of the longest touchdown pass and the length of the longest field goal?

    Example 2:
    Original Question: Which player kicked the only field goal of the game?
    Sub-questions:
    1. Was there a field goal in the game?
    2. Which player kicked the field goal?

    Original Question: {question}
    Sub-questions:
    """

    try:
        sub_questions_str = call_llm(decomposition_prompt, "You are an expert at question decomposition.")
        sub_questions = [q.strip() for q in sub_questions_str.split('\n') if q.strip()]  # Splits and cleans
    except Exception as e:
        print(f"Error decomposing question: {e}")
        return "Error decomposing question."

    # Step 2: Answer each sub-question
    sub_answers = []
    for i, sub_question in enumerate(sub_questions):
        answer_prompt = f"""
        Answer the following sub-question concisely.

        Sub-question: {sub_question}

        Answer:
        """
        try:
            answer = call_llm(answer_prompt, "You are an expert at answering questions concisely.")
            sub_answers.append(answer)
        except Exception as e:
            print(f"Error answering sub-question {i+1}: {e}")
            sub_answers.append(f"Error answering sub-question {i+1}.")

    # Step 3: Synthesize the sub-answers into a final answer
    synthesis_prompt = f"""
    Synthesize the following sub-answers into a final, comprehensive answer to the original question.

    Original Question: {question}
    Sub-questions and Answers:
    {chr(10).join([f"{i+1}. {q}: {a}" for i, (q, a) in enumerate(zip(sub_questions, sub_answers))])}

    Final Answer:
    """

    try:
        final_answer = call_llm(synthesis_prompt, "You are an expert at synthesizing information.")
        return final_answer
    except Exception as e:
        print(f"Error synthesizing final answer: {e}")
        return "Error synthesizing final answer."
```

=== SCRIPT FROM ITERATION 2 (exploit, ACCURACY: 1.00) ===
Approach: The script uses a hybrid approach, combining direct LLM calls with multi-stage analysis for question answering. It decomposes the problem into keyword identification and passage simplification, information extraction with examples, and verification. There are two agent roles: Passage Simplifier, Information Extractor and Answer Verifier and Corrector.

The main function orchestrates the process, first using `call_llm` with a "Passage Simplifier" to simplify the input passage. It then calls `call_llm` again with an "Information Extractor" to extract a concise answer. Finally, it calls `call_llm` a third time with an "Answer Verifier and Corrector" to verify the extracted answer and correct it if needed. The `call_llm` function is a wrapper for interacting with the Gemini LLM, taking a prompt and optional system instruction as input.

```python
import os
import re
import math
from google import genai
from google.genai import types

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question):
    """
    Hybrid approach: Combines the direct approach of Script 1 with the multi-stage analysis of Script 2.
    This aims to leverage the speed and simplicity of the direct approach while incorporating targeted
    information extraction and verification from the multi-stage method to improve accuracy.
    
    Specific Synthesis Elements:
    - Direct LLM Call: Incorporates the direct, concise LLM call from Script 1 to provide an initial answer.
    - Keyword Identification and Passage Simplification: Uses keyword extraction and passage simplification from Stage 1 of Script 2 to focus on relevant information.
    - Information Extraction with Examples:  Employs a prompt with multiple examples for targeted information extraction to improve precision, addressing the issue of over-descriptive answers.
    - Verification: Adds a final verification step to validate extracted information and refine the answer, as done in Script 2.
    
    Improvements Addressed:
    - Overly verbose answers: Prompt engineering focuses on conciseness.
    - Incorrect Numerical Extraction: Explicit association of values with descriptors in information extraction prompts
    - Numerical Reasoning Errors: Implement basic validation
    """
    
    # --- Keyword Identification and Passage Simplification (from Script 2) ---
    keywords_prompt = f"""
    Identify the key entities and concepts in the question and passage. Use these keywords to simplify the passage,
    focusing on the most relevant sentences.

    Example 1:
    Question: Which player kicked the only field goal of the game?
    Passage: Game SummaryComing off their Thanksgiving road win over the Falcons, the Colts went home for a Week 13 AFC South rematch with the Jacksonville Jaguars.  In the first quarter, Indianapolis scored first with QB Peyton Manning completing a 5-yard TD pass to TE Dallas Clark, along with a 48-yard TD pass to WR Reggie Wayne.  In the second quarter, the Jaguars got on the board with RB Maurice Jones-Drew getting a 2-yard TD run. Afterwards, the Colts replied with Manning and Clark hooking up with each other again on a 14-yard TD pass. In the third quarter, Jacksonville tried to come back as QB David Garrard completed a 2-yard TD pass to TE Mercedes Lewis for the only score of the period. In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal. However, the Colts responded with Manning completing a 1-yard TD pass to RB Luke Lawton. Afterwards, Jacksonville tried to come back as Garrard completed a 17-yard TD pass to WR Dennis Northcutt (along with getting the 2-point conversion run). Indianapolis' defense managed to seal the deal. With their season-sweep over the Jaguars, the Colts improved to 10-2. During the game, the Colts gave Garrard his first interception of the year, courtesy of Safety Antoine Bethea.

    Keywords: player, field goal
    Simplified Passage: In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.

    Example 2:
    Question: Which star has a smaller mass, Nu Phoenicis or Gliese 915?
    Passage: Nu Phoenicis is a yellow-white main sequence star of spectral type F9V and magnitude 4.96. Lying some 49 light years distant, it is around 1.2 times as massive as our sun, and likely to be surrounded by a disk of dust. It is the closest star in the constellation that is visible with the unaided eye. Gliese 915 is a white dwarf only 26 light years away. It is of magnitude 13.05, too faint to be seen with the naked eye. White dwarfs are extremely dense stars compacted into a volume the size of the Earth. With around 85% of the mass of the Sun, Gliese 915 has a surface gravity of 108.39 ± 0.01 (2.45 · 108) centimetre·second−2, or approximately 250,000 of Earths gravity.

    Keywords: star, mass, Nu Phoenicis, Gliese 915
    Simplified Passage: Nu Phoenicis is around 1.2 times as massive as our sun. Gliese 915 has around 85% of the mass of the Sun.

    Example 3:
    Question: How many yards longer was the longest touchdown pass than the longest field goal?
    Passage: Hoping to rebound from their first loss of the season, the Broncos returned home for an AFC West divisional rematch with the Kansas City Chiefs. After Peyton Manning became the NFL's all-time leader in regular season passing yardage, the game turned sour for the Broncos. Following a Manning interception, the Chiefs capitalized, with a 4-yard touchdown run by running back Charcandrick West. The Broncos' offense went three-and-out on their next two possessions, and the Chiefs increased their lead to 10-0, with a 48-yard field goal by placekicker Cairo Santos. The Chiefs increased their lead to 19-0 at halftime, with three more field goals by Santos &#8212 from 49, 34 and 33 yards out. By halftime, Manning had thrown three interceptions and the Broncos' offense had earned only one first down. The Broncos went three-and-out on their first possession of the second half, and a 50-yarder field goal by Santos increased the Chiefs' lead to 22-0. After Manning threw his fourth interception of the game on the Broncos' next possession, he was pulled and replaced by backup quarterback Brock Osweiler for the remainder of the game. Osweiler drove the Broncos' into the red zone early in the fourth quarter, but was intercepted by Chiefs' safety Eric Berry. Two plays later, the Chiefs increased their lead to 29-0, when quarterback Alex Smith connected with West on an 80-yard touchdown pass. The Broncos' finally got on the scoreboard with 5:31 remaining in the game, with running back Ronnie Hillman rushing for a 1-yard touchdown (two-point conversion attempt unsuccessful), followed by a 7-yard touchdown pass from Osweiler to wide receiver Andre Caldwell, but the Chiefs' lead was too much for the Broncos to overcome. Peyton Manning finished the day with the first 0.0 passer rating of his career.

    Keywords: yards, longest touchdown pass, longest field goal
    Simplified Passage: the Chiefs increased their lead to 10-0, with a 48-yard field goal by placekicker Cairo Santos. The Chiefs increased their lead to 29-0, when quarterback Alex Smith connected with West on an 80-yard touchdown pass.

    Question: {question}
    Keywords:
    Simplified Passage:
    """

    try:
        keywords_and_simplified = call_llm(keywords_prompt, "You are an expert at simplifying passages by extracting keywords and relevant sentences.")
        simplified_passage = keywords_and_simplified.split("Simplified Passage:")[1].strip()
    except:
        simplified_passage = question  # If keyword extraction fails, use the original question

    # --- Direct LLM call with examples for targeted information extraction (enhanced from Script 2) ---
    extraction_prompt = f"""
    Based on the simplified passage, extract the answer to the question. Provide ONLY the answer entity (name, value, etc.) without extra words or context.
    Focus on precision and conciseness.

    Example 1:
    Question: Which player kicked the only field goal of the game?
    Passage: In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.
    Answer: Josh Scobee

    Example 2:
    Question: Which star has a smaller mass, Nu Phoenicis or Gliese 915?
    Passage: Nu Phoenicis is around 1.2 times as massive as our sun. Gliese 915 has around 85% of the mass of the Sun.
    Answer: Gliese 915

    Example 3:
    Question: How many yards longer was the longest touchdown pass than the longest field goal?
    Passage: the Chiefs increased their lead to 10-0, with a 48-yard field goal by placekicker Cairo Santos. The Chiefs increased their lead to 29-0, when quarterback Alex Smith connected with West on an 80-yard touchdown pass.
    Answer: 32

    Question: {question}
    Passage: {simplified_passage}
    Answer:
    """

    try:
        answer = call_llm(extraction_prompt, "You are an information extraction expert. Give the single most relevant entity as the answer.").strip()
    except:
        answer = "Could not extract answer."

    # --- Verification (from Script 2) ---
    verification_prompt = f"""
    Verify that the extracted answer directly and concisely answers the question based on the original passage.
    If the answer is incorrect or incomplete, provide the correct answer without any extra information.

    Example 1:
    Question: Which player kicked the only field goal of the game?
    Extracted Answer: Josh Scobee
    Correct: True

    Example 2:
    Question: Which star has a smaller mass, Nu Phoenicis or Gliese 915?
    Extracted Answer: Gliese 915
    Correct: True

    Example 3:
    Question: How many yards longer was the longest touchdown pass than the longest field goal?
    Extracted Answer: 32
    Correct: True

    Question: {question}
    Extracted Answer: {answer}
    Correct:
    """
    try:
        correctness = call_llm(verification_prompt, "You are a precise answer checker.").strip()
        if "False" in correctness:
            answer = call_llm(f"Based on the question {question} and the original passage, what is the CORRECT answer without any extra information or verbosity?", "You are a precise information retriever.")
    except:
        pass

    return answer
```


            LEARNINGS FROM PREVIOUS ITERATIONS:
            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        ```text
## DATASET PATTERNS & CHARACTERISTICS

*   **Contextual Passage:** Every question is accompanied by a passage of text, which is often a sports report, news article, or description of astronomical objects. Answers cannot be derived from general knowledge *alone*; they must be explicitly stated or inferable from the provided text, *potentially combined with external knowledge.*
*   **Fact-Based:** The questions are primarily fact-based and require extracting specific information directly or indirectly from the passage.
*   **Varied Question Types:** Questions vary (who, what, how many, how much, which, etc.) and cover diverse topics like player names, score differences, stellar masses, touchdown passes, etc.
*   **Implicit Relationships:** Some questions require understanding relationships between different events or entities in the passage.
*   **Numerical Reasoning:** Some questions involve numerical operations (addition, subtraction, comparison) based on the information within the passage.
    *   Questions frequently require numerical reasoning and comparison, such as finding the difference between two quantities mentioned in the passage.
    *   Many questions necessitate extracting multiple numerical values from the passage and performing arithmetic operations (addition, subtraction) to arrive at the correct answer. Example: calculating the final score difference between two teams.
*   **Numerical & Temporal Reasoning:** The dataset features questions that require precise numerical and temporal reasoning based on information presented in the passage. Many questions involve extracting specific dates or numbers and performing arithmetic operations (addition, subtraction, or comparisons) on them.
*   **Temporal Reasoning:** Some questions require temporal reasoning or comparison (e.g., "What happened second..."). This demands careful parsing of the passage to establish chronological order.
*   **Quantitative Reasoning:** Many questions involve numerical answers ("How many points..."). This necessitates accurate extraction and potentially simple calculations based on the passage's content.
*   **Short Answers:** The answers are typically short, concise phrases or numerical values.
*   **Directly Extractable:** Many answers are directly present in the passage, although they might need some minimal processing or inference.
*   **Consistency in Format:** The expected format of the answer is usually a string or a number.
*   **Context-Dependent:** The answer's meaning is entirely dependent on the context of the passage.
*   **Domain Knowledge:**
    *   General Knowledge is assumed.
    *   Sports (Primarily American Football): Understanding basic football terminology is crucial for interpreting passages.
    *   Astronomy: Some passages require basic astronomy knowledge (e.g., stars, mass).
    *   Basic Arithmetic is needed to answer some questions.
    *   Units: Need to understand units being used. (e.g., yards, points, solar masses)
*   **Historical Context & Reasoning:** Many questions require reasoning about historical figures and events beyond direct statements in the passage. The questions often ask about relationships between people and time periods.
*   **Multi-Sentence Synthesis:** Answering frequently requires synthesizing information from multiple sentences within the passage, not just extracting a single fact.
*   **Combination of Explicit and Implicit Information:** Questions often require combining explicitly stated information from the passage with common-sense or world knowledge not present in the passage.
*   **Question Types:**
    *   Extraction: Directly extracting information from the passage (e.g., "Who caught the final touchdown?"). "Which star has a smaller mass, Nu Phoenicis or Gliese 915?" Requires directly locating the answer within the text.
    *   Counting: Counting occurrences of entities or events (e.g., "How many running backs ran for a touchdown?").
    *   Comparison: Comparing numerical values within the passage (e.g., "How many yards longer was Sebastian Janikowski's first field goal compared to his second?"). Also, "Which star has a smaller mass, Nu Phoenicis or Gliese 915?" requires comparison.
    *   Identification: Identifying a specific agent or object. Example: "Who threw the second longest touchdown pass?"
    *   Seeking Specifics: Example: "Which player kicked the only field goal of the game?".
*   **Reasoning Types:**
    *   Direct Extraction: Locating the relevant sentence and extracting the answer verbatim.
    *   Simple Inference: Combining information from multiple sentences or clauses.
    *   Numerical Reasoning: Performing arithmetic operations based on passage data.
    *   Relationship Identification: Understanding the relationships between players, teams, events, or astronomical objects.
    *   Information Synthesis: The answers are often not explicitly stated in a single sentence but require synthesizing information from different parts of the passage.
*   **Distractors:** Passages often contain extraneous information not directly relevant to answering the question. This requires the model to filter information.
*   **Passage Structure:** Dataset features complex, multi-sentence passages, often describing sporting events.
*   **Numerical Embedding:** Numerical values are often embedded within descriptive text, increasing the difficulty of accurate extraction.
*   **Numerical Facts:** The questions frequently require extracting specific numerical facts (e.g., "How many points?", "How many yards?").
*   **Temporal/Sequential Reasoning:** Questions often involve temporal or sequential reasoning (e.g., what happened "in the first quarter", "after a series of penalties").
*   **Multiple Dates & Numbers:** Passages often contain multiple dates and numbers, increasing the chance of incorrect extraction.
*   **Passage Length Variation:** Passages vary in length, but relevant information is often spread throughout the text, demanding comprehensive parsing.

## EFFECTIVE TASK-SPECIFIC STRATEGIES

*   **Keyword Matching and Extraction:**
    *   Identify keywords from the question.
    *   Search the passage for sentences containing those keywords.
    *   Extract relevant information from those sentences.
*   **Question Type Classification:**
    *   Classify the question type (extraction, counting, comparison, calculation).
    *   Use a specialized approach for each question type.
*   **Sentence Similarity:**
    *   Calculate the similarity between the question and each sentence in the passage.
    *   Extract the answer from the most similar sentence(s).
*   **Chain-of-Thought Prompting:**
    *   Prompt the LLM to first explain its reasoning process step-by-step before providing the final answer. This helps to surface and correct errors in reasoning.
    *   Example: "First, I will identify the relevant information in the passage. Then, I will perform the necessary calculations. Finally, I will provide the answer."
*   **Decomposition:**
    *   Break down complex questions into simpler sub-questions.
    *   Answer each sub-question individually.
    *   Combine the answers to the sub-questions to obtain the final answer.
    *   **Numerical Questions:**
        1.  Identify the relevant numbers in the passage.
        2.  Determine the operation to perform (addition, subtraction, etc.).
        3.  Perform the calculation.
        4.  Present the result.
    *   **Who/What/Which Questions:**
        1.  Identify the entity or event that the question is asking about.
        2.  Locate the sentence(s) in the passage that describe that entity or event.
        3.  Extract the relevant information.
    *   *Note:* While decomposition seems logical, its effectiveness is highly dependent on the accuracy of the information extracted for each sub-question. Inaccurate extraction during the decomposition phase can lead to error propagation.
        *   If a numerical computation is necessary, explicitly decompose the question into sub-questions that isolate the numerical values, then compute the final answer in a separate step.
*   **Prompt Engineering for Conciseness:** Explicitly instruct the LLM to provide only the single most relevant entity/phrase as the answer, avoiding full sentences. Examples: "Answer with only the name," or "Provide only the single word answer". This addresses the issue of overly verbose answers.
*   **Post-Processing:** Implement a post-processing step to extract the core entity from the LLM's response. This could involve techniques like Named Entity Recognition (NER) or simple string manipulation to isolate the key phrase.
*   **Few-shot examples:** Incorporate few-shot examples in the prompt to demonstrate the desired answer format (i.e., short, entity-focused answers).
*   **Fact Verification using Retrieval System:** Add a retrieval system for verifying facts and entities. If the model is uncertain about the answer, the retrieval system can provide additional context and information to help the model make a more informed decision. This is crucial in cases where the answer is present in passage but the model lacks the details. Fact verification helps to improve accuracy.
    *   When a question requires numerical reasoning, direct the fact-verification step to specifically validate the extracted numerical values and the intermediate steps in the calculation.
*   **Passage Structure Awareness:** Sporting event reports often follow a predictable structure (quarter-by-quarter summary). Use this to focus the search for information.
*   **Question Focus:** Questions related to a specific player often have answers near the first mention of that player in the passage.
*   **Predefined Answer Lists:** For certain question types (e.g., "Which team won?"), you could predefine a list of possible teams mentioned in the passage and select from that list.
*   **Negative Constraints:** Using the LLM's reasoning capabilities, generate negative constraints from the passage. For example, if the question is "Who *wasn't* playing?" generate a list of players who *were* playing and eliminate those.
*   **Multi-Stage LLM Approach:** A multi-stage LLM approach shows promise by breaking down the problem.
    *   Modularity of the Simplification, Extraction, and Verification stages allows for error diagnosis and iterative refinement.
    *   Decomposes the question answering task into smaller, manageable sub-tasks.
*   **Hybrid Approach (Simplification, Extraction, Verification):** This approach proves effective in extracting numerical information within a constrained context. The breakdown of tasks between Simplifier, Extractor and Verifier allows the system to successfully navigate the passages and identify correct answers (as demonstrated in Iteration 2).
*   **Explicit Association of Values and Descriptors**: Refine the prompt for the information extraction expert to explicitly request association of numerical values with their descriptors. For instance, ask the LLM to identify "The longest touchdown pass: [value]" and "The longest field goal: [value]". This is aimed at improving numerical extraction.
*   **Answer Checker Agent:** Provide more explicit instructions to the "answer checker" agent to verify that the extracted numbers are relevant to answering the question and not just any number in the passage.
*   **Dedicated System Instructions for Modularity:** Using different system instructions for decomposition, answering, and synthesis *could* be effective if each module performed its specific task accurately. (Needs Improvement: Accuracy suggests that decomposition itself isn't inherently helpful *without* improvements to the extraction).
*   **Prompt Engineering for Extraction:** Experiment with prompts specifically designed to elicit accurate numerical and temporal values. Use formats like "Extract the date of X in the format YYYY-MM-DD" or "What number represents Z?"
*   **Few-Shot Examples for Extraction:** Provide few-shot examples in the system prompt that demonstrate precise extraction of dates and numbers from similar text.
*   **Self-Correction:** Self-correction helps to catch and fix mistakes in the initial answer.
*   **External Knowledge Retrieval & Integration (Proposed):** Augment the system to retrieve external knowledge from a knowledge base (e.g., Wikipedia) when the LLM determines it's necessary to answer a question. Develop a mechanism to integrate retrieved external knowledge with the passage information before the final validation stage, potentially creating a combined context for the LLM. Refine prompts to specifically instruct the LLM to utilize both the passage and any retrieved external knowledge to derive the final answer. Explicitly tell it to perform a search if needed, and then cite the source.
*   **Ineffective Strategies:**
    *   **Simple Query-and-Synthesize Approach:** The two-step process of query generation followed by answer synthesis, *in its current form*, is not effectively leveraging the information available in the passage. Rejected hypothesis: A simple query-and-synthesize approach is *not* effective for this dataset.
    *   **Multi-Stage LLM-Driven Fact Verification and Information Extraction Alone:** The hypothesis that multi-stage, LLM-driven fact verification and information extraction *alone* would be sufficient for this dataset is rejected. The reliance on passage-contained facts is a critical limitation when external knowledge is required.

## COMMON FAILURE MODES ON THIS DATASET

*   **Passage Length:** Long passages can make it difficult to locate the relevant information.
*   **Synonym Use:** The passage might use synonyms or paraphrases that require understanding to connect to the question.
*   **Complex Sentence Structure:** Complex sentences with multiple clauses can increase the cognitive load.
*   **Implicit Information:** The answer might not be stated explicitly but needs to be inferred from the passage.
*   **Distraction:** Irrelevant information in the passage can distract the model from the correct answer.
*   **Ambiguity:** Rare, but possible, the passage could be ambiguous leading to multiple possible valid answers.
*   **Multiple Occurrences:** The entity mentioned in the question might appear multiple times in the passage, requiring disambiguation.
*   **Negation:** The question or passage might contain negations that need to be handled correctly.
*   **Coreference Resolution:** Identifying which entity a pronoun or other reference refers to.
*   **Units Conversion:** (Not present in these examples, but a potential complexity) Needing to convert between units (e.g., feet to yards).
*   **Time Sensitivity:** If time is involved, the model needs to understand relative time references (e.g., "later on").
*   **Overly Verbose Answers:** The LLM includes unnecessary context or restates the question in its answer, causing a mismatch with the concise ground truth. *Example:* For "Which star has a smaller mass, Nu Phoenicis or Gliese 915?", the expected answer is "Gliese 915," but the LLM outputs "Gliese 915 has a smaller mass than Nu Phoenicis."
*   **Incorrect Entity Identification:** The LLM sometimes identifies the wrong entity within the passage as the answer. *Example:* For "Who threw the second longest touchdown pass?", the model incorrectly answers "Joe Flacco" instead of "Brett Favre".
*   **Lack of Precise Information Retrieval:** Even when the correct answer is identified within the passage, the model may fail to extract it correctly. *Example:* For "Which player kicked the only field goal of the game?", the question expects the name of the player. The LLM answer is unknown due to lack of details in the provided output.
*   **Numerical Reasoning Errors:** Incorrect calculations or misinterpretation of numerical information in the passage.
*   **Incorrect Numerical Extraction:** The primary failure mode is the inaccurate extraction of numerical values from the passage. The LLM struggles to correctly identify the "longest touchdown pass" and "longest field goal" when these values are presented within a narrative context. The LLM may pick a number that's simply mentioned, not the one relevant to the question.
*   **Inaccurate Numerical/Temporal Extraction:** The most prominent failure mode is the LLM's inability to accurately extract dates and numerical values from the passage. Even a single incorrect extraction can derail the entire reasoning process, leading to a wrong final answer (as demonstrated in the provided error examples). For example, in the "Price Is Right" question, misidentifying the dates of the initial release and the preview event made it impossible to calculate the correct time difference. The "Albania revolt" question similarly demonstrates incorrect date extraction and comparison.
*   **Lack of Contextual Understanding:** The model fails to associate the numerical values with their correct descriptors (e.g., associating "80" with "touchdown pass" and "50" with "longest field goal") even when the passage contains sufficient information.
*   **Complex Reasoning:** Questions requiring complex reasoning with multiple steps, such as aggregating information from different parts of the passage or performing arithmetic calculations. The agent may struggle with these types of multi-hop reasoning tasks.
*   **Ambiguous Questions:** Ambiguous questions within the context of the passage may result in incorrect or incomplete information. The model may misinterpret the question and thus extract incorrect information.
*   **Failure to Filter Irrelevant Information:** If the passage contains large amounts of irrelevant information, the Information Extractor may fail to identify the relevant pieces to answer the question. (Note: This wasn't observed in iteration 2, as accuracy was perfect, but it remains a potential failure mode.)
*   **Error Propagation:** When the question decomposition step extracts incorrect information, this error propagates through the subsequent answering and synthesis steps, compounding the problem. The "Texans vs. Jaguars" question demonstrates how misidentifying the scores led to an incorrect final point total.
*   **Lack of Precise Matching:** The LLM seems to struggle with precise matching of entities, often picking similar but ultimately incorrect values from the passage. This can result in the "right kind of answer" but with the "wrong details."
*   **Numbers within Descriptive Text**: Numerical values are often embedded within descriptive text, increasing the difficulty of accurate extraction.
*   **Complex Numerical Calculation Failure:** The primary failure mode is the inability to correctly identify all relevant numerical information and perform the necessary arithmetic. For instance, in the question about the points scored by the Ravens and Steelers, the system failed to determine the final score for each team, thus missing the answer.
*   **Information Synthesis Failure:** When the answer requires combining information from multiple sentences or paragraphs, the system struggles to synthesize the relevant details into a cohesive answer.
*   **Over-Elaboration and Misinterpretation:** The system provides more information than necessary (e.g., detailing scoring breakdown instead of just stating the point difference). This suggests a failure to understand the precise scope and intent of the question.
*   **Entity Confusion and Misidentification:** In the Zagorichani example, the system confuses "massacre" with "political murders," indicating a semantic understanding issue. The system needs to be more precise in extracting and relating information.
*   **Inability to perform basic arithmetical reasoning:** The system was unable to accurately subtract, even when it extracted the correct numbers from the passage.
*   **External Knowledge Dependence Failure:** The core failure mode is the inability to leverage external knowledge. The system failed when asked "Who was in power the year that Boris Yeltsin was born?" because it correctly identified that the passage didn't state Yeltsin's birth year and stopped there, failing to incorporate the implicit task of looking up Yeltsin's birth year and then checking who was in power at that time (according to the passage). The system's fact verification is limited to the context window.
*   **Lack of External Knowledge Integration:** Even with passage references to key figures and timelines, the system fails to integrate external knowledge needed for reasoning.

## EXPERIMENT LOG & FINDINGS

*   **2025-06-01 (Baseline Experiment):**
    *   **Accuracy:** 80%
    *   **Findings:** The baseline LLM demonstrates a decent ability to comprehend passages and identify relevant information, but struggles with providing concise answers in the expected format. It tends towards generating more complete, natural-sounding sentences.
    *   **Failure Analysis:** Overly verbose answers, incorrect entity identification, and failure to precisely extract information even when identified in the passage.
    *   **Next Steps:** Prompt engineering for conciseness, post-processing for entity extraction, few-shot examples for answer format, and implementing a fact verification retrieval system.
*   **[Iteration 1]:**
    *   **Accuracy:** 0.67
    *   **Findings:** The multi-stage LLM approach shows promise by breaking down the problem, but the current implementation struggles with tasks that require both numerical reasoning and deep contextual understanding. The initial hypothesis that decomposing the problem would improve accuracy is partially supported, but the numerical extraction stage needs significant improvement.
    *   **Failure Analysis:** The primary failure mode is the inaccurate extraction of numerical values from the passage. Specifically, the LLM struggles to correctly identify the "longest touchdown pass" and "longest field goal" when these values are presented within a narrative context. The LLM may pick a number that's simply mentioned, not the one relevant to the question. The model fails to associate the numerical values with their correct descriptors (e.g., associating "80" with "touchdown pass" and "50" with "longest field goal") even when the passage contains sufficient information.
    *   **Next Steps:** Improve Numerical Extraction, Enhance Contextual Understanding and Error Analysis.
*   **[Iteration 2]:**
    *   **Accuracy:** 1.0
    *   **Findings:** High accuracy suggests the multi-stage (Simplification, Extraction, Verification) strategy is well-suited for the dataset, given the sample questions. The hybrid approach proves effective in extracting numerical information within a constrained context. The breakdown of tasks between Simplifier, Extractor and Verifier allows the system to successfully navigate the passages and identify correct answers. The modularity of the multi-stage approach facilitates error diagnosis and iterative refinement.
    *   **Failure Analysis:** (Hypothetical - no failures were recorded in this run). Potential failure modes include: Complex reasoning with multiple steps, ambiguous questions leading to misinterpretation, and failure to filter large amounts of irrelevant information.
    *   **Next Steps:** Introduce more complex questions that require multi-hop reasoning, arithmetic operations, or temporal reasoning. Refine the prompts for the Simplifier to preserve numerical data integrity and contextual cues relevant to question answering. Implement error logging to capture failure cases and facilitate targeted improvements. Evaluate the model on passages with higher information density and more irrelevant details to test its ability to filter and extract relevant information accurately. Expand the verification step to perform more rigorous consistency checks and resolve potential ambiguities in extracted information. Further stress-test the model with more complex questions and passages that require deeper reasoning.
*   **[Iteration 3]:**
    *   **Accuracy:** 0.50
    *   **Findings:** The low accuracy suggests that the current question-decomposition and answer-synthesis approach, while conceptually sound, is highly sensitive to errors in information extraction. The experiment confirmed that the LLM's reasoning abilities are limited by the accuracy of its initial information retrieval. Decomposition alone doesn't solve the problem of inaccurate extraction. While the strategy attempts to decompose the questions, the accuracy suggests the decomposition itself isn't inherently helpful *without* improvements to the extraction. A good decomposition relies on accurate information gathering for each sub-question.
    *   **Failure Analysis:** The most prominent failure mode is the LLM's inability to accurately extract dates and numerical values from the passage. Even a single incorrect extraction can derail the entire reasoning process, leading to a wrong final answer. The LLM seems to struggle with precise matching of entities, often picking similar but ultimately incorrect values from the passage. This can result in the "right kind of answer" but with the "wrong details." Error Propagation occurs when the question decomposition step extracts incorrect information, this error propagates through the subsequent answering and synthesis steps, compounding the problem.
    *   **Next Steps:** Prioritize improving the accuracy of information extraction, especially for numerical and temporal data. Implement Verification/Consistency Checks after the synthesis to check the consistency of the final answer with the information extracted from the passage.
*   **[Iteration 4]:**
    *   **Accuracy:** [To be populated with the real accuracy]
    *   **Findings:** The current fact-verification and self-correction approach, while helpful, isn't sufficient to overcome the challenges of complex numerical reasoning on this dataset. The experiment highlights that the LLM struggles with questions that involve implicit computations based on textual information.
    *   **Failure Analysis:** Complex Numerical Calculation Failure: The primary failure mode is the inability to correctly identify all relevant numerical information and perform the necessary arithmetic. For instance, in the question about the points scored by the Ravens and Steelers, the system failed to determine the final score for each team, thus missing the answer. Information Synthesis Failure: When the answer requires combining information from multiple sentences or paragraphs, the system struggles to synthesize the relevant details into a cohesive answer.
    *   **Next Steps:** Enhance Numerical Reasoning, Decomposition of Numerical Tasks, Focused Fact Verification for Numerical Values.
*   **[Iteration 5]:**
    *   **Accuracy:** Low (specific value to be populated).
    *   **Findings:** The initial hypothesis that a simple query-and-synthesize approach would be effective for this dataset is rejected. The complexity of the questions (requiring temporal, quantitative, and precise semantic understanding) demands a more sophisticated approach. The two-step query generation and answer synthesis is not performing as intended with the current prompts.
    *   **Failure Analysis:** Over-Elaboration and Misinterpretation, Entity Confusion and Misidentification, Inability to perform basic arithmetical reasoning.
    *   **Next Steps:** Refine LLM Prompts for Precision, Implement explicit numerical calculation, Implement Passage-Focused Query Generation, Incorporate Named Entity Recognition (NER), Explore Different LLM Architectures.
*   **[Iteration 6]:**
    *   **Accuracy:** Low (Specific Value to be Populated)
    *   **Findings:** The current architecture is effective for questions answerable directly from the passage, but insufficient when external knowledge or more complex reasoning is required. The hypothesis that multi-stage, LLM-driven fact verification and information extraction alone would be sufficient for this dataset is rejected. The reliance on passage-contained facts is a critical limitation.
    *   **Failure Analysis:** External Knowledge Dependence Failure: The core failure mode is the inability to leverage external knowledge. The system failed when asked "Who was in power the year that Boris Yeltsin was born?" because it correctly identified that the passage didn't state Yeltsin's birth year and stopped there, failing to incorporate the implicit task of looking up Yeltsin's birth year and then checking who was in power at that time (according to the passage). The system's fact verification is limited to the context window.
    *   **Next Steps:** Implement External Knowledge Retrieval, Knowledge Integration Mechanism, Prompt Engineering for Knowledge Integration.

## NEXT RESEARCH DIRECTIONS

*   **Implement External Knowledge Retrieval:** Augment the system to retrieve external knowledge from a knowledge base (e.g., Wikipedia) when the LLM determines it's necessary to answer a question. This retrieval should be triggered when the initial answer generation identifies a missing piece of information.
*   **Knowledge Integration Mechanism:** Develop a mechanism to integrate retrieved external knowledge with the passage information before the final validation stage. This might involve creating a combined context for the LLM.
*   **Prompt Engineering for Knowledge Integration:** Refine prompts to specifically instruct the LLM to utilize both the passage and any retrieved external knowledge to derive the final answer. Explicitly tell it to perform a search if needed, and then cite the source.
*   **Focus on Improving Information Extraction:** Prioritize improving the accuracy of information extraction, especially for numerical and temporal data.
    *   **Prompt Engineering for Extraction:** Experiment with prompts specifically designed to elicit accurate numerical and temporal values. Use formats like "Extract the date of X in the format YYYY-MM-DD" or "What number represents Z?"
    *   **Few-Shot Examples for Extraction:** Provide few-shot examples in the system prompt that demonstrate precise extraction of dates and numbers from similar text.
    *   **Output Validation:** Implement a validation step to check if extracted values are plausible (e.g., ensure dates are within a reasonable range).
*   **Enhance Numerical Reasoning:** Incorporate a more robust numerical reasoning module, potentially using specialized tools or libraries to extract and process numerical data. Implement explicit numerical calculation in the system.
*   **Decomposition of Numerical Tasks:** If a numerical computation is necessary, explicitly decompose the question into sub-questions that isolate the numerical values, then compute the final answer in a separate step.
*   **Focused Fact Verification for Numerical Values:** When a question requires numerical reasoning, direct the fact-verification step to specifically validate the extracted numerical values and the intermediate steps in the calculation.
*   **Implement Verification/Consistency Checks:** Add a verification step after the synthesis to check the consistency of the final answer with the information extracted from the passage. This can help catch errors introduced during the reasoning process.
*   **Datasets for Number and Date Extraction:** Consider using or fine-tuning with existing datasets focusing on number and date extraction to improve the LLM's capabilities in these areas.
*   **Iterative Refinement:** Systematically analyze failure cases to identify specific types of numerical/temporal reasoning errors and refine the prompting strategy accordingly.
*   **Introduce Complexity:** Introduce more complex questions that require multi-hop reasoning, arithmetic operations, or temporal reasoning to further test the multi-stage approach.
*   **Detailed Error Analysis:** Conduct a more granular error analysis to categorize failure modes and their frequencies. Focus on understanding the root causes of incorrect entity identification and numerical reasoning errors. Manually analyze more error cases to identify recurring patterns and fine-tune the prompts and post-processing steps accordingly. *Even though Iteration 2 had perfect accuracy, proactively prepare for failures by defining clear error analysis procedures.*
*   **Prompt Engineering Iteration:** Systematically experiment with different prompt formulations to optimize for conciseness and accuracy. Explore different instruction styles, constraints, and the use of chain-of-thought prompting. Refine the prompts for the Simplifier to preserve numerical data integrity and contextual cues relevant to question answering.
*   **Post-Processing Techniques:** Investigate various post-processing techniques for entity extraction, including NER, regular expressions, and string similarity measures. Evaluate their effectiveness in correcting overly verbose answers.
    *   Implement a rule-based post-processing step to validate extracted values. For example, check if the extracted values are plausible within the context of a football game (e.g., a field goal cannot be longer than 60 yards).
*   **Few-Shot Example Selection:** Explore different strategies for selecting effective few-shot examples. Focus on examples that demonstrate the desired answer format and cover different question types and domains.
*   **Retrieval System Integration:** Implement and evaluate different retrieval systems for fact verification. Explore various indexing and retrieval algorithms, such as BM25 and TF-IDF.
*   **Knowledge Graph Integration:** Explore the use of knowledge graphs to represent the information in the passages. This could enable more sophisticated reasoning and inference.
*   **Fine-tuning:** If a large, labeled dataset is available, consider fine-tuning the LLM on the specific task of question answering.
*   **Address Ambiguity:** Develop methods for handling ambiguous questions or passages, such as returning multiple possible answers or asking clarifying questions.
*   **Context Window Optimization:** Experiment with different context window sizes to determine the optimal trade-off between information access and computational cost.
*   **Cross-Domain Generalization:** Evaluate the model's performance on different domains and assess its ability to generalize to new types of passages and questions.
*   **Reasoning Chain Generation:** Explore using the LLM to generate a "reasoning chain" before providing the final answer, forcing it to explicitly state its understanding of the passage and its reasoning for choosing specific numbers. This is to enhance contextual understanding.
*   **Information Density Evaluation:** Evaluate the model on passages with higher information density and more irrelevant details to test its ability to filter and extract relevant information accurately.
*   **Verification Expansion:** Expand the verification step to perform more rigorous consistency checks and resolve potential ambiguities in extracted information.
*   **Error Logging:** Implement error logging to capture failure cases (when they occur) and facilitate targeted improvements.
*   **Implement Passage-Focused Query Generation:** Instead of generating a broad search query, design the first LLM call to identify the specific sentence(s) in the passage that contain the answer. This could involve instructing the LLM to output the relevant sentence indices.
*   **Incorporate Named Entity Recognition (NER):** Use NER to identify and categorize entities (dates, numbers, locations, people) within the passage. This structured information can improve the LLM's ability to perform temporal and relational reasoning.
*   **Explore Different LLM Architectures:** Consider fine-tuning a smaller, more efficient LLM on a dataset of question-answer pairs derived from similar passages. This might improve the model's ability to focus on the relevant information.
```
        

            CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        

            EXPLORATION GUIDANCE:
            1. Review the historical approaches, error patterns, and accumulated learnings carefully
            2. Review the FULL CODE of previous scripts to understand what has already been tried
            3. Design a new approach that is DISTINCTLY DIFFERENT from previous attempts. This approach should have a specific NEW HYPOTHESIS or variable you are trying to test. 
            4. CRITICAL: Include EMBEDDED EXAMPLES directly within your LLM prompts
            5. For each key function, show a complete worked example, or include multiple examples, including:
               - Input example that resembles the dataset
               - Step-by-step reasoning through the example
               - Properly formatted output
            6. Apply the insights from the ACCUMULATED LEARNINGS section to avoid repeating past mistakes
            7. Pay SPECIAL ATTENTION to the weaknesses and improvement suggestions from the capability assessment
            8. Consider implementing one or more of these LLM usage patterns:
               - Repeated validation with feedback loops
               - Multi-perspective analysis with synthesis
               - Dynamic input-dependent routing with an orchestrator
               - Hybrid approaches combining LLM with deterministic functions
               - Best-of-n solution generation and selection
               - ReAct pattern for interactive reasoning and action
               - If it is unknown how successful a processing state or part of the pipeline is, include verification steps to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
               - Answer checkers to validate the final answer against the problem statement. If the answer is incorrect, the checker can send the answer back to an earlier part of the system for for refinement with feedback

            Here's how to call the Gemini API. Use this example without modification and don't invent configuration options:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

            Since this is an EXPLORATION phase:
            - Try a fundamentally different approach to reasoning about the problem. Test a NEW HYPOTHESIS or variable, and add verification steps to deduce if this new change is helpful.
            - THIS IS KEY: Break down the problem into new, distinct reasoning steps based on past performance before you start coding
            - For EACH key LLM prompt, include a relevant example with:
              * Sample input similar to the dataset
              * Expected reasoning steps
              * Desired output format
            - Apply a verifier call to different parts of the pipeline in order to understand what parts of the pipeline of calls is successful and where the system is breaking
            - Pay special attention to addressing the primary issues from previous iterations
            - Ensure your new approach addresses the weaknesses identified in the capability assessment

            CRITICAL REQUIREMENTS:
            1. The script MUST properly handle all string literals - be extremely careful with quotes and triple quotes
            2. The script MUST NOT exceed 150 lines of code to prevent truncation
            3. Include detailed comments explaining your reasoning approach
            4. EVERY SINGLE LLM PROMPT must include at least one embedded example showing:
               - Sample input with reasoning
               - Desired output format
            5. Make proper use of error handling
            6. Implement robust capabilities to address the specific weaknesses identified in the capability assessment
            7. Do NOT use json.loads() in the LLM calls to process input data. JSON formatting is good to use to structure information as inputs and outputs, but attempting to have functions process JSON data explicitly with strict built-in functionality is error prone due to formatting issues and additional text that appears as documentation, reasoning, or comments. When passing data into another LLM call, you can read it as plain text rather than trying to load it in strict json format, is the better approach.

            Return a COMPLETE, RUNNABLE Python script that:
            1. Has a main function that takes a question string as input and returns the answer string
            2. Makes multiple LLM calls for different reasoning steps
            3. Has proper error handling for API calls
            4. Includes embedded examples in EVERY LLM prompt
            5. Is COMPLETE - no missing code, no "..." placeholders
            6. Closes all string literals properly

            This should be FUNDAMENTALLY DIFFERENT from all previous approaches. Do not reuse the same overall structure.

            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            