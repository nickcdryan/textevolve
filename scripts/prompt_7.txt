
            You are developing a Python script to solve problems using LLM reasoning capabilities.
            You are in the EXPLORATION PHASE. You must generate a NEW approach that's different from previous approaches but informed by their successes and failures. With this approach, you will have a specific NEW HYPOTHESIS or variable you are trying to test. Your goal is to see if this new approach works, and you must add verification and validation steps to deduce if this new change is helpful. You may also test RADICAL NEW APPROACHES that are substantially different from previous approaches. 
            
            You should try NEW THINGS:
            
            Break down the problem into smaller pieces
            Think CREATIVELY about how to solve your problem if other approaches aren't working
            Transform data into different formats to see if it helps

            # YOUR TASK
            You are deeply familiar with prompting techniques and the agent works from the literature. 
            Your goal is to maximize the specified performance metrics by proposing interestingly new agents.
            Observe the past discovered agents and scripts carefully and think about what insights, lessons, or stepping stones can be learned from them.
            Be creative when thinking about the next interesting agent to try. You are encouraged to draw inspiration from related agent papers or academic papers from other research areas.
            Use the knowledge from the archive and inspiration from academic literature to propose the next interesting agentic system design.
            THINK OUTSIDE THE BOX.
            

            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "In which month and year was Satyanarayan Gangaram Pitroda (an Indian telecommunication engineer and entrepreneur) appointed as advisor to Indian Prime Minister Manmohan Singh on Public Information Infrastructure and Innovations with the rank of Cabinet Minister?",
    "answer": " October 2009"
  },
  {
    "id": 1,
    "question": "During which years was Otto Schl\u00fcter a professor of geography at the University of Halle?",
    "answer": "1911 to 1959"
  },
  {
    "id": 2,
    "question": "Who was the first president of the Supreme Court of Argentina?",
    "answer": "Francisco de las Carreras"
  }
]

            HISTORICAL CONTEXT:
            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 7
        - Current explore/exploit balance: 50/20
        - Best accuracy achieved: 0.67 (iteration 2)

        APPROACH HISTORY (last 7 iterations):
        [
  {
    "iteration": 0,
    "strategy": "baseline",
    "accuracy": 0.1,
    "approach": "Simple baseline script: Direct LLM call without sophisticated techniques"
  },
  {
    "iteration": 1,
    "strategy": "explore",
    "accuracy": 0.0,
    "approach": "This script answers questions using LLMs by decomposing the problem into entity/relationship extraction, search query generation, information retrieval, answer generation, and answer validation. Each step leverages the Gemini LLM with a specific system instruction to act as a specialized agent (e.g., information extractor, query generator). The script uses a few functions: `call_llm` for interacting with the Gemini model, `extract_entities_and_relationships` to identify key information, `generate_search_query` to create a search query, `retrieve_information` to simulate a search engine, `generate_answer` to formulate an answer, and `validate_answer` to check the answer's correctness. The overall workflow involves sequentially calling these functions, passing the output of one as input to the next, ultimately returning the validated answer or an error message."
  },
  {
    "iteration": 2,
    "strategy": "exploit",
    "accuracy": 0.6666666666666666,
    "approach": "The script employs a validation-driven problem-solving approach using the Gemini LLM, where a problem is first solved and then iteratively refined based on validation feedback. The problem is decomposed into solution generation and solution validation steps. There are two agent roles: a solver and a validator, each with specific system instructions. `call_llm` is used to interact with the LLM by passing prompts and instructions, while `solve_with_validation_loop` manages the iterative solving and validation process, and `main` is the entry point that calls `solve_with_validation_loop`. The overall workflow involves generating an initial solution with `call_llm` using the solver agent, validating it with `call_llm` using the validator agent, and refining the solution using `call_llm` with feedback until it's deemed valid or the maximum number of attempts is reached."
  },
  {
    "iteration": 3,
    "strategy": "explore",
    "accuracy": 0.3333333333333333,
    "approach": "The script uses LLM-based information retrieval and answer generation. It decomposes the problem into retrieving relevant context and then generating an answer using that context. The `retrieve_relevant_context` function acts as a search agent, generating search queries, simulating search, and validating the retrieved context for relevance. The `generate_answer_with_context` function acts as an expert at answering questions using the provided context.\n\nThe `call_llm` function is used to interact with the Gemini LLM, taking prompts and system instructions as input. The `retrieve_relevant_context` calls `call_llm` to generate search queries, simulate search, and validate context relevance; this function is then called by the `main` function, which then calls `generate_answer_with_context`, which then calls `call_llm` to create an answer using the retrieved context, finally returning it. The `main` function orchestrates the workflow by calling both functions sequentially."
  },
  {
    "iteration": 4,
    "strategy": "explore",
    "accuracy": 0.6666666666666666,
    "approach": "The script implements a retrieval-augmented generation (RAG) approach to answer questions using an LLM. It decomposes the problem into query generation, search snippet validation, and answer generation. The `generate_query_and_validate` function uses an LLM with the \"expert at generating effective search queries\" role to create a search query and then validates the query using another LLM call with the role \"expert at validating search snippets\" before proceeding; It uses a few-shot validation technique. The `generate_answer_with_snippets` function uses an LLM with the role \"expert at answering questions given relevant search snippets\" to formulate an answer based on the validated search snippets.\n\nThe overall workflow begins in `main` which calls `generate_query_and_validate` with a question, which in turn uses `call_llm` to generate a search query and validate the results. `main` then calls `generate_answer_with_snippets` with the original question and the validated search snippets, which in turn uses `call_llm` to generate the final answer. The `call_llm` function is used throughout the script to interface with the Gemini LLM for query generation, snippet validation, and answer generation."
  },
  {
    "iteration": 5,
    "strategy": "exploit",
    "accuracy": 0.0,
    "approach": "The script implements a RAG-based approach with a validation loop for answering questions. It decomposes the problem into query generation, search snippet retrieval (simulated), answer generation, and solution validation. The script uses two agent roles: a problem solver/answer generator and a validator, both driven by the `call_llm` function.\n\nKey functions:\n*   `call_llm`: Interacts with the Gemini model.\n*   `generate_query_and_validate`: Generates and validates a search query against search snippets to ensure relevance.\n*   `generate_answer_with_snippets`: Generates an answer based on the provided search snippets.\n*   `solve_with_validation_loop`: Orchestrates the RAG process, incorporating a validation loop to refine the answer.\n*   `main`: Calls `solve_with_validation_loop` to return an answer to the user's question\n\nThe workflow starts with `solve_with_validation_loop`, which calls `generate_query_and_validate` and `generate_answer_with_snippets` to get an initial answer. This answer is then iteratively validated, and if found invalid, the query and answer generation steps are rerun."
  },
  {
    "iteration": 6,
    "strategy": "exploit",
    "accuracy": 0.3333333333333333,
    "approach": "The script uses a combination of RAG and a validation loop to answer questions. It first generates a search query, retrieves and validates search snippets, and then generates an initial answer using these snippets. The answer is then iteratively validated and refined through a validation loop, with the LLM acting as a validator and a refiner, until a valid answer is found or the maximum attempts are reached. The functions used include `call_llm` for LLM interaction, `generate_query_and_validate` for RAG, `generate_answer_with_snippets` for answer generation, `solve_with_validation_loop` for the iterative process, and `main` to orchestrate everything."
  }
]

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 3 iterations):
        [
  {
    "iteration": 0,
    "issue": "The most critical problem is the system's reliance on an unreliable knowledge source which leads to the retrieval and provision of factually incorrect information. The lack of a verification mechanism exacerbates this issue, as the system blindly trusts the incorrect information."
  },
  {
    "iteration": 1,
    "issue": "The most critical problem is the **inaccurate and unreliable information extraction process, coupled with insufficient validation and error detection**. The system needs to be significantly improved in its ability to pinpoint the specific information required to answer the question correctly and verify that the retrieved information is accurate and relevant."
  },
  {
    "iteration": 2,
    "issue": "The primary issue is **inaccurate knowledge retrieval**. The system provides a definite answer that is factually incorrect, indicating a flaw in its information gathering or database. This highlights the need for improved source reliability and validation."
  },
  {
    "iteration": 3,
    "issue": "The primary issue is **inaccurate factual recall, particularly involving dates and specific names**. This leads to the system providing incorrect answers even when it understands the question's intent. The root cause could stem from flaws in the training data, the knowledge retrieval mechanism, or the final answer selection process."
  },
  {
    "iteration": 4,
    "issue": "The single most critical problem is the **inadequate retrieval of relevant information from search snippets**. The search queries and information extraction methods used by the system are not specific or robust enough to find the required details, leading to \"information not present\" answers even when the information is potentially available."
  },
  {
    "iteration": 5,
    "issue": "The primary issue is **inaccurate and/or incomplete information retrieval from the knowledge source.** This manifests as providing imprecise answers, failing to find existing answers, or providing incorrect specific details. The system's retrieval mechanism needs to be improved to ensure accuracy and completeness."
  },
  {
    "iteration": 6,
    "issue": "The system's inability to extract precise information, specifically dates and time periods, from the available data is the most critical problem. It relies too heavily on exact matches and cannot synthesize or infer answers when precise details are missing."
  }
]

        TARGETED IMPROVEMENTS:
        [
  "Stage 2: Semantic Filtering:** Filter the retrieved information based on semantic relevance to the question. Use techniques like sentence similarity or knowledge graph traversal to identify the most relevant pieces of information.",
  "Fact Verification Module:** Integrate a fact verification module that cross-references potential answers against multiple external sources to assess their accuracy and reliability.",
  "Stage 1: Focused Retrieval:** Retrieve a narrow set of potentially relevant information based on keywords and entity recognition.",
  "Confidence Scoring and Thresholding:** Develop a confidence scoring mechanism for potential answers and set a threshold for acceptance. Answers below the threshold should be rejected, and the system should indicate its uncertainty or seek clarification.",
  "Improved Ranking Algorithm:** Experiment with different ranking algorithms to prioritize answers based on factors such as source credibility, frequency of mention, and relevance to the question's context. For example, prioritize information extracted from reputable encyclopedias or biographical databases.",
  "Stage 3: Verification and Validation:** Verify the accuracy and consistency of the filtered information by cross-referencing with other sources, applying logical rules, or using external validation tools.",
  "Data Verification and Augmentation:** Implement automated and manual processes to verify and correct information in the knowledge base. Focus on fact-checking and updating numerical data (especially dates). Consider augmenting the knowledge base with more comprehensive biographical data, especially for prominent figures."
]
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity ✓
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity ✓
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity ✓
    4. All operations have O(1) average time complexity ✓
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        

            PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS). YOUR APPROACH MUST BE SUBSTANTIVELY DIFFERENT THAN THESE:
            
PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS):

=== SCRIPT FROM ITERATION 6 (exploit, ACCURACY: 0.33) ===
Approach: The script uses a combination of RAG and a validation loop to answer questions. It first generates a search query, retrieves and validates search snippets, and then generates an initial answer using these snippets. The answer is then iteratively validated and refined through a validation loop, with the LLM acting as a validator and a refiner, until a valid answer is found or the maximum attempts are reached. The functions used include `call_llm` for LLM interaction, `generate_query_and_validate` for RAG, `generate_answer_with_snippets` for answer generation, `solve_with_validation_loop` for the iterative process, and `main` to orchestrate everything.

```python
import os
import re
import json
import math # for react
from google import genai
from google.genai import types

# Combination of Iteration 2 (Validation Loop) and Iteration 4 (RAG with validated query)

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def generate_query_and_validate(question, max_attempts=3):
    """
    Generates a search query from a question and validates its effectiveness.
    (Adapted from Iteration 4)
    """
    system_instruction_query_gen = "You are an expert at generating effective search queries that help answer questions."
    system_instruction_search_validator = "You are an expert at validating whether a set of search snippets are relevant to answering the question"

    for attempt in range(max_attempts):
        # Step 1: Generate Search Query with Examples
        query_prompt = f"""
        Generate a search query to retrieve information needed to answer the question. Be specific and target the precise information needed.

        Example 1:
        Question: What was the first name of Ralph E. Oesper?
        Search Query: "Ralph E. Oesper first name" exact

        Example 2:
        Question: In which year did Maharaj Kishan Bhan receive the Padma Bhushan for civil services?
        Search Query: "Maharaj Kishan Bhan Padma Bhushan year"

        Example 3:
        Question: What day, month, and year was the first Sazae-san strip run by the Asahi Shimbun published?
        Search Query: "first Sazae-san strip published date Asahi Shimbun"

        Question: {question}
        Search Query:
        """
        search_query = call_llm(query_prompt, system_instruction_query_gen)

        # Step 2: Simulate Retrieving Top Search Snippets - IMPORTANT: IN A REAL SYSTEM THIS WOULD BE SEARCH API
        search_snippets = call_llm(f"Provide top 3 search snippets for: {search_query}", "You are a helpful search engine providing realistic search results.")

        # Step 3: Validate Relevance of Search Snippets with Examples
        validation_prompt = f"""
        Determine if the following search snippets are relevant to answering the question. If they are, respond with "RELEVANT: [brief explanation]". If not, respond with "IRRELEVANT: [detailed explanation]".

        Example 1:
        Question: What was the first name of Ralph E. Oesper?
        Search Snippets: Ralph Oesper was a professor...; His middle name was E...; There is no information on his first name.
        Validation: IRRELEVANT: The snippets don't reveal his first name.

        Example 2:
        Question: In which year did Maharaj Kishan Bhan receive the Padma Bhushan for civil services?
        Search Snippets: Maharaj Kishan Bhan received the Padma Bhushan in 2013; He was a scientist; He worked in civil services.
        Validation: RELEVANT: Snippets contain MKB and the year he received the award.

        Example 3:
        Question: What day, month, and year was the first Sazae-san strip run by the Asahi Shimbun published?
        Search Snippets: The first Sazae-san strip was published on November 30, 1949.
        Validation: RELEVANT: Snippet provides the full date of publication.

        Question: {question}
        Search Snippets: {search_snippets}
        Validation:
        """
        validation_result = call_llm(validation_prompt, system_instruction_search_validator)

        if "RELEVANT:" in validation_result:
            return search_query, search_snippets  # Return both the search query and relevant snippets
        else:
            print(f"Attempt {attempt + 1}: Search snippets deemed irrelevant. Trying again...")

    return None, None  # Return None if no relevant context is found

def generate_answer_with_snippets(question, search_snippets):
    """
    Generates an answer using the validated search snippets. (Adapted from Iteration 4)
    """
    system_instruction = "You are an expert at answering questions given relevant search snippets. Provide the most accurate and complete answer possible, extracting key details. If the answer is not explicitly found, clearly state 'Answer not found in snippets'."

    answer_prompt = f"""
    Answer the question using ONLY the information present in the search snippets. Extract as much detail as possible. If the answer is not found, clearly state "Answer not found in snippets".

    Example 1:
    Question: What was the first name of Ralph E. Oesper?
    Search Snippets: No results found.
    Answer: Answer not found in snippets.

    Example 2:
    Question: In which year did Maharaj Kishan Bhan receive the Padma Bhushan for civil services?
    Search Snippets: Maharaj Kishan Bhan was awarded the Padma Bhushan in 2013.; He was a famous scientist.
    Answer: 2013

    Example 3:
    Question: What day, month, and year was the first Sazae-san strip run by the Asahi Shimbun published?
    Search Snippets: The first Sazae-san strip by Sazae-san was published by the Asahi Shimbun on November 30, 1949.
    Answer: November 30, 1949

    Question: {question}
    Search Snippets: {search_snippets}
    Answer:
    """
    answer = call_llm(answer_prompt, system_instruction)
    return answer

def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop.
    This is primarily adapted from Iteration 2, but using RAG from Iteration 4"""

    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements, ensuring factual correctness and completeness."

    # RAG process from Iteration 4
    search_query, search_snippets = generate_query_and_validate(problem)

    if not search_query or not search_snippets:
        return "Answer not found after multiple attempts."

    initial_solution = generate_answer_with_snippets(problem, search_snippets)

    # Validation loop (from Iteration 2, but adapted to RAG output)
    solution = initial_solution
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly answers all aspects of the problem, is consistent with the search snippets, and is factually correct.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues (e.g. inaccuracies, missing information, or inconsistency with the search snippets), respond with "INVALID: [detailed explanation of issues]".

        Example 1:
        Problem: What is the capital of France?
        Proposed Solution: Paris
        Validation: VALID: The capital of France is indeed Paris.

        Example 2:
        Problem: Who painted the Mona Lisa?
        Proposed Solution: Leonardo da Vinci
        Validation: VALID: The Mona Lisa was painted by Leonardo da Vinci.

        Example 3:
        Problem: What year did World War II begin?
        Proposed Solution: 1940
        Validation: INVALID: World War II began in 1939, not 1940.

        Problem: {problem}
        Proposed Solution: {solution}
        Search Snippets: {search_snippets}
        Validation:
        """

        validation_result = call_llm(validation_prompt, system_instruction_validator)

        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution

        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed. Ensure that your solution uses ONLY information extracted from the search snippets provided, and that it is factually correct.

        Problem: {problem}
        Proposed Solution: {solution}
        Search Snippets: {search_snippets}
        Validation feedback: {validation_result}

        Example of a corrected solution based on validation feedback:

        Problem: When did the Titanic sink?
        Proposed Solution: April 1912
        Search Snippets: The Titanic sank on April 15, 1912.
        Validation Feedback: INVALID: Missing the specific day that the Titanic sank which the search snippets contained.

        Corrected Solution: April 15, 1912

        Generate an improved solution. Use the search snippets to extract the correct answer and all required details. If the snippets don't contain the answer, state "Answer not found in snippets."
        """
        solution = call_llm(refined_prompt) #Force resynthesis based on instructions and snippets
        if "Answer not found in snippets" in solution: #Break if snippets truly don't have the answer
             return solution

    return solution # If validation loop fails, return the last solution

def main(question):
    """
    Main function that orchestrates the solution process using solve_with_validation_loop.
    This function now incorporates RAG and an iterative validation loop for enhanced accuracy.
    """
    answer = solve_with_validation_loop(question)
    return answer
```

=== SCRIPT FROM ITERATION 5 (exploit, ACCURACY: 0.00) ===
Approach: The script implements a RAG-based approach with a validation loop for answering questions. It decomposes the problem into query generation, search snippet retrieval (simulated), answer generation, and solution validation. The script uses two agent roles: a problem solver/answer generator and a validator, both driven by the `call_llm` function.

Key functions:
*   `call_llm`: Interacts with the Gemini model.
*   `generate_query_and_validate`: Generates and validates a search query against search snippets to ensure relevance.
*   `generate_answer_with_snippets`: Generates an answer based on the provided search snippets.
*   `solve_with_validation_loop`: Orchestrates the RAG process, incorporating a validation loop to refine the answer.
*   `main`: Calls `solve_with_validation_loop` to return an answer to the user's question

The workflow starts with `solve_with_validation_loop`, which calls `generate_query_and_validate` and `generate_answer_with_snippets` to get an initial answer. This answer is then iteratively validated, and if found invalid, the query and answer generation steps are rerun.

```python
import os
import re
import json
import math # for react
from google import genai
from google.genai import types

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def generate_query_and_validate(question, max_attempts=3):
    """
    Generates a search query from a question and validates its effectiveness.

    This function incorporates elements from Iteration 4 (RAG approach)
    and enhances the query generation and validation steps.
    """
    system_instruction_query_gen = "You are an expert at generating effective search queries that help answer questions."
    system_instruction_search_validator = "You are an expert at validating whether a set of search snippets are relevant to answering the question. Focus on factual recall and completeness."

    for attempt in range(max_attempts):
        # Step 1: Generate Search Query with Examples (From Iteration 4)
        query_prompt = f"""
        Generate a search query to retrieve information needed to answer the question. Focus on generating queries to help factually answer the questions.

        Example 1:
        Question: What was the first name of Ralph E. Oesper?
        Search Query: Ralph E. Oesper first name

        Example 2:
        Question: In which year did Maharaj Kishan Bhan receive the Padma Bhushan for civil services?
        Search Query: Maharaj Kishan Bhan Padma Bhushan year

        Example 3:
        Question: Which of the three Olympic fencing weapons was the last one to transition to using electrical equipment?
        Search Query: Olympic fencing weapons electrical equipment transition

        Question: {question}
        Search Query:
        """
        search_query = call_llm(query_prompt, system_instruction_query_gen)

        # Step 2: Simulate Retrieving Top Search Snippets
        search_snippets = call_llm(f"Provide top 3 search snippets for: {search_query}", "You are a helpful search engine providing realistic search results. Focus on factual and complete information")

        # Step 3: Validate Relevance of Search Snippets with Examples (From Iteration 4, enhanced)
        validation_prompt = f"""
        Determine if the following search snippets are relevant to answering the question. If they are, respond with "RELEVANT: [brief explanation]". If not, respond with "IRRELEVANT: [detailed explanation]". Prioritize factual recall.

        Example 1:
        Question: What was the first name of Ralph E. Oesper?
        Search Snippets: Ralph Oesper was a professor...; His middle name was E...; There is no information on his first name.
        Validation: IRRELEVANT: The snippets don't reveal his first name or a direct answer.

        Example 2:
        Question: In which year did Maharaj Kishan Bhan receive the Padma Bhushan for civil services?
        Search Snippets: Maharaj Kishan Bhan received the Padma Bhushan in 2013; He was a scientist; He worked in civil services.
        Validation: RELEVANT: Snippets contain MKB and the year he received the award providing a direct answer.

        Example 3:
        Question: In which of the three Olympic fencing weapons was the last one to transition to using electrical equipment?
        Search Snippets: The sabre was the last of the three weapons to be electrified.
        Validation: RELEVANT: Provides answer as to which weapon and the electrical equipment transition

        Question: {question}
        Search Snippets: {search_snippets}
        Validation:
        """
        validation_result = call_llm(validation_prompt, system_instruction_search_validator)

        if "RELEVANT:" in validation_result:
            return search_query, search_snippets
        else:
            print(f"Attempt {attempt + 1}: Search snippets deemed irrelevant. Trying again...")

    return None, None

def generate_answer_with_snippets(question, search_snippets):
    """
    Generates an answer using the validated search snippets.

    This function is from Iteration 4, but adds emphasis to extract specific
    facts, particularly years and names.
    """
    system_instruction = "You are an expert at answering questions given relevant search snippets. Focus on extracting specific facts like names and years."

    answer_prompt = f"""
    Answer the question using ONLY the information present in the search snippets.

    Example 1:
    Question: What was the first name of Ralph E. Oesper?
    Search Snippets: No results found.
    Answer: Answer not found.

    Example 2:
    Question: In which year did Maharaj Kishan Bhan receive the Padma Bhushan for civil services?
    Search Snippets: Maharaj Kishan Bhan was awarded the Padma Bhushan in 2013.; He was a famous scientist.
    Answer: 2013

    Example 3:
    Question: Which of the three Olympic fencing weapons was the last one to transition to using electrical equipment?
    Search Snippets: The sabre was the last of the three weapons to be electrified.
    Answer: Sabre

    Question: {question}
    Search Snippets: {search_snippets}
    Answer:
    """
    answer = call_llm(answer_prompt, system_instruction)
    return answer

def solve_with_validation_loop(question, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop.
    This incorporates the validation loop approach from Iteration 2
    to refine the answer generated from the RAG approach of Iteration 4.
    """
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions. Focus on factual accuracy and completeness."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements, ensuring factual correctness and completeness. Focus on directly answering the question."

    # Initial solution generation using RAG
    search_query, search_snippets = generate_query_and_validate(question)
    if search_query and search_snippets:
        solution = generate_answer_with_snippets(question, search_snippets)
    else:
        solution = "Answer not found."

    # Validation loop (From Iteration 2)
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem. Ensure factual correctness and completeness. The response should directly answer the question.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues, including specific factual errors or omissions]".

        Example 1:
        Problem: What is the capital of France?
        Solution: Paris
        Validation: VALID: The capital of France is indeed Paris. Provides a factually correct response.

        Example 2:
        Problem: Who painted the Mona Lisa?
        Solution: Leonardo DaVinci
        Validation: VALID: The Mona Lisa was painted by Leonardo da Vinci and this directly answers the question.

        Example 3:
        Problem: What year did World War II begin?
        Solution: 1940
        Validation: INVALID: World War II began in 1939, not 1940. Contains factually incorrect information and should be revised.

        Problem:
        {question}

        Proposed Solution:
        {solution}
        """

        validation_result = call_llm(validation_prompt, system_instruction_validator)

        # Check if solution is valid
        if "VALID:" in validation_result:
            return solution

        # If invalid, refine the solution. Attempt to regenerate the answer given the error,
        # as the initial search query may have been inadequate

        search_query, search_snippets = generate_query_and_validate(question)  # Re-run to generate new snippets
        if search_query and search_snippets:
            solution = generate_answer_with_snippets(question, search_snippets)
        else:
            solution = "Answer not found."

    return solution # Returns best attempt by end
def main(question):
    """
    Main function that orchestrates the solution process using solve_with_validation_loop.
    """
    answer = solve_with_validation_loop(question)
    return answer
```

=== SCRIPT FROM ITERATION 4 (explore, ACCURACY: 0.67) ===
Approach: The script implements a retrieval-augmented generation (RAG) approach to answer questions using an LLM. It decomposes the problem into query generation, search snippet validation, and answer generation. The `generate_query_and_validate` function uses an LLM with the "expert at generating effective search queries" role to create a search query and then validates the query using another LLM call with the role "expert at validating search snippets" before proceeding; It uses a few-shot validation technique. The `generate_answer_with_snippets` function uses an LLM with the role "expert at answering questions given relevant search snippets" to formulate an answer based on the validated search snippets.

The overall workflow begins in `main` which calls `generate_query_and_validate` with a question, which in turn uses `call_llm` to generate a search query and validate the results. `main` then calls `generate_answer_with_snippets` with the original question and the validated search snippets, which in turn uses `call_llm` to generate the final answer. The `call_llm` function is used throughout the script to interface with the Gemini LLM for query generation, snippet validation, and answer generation.

```python
import os
import re
import math # for react
from google import genai
from google.genai import types

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def generate_query_and_validate(question, max_attempts=3):
    """
    Generates a search query from a question and validates its effectiveness by assessing
    if the top search snippets contain key entities and relationships needed to answer the question.
    Returns both the generated query and top search snippets.
    """
    system_instruction_query_gen = "You are an expert at generating effective search queries that help answer questions."
    system_instruction_search_validator = "You are an expert at validating whether a set of search snippets are relevant to answering the question"
    # Hypothesis: By generating and validating the query BEFORE retrieving the information, we can significantly improve the information retrieval and hallucination problems that are causing the pipeline to fail
    for attempt in range(max_attempts):
        # Step 1: Generate Search Query with Examples
        query_prompt = f"""
        Generate a search query to retrieve information needed to answer the question.

        Example 1:
        Question: What was the first name of Ralph E. Oesper?
        Search Query: Ralph E. Oesper first name

        Example 2:
        Question: In which year did Maharaj Kishan Bhan receive the Padma Bhushan for civil services?
        Search Query: Maharaj Kishan Bhan Padma Bhushan year

        Question: {question}
        Search Query:
        """
        search_query = call_llm(query_prompt, system_instruction_query_gen)
        # Step 2: Simulate Retrieving Top Search Snippets - IMPORTANT: IN A REAL SYSTEM THIS WOULD BE SEARCH API
        search_snippets = call_llm(f"Provide top 3 search snippets for: {search_query}", "You are a helpful search engine providing realistic search results.")

        # Step 3: Validate Relevance of Search Snippets with Examples
        validation_prompt = f"""
        Determine if the following search snippets are relevant to answering the question. If they are, respond with "RELEVANT: [brief explanation]". If not, respond with "IRRELEVANT: [detailed explanation]".

        Example 1:
        Question: What was the first name of Ralph E. Oesper?
        Search Snippets: Ralph Oesper was a professor...; His middle name was E...; There is no information on his first name.
        Validation: IRRELEVANT: The snippets don't reveal his first name.

        Example 2:
        Question: In which year did Maharaj Kishan Bhan receive the Padma Bhushan for civil services?
        Search Snippets: Maharaj Kishan Bhan received the Padma Bhushan in 2013; He was a scientist; He worked in civil services.
        Validation: RELEVANT: Snippets contain MKB and the year he received the award

        Question: {question}
        Search Snippets: {search_snippets}
        Validation:
        """
        validation_result = call_llm(validation_prompt, system_instruction_search_validator)

        if "RELEVANT:" in validation_result:
            return search_query, search_snippets # Return both the search query and relevant context
        else:
            print(f"Attempt {attempt + 1}: Search snippets deemed irrelevant. Trying again...")

    return None, None  # Return None if no relevant context is found
def generate_answer_with_snippets(question, search_snippets):
    """
    Generates an answer using the validated search snippets, ensuring that the answer
    is directly supported by the information in the snippets.
    """
    system_instruction = "You are an expert at answering question given relevant search snippets"
    # Now we leverage the search snippets to answer the question directly
    answer_prompt = f"""
    Answer the question using ONLY the information present in the search snippets.

    Example 1:
    Question: What was the first name of Ralph E. Oesper?
    Search Snippets: No results found.
    Answer: Answer not found.

    Example 2:
    Question: In which year did Maharaj Kishan Bhan receive the Padma Bhushan for civil services?
    Search Snippets: Maharaj Kishan Bhan was awarded the Padma Bhushan in 2013.; He was a famous scientist.
    Answer: 2013

    Question: {question}
    Search Snippets: {search_snippets}
    Answer:
    """
    answer = call_llm(answer_prompt, system_instruction)
    return answer

def main(question):
    """
    Main function to orchestrate the validated query generation, information retrieval (simulated),
    and answer generation process.
    """
    search_query, search_snippets = generate_query_and_validate(question)

    if search_query and search_snippets:
        answer = generate_answer_with_snippets(question, search_snippets)
        return answer
    else:
        return "Answer not found." # If not able to retrieve reliable context then return not found
```

=== SCRIPT FROM ITERATION 3 (explore, ACCURACY: 0.33) ===
Approach: The script uses LLM-based information retrieval and answer generation. It decomposes the problem into retrieving relevant context and then generating an answer using that context. The `retrieve_relevant_context` function acts as a search agent, generating search queries, simulating search, and validating the retrieved context for relevance. The `generate_answer_with_context` function acts as an expert at answering questions using the provided context.

The `call_llm` function is used to interact with the Gemini LLM, taking prompts and system instructions as input. The `retrieve_relevant_context` calls `call_llm` to generate search queries, simulate search, and validate context relevance; this function is then called by the `main` function, which then calls `generate_answer_with_context`, which then calls `call_llm` to create an answer using the retrieved context, finally returning it. The `main` function orchestrates the workflow by calling both functions sequentially.

```python
import os
import re
import math # for react
from google import genai
from google.genai import types

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def retrieve_relevant_context(question, max_attempts=3):
    """
    Retrieve relevant context for the given question using LLM-based search query generation and a simulated search engine.
    This directly addresses the hallucination and inaccurate retrieval issues from previous iterations.
    Includes a validation loop to ensure the retrieved context is relevant.
    """
    system_instruction = "You are an expert at generating search queries to retrieve relevant information."

    for attempt in range(max_attempts):
        # Step 1: Generate search query with examples
        query_prompt = f"""
        Generate a concise search query to find relevant information for the given question.

        Example 1:
        Question: In which year was Jamini Roy (an Indian painter) awarded the Padma Bhushan by the Government of India?
        Search Query: "Jamini Roy Padma Bhushan award year"

        Example 2:
        Question: Which architect was tasked with finishing the chapel in the newly built Papal apartment when its construction remained incomplete after Pope Paul IV moved in, in October 1556?
        Search Query: "architect finishing Papal apartment Pope Paul IV 1556"

        Example 3:
        Question: In 1993, Vaughan Jones was elected to which academy?
        Search Query: "Vaughan Jones elected academy 1993"

        Question: {question}
        Search Query:
        """
        search_query = call_llm(query_prompt, system_instruction)

        # Step 2: Simulate search and retrieve context
        context = call_llm(f"Provide concise information about: {search_query}", "You are a helpful search engine.")

        # Step 3: Validate context relevance with examples
        validation_prompt = f"""
        Validate if the retrieved context is relevant to the question.
        If relevant, respond with "RELEVANT: [brief explanation]".
        If not relevant, respond with "IRRELEVANT: [detailed explanation]".

        Example 1:
        Question: In which year was Jamini Roy awarded the Padma Bhushan?
        Context: Jamini Roy received the Padma Bhushan in 1954.
        Validation: RELEVANT: The context directly answers the question about the award year.

        Example 2:
        Question: Which architect finished the Papal apartment chapel in 1556?
        Context: Pirro Ligorio completed the chapel in the Papal apartment in October 1556.
        Validation: RELEVANT: The context identifies the architect and the task.

        Question: {question}
        Context: {context}
        Validation:
        """
        validation_result = call_llm(validation_prompt, "You are an expert at determining the relevance of a text to a question.")

        if "RELEVANT:" in validation_result:
            return context
        else:
            print(f"Attempt {attempt + 1}: Retrieved context is irrelevant. Retrying...")

    return "No relevant context found." # Fallback after multiple attempts

def generate_answer_with_context(question, context):
    """
    Generate the answer using the retrieved context.
    This leverages the LLM's reasoning capabilities to synthesize an answer based on the context.
    """
    system_instruction = "You are an expert at answering questions based on provided context."

    prompt = f"""
    Answer the question using the provided context. If the context does not contain the answer, state "Answer not found in context."

    Example 1:
    Question: In which year was Jamini Roy awarded the Padma Bhushan?
    Context: Jamini Roy received the Padma Bhushan in 1954.
    Answer: 1954

    Example 2:
    Question: Which architect finished the Papal apartment chapel in 1556?
    Context: Pirro Ligorio completed the chapel in the Papal apartment in October 1556.
    Answer: Pirro Ligorio

    Question: {question}
    Context: {context}
    Answer:
    """
    return call_llm(prompt, system_instruction)

def main(question):
    """
    Main function: Orchestrates context retrieval and answer generation.
    """
    context = retrieve_relevant_context(question)
    if "No relevant context found" in context:
        return "Could not find the answer."

    answer = generate_answer_with_context(question, context)
    return answer
```

=== SCRIPT FROM ITERATION 2 (exploit, ACCURACY: 0.67) ===
Approach: The script employs a validation-driven problem-solving approach using the Gemini LLM, where a problem is first solved and then iteratively refined based on validation feedback. The problem is decomposed into solution generation and solution validation steps. There are two agent roles: a solver and a validator, each with specific system instructions. `call_llm` is used to interact with the LLM by passing prompts and instructions, while `solve_with_validation_loop` manages the iterative solving and validation process, and `main` is the entry point that calls `solve_with_validation_loop`. The overall workflow involves generating an initial solution with `call_llm` using the solver agent, validating it with `call_llm` using the validator agent, and refining the solution using `call_llm` with feedback until it's deemed valid or the maximum number of attempts is reached.

```python
import os
import re
import json
import math # for react
from google import genai
from google.genai import types

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop.
    This is based on successful patterns from previous iterations, particularly in Iteration 0,
    but enhanced with iterative refinement for better accuracy."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions. Focus on factual accuracy and completeness."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements, ensuring factual correctness and completeness."

    # Initial solution generation - Enhanced with multi-example prompting
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements. Focus on factually accurate and complete answers.

    Example 1:
    Problem: What is the name of the university where Ana Figueroa, a political activist and government official, studies and graduates from?
    Solution: University of Chile

    Example 2:
    Problem: Which genus was the ruby-throated bulbul moved to from *Turdus* before finally being classified in the genus *Rubigula*?
    Solution: Genus Pycnonotus

    Example 3:
    Problem: In what year did Etta Cone last visit Europe?
    Solution: 1938

    Problem:
    {problem}
    """

    solution = call_llm(solution_prompt, system_instruction_solver)

    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution - Enhanced with specific validation examples
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem. Ensure factual correctness and completeness.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues, including specific factual errors or omissions]".

        Example 1:
        Problem: What is the capital of France?
        Solution: Paris
        Validation: VALID: The capital of France is indeed Paris.

        Example 2:
        Problem: Who painted the Mona Lisa?
        Solution: Leonardo DaVinci
        Validation: VALID: The Mona Lisa was painted by Leonardo da Vinci.

        Example 3:
        Problem: What year did World War II begin?
        Solution: 1940
        Validation: INVALID: World War II began in 1939, not 1940.

        Problem:
        {problem}

        Proposed Solution:
        {solution}
        """

        validation_result = call_llm(validation_prompt, system_instruction_validator)

        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution

        # If invalid, refine the solution - Provides multi-example based feedback to ensure robust refinement
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed. Ensure that you only use information from the original problem in your response, and ensure that the response is factually correct and as complete as possible.

        Problem:
        {problem}

        Your previous solution:
        {solution}

        Validation feedback:
        {validation_result}

        Example of a corrected solution based on validation feedback:

        Problem: When did the Titanic sink?
        Your previous solution: April 1912
        Validation Feedback: INVALID: The Titanic sank on April 15, 1912, include the day.

        Corrected Solution: April 15, 1912

        Please provide a completely revised solution that addresses all the issues mentioned. Be as factual as possible. Do not attempt to create new information that is not present in the original response.
        """

        solution = call_llm(refined_prompt, system_instruction_solver)

    return solution

def main(question):
    """
    Main function that orchestrates the solution process using solve_with_validation_loop.
    This function now incorporates the iterative validation loop for enhanced accuracy.
    This is a hybrid approach combining elements from Iteration 0 (direct LLM call) with the idea
    of iterative refinement from later iterations.
    """
    answer = solve_with_validation_loop(question)
    return answer
```


            LEARNINGS FROM PREVIOUS ITERATIONS:
            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        ```
# Dataset-Specific Experiment Log: Question Answering

This document serves as a continuously updated log of patterns, strategies, and findings related to the question-answering task for this specific dataset. It prioritizes concrete, task-specific insights over general principles.

## 1. DATASET PATTERNS & CHARACTERISTICS

*   **Question Type Dominance:** The dataset predominantly features *Who* questions, seeking individuals or groups associated with specific events, creations, or awards. Example: "Who created Groove Coaster?". There is also at least one example of a "What" question focusing on a specific part of a person's name. Example: "What was the first name of Ralph E. Oesper?".
*   **Answer Type:** Answers are typically short-form, factual names of people, groups, sometimes dates/numbers, or numerical facts. They represent precise details directly related to the question. Example: "How many losses...?" require a numerical answer.
*   **Knowledge Breadth:** Questions span a wide range of topics, requiring broad domain knowledge. Examples include music production (Groove Coaster), chemistry history (Ralph E. Oesper), oceanography (Jerlov Award), music band creation (Sho?), and TV series details ("El guardián invisible"). Questions also require knowledge of historical events or biographies.
*   **Question Specificity:** Varies from very precise to allowing some interpretation.
*   **Structure and Format:** Questions are natural language sentences. Answers are simple noun phrases or names. Each entry has an ID field (string).
*   **Reasoning Type:** Primarily fact retrieval. Answers are facts needing extraction from a knowledge source.
*   **Entity and Relationship Focus:** Questions often contain specific entities (person, place, organization) and relationships (purchased, announced).
*   **Date Sensitivity:** Correctness is highly sensitive to dates; even slight variations are considered incorrect.
    *   *Example:* Needs to be able to distinguish between "October 20" vs. "21 of October".
    *   Questions often demand precise factual recall, including specific years ("In which year did..."), months ("In which month of 2005...").
*   **Precise Date & Identity Focus:** Many questions require pinpoint accuracy regarding dates (day, month, year) or specific individual identities (e.g., the name of a scientist who received a specific award). Partial or approximate answers are considered incorrect.
*   **Factual Recall, Not Reasoning:** The questions predominantly test factual recall of specific details rather than requiring complex reasoning or inference. The answers are likely directly stated in the (simulated) knowledge source, but finding the exact right snippet is crucial.
*   **Varied Temporal Scope:** The questions span a range of historical periods, meaning that the LLM and retrieval mechanisms need to handle diverse and potentially obscure historical information.
*   **Need for Completeness:** Answers must include all parts requested in the question (e.g., day, month, and year when asked for).
*   **Complex Relationships:** Questions frequently involve identifying relationships between entities across different time periods or classification schemes (e.g., genus changes, visit dates). This requires the system to track and correlate information from potentially disparate sources.
*   **Implicit Assumptions:** Some questions rely on implicit assumptions or background knowledge that the LLM might not possess. For example, understanding the taxonomic hierarchy to follow genus changes.
*   **Comparative Reasoning:** The dataset requires the system to compare and contrast information (e.g., "moved *to* from *Turdus* *before* finally being classified"). This necessitates accurate tracking of changes in classification over time.
*   **Specific Factual Recall:** The questions demand precise factual recall, often involving dates, names, and associations (e.g., "In which year was X awarded Y?").
*   **Entity Recognition & Disambiguation:** Questions involve named entities (people, organizations, awards) that require the LLM to correctly identify and disambiguate them. The failure highlights the importance of accurately mapping entities and their properties.
*   **Complex Relational Queries:** The questions often require understanding the relationship between multiple entities (e.g., person, achievement, organization). This goes beyond simple fact retrieval and requires the LLM to understand and reason about the relationships between different pieces of information.
*   **Compound Information:** Questions often contain multiple pieces of information (e.g., name, title, location), requiring the system to integrate and filter information accurately. *Example:* "In which month of 2005 was the Lal Bahadur Shastri Dam, also known as the Almatti Dam, located on the Krishna River in North Karnataka, India, completed?"
*   **Ambiguity in "First" or "Pioneer" Questions:** Questions asking about "first" or "pioneer" individuals or events may have multiple valid answers or lack a definitive answer, leading to discrepancies between the expected answer and the system's response.
*   **Temporal Specificity:** Many questions require extraction of specific dates or time periods. The questions are often phrased to directly ask "when" something occurred or "during which years" something happened.
*   **Entity-Rich Context:** The questions frequently include detailed contextual information about the entities involved (e.g., "Satyanarayan Gangaram Pitroda (an Indian telecommunication engineer and entrepreneur)"). This suggests a need for the model to filter relevant information based on these contextual clues.
*   **Fact Verification Challenge:** The questions target factual knowledge that may require precise lookup. This highlights the need for the model to differentiate between generally true statements and specific answers to the questions being asked, potentially requiring numerical reasoning.

## 2. EFFECTIVE TASK-SPECIFIC STRATEGIES

*   **NONE:** (Iteration 5 result) No aspect of the current strategy is working effectively for this dataset.
*   **Ineffective:** Direct LLM question answering without knowledge retrieval or verification (Baseline Experiment). Accuracy was only 10%.
*   **Ineffective:** Simple chain-of-thought approach using specialized LLM agents for each step (extraction, query generation, retrieval, answer generation, validation). Accuracy was 0%.
*   **Ineffective:** RAG implementation with current settings and the "explore" strategy is not effective (Iteration 4). RAG architecture with current validation loop (Iteration 5).
*   **Ineffective:** The validation loop alone, even with RAG, is insufficient for this dataset. While the validation loop might filter out some incorrect answers, it does not address the fundamental issue of the LLM's inability to extract specific temporal information when it's not explicitly stated in the retrieved snippets.
*   **Potentially Effective:** The validation loop approach has promise, but it needs to be coupled with more robust information retrieval to be effective for this dataset. The idea of iterative refinement based on validation feedback is sound, but only if the initial information provided to the solver is accurate. However, iterations 2 and 5 revealed that the validation loop is failing to correct retrieval or generation errors, and the validator fails to detect answers that are partially correct, incorrect or missing.
*   **Untested:** Knowledge Base Retrieval (using LLM to formulate queries for external knowledge bases)
*   **Untested:** Hybrid Approach (LLM for query rephrasing and search results informing answer generation)
*   **Untested:** Entity and Relation Extraction before Knowledge Retrieval (though initial experiments highlight the need for *precise* extraction).
*   **Untested:** Structured Query Generation (e.g., SPARQL)
*   **Untested:** Few-Shot Learning, Chain-of-Thought Prompting, Answer Verification Prompting, Specialized "Who" question prompts.

## 3. COMMON FAILURE MODES ON THIS DATASET

*   **Factual Inaccuracy (Hallucination):** LLM provides incorrect facts not supported by evidence. Example: Incorrect details about "Barcelona corners" or the "Belmont purchaser" in the baseline experiment. This indicates the LLM is "hallucinating" facts. In Iteration 2, the LLM incorrectly stated "The Royal Society" instead of "American Academy of Arts and Science." This indicates the LLM's knowledge base or retrieval mechanism is unreliable for this specific information.
*   **Date Discrepancies:** Small differences in dates (e.g., "October 20" vs. "21 of October") are marked as incorrect, showing the need for high date precision.
*   **Incomplete Answers:** LLM fails to provide all parts of the answer requested by the question. Example: Providing only the month and year when the question asks for the day, month, and year.
*   **Ambiguity:** (Hypothesized from initial analysis) Some questions could be ambiguous if taken out of context.
*   **Multiple Valid Answers:** (Hypothesized from initial analysis) Some questions might have multiple correct answers, or answers that vary in specificity.
*   **Name Variations:** (Hypothesized from initial analysis) People's names can be written in different ways (e.g., "Robert" vs. "Bob").
*   **Cultural Differences:** (Hypothesized from initial analysis) Name formats differ across cultures. The dataset might contain names from various regions.
*   **Misspellings:** (Hypothesized from initial analysis) Questions might contain misspellings of names or terms, which could make retrieval difficult.
*   **Incorrect Entity/Relationship Extraction:** The system fails to accurately extract the precise entities and relationships needed to answer the question. For example, the system extracted "American University" which was not present in the gold answer, and was not relevant in Iteration 1. In Iteration 5, the system incorrectly provided "Robert P. Sharp" instead of "Carl Owen Dunbar".
*   **Inability to Discern Temporal Order:** The system struggles to correctly identify the sequence of events or changes over time. This is evident in the ruby-throated bulbul question, where the system failed to establish the order of genus classifications. The last visit to Europe question also shows this.
*   **Inaccurate Information Retrieval & Validation:** The system struggles to validate if information retrieved is in fact valid. This is demonstrated across all samples in Iteration 1, Iteration 3, Iteration 4, and Iteration 5.
*   **Lack of Reliable Source Attribution:** Since the responses aren't grounded in verifiable sources, it is difficult to determine the source of the error. This makes debugging and improving the system challenging.
*   **Incorrect Year Retrieval:** The system incorrectly retrieves the year in several questions, indicating a failure in the information retrieval or context validation steps. *Example:* For the question about Maharaj Kishan Bhan in Iteration 3, the system returned "1999" instead of "2013". This suggests a problem in filtering or prioritizing information within the retrieved context.
*   **Handling Ambiguity in "First" Questions:** The system struggled with questions seeking the "first" of something in Iteration 3. Instead of providing the exact expected answer ("Lydia Canaan"), it offered multiple possibilities or disclaimers about the lack of a single definitive answer. This indicates a failure in resolving ambiguity and selecting the most appropriate answer based on the context.
*   **Context Validation Inadequacy**: In Iteration 3, the LLM struggles to validate the context against the question and extracts the incorrect named entity, date, or other fact from the search results. The system reports "information not present" even when the information might be obtainable, pointing to overly conservative validation or poor snippet content comprehension (Iteration 4). In Iteration 5, the validator fails to detect answers that are partially correct, incorrect or missing.
*   **Script Errors:** Script errors encountered during attempted repairs, indicating fragility of the codebase and the need for more robust error handling. Examples:
    *   `ERROR: Answer not found in context`
    *   `ERROR: Gemini API call failed with AttributeError: 'module' object has no attribute 'GenerativeModel'`
    *   `ERROR: Gemini API key error and search snippets deemed irrelevant.`
*   **Failure to Extract Numerical Answers:** The RAG approach fails when questions require numerical information. The system fails to extract the numerical answer, even when the relevant context may be available within the retrieved snippets. Example: For the question about Adolf Anderssen's losses, the system *might* have retrieved snippets discussing the match but failed to identify and extract the specific number "3". The validation step then incorrectly deemed the snippets unhelpful (Iteration 4).
*   **Granularity Mismatch in Date Retrieval:** (Iteration 5) The system fails when a question demands a precise date (e.g., "November 30, 1949") but the retrieved information provides a broader range (e.g., "1949 to February 21, 1974"). The retrieval process is not filtering with sufficient precision to isolate the exact date.
*   **"Answer Not Found" Errors for Existing Answers:** (Iteration 5) The system incorrectly reports "Answer not found" even when the correct answer exists in the knowledge source. This suggests issues with query formulation, search strategy, or the ability to match question intent with relevant document content.
*   **Query Validation Ineffective:** The `generate_query_and_validate` function isn't ensuring that the generated queries are precise enough to retrieve the exact information needed. This is a critical bottleneck in the current approach. (Iteration 5)
*   **Date Extraction Bottleneck:** The system fails when it cannot find the exact date or time period mentioned in the golden answer within the retrieved snippets. The first error illustrates this clearly where the system returns "Answer not found in snippets," missing the "October 2009" answer. It seems unable to infer or synthesize temporal information.
*   **Insufficient Temporal Reasoning:** The system acknowledges Otto Schluter was a professor but cannot extract the date range (1911-1959). It appears the system lacks the ability to extract start and end dates and perform basic subtraction to derive a duration, it needs the exact answer in the snippets.

## 4. EXPERIMENT LOG & FINDINGS

*   **Iteration 0 (Baseline):**
    *   **Approach:** Direct LLM call with a basic prompt.
    *   **Runtime:** Not explicitly recorded, but assumed to be fast due to direct prompting.
    *   **Accuracy:** 10%.
    *   **Key Findings:** Direct LLM prompting is insufficient. Requires knowledge retrieval and verification.
    *   **Error Analysis:** Factual inaccuracies and date discrepancies were the primary error types.

*   **Iteration 1:**
    *   **Approach:** Chain-of-thought approach using specialized LLM agents for each step (extraction, query generation, retrieval, answer generation, validation).
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** 0%.
    *   **Key Findings:** A simple chain-of-thought approach is not sufficient. Inaccuracies at the extraction stage propagate through the entire pipeline. The LLM struggles with understanding temporal information, tracking relationships across multiple sources, and performing accurate information validation.
    *   **Error Analysis:** Incorrect entity/relationship extraction, inability to discern temporal order, and inaccurate information retrieval and validation.

*   **Iteration 2:**
    *   **Approach:** Validation loop with specialized LLM agents. (Details of the individual components as in Iteration 1 were not re-stated but assumed).
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** Not explicitly recorded, but noted that validation alone was insufficient.
    *   **Key Findings:** The validation loop did not catch the factual inaccuracy in the example. This suggests the validator agent needs to be more rigorous in verifying factual claims, and needs access to reliable information to do so.
    *   **Error Analysis:** Incorrect Factual Recall. Validation alone is insufficient. Need for External Knowledge Injection. Lack of Reliable Source Attribution.

*   **Iteration 3:**
    *   **Approach:** LLM-based information retrieval and answer generation (details of the specific implementation not provided).
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** 0.33%.
    *   **Key Findings:** The current LLM-based information retrieval and answer generation approach, while conceptually sound, does not achieve satisfactory accuracy on this dataset. Contextual Understanding is Not Enough. The approach can be adapted by changing the prompt template, finetuning the LLM, or by giving it a validation step after information retrieval and before answer generation. More data alone may not be the solution. Prompt engineering or other improvements may be more beneficial.
    *   **Error Analysis:** Incorrect Year Retrieval. Handling Ambiguity in "First" Questions. Context Validation Inadequacy.

*   **Iteration 4:**
    *   **Approach:** RAG implementation (details not fully specified, but presumed to involve retrieval of context and generation of answer). "Explore" strategy used.
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** 0.67%.
    *   **Key Findings:** The "explore" strategy with the current RAG implementation is not effective for this dataset. The system struggles with questions requiring numerical information, and the validation step may be overly conservative, discarding useful information. The hypothesis that an LLM can effectively validate search snippets based on few-shot examples is not supported by these results.
    *   **Error Analysis:** Failure to extract numerical answers. Overly conservative snippet validation.

*   **Iteration 5:**
    *   **Approach:** RAG architecture with a validation loop (details not fully specified).
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** 0.00%.
    *   **Key Findings:** The RAG architecture, with its current validation loop, is failing to provide accurate answers for questions requiring precise factual recall from the simulated knowledge source. The validation loop isn't effective at correcting retrieval or generation errors. The `generate_query_and_validate` function isn't ensuring that the generated queries are precise enough to retrieve the exact information needed. This is a critical bottleneck in the current approach.
    *   **Error Analysis:** Granularity Mismatch in Date Retrieval. "Answer Not Found" Errors for Existing Answers. Incorrect Entity Resolution. Query Validation Ineffective. Validation Fails to Catch Inaccuracies.

*   **Iteration 6:**
    *   **Approach:** RAG architecture with validation loop (details not fully specified).
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** Not explicitly recorded, due to identified errors.
    *   **Key Findings:** The validation loop alone, even with RAG, is insufficient for this dataset. While the validation loop might filter out some incorrect answers, it does not address the fundamental issue of the LLM's inability to extract specific temporal information when it's not explicitly stated in the retrieved snippets. The hypothesis that iterative validation and refinement would overcome the limitations of initial answer generation is rejected. The system gets stuck because the initial answer (or retrieved snippets) doesn't contain enough precise information.
    *   **Error Analysis:** Date Extraction Bottleneck. Insufficient Temporal Reasoning.

## 5. NEXT RESEARCH DIRECTIONS

*   **Enhance Temporal Reasoning:** Implement a module specifically designed to extract and reason about dates and time periods. This could involve:
    *   Training a separate model to identify date entities and duration from text.
    *   Using regular expressions or other pattern matching techniques to extract date information from retrieved snippets.
    *   Incorporating a date normalization library to handle different date formats.
*   **Implement Date Inference:** Extend the system to infer dates or date ranges from contextual clues. For example, if a snippet mentions an event happening "in the early 20th century," the system could be trained to identify the potential range of years associated with that phrase.
*   **Fine-tune LLM on Temporal Tasks:** Fine-tune the base LLM on a dataset of question-answer pairs where the answers involve specific dates or time periods. This could improve the LLM's ability to extract and reason about temporal information.
*   **Query Expansion for Temporal Information:** Augment the search query to explicitly request temporal information. For example, instead of just searching for "Otto Schluter professor University of Halle," the query could be expanded to "Otto Schluter professor University of Halle years" or "Otto Schluter University of Halle professorship duration."
*   **Improve Query Formulation for Precision:** Focus on refining the query generation process to create more specific and targeted queries, especially when questions involve dates or named entities. Consider adding constraints to the query generation process to explicitly request the exact date/name.
*   **Enhance Retrieval Granularity:** Implement techniques to improve the granularity of the retrieval process. Explore methods for ranking search snippets based on the precision of date or entity matches. Experiment with chunking strategies that isolate key facts.
*   **Strengthen Entity Resolution:** Integrate entity linking or named entity recognition (NER) techniques to improve the system's ability to identify and disambiguate entities within the questions and the retrieved documents.
*   **Refine Validation Logic:** Re-evaluate the validation criteria and implementation. The validator needs to be more sensitive to partial matches, contradictions, and missing information. Explore using a separate, more robust LLM for validation.
*   **Dataset Augmentation for Negative Examples:** Augment the dataset with negative examples, specifically questions paired with irrelevant or misleading snippets, to train the validation component to better identify incorrect answers.
*   **Improve Search Query Specificity:** Refine the prompt for query generation to emphasize the need for queries that specifically target numerical answers or quantifiable facts. For example, add phrases like "return the number of..." or "how many..." to the query generation prompt.
*   **Enhance Information Extraction:** Implement a more robust information extraction mechanism, potentially using regular expressions or specialized NER models, to identify and extract numerical answers from the validated snippets. Instead of relying solely on the LLM for answer generation, focus on extracting key facts.
*   **Refine Snippet Validation:** Loosen the validation criteria or explore alternative validation strategies. The current approach is likely discarding snippets that contain the necessary information but don't perfectly align with the expected format. Consider adding a scoring mechanism to evaluate snippets based on relevance instead of strict acceptance/rejection. Specifically, explore if retrieving more snippets will help, then re-rank them.
*   **Implement Numerical Reasoning Checks:** After extracting a numerical answer, add a simple check to ensure it makes sense in the context of the question (e.g., is it a reasonable number of losses in a chess match?). This could prevent nonsensical or hallucinated answers.
*   **Implement Knowledge Retrieval:** Integrate a search engine or knowledge base to retrieve supporting information *before* answer generation. This is crucial for addressing factual inaccuracies.
*   **Implement Answer Verification:** Verify the LLM's answer against a reliable external source and correct it if discrepancies are found. This should reduce hallucinations.
*   **Date Normalization:** Standardize date formats in questions and retrieved information to enable accurate comparison.
*   **Prompt Engineering for Completeness:** Revise the prompt to ensure the model provides all parts of the answer or responds with an appropriate error message (e.g., "Unable to determine the day."). Example prompt update: "Question: {question}. Answer: If all requested parts of the answer (day, month, year) are known, provide them all. Otherwise, state 'Insufficient Information'."
*   **Explore Hybrid Approach:** Use LLM for query rephrasing to improve search engine results, and then use those results to generate an answer.
*   **Enhance Entity and Relationship Extraction:** Implement more robust methods for entity and relationship extraction, potentially using named entity recognition (NER) models finetuned on similar datasets. Focus on precise extraction of the specific entities and relationships relevant to the question.
*   **Incorporate Temporal Reasoning:** Add a dedicated temporal reasoning module to track changes over time. This could involve using specialized data structures or algorithms to represent temporal relationships and perform reasoning about event sequences.
*   **Improve Answer Validation:** Implement more rigorous answer validation techniques, such as cross-referencing information from multiple sources and using a separate validation model to assess the answer's correctness. Implement a system that assesses the quality of the extracted entities *before* query generation.
*   **Iterative Refinement with Feedback:** Create a feedback loop where incorrect answers are analyzed to identify the specific errors made by each component in the pipeline. Use this feedback to iteratively refine the system and improve its performance.
*   **Test Structured Query Generation:** Convert the natural language question into a structured query (e.g., SPARQL) to retrieve information from knowledge graphs like Wikidata.
*   **Investigate Few-Shot Learning:** Provide the LLM with examples of question-answer pairs to improve performance.
*   **Evaluate Chain-of-Thought Prompting:** Prompt the LLM to explain its reasoning process step-by-step (though a simple version of this failed).
*   **Develop Answer Verification Prompting:** Use a separate prompt to ask the LLM to verify the answer's accuracy.
*   **Create Specialized "Who" Question Prompts:** Given the dominance of "Who" questions, design highly optimized prompts.
*   **Track Latency:** Measure and log the runtime (latency) of each experiment, to understand the performance impact of different strategies.
*   **Analyze Failure Cases:** Perform detailed analysis of failure cases to identify patterns and refine strategies.
*   **Implement Fact-Checking Mechanism:** Integrate a mechanism to fact-check the LLM's answers against a reliable knowledge base (e.g., Wikipedia, a curated database). The validator agent should use this to verify the LLM's claims.
*   **Implement Source Tracking:** Modify the `call_llm` function to include source tracking. This will allow the system to track where the information comes from and identify potential sources of error.
*   **RAG Implementation**: Explore Retrieval-Augmented Generation (RAG) to provide the LLM with relevant context from external sources during the solution generation phase. This will help to ground the LLM's answers in verifiable evidence. This should be implemented as part of the `call_llm` function.
*   **Improve Context Validation:** Implement a more robust context validation mechanism in the `retrieve_relevant_context` function. This could involve techniques like verifying the presence of key entities (names, dates) and assessing the overall relevance of the retrieved context to the specific question.
*   **Fine-tune Answer Selection Logic:** Refine the answer selection logic in the `generate_answer_with_context` function to prioritize precise factual matches and resolve ambiguity in "first" or "pioneer" questions. Explore techniques like confidence scoring or rule-based filtering to select the most appropriate answer.
*   **Prompt Engineering for Date Retrieval:** Optimize the prompts used for generating search queries and validating context to emphasize the importance of accurate date retrieval. Include explicit instructions to prioritize sources that provide specific dates.
*   **Prompt Engineering for validation:** Give the LLM the ability to "check its work" by validating its initial answer against the original question and the retrieved context. For example, "Does this answer the question completely? Are all the details consistent with the provided context?"
*   **Implement robust error handling and logging** to better understand and recover from script errors.
```
        

            CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        

            EXPLORATION GUIDANCE:
            1. Review the historical approaches, error patterns, and accumulated learnings carefully
            2. Review the FULL CODE of previous scripts to understand what has already been tried
            3. Design a new approach that is DISTINCTLY DIFFERENT from previous attempts. This approach should have a specific NEW HYPOTHESIS or variable you are trying to test. 
            4. CRITICAL: Include EMBEDDED EXAMPLES directly within your LLM prompts
            5. For each key function, show a complete worked example, or include multiple examples, including:
               - Input example that resembles the dataset
               - Step-by-step reasoning through the example
               - Properly formatted output
            6. Apply the insights from the ACCUMULATED LEARNINGS section to avoid repeating past mistakes
            7. Pay SPECIAL ATTENTION to the weaknesses and improvement suggestions from the capability assessment
            8. Consider implementing one or more of these LLM usage patterns:
               - Repeated validation with feedback loops
               - Multi-perspective analysis with synthesis
               - Dynamic input-dependent routing with an orchestrator
               - Hybrid approaches combining LLM with deterministic functions
               - Best-of-n solution generation and selection
               - ReAct pattern for interactive reasoning and action
               - If it is unknown how successful a processing state or part of the pipeline is, include verification steps to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
               - Answer checkers to validate the final answer against the problem statement. If the answer is incorrect, the checker can send the answer back to an earlier part of the system for for refinement with feedback

            Here's how to call the Gemini API. Use this example without modification and don't invent configuration options:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

            Since this is an EXPLORATION phase:
            - Try a fundamentally different approach to reasoning about the problem. Test a NEW HYPOTHESIS or variable, and add verification steps to deduce if this new change is helpful.
            - THIS IS KEY: Break down the problem into new, distinct reasoning steps based on past performance before you start coding
            - For EACH key LLM prompt, include a relevant example with:
              * Sample input similar to the dataset
              * Expected reasoning steps
              * Desired output format
            - Apply a verifier call to different parts of the pipeline in order to understand what parts of the pipeline of calls is successful and where the system is breaking
            - Pay special attention to addressing the primary issues from previous iterations
            - Ensure your new approach addresses the weaknesses identified in the capability assessment

            CRITICAL REQUIREMENTS:
            1. The script MUST properly handle all string literals - be extremely careful with quotes and triple quotes
            2. The script MUST NOT exceed 150 lines of code to prevent truncation
            3. Include detailed comments explaining your reasoning approach
            4. EVERY SINGLE LLM PROMPT must include at least one embedded example showing:
               - Sample input with reasoning
               - Desired output format
            5. Make proper use of error handling
            6. Implement robust capabilities to address the specific weaknesses identified in the capability assessment
            7. Do NOT use json.loads() in the LLM calls to process input data. JSON formatting is good to use to structure information as inputs and outputs, but attempting to have functions process JSON data explicitly with strict built-in functionality is error prone due to formatting issues and additional text that appears as documentation, reasoning, or comments. When passing data into another LLM call, you can read it as plain text rather than trying to load it in strict json format, is the better approach.

            Return a COMPLETE, RUNNABLE Python script that:
            1. Has a main function that takes a question string as input and returns the answer string
            2. Makes multiple LLM calls for different reasoning steps
            3. Has proper error handling for API calls
            4. Includes embedded examples in EVERY LLM prompt
            5. Is COMPLETE - no missing code, no "..." placeholders
            6. Closes all string literals properly

            This should be FUNDAMENTALLY DIFFERENT from all previous approaches. Do not reuse the same overall structure.

            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            