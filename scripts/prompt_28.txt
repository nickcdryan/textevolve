
            You are improving a Python script that solves problems from a dataset.
            Your goal is to REFINE and ENHANCE the best performing approaches by combining their strengths and addressing specific weaknesses identified in error analysis.

            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "PASSAGE: There were many factors that made the Bills underdogs before Week 4- an 0-3 start coming off of a demoralizing loss to the New England Patriots, a starting quarterback in Trent Edwards who was starting his very first NFL game, numerous defensive injuries of starters, and an opponent in the New York Jets that had just gotten its first win over the Miami Dolphins. Things started off well for the Bills as Edwards led the offense down into Jets territory, but a Robert Royal fumble caused by Jonathan Vilma ended a promising drive. The Jets could not get any sort of momentum from the turnover, and were forced to punt. In the second quarter, things heated up as the Bills once again managed to gain several first downs, only to see the Jets force another turnover as a deep ball by Edwards meant for Lee Evans was intercepted by Andre Dyson. Three events late in the first half led to a flurry of big plays- Bills safety Donte Whitner tackled Jets RB Leon Washington inbounds to keep the clock running, quarterback Chad Pennington faked a spike and completed a pass to put the Jets in field goal range with a couple of seconds left, and Jets kicker Mike Nugent clanged the ball off the right upright, leaving the game scoreless at the half. The second half saw the Bills take the lead on a 10-yard run by Marshawn Lynch, his second of the year. It was all set up by Edwards going 4 for 4 on the drive. The Jets tied it at 7 on an inspired drive by Pennington, who found Laveranues Coles in the end zone for a touchdown. The fourth quarter saw the Bills take advantage of the Jets when reserve defensive back Jabari Greer intercepted Pennington. The Bills cashed in as Edwards found reserve tight end Michael Gaines on a bootleg 4th down play at the Jets 1-yard line, and once again the Bills got more points when kicker Rian Lindell kicked a 46-yard field goal to make it 17-7. The Jets came right back with points of their own as Leon Washington rambled in to cut the lead to 17-14. After a Buffalo punt with less than 2 minutes remaining, the Jets failed to score as a forced pass by Pennington with 6 seconds left ended up in the acrobatic arms of Terrence McGee, preserving the 17-14 victory. The Bills took over second place in the AFC East at 1-3 because of the win, while the Jets fell to 1-3.\n\nQUESTION: How many wins did the Jets have after this game?",
    "answer": "1"
  },
  {
    "id": 1,
    "question": "PASSAGE: The following year, the feud between the nobility and plundering by robber knights continued. In 1385, the castle of Ricklingen, where the Mandelslohs resided, was besieged. When a catapult threw a heavy rock at the troops of Duke Albert, he was hit, and died on 28 June 1385. In the wake of his death, Elector Wenceslas appointed Bernard, his brother-in-law, as co-regent involved him in the government. But his younger brother Henry did not agree with this ruling, and after vain attempts to reach an agreement, the fight flared up again in the spring of 1388. Elector Wenceslas had to assemble an army without the help of Bernard, supported by the town of L\u00fcneburg. From Winsen an der Aller, he wanted to attack Celle, which was held by Henry and his mother. During the preparations, however, Elector Wenceslas fell seriously ill and died shortly thereafter. According to legend, he was poisoned. L\u00fcneburg continued the preparations, formed an alliance with the Bishop of Minden and Count of Schaumburg and set up his own army. On 28 May 1388, battle was joined at Winsen an der Aller; it ended in victory for Henry. According to the provisions of the Treaty of Hanover from the year 1373, after the death of Wensceslas, the Principality passed to the House of Welf. In 1389, a inheritance agreement between the Welfs and the Ascanians was concluded, the treaty of 1374 was abolished, and the Principality was finally secured for the Welfs.\n\nQUESTION: How many of the nobility did L\u00fcneburg ally with?",
    "answer": "2"
  },
  {
    "id": 2,
    "question": "PASSAGE: In their first home game in nearly a month, the Patriots faced the Broncos, winners of 16 of the last 19 games between the two teams, on Monday Night Football; earlier that day, the Patriots placed Maroney on injured reserve with a shoulder injury after he had missed two of the team's last three games, and activated offensive lineman Stephen Neal from the physically unable to perform list. After winning the coin toss, the Patriots deferred their pick to the start of the second half; the Broncos elected to receive. On the opening series, Broncos quarterback Jay Cutler injured his finger on the first play, but led the Broncos into Patriots territory, where running back Andre Hall, who replaced injured starter Michael Pittman two plays prior, fumbled at the Patriots' 30-yard line; Patriots defensive lineman Le Kevin Smith's recovery gave the Patriots the ball at their own 32-yard line to begin their first drive. A 29-yard reception by Watson brought the Patriots into the Broncos' red zone, where the drive ended and a 31-yard Gostkowski field goal gave the Patriots a 3-0 lead. On the fourth play of the Broncos' ensuing drive, Hall fumbled again; this time, a scramble for the ball pushed it 25 yards in the Patriots' favor to the Broncos' 37-yard line, where Mayo recovered it. A Broncos unnecessary roughness penalty on the play gave the ball to the Patriots at the Broncos' 22-yard line, but after failing to make a first down, a 40-yard Gostkowski field goal extended the Patriots' lead to 6-0. An exchange of three punts brought the game into the second quarter, where Morris ran for 34 yards on the second play of a Patriots drive to put them in the Broncos' red zone yet again. Four plays later, on 4th-and-1 from the Broncos' 4-yard line, Morris ran for a 4-yard touchdown that gave the Patriots a 13-0 lead. The Broncos' next drive ended with Meriweather intercepting a deep throw by Cutler, setting up the Patriots' next series at their own 16-yard line. After a sack of Cassel put the Patriots even deeper into their own territory, a facemask penalty on Cassel on third down by Broncos linebacker Jamie Winborn gave the Patriots a new set of downs. Five plays later, on another 4th-and-1, Morris ran for 29 yards to give the Patriots a first down from the Broncos' 13-yard line; he ran for a total of 138 yards on 16 carries. On the next play, Moss caught a 13-yard touchdown pass from Cassel to extend the Patriots' lead to 20-0. The Broncos' next drive began with backup Patrick Ramsey replacing Cutler at quarterback. On the fourth play, defensive lineman Mike Wright sacked Ramsey and forced a fumble, which undrafted free agent linebacker Gary Guyton caught to give the Patriots the ball at midfield. Cassel suffered his fifth sack on the next play, the final of the first half. The Patriots chose to receive the ball to open the second half. Another undrafted rookie, running back BenJarvus Green-Ellis, elevated from the Patriots' practice squad before the Chargers game, replaced Morris, who left the game with a knee injury. A Patriots punt was followed by a Broncos three-and-out; Welker returned the punt 44 yards to give the Patriots the ball at the Broncos' 28-yard line. After two Green-Ellis carries gained one yard, Cassel used a screen play to throw a 27-yard touchdown pass to Moss to extend the Patriots' lead to 27-0. Three completed passes and an unnecessary roughness penalty on Patriots defensive lineman Vince Wilfork moved the Broncos' next drive into Patriots' territory before the Broncos turned the ball over for the fifth and final time, with safety James Sanders intercepting a Cutler pass; a second unnecessary roughness call, originally on Wilfork but later changed to be on Sanders, gave the Patriots the ball at their own 20-yard line to begin the ensuing drive. Three plays after a 27-yard Welker catch put the Patriots in Broncos territory, Broncos safety Calvin Lowry appeared to have intercepted an errant Cassel throw at the Broncos' 1-yard line, but, when the Patriots challenged, the pass was deemed incomplete, as the ball touched the ground before Lowry had possession. Two plays later, Welker caught his first touchdown pass of the season, this one from 6 yards out, to give the Patriots a 34-0 lead. Four plays later, former Patriots tight end Daniel Graham caught a 10-yard touchdown pass from Cutler to cut the Patriots' lead to 34-7. The Patriots' next drive got as far as the Broncos' 33-yard line but eventually ended in a punt. The Broncos, facing 3rd-and-21 on their next drive, had a 21-yard completion by Cutler pushed back by a Broncos holding penalty; Richard Seymour sacked Cutler on the next play, forcing the Broncos to punt on 4th-and-23 with 6:35 left in the game. On the ensuing Patriots drive, two 10-yard runs by Green-Ellis and 16- and 15-yard runs by Faulk put the Patriots in the red zone, where Green-Ellis earned his first NFL touchdown, running in untouched from the 1-yard line, to score the final points of the game. Another Broncos punt gave the Patriots the ball to end the game with a 41-7 victory that took the Patriots' record to 4-2. The Patriots accumulated 257 rushing yards, their largest total since the 1985 season. Cassel, who completed 18 of 24 passes for 185 yards, three touchdowns, and no interceptions, finished with a passer rating of 136.3, one of the ten highest single-game ratings in Patriots history, and was named AFC Offensive Player of the Week for his performance. On the final play of the third quarter, as the Broncos were driving into Patriots territory, Harrison was injured while attempting to tackle Cutler. Harrison, whose 2005 and 2006 seasons were cut short by knee injuries that caused ligament damage, suffered a torn quadriceps muscle in his right leg. Harrison was carted off the field and placed on injured reserve two days later. Safety Antwain Spann was then activated from the practice squad. The Broncos also lost players to injury: Ramsey and linebacker Boss Bailey were also placed on injured reserve, while cornerback Champ Bailey was sidelined for several weeks by a torn groin muscle.\n\nQUESTION: How many points were scored in the first quarter?",
    "answer": "6"
  }
]

            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 28
        - Current explore/exploit balance: 15/85
        - Best accuracy achieved: 1.00 (iteration 1)

        APPROACH HISTORY (last 10 iterations):
        [
  {
    "iteration": 18,
    "strategy": "Exploitation",
    "accuracy": 0.9,
    "approach": "The script uses a multi-stage LLM approach to answer questions, incorporating chain-of-thought reasoning and verification. The problem is decomposed into question analysis, passage extraction, answer generation, and answer verification, each handled by a distinct LLM prompt. Each stage utilizes the `call_llm` function with specific system instructions to guide the LLM's role as an expert in that stage. The workflow begins with `main` calling `analyze_question`, which then calls `extract_relevant_passage`, followed by `generate_answer` and finally `verify_answer`, with each function passing its output to the next, and error handling at each step."
  },
  {
    "iteration": 19,
    "strategy": "Exploitation",
    "accuracy": 0.7,
    "approach": "The script uses a multi-stage LLM approach to answer questions by analyzing the question, extracting a relevant passage, generating an initial answer, and then verifying it. It decomposes the problem into question analysis, passage extraction, answer generation, and answer verification, assigning an \"expert\" role to the LLM in each stage with chain-of-thought prompting. The functions `analyze_question` extracts question type and keywords, `extract_relevant_passage` finds the passage containing the answer, `generate_answer` creates an answer and `verify_answer` confirms the answer using the passage; each of these functions calls `call_llm` to interact with the Gemini LLM. The overall workflow involves sequentially calling these functions, passing the output of one as input to the next, to refine and verify the final answer."
  },
  {
    "iteration": 20,
    "strategy": "Exploitation",
    "accuracy": 0.9,
    "approach": "The script uses a multi-stage LLM approach to answer questions by analyzing the question, extracting relevant information, generating an initial answer, and then verifying the answer, potentially involving calculations. The problem is decomposed into question analysis, passage extraction, answer generation, and answer verification. The LLM acts as an expert in each stage, guided by specific system instructions and few-shot examples.\n\nKey functions include: `main` (orchestrates the workflow), `analyze_question` (identifies question type and keywords), `extract_relevant_passage` (extracts relevant text), `generate_answer` (generates the initial answer), `verify_answer` (verifies and potentially calculates the answer), and `call_llm` (interacts with the LLM). The `main` function calls the other functions in sequence, passing the results from one to the next to refine the answer. The workflow starts with question analysis, uses that to extract the relevant passage, and then utilizes both of those to generate and verify the final answer."
  },
  {
    "iteration": 21,
    "strategy": "Exploitation",
    "accuracy": 1.0,
    "approach": "The script implements a multi-stage question answering system using the Gemini LLM. It decomposes the problem into analyzing the question, extracting relevant information, generating an initial answer, and then verifying the answer, performing calculations if required. The agent roles are defined in the system instructions of each function, acting as experts in question analysis, passage extraction, answer generation and answer verification. The `main` function orchestrates the process, calling `analyze_question`, `extract_relevant_passage`, `generate_answer`, and `verify_answer` sequentially, using `call_llm` to interact with the Gemini model, with each function building on the result of the previous one."
  },
  {
    "iteration": 22,
    "strategy": "Exploitation",
    "accuracy": 0.8,
    "approach": "The script uses a multi-stage LLM approach with techniques like question analysis and answer verification to solve a question. The problem is decomposed into identifying question type, extracting a relevant passage, generating an answer, and verifying the answer, incorporating numerical reasoning where necessary. The agent roles include an expert in question analysis, passage extraction, answer generation, and answer verification.\n\nThe function `main` orchestrates the process. `analyze_question` identifies the question type and keywords. `extract_relevant_passage` retrieves relevant information. `generate_answer` formulates an initial answer. `verify_answer` checks the answer for correctness, performing calculations if needed. `call_llm` is used to interact with the Gemini model and execute the prompts. The overall workflow involves sequentially calling these functions, passing the output of one as input to the next, to arrive at a verified answer."
  },
  {
    "iteration": 23,
    "strategy": "Exploitation",
    "accuracy": 0.9,
    "approach": "The script uses a multi-stage LLM approach to answer questions by analyzing the question, extracting relevant information, generating an initial answer, and then verifying it. It decomposes the problem into four main steps: question analysis, passage extraction, answer generation, and answer verification. The agent roles are defined implicitly through system instructions, such as \"expert at analyzing questions\" or \"expert at verifying answers,\" guiding the LLM's behavior in each function. The script uses `analyze_question` to determine question type and keywords, `extract_relevant_passage` to find the most relevant text, `generate_answer` to produce an answer, and `verify_answer` to validate the answer, with `call_llm` serving as the underlying function to interact with the Gemini LLM for each step. The overall workflow involves sequentially calling these functions, passing the output of one as input to the next, to arrive at a verified answer."
  },
  {
    "iteration": 24,
    "strategy": "Exploitation",
    "accuracy": 1.0,
    "approach": "The script employs a multi-stage LLM approach, decomposing the problem into question analysis, relevant passage extraction, answer generation, and answer verification, using chain-of-thought prompting with examples. It uses a single agent role, with the LLM acting as an expert in question analysis, passage extraction, answer generation, and verification. The functions `analyze_question`, `extract_relevant_passage`, `generate_answer`, and `verify_answer` sequentially refine the response using the LLM, with `call_llm` acting as the interface to the Gemini model. The overall workflow involves analyzing the question, extracting a relevant passage, generating an initial answer, and then verifying/correcting that answer, with error handling at each stage."
  },
  {
    "iteration": 25,
    "strategy": "Exploitation",
    "accuracy": 0.8,
    "approach": "The script uses a multi-stage chain-of-thought approach with verification to answer questions by decomposing the problem into question analysis, relevant passage extraction, answer generation, and answer verification. Each stage employs an LLM with a specific role (e.g., question analyzer, passage extractor, answer generator, and answer verifier) and few-shot examples in the prompt to guide the LLM. `main()` orchestrates the workflow, calling `analyze_question()` to determine the question type and keywords, `extract_relevant_passage()` to find relevant information, `generate_answer()` to form an initial answer, and `verify_answer()` to confirm the answer's correctness. All LLM interactions are handled by the `call_llm()` function."
  },
  {
    "iteration": 26,
    "strategy": "Exploitation",
    "accuracy": 0.8,
    "approach": "The script uses a multi-stage LLM approach to answer questions, incorporating question analysis, information extraction, answer generation, and verification. The problem is decomposed into these four distinct steps, each handled by a specialized LLM agent. The `analyze_question` function identifies the question type and keywords, `extract_relevant_passage` retrieves the necessary information, `generate_answer` produces the answer, and `verify_answer` checks its correctness, using numerical checks when needed. The `call_llm` function is used in each step to interface with the Gemini model by packaging the prompt and system instruction. The `main` function orchestrates these steps sequentially, handling potential errors along the way."
  },
  {
    "iteration": 27,
    "strategy": "Exploitation",
    "accuracy": 1.0,
    "approach": "The script employs a multi-stage LLM approach to answer questions by first analyzing the question type and keywords, then extracting a relevant passage, generating an initial answer, and finally verifying the answer. It decomposes the problem into distinct steps handled by specialized LLM agents (question analyzer, passage extractor, answer generator, and answer verifier), each defined by specific system instructions. The functions `analyze_question`, `extract_relevant_passage`, `generate_answer`, and `verify_answer` orchestrate these steps, using `call_llm` to interact with the Gemini API for each stage, and `main` to define the overall workflow."
  }
]

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 3 iterations):
        [
  {
    "iteration": 18,
    "issue": "The most critical problem is the system's failure in basic inferential reasoning and information synthesis. It cannot perform simple calculations like adding percentages to derive a total."
  },
  {
    "iteration": 19,
    "issue": "The primary issue is the system's limited reasoning capability. It can extract information but cannot reliably process or manipulate it to answer questions that require calculations or inferences."
  },
  {
    "iteration": 20,
    "issue": "The primary issue is **misinterpreting the question's expected output format or level of aggregation required.** The system correctly extracts individual numerical facts but fails to combine or present them in the way the question demands, leading to mismatches with the golden answer."
  },
  {
    "iteration": 21,
    "issue": "The most critical problem is the **shallow reasoning and limited contextual understanding**. The system appears to rely heavily on direct information extraction and lacks the capacity for complex reasoning and inference necessary for more challenging questions."
  },
  {
    "iteration": 22,
    "issue": "The primary issue is **deficient semantic understanding** and a consequential **failure in information extraction**. The system struggles to correctly identify the target of the question within the context of the provided passage."
  },
  {
    "iteration": 23,
    "issue": "The most critical problem is the system's **inability to perform the last step of reasoning** after extracting relevant information, leading to missed or incomplete answers."
  },
  {
    "iteration": 24,
    "issue": "Without error cases, it is impossible to pinpoint the primary issue."
  },
  {
    "iteration": 25,
    "issue": "The primary issue is the system's inability to perform high-level reasoning and inference and deduce implied information needed to answer questions accurately, leading to misinterpretations of question and passage semantics and incorrect numerical reasoning."
  },
  {
    "iteration": 26,
    "issue": "The primary issue is the **inaccurate interpretation of questions**, particularly those involving negation or subtle contextual constraints. This leads to the system performing calculations or extractions based on a flawed understanding, resulting in incorrect answers."
  },
  {
    "iteration": 27,
    "issue": "Without error cases, it is difficult to determine the primary issue. Assuming that the goal is to solve increasingly complex questions, the primary issue is the system's limited ability to perform complex reasoning over the provided text."
  }
]

        TARGETED IMPROVEMENTS:
        [
  "Implement Rule-Based Deduction:** Implement rule-based deduction mechanisms to help the system infer implicit information. For example, rules could be defined to deduce the absence of an event when there is no mention of it in a context where it would be expected.",
  "Add Intermediate Reasoning Steps:** Add intermediate reasoning steps that explicitly show the system's thought process. This will help in debugging and identifying where the reasoning goes wrong. For example, add a step to explicitly identify all the events/scores/occurrences mentioned in the passage and then relate them to the question.",
  "Introduce Simple Rule-Based Inference:** Implement a set of simple rules to infer new facts from the extracted information. For example, if the question asks for a difference, the system should identify the relevant numerical values and subtract them.",
  "Enhance Keyword and Constraint Extraction:** Improve the system's ability to identify keywords and constraints within the question. For example, explicitly mark keywords and their relationships to the entities within the passage.",
  "Implement a Question Type Classifier:** Train a classifier to categorize questions based on their intent (e.g., negation, comparison, calculation, fact retrieval). This would allow the system to apply different reasoning strategies based on the question type.",
  "Add Assertions and Checks:** When performing numerical reasoning, add assertions and checks to ensure that the intermediate results are within expected ranges and consistent with the information in the passage.",
  "Introduce a \"Sense-Making\" Step:** Before performing calculations or extractions, introduce a \"sense-making\" step where the system explicitly reasons about the question's intent and the relevant information in the passage. This could involve generating a summary of the question's requirements and the relevant context from the passage.",
  "Introduce more test cases with varied sentence structures:** This will help to ensure that the system is able to understand the question and the context of the passage.",
  "Add Print Statements:** Add print statements during execution to show what the system is doing at different steps, and add the current values of the relevant variables that will allow debugging later.",
  "Fine-tune Language Model:** Fine-tune the underlying language model on a dataset specifically designed to test semantic understanding and deductive reasoning. This dataset should include examples where the answer requires combining information from different parts of the passage or making logical inferences."
]
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity ✓
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity ✓
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity ✓
    4. All operations have O(1) average time complexity ✓
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        

            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        Okay, I will synthesize the existing knowledge with the new learnings from Iteration 27, ensuring that all key insights are preserved while staying within the token limit. I will prioritize dataset-specific information and condense where possible.

```
# DATASET-SPECIFIC LEARNINGS & EXPERIMENT LOG

This document serves as a running log of our learnings, experiments, and findings specific to the question-answering task on this particular dataset of sports narratives and demographic data.

## 1. DATASET PATTERNS & CHARACTERISTICS

*   **Passage Dependency:** Questions heavily reliant on the provided passage; external knowledge is rarely needed.
*   **Question Structure:** Questions consistently formatted as `PASSAGE:\n... \n\nQUESTION: ...`.
*   **Passage Content Variability:** Passages vary in content (sports - especially American Football, demographics, geography, nickel production, history, census data), length, and contain extraneous information. Requires differentiating between narrative descriptions and statistical data. Passages are generally self-contained snippets of text.
*   **Mixed Answer Types:** The dataset contains questions that require different answer formats, including numerical values, proper nouns, and locations.
*   **Varied Question Complexity:** Questions range from simple fact retrieval to complex comparisons and calculations.
*   **Dominant Topics:** Significant portion of questions related to sports (American Football specifically) and historical events, census and demographics. Questions often focus on scores and timing of events in sports games.
*   **Sports Game Summaries:** A significant portion of the passages summarize sports games (likely American football), but also other content requiring adaptability from the model.
*   **Specific Factual Questions:** Many questions ask for specific factual details directly stated in the passage, such as player names, yardage, or point totals.
*   **Answer Types:** Typically concise and factual, directly extracted or derived from the passage, including proper nouns (player names), numerical values (counts, yardage, rates, percentages), and dates.
*   **Question Types:**
    *   **Fact Extraction:** Directly retrieving information from the passage (e.g., "Who caught the final touchdown of the game?", "How many receiving yards...").
    *   **Counting:** Determining the number of occurrences of an event or entity (e.g., "How many running backs ran for a touchdown?"). Requires constraint awareness and distinguishing between similar events (e.g., rushing touchdowns vs. all touchdowns).
    *   **Calculation:** Performing arithmetic operations on values extracted from the passage (e.g., "How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?"). Requires unit awareness. "How many more housing units are there than households?". Often requires calculating total points from touchdowns.
    *   **Comparison/Difference:** Comparing values or entities described in the passage (e.g., "How many more TFR did Malays have in 2004 compared to Ethnic Chinese TFR in 2004?", "How many yards longer..."). Requires numerical reasoning.
    *   **Ordinality:** Questions may include ordinal terms like "second longest", requiring ranking and comparison.
    *   **Multiple Instances:** Question has multiple valid answers in the passage (e.g., "List all the countries...", "What touchdowns did Marc Bulger make?").
    *   **Score Tracking:** Questions that require tracking and aggregating scores based on individual plays or events described in the passage. Requires addition/subtraction of point values. "How many total points were scored?". Explicit Scoring Rule Definition is important.
    *   **Implicit Numerical Reasoning:** Questions require *implicit* numerical reasoning or inference.
    *   **Temporal Reasoning:** Questions often involve temporal aspects, requiring understanding the sequence of events (e.g., "final touchdown," "first field goal"). Passages are often chronological. Involves calculating the duration between two events (e.g., "Was Peck in more films in the 1950s or 1960s?").
    *   **Complex Date/Event Identification:** Correctly pinpointing the exact start and end dates of events can be challenging due to varied phrasing and implied timelines.
    *   **Holistic Understanding for Final Score Calculation:** Some questions require a holistic understanding of the passage to determine the final score.
    *   **Numerical Reasoning and Aggregation:** Many questions require aggregating numerical information presented in the passage.
    *   **Percentage Interpretation:** A significant subset of questions involves understanding and manipulating percentages.
    *   **Implicit Information Retrieval:** Some questions require synthesizing information from multiple parts of the passage or performing logical inference.
    *   **Negation:** Questions involving negation (e.g., "How many percent are *not* non-families?") require understanding and reversing the direct interpretation of information.
*   **Quantity-based Questions Dominate:** A significant portion of questions revolve around extracting numerical quantities and performing simple arithmetic. Example: "How many total passing touchdown yards did Dalton have?".
*   **Unit Sensitivity:** The presence of units of measurement (e.g., "yards," "percent," "months") is crucial for answer correctness. Unit Omission is a common failure mode. Questions often *imply* units (e.g., "How many months..." implies months).
*   **Precision Requirements:** Questions often require precise numerical answers. Precision Mismatch can occur.
*   **Synonyms and Paraphrasing:** The question might use different wording than the passage to refer to the same entity or event. Test robustness against paraphrasing.
*   **Multiple Occurrences:** An event (e.g., a touchdown) might occur multiple times; the question could be specific about which occurrence to consider (e.g., "first," "last," "second longest").
*   **Ambiguity in Expected Answer Granularity:** The "correctness" of an answer can be subjective. Example: providing "12 Dutch and 10 English" when the answer is expected to be "22".
*   **Explicit Information Assumption:** The questions assume the information needed to answer them is explicitly stated in the passage. Emphasis on direct information retrieval (named entities, numbers).
*   **Numerical Reasoning & Misdirection:** Questions frequently require numerical reasoning, often involving addition, subtraction, or comparison of numbers extracted from the provided text. The dataset also includes "trick" questions that introduce irrelevant entities.
*   **Complex Sentence Structure:** The passages often feature complex sentence structures.
*   **Entity Recognition and Coreference:** Questions frequently hinge on identifying specific entities (people, teams, locations) and resolving coreferences (pronouns referring back to those entities).
*   **Topic Variation:** Passages vary in topic (statistical data, filmographies, sports summaries), requiring adaptability from the model. Example: "How did Zhang enhance the economy?".
*   **Constraint-Based Questions:** Questions often include constraints on the type of events to count (e.g., "rushing touchdowns" vs. all touchdowns). Requires differentiating between general event descriptions and countable instances.
*   **Numerical Extraction and Aggregation Emphasis:** Many questions require extracting multiple numerical facts from the passage and performing a calculation (addition, subtraction, or sometimes implicit aggregation) to arrive at the final answer.
*   **Census-Style Data Passages:** A subset of the dataset includes census-style passages containing population statistics (number of people, households, families).
*   **Event-Based Questions with Dates:** Many questions are event-based, including dates. They require extracting numerical information tied to these events and then performing time-based calculations (e.g., years between events).
*   **Pronoun Resolution:** Questions may involve pronouns.
*   **Multi-Event Summarization:** Some questions require summarizing information across multiple events or entities within the passage (e.g. longest run for a touchdown may require looking across multiple runs).
*   **Complex Reasoning Required:** Questions often require more than direct information retrieval: aggregation (e.g., "How many wins did the Jets have"), comparison, or arithmetic.

## 2. EFFECTIVE TASK-SPECIFIC STRATEGIES

*   **Multi-Stage Approach:** A multi-stage approach (question analysis, passage retrieval/extraction, answer generation, and verification) shows promise. Isolates errors and allows for targeted improvements. High accuracy (90% in Iteration 23) suggests breaking down the problem is effective. The current multi-stage chain-of-thought approach serves as a reasonable baseline.
*   **Chain-of-Thought Prompting (LLM-focused):** Encouraging the LLM to explicitly show its reasoning steps can improve accuracy and allow for debugging.
*   **Example-Driven Reasoning (LLM-focused):** Providing the LLM with a few examples of questions and answers to guide its reasoning.
*   **Few-Shot Learning (LLM-focused):** Providing the LLM with a few examples of questions and answers to guide its reasoning. Few-shot examples in prompts can guide the LLMs to extract information, manipulate data, and derive answers.
*   **Answer Validation Prompt (LLM-focused):** Using a separate prompt to validate the generated answer, justifying its answer based on the passage. Important for questions that require calculations. `verify_answer` is an important step.
*   **Constrained Decoding (LLM-focused):** If the answer is known to be a number, use constrained decoding.
*   **Explicit Scoring Rule Definition:** Explicitly define the scoring rules for each type of play in prompt engineering.
*   **Leveraging Expert Roles:** When guided with specific system instructions ("expert roles"), the LLM can effectively perform question analysis, passage extraction, answer generation, and verification. Using LLMs with specific roles (question analyzer, passage extractor, etc.) helps to modularize the reasoning process.
*   **Keyword-Based Passage Extraction:** Using keywords from the question to extract relevant passages works well for narrowing down the information scope.
*   **Focused Passage Extraction:** Extracting only the most relevant portions of the passage helps to reduce noise and computational load.
*   **Error Handling:** Explicitly incorporating error handling within the code ensures that the system gracefully manages unexpected situations.
*   **Numerical Reasoning:** Numerical reasoning is crucial for questions that require the understanding of proportions or percentages, and extraction of numerical answers from a complex context.
*   **Expert Agent Role:** Using a single agent that is an expert in all stages could be beneficial.
*   **Overall Architecture Sound:** The architecture of question analysis, extraction, generation, and verification offers potential.
*   **Specialized LLM Agents:** Specialized agents with specific system instructions helps focus the LLM's reasoning at each stage.

## 3. COMMON FAILURE MODES ON THIS DATASET

*   **Numerical Reasoning Errors:** The system struggles with questions that require inference, comparison beyond single sentence lookup, or multi-step calculations. The system can extract numbers, but fails to reason about them. The LLM incorrectly calculates differences, sums, or percentages. Root cause is arithmetic/logical errors in deriving the answer. Example: Calculating 51.0% - 47.2% incorrectly.
*   **Arithmetic Errors:** The system consistently fails when arithmetic operations are required to derive the correct answer (addition, subtraction, etc.). "How many more housing units are there than households?". LLM correctly identifies the touchdown but fails to calculate the total points for the fourth quarter, even when knowing the point value of a touchdown.
*   **Inference and Synthesis:** The system struggles when the answer isn't directly stated but requires inference or synthesis of information from different parts of the passage. For example, calculating the total points scored in a game ("How many total points were scored?") requires summing the scores mentioned for each team.
*   **Incorrect Answer Type Handling:** The system frequently fails to identify the correct answer type. For example, instead of providing a numerical answer to a "How many..." question, it returns a verification status ("Correct").
*   **Over-reliance on Verification:** The system seems to prematurely stop at verification, without extracting the specific answer from the extracted passage. This leads to responses like "True" or "Correct" when a more detailed answer is required.
*   **Passage Understanding Deficiencies:** While the passages contain the answer, the system struggles to extract the correct information, even when it correctly identifies the relevant passage.
*   **Incomplete Numerical Extraction:** The system struggles to extract *all* relevant numbers required for calculations.
*   **Difficulty in Combining Numerical Data:** The system fails to correlate and aggregate information across the passage (e.g., "How many touchdown runs were made for the same yardage?").
*   **Missing Implicit Information:** Errors occur when the model fails to recognize and use implicit information.
*   **Confusion Between Implied and Explicit Information:** Difficulty with deriving at the right answer when implied by the text. *Example*: LLM assumes the Giants scored 0 points in the third quarter, when in reality they didn't.
*   **Incorrect Date Identification:** The system struggles to consistently identify the correct start and end dates for calculating time differences.
*   **Inaccurate Handling of Ordinality:** Inability to correctly answer questions involving ordinality (e.g., "second longest").
*   **Precision Mismatch:** Providing a range when a precise answer is expected is a frequent failure mode.
*   **Failure to Extract All Relevant Instances:** The system sometimes fails to identify all instances of events related to the question.
*   **Inability to Filter Extraneous Information:** Difficulty in filtering out irrelevant details from the passage.
*   **Missing Numerical Aggregation:** The system fails when questions require combining multiple numbers from the passage into a single answer. Example: "How many Dutch and English warships were blocking Spanish support?" expects the sum of 12 Dutch and 10 English warships (22), but the system returns "12 Dutch and 10 English".
*   **Lack of Calculation Awareness and Execution:** The system is not reliably detecting the need for any calculation. Example: "How many in percent from the census weren't African American?" requires subtracting the percentage of African Americans from 100%, but the system fails to perform this operation and gives 2.8% instead of 97.2%.
*   **Ambiguity Resolution:** Difficulty in disambiguating similar information or names within the passage.
*   **Incomplete Information Inference:** Failing to correctly infer the answer when it requires synthesizing information from multiple parts of the passage.
*   **Misinterpreting Data:** The system misinterpreted the question "How many total points were scored in the game?".
*   **Temporal Reasoning Errors:** The system struggles with precise temporal calculations, particularly when dealing with months and approximate time phrases (e.g., "roughly").
*   **Semantic Equivalence Failure:** The system fails to understand that the golden answer, a number, is semantically different from a quantity like "one month".
*   **Incorrect Information Extraction:** The system sometimes extracts and provides incorrect or unrelated pieces of information from the passage.
*   **Inaccurate Question Analysis:** Incorrectly identifying the question type leads to extracting irrelevant information and generating wrong answers.
*   **Inability to Handle Temporal Reasoning:** The system struggles with questions requiring temporal reasoning or comparison, such as "Which happened later..." questions.
*   **Unit Omission:** The most prominent failure mode is the omission of units of measurement in the answer. Even if the numerical value is correct, the absence of the correct unit leads to a mismatch with the expected answer (e.g., "77 and 1" instead of "1-yard, 77-yard").
*   **Incorrect Ordering of Answer Components:** When multiple pieces of information are required in the answer, the system sometimes provides them in the wrong order.
*   **Lack of Contextual Understanding:** Demonstrates a lack of robust contextual understanding leading to formatting errors.
*   **Misdirection and Contextual Errors:** The system fails to correctly interpret the context and is easily misled by irrelevant information. *Example:* The question asking about "the Suns".
*   **Entity Confusion:** The system struggles to correctly link information to the correct entity when multiple entities are mentioned in the passage. *Example:* Questions about which group in the census is larger.
*   **Negation Interpretation:** The system struggles with questions involving negation (e.g., "How many percent are not non-families?"). It fails to correctly subtract the negated quantity from the whole.
*   **Contextual Numerical Extraction:** The system incorrectly identifies numbers due to a failure to determine context.
*   **Miscounting Events:** The system fails to accurately count specific events (e.g., rushing touchdowns) when the question requires distinguishing between different types of the same event (e.g., rushing vs. passing touchdowns).
*   **Lack of Precise Understanding of Question Constraints:** The system struggles to incorporate the full constraints of the question. In the "rushing touchdowns" example, the system apparently counted *all* touchdowns instead of *only* rushing touchdowns.
*   **Lack of Numerical Synthesis:** The system struggles to synthesize numerical information presented in different forms within the passage (e.g., combining multiple percentage values to reach a total).
*   **Inability to Perform Basic Calculations:** The primary failure mode is the system's inability to perform basic arithmetic operations, particularly addition and subtraction, necessary to derive answers from the passage.
*   **Misinterpreting Expected Output Format:** The system can correctly extract the necessary numbers but fails when the question requires combining them into a single aggregated answer (e.g., summing multiple counts when the desired answer is a single total).
*   **Lack of Explicit Aggregation Logic:** The system lacks explicit instructions or constraints on how to combine or aggregate the extracted numerical information, especially when a single number is expected as the golden answer.
*   **Inadequate Question Analysis for Aggregation:** The `analyze_question` function doesn't reliably identify the need for aggregation or calculation when the question implicitly requires it.
*   **Contextual Misunderstanding:** The system may misunderstand the context of the question, leading to extraction of irrelevant information or incorrect calculations, especially if the question contains implicit references or requires understanding relationships between entities (e.g., understanding the "game" refers to the one in the passage).
*   **Incorrect Entity Association:** The system misinterprets the scope or target of the percentage, linking a numerical value to the wrong subject (e.g., Haiti's international commerce vs. Hispaniola).
*   **Reliance on Keyword Matching:** The system relies too heavily on keywords (e.g., "percent") without properly understanding the precise entity to which that value applies.
*   **Insufficient Contextual Reasoning:** The system struggles with drawing inferences and understanding implied relationships within the passage. Model seems to have trouble with multi-step reasoning, especially when arithmetic is involved.
*   **Synonym Usage:** Potential failure to extract passage or identify correct answer if the question and the passage contains different vocabularies although they mean the same.
*   **Verification Stage Ineffectiveness:** The verification stage may not be effective to the point of correcting numerical reasoning errors.
*   **Misinterpreting Negation:** The model fails when questions involve negation, providing the opposite of the correct answer (e.g., percentage of families instead of non-families). Highlights a flaw in the question analysis stage.
*   **Incorrect Event Association:** The model may fail to correctly connect an event from the passage with the required entity, suggesting extraction of the first encountered value instead of considering all possibilities (e.g., longest run for a touchdown).
*   **Lack of contextual awareness**: The example about the longest run suggests a failure to identify all touchdown runs from different players in the passage before selecting the longest.
*   **Complex Reasoning Failure:** Inability to perform complex reasoning, such as aggregating information, comparing values, or performing simple arithmetic.

## 4. EXPERIMENT LOG & FINDINGS

*   **2025-05-19 14:27:04 - INITIAL DATASET ANALYSIS:** Comprehensive analysis of dataset characteristics, challenges, potential approaches, and implementation recommendations.
*   **ITERATION 0:** Initial multi-stage approach achieved 67% accuracy. Highlighted the need for improved handling of questions involving comparisons and ordinality. The system needs mechanisms for systematically extracting and processing multiple instances of events from the passage.
*   **ITERATION 1:** Achieved 1.0 accuracy using a multi-stage LLM approach with chain-of-thought prompting. Proved highly effective for the tested subset of the dataset.
*   **ITERATION 2:** Achieved 1.0 accuracy using a multi-stage LLM approach with example-driven reasoning. The modular approach is effective.
*   **ITERATION 3:** The multi-stage LLM approach struggles with questions that necessitate any form of numerical calculation or aggregation, even when the component numbers are correctly extracted from the passage. Rejected the hypothesis that LLMs can inherently perform complex reasoning from raw text, at least without explicit instruction and verification steps targeted at numerical operations. The system also exhibited a tendency to answer "The answer is correct" instead of providing the actual answer.
*   **ITERATION 4:** The initial hypothesis that a multi-stage LLM approach can effectively answer questions based on text passages is partially confirmed. The system achieves high accuracy on simpler questions but fails when arithmetic reasoning is involved (90% accuracy on simpler questions). Rejected the hypothesis that the current multi-stage approach is sufficient for all types of questions, as demonstrated by the failure to handle questions requiring score tracking and comparison.
*   **ITERATION 5:** The "exploitation" strategy does not adequately address the specific challenges of temporal reasoning and arithmetic calculation present in this dataset. The 80% accuracy indicates a ceiling is being reached with the current approach.
*   **ITERATION 6:** The multi-stage LLM approach shows promise in decomposing the problem but needs to be augmented with a dedicated arithmetic reasoning component to solve quantitative questions. Errors occur primarily in the "generate" and "verify" stages due to the lack of arithmetic capability.
*   **ITERATION 7:** Rejected the hypothesis that a multi-stage LLM approach with question analysis and answer verification would be sufficient for this dataset. The 40% accuracy indicates that the current implementation needs significant improvements. The system struggles with correct answer type identification, and prematurely stops at verification, without extracting the specific answer from the extracted passage. Passage understanding also remains deficient. The question analysis stage is likely a bottleneck.
*   **ITERATION 8:** The multi-stage approach shows promise (70% accuracy), but the LLM's inherent limitations in precise numerical reasoning and information extraction are the bottleneck. The successful cases likely involve questions where the relevant passage is easily identifiable and the required calculations are straightforward and explicitly stated in the passage.
*   **ITERATION 9:** Achieved 1.00 accuracy using a multi-stage approach. The system's ability to handle quantitative reasoning, as demonstrated by the sports question (calculating the difference in yards), is crucial for this dataset. `verify_answer` is an important step. The success indicates that the LLM, when guided with specific system instructions ("expert roles"), can effectively perform question analysis, passage extraction, answer generation, and verification.
*   **ITERATION 10:** Accuracy of 0.70. The use of multiple LLM-based verifiers is not sufficient to overcome the temporal reasoning weaknesses. The iterative refinement approach doesn't appear to adequately address the challenges posed by time-related questions in this dataset. The fact verification is either insufficient or not properly utilized to solve the temporal reasoning failures. The experiment highlights that while LLMs can extract facts, accurately calculating durations and dealing with nuances of time requires more specialized processing or fine-tuning.
*   **ITERATION 11:** Accuracy of 0.70. The results highlight that while the overall architecture of the system is reasonable, the individual components, particularly the answer generation and verification steps, need improvement. The reliance on LLMs for arithmetic and logical reasoning is a weak point. LLMs may not reliably perform these operations without specific training or tools.
*   **ITERATION 12:** Results highlight the need for explicit unit handling in both answer generation and verification. The single run underscores the LLM's struggle with output formatting (unit omission and incorrect ordering) and the limitations of the current verification stage in correcting these issues.
*   **ITERATION 13:** The Exploitation strategy, while leveraging a multi-stage approach, highlights the limitations of the LLM in performing arithmetic and complex reasoning tasks directly. The initial question analysis and focused passage extraction are promising steps, but the LLM needs additional support for accurate calculation and contextual awareness.
*   **ITERATION 14:** Accuracy of 80%. The "Exploitation" strategy has revealed critical weaknesses in handling numerical reasoning and negated queries within this specific dataset. Confirms the potential of a multi-stage approach, but rejects the sufficiency of the current implementation.
*   **ITERATION 15:** Achieved 1.00 accuracy using a multi-stage LLM-driven approach with chain-of-thought reasoning. The decomposition into question analysis, passage extraction, answer generation, and verification, with specialized agents for each function, proved highly effective. Explicit error handling also contributed to the success. Validates the effectiveness of the modular, LLM-driven approach.
*   **ITERATION 16:** Exploitation strategy, achieving 80% accuracy, reveals a critical weakness in precise counting and contextual understanding, especially for sports narratives and event-based questions. The "Calculation Needed" flag indicates the system recognizes the need for some form of numerical reasoning, but the subsequent calculation is often incorrect, and the verification stage is not robust enough to catch miscounting errors.
*   **ITERATION 17:** Reported 1.00 accuracy using a multi-stage approach. While promising, this result is questionable due to the lack of reported error cases. A 1.0 accuracy claim without supporting error analysis is unreliable. Further validation with comprehensive error logging is required. This iteration highlights the *critical importance* of tracking and analyzing failure modes to accurately assess system performance.
*   **ITERATION 18:** Hypothesis Rejected: The initial hypothesis that the multi-stage LLM approach could effectively handle quantitative reasoning based on the provided passages was rejected. The system demonstrated a significant weakness in performing even simple calculations. The experiment highlighted a key limitation of the LLM in its current configuration: a lack of capacity for basic numerical processing and synthesis, even when provided with all the necessary information within the passage.
*   **ITERATION 19:** Multi-stage approach yields 70% accuracy. The system demonstrates limitations in numerical reasoning (failing to perform subtractions) and inference (failing to synthesize information from different parts of the passage). Highlights that the system extracts information well, but struggles to process it.
*   **ITERATION 20:** The multi-stage approach has potential, but the "verify\_answer" stage is not robust enough to handle cases where aggregation or calculation is necessary. The high-level strategy of breaking down the problem into smaller steps is promising but needs refinement in how numerical information is processed and combined.
*   **ITERATION 21:** Perfect accuracy on simple extraction tasks shows the multi-stage approach is effective when questions are relatively straightforward and require minimal reasoning or inference. The analysis points out that the most critical problem is the shallow reasoning and limited contextual understanding.
*   **ITERATION 22:** Achieved 80% accuracy. Primary failure is misinterpreting the scope of percentages and linking values to the wrong subject, showing weakness in semantic understanding and entity resolution.
*   **ITERATION 23:** Exploitation strategy, leveraging the multi-stage LLM approach, achieves a strong baseline (90% accuracy). Analysis confirms the modular approach to question answering can be effective, but highlights need for improvements in the final reasoning/calculation stage. Rejected the assumption that LLM would automatically perform necessary calculations once relevant information was extracted.
*   **ITERATION 24:** Achieved perfect accuracy (1.00) using a multi-stage LLM with chain-of-thought prompting. Due to the perfect accuracy, specific failure modes cannot be identified.
*   **ITERATION 25:** The exploitation strategy achieved an accuracy of 80%, indicating a good baseline for the performance of the approach. The primary issue lies in numerical reasoning and semantic understanding rather than the overall architecture of the chain-of-thought approach. Verification stage may not be effective to the point of correcting numerical reasoning errors.
*   **ITERATION 26:** The exploitation strategy achieved an accuracy of 80%, further solidifying this architecture's potential. However, key weaknesses persist in negation handling, event association, and contextual reasoning. The multi-stage approach is promising but sensitive to errors in the question analysis stage, which then propagate through the subsequent stages. Verification strategies need to be more robust to catch errors in interpretation and extraction, particularly with numerical answers.
*   **ITERATION 27:** Multi-stage approach with specialized agents shows potential but struggles with complex reasoning (aggregation, comparison, arithmetic) and contextual grounding. The implementation is insufficient for complex reasoning.

## 5. NEXT RESEARCH DIRECTIONS

*   **Enhance Reasoning Capabilities:**
    *   Augment the "answer generator" agent with capabilities for performing calculations and comparisons. For example, provide it with a tool or function that can evaluate simple arithmetic expressions.
    *   Implement a calculator module to perform arithmetic operations instead of relying on the LLM.
*   **Improve Question Analysis:**
    *   Refine the "question analyzer" to better identify question types that require specific reasoning skills (counting, comparison, arithmetic). This will allow the system to invoke appropriate reasoning tools. Enhance the `analyze_question` function to better identify question types that require calculations or inferences and to identify and encode constraints within the question. Improve aggregation/calculation detection. Enhance Question Analysis for Aggregation/Calculation: Modify the `analyze_question` function to specifically identify whether the question requires aggregation or calculation of extracted numerical facts. Use few-shot examples to prompt engineering that cover scenarios involving sums, differences, or other arithmetic operations. Explicitly prompt the LLM to determine if calculation will be needed.
    *   Enhance the question analysis stage to correctly identify question type, intent, and constraints (e.g., negative constraints) more accurately. Use more examples that show subtle differences in questions.
*   **Contextual Awareness:**
    *   Add a verification step that enforces that the answer is specific to the context of the provided passage. This could involve checking the generated answer for keywords from the passage or ensuring that it refers to entities mentioned in the passage.
    *   Improve the "extract relevant passage" stage by incorporating techniques that understand the *context* of numerical values. Improve Entity Tracking and add Misdirection Filtering.
    *   Incorporate a "relationship extraction" step within `analyze_question` to identify the *type* of relationship being asked about (e.g., ownership, demographic proportion, relative size). The "extract relevant passage" stage should incorporate techniques that understand the *context* of numerical values. Focus on training and fine-tuning the LLMs on the specific error cases to improve their robustness. Create a dataset of similar challenging examples.
*   **Enhance Negation Handling:** Implement explicit rules or training data to improve the model's ability to handle negation in questions. This could involve pre-processing questions to rephrase them in a positive form, or augmenting the training data with examples that specifically target negation. A specific module within the "question analysis" stage should explicitly identify and handle negation.
*   **Improve Numerical Reasoning:** Strengthen the numerical reasoning capabilities by:
    *   Explicitly identifying and extracting all relevant numerical values with units from the passage, then performing calculations.
    *   Implementing more sophisticated verification checks that not only compare numerical values but also validate the logical steps used to arrive at the answer.
*   **Improve Event Association**: Enhance the models ability to associate events to the entity in question. This may require a separate question decomposition stage that parses the question into events and entities.
*   **Implement a Rationale Checker:** Introduce a new verification stage that checks the *reasoning process* rather than just the final answer. This stage will evaluate if the steps taken to reach the answer were logically sound. Verification strategies need to be more robust to catch errors in interpretation and extraction, particularly with numerical answers.
*   **Fine-tune LLMs on Numerical Reasoning Datasets:** Fine-tune each LLM on datasets that focus on numerical reasoning and question answering.
*   **Introduce More Complex Questions:** Modify the dataset to include more complex questions that require numerical reasoning, pronoun resolution, negation, and comparative reasoning.
*   **Increase Dataset Diversity:** Expand the dataset to include different sports or different styles of game summaries.
*   **Error Analysis with Imperfect Runs:** Intentionally perturb the system (e.g., shorten the passage, introduce noise, use a smaller model) to induce errors and then analyze those errors to gain deeper insights.
*   **Ablation Studies:** Remove or modify components of the approach (e.g., remove the verification step, simplify the question analysis) to assess their individual contributions to overall performance. Once failure modes have been identified, conduct ablation studies to determine their contribution to the overall performance.
*   **Evaluate on Unseen Data:** Hold out a portion of the data and evaluate the model's performance on it to ensure that it generalizes well to new examples.
*   **Test robustness against paraphrasing:** Create adversarial examples of questions where words are changed but mean the same thing. This can expose weaknesses in the model's ability to understand synonyms.
*   **Explicitly instruct the 'answer generation' stage to perform calculations:** Add specific instructions like "After extracting the relevant information, perform any necessary calculations (addition, subtraction, etc.) to derive the final answer to the question. The answer should be a number if applicable."
*   **Incorporate a "Calculation and Aggregation" stage:** Design a stage to perform the final arithmetic and logical reasoning. This could involve feeding the extracted information into a smaller, more focused LLM or even a deterministic calculator.
*   **Fine-tune the 'verify\_answer' stage to specifically check for calculation completeness:** It should verify that the final answer is present and the correct reasoning is performed. The system needs to know how to calculate points from touchdowns, field goals, etc. Add test examples that include more complex score calculations. Verify that the calculated value is associated with the target entity specified in the question, requiring a deeper understanding of the passage context.
*   **Improve Entity Linking:** Enhance the `extract_relevant_passage` stage to perform more robust entity linking: identify all entities, determine relationships, and use those relationships to filter answer passages.
*   **Implement a Negative Constraint:** Add a constraint during passage extraction to explicitly exclude passage segments that directly contradict the question's implied entity relationships.
*   **Prompt Engineering:** Refine the prompts used in `call_llm` to explicitly encourage reasoning about entity relationships. Refine prompting for Verification and for Unit Inclusion. Add explicit instructions in the prompt to the answer generator specifying the desired order of answer components based on the question's structure.
*   **Enhance reasoning and inference capabilities:** Incorporate mechanisms for identifying relationships between entities, performing multi-step reasoning, and making inferences based on the passage content. This could involve techniques like graph-based representation of information or more sophisticated prompt engineering for the reasoning stage.
*   **Implement more robust numerical reasoning:** Improve the system's ability to perform calculations and comparisons involving numbers extracted from the passage, including handling units of measurement and implicit numerical relationships.
*   **Introduce question type classification and targeted strategies:** Categorize questions based on their type (e.g., extraction, comparison, inference) and apply different strategies accordingly. For example, inference questions could trigger a more elaborate reasoning process.
*   **Introduce Aggregation Logic in Verify\_Answer:** Incorporate an explicit aggregation/calculation step in the `verify_answer` function, guided by the analysis from the `analyze_question` function. If `analyze_question` identifies a "sum" question type, `verify_answer` should attempt to sum the extracted numbers.
*   **Refine Few-Shot Examples for Aggregation:** Add few-shot examples that specifically demonstrate how to combine multiple numbers to get the final answer.
*   **Add Output Format Constraints:** Experiment with system instructions or few-shot examples that constrain the model to produce a single numerical answer when appropriate (e.g., "Your answer should be a single number representing the total.").
*   **Evaluate on Number-Focused Subsets:** Create a dataset subset with a greater focus on the number manipulation aspects to focus the work.
*   **Prioritize Error Analysis:** The most crucial next step is to identify and analyze failure cases. Run the script on a larger subset of the dataset and *specifically log all instances where the answer is incorrect*. Collect a diverse set of questions to expose different failure modes. Conduct a more thorough error analysis of a larger set of failures to identify additional error patterns. Use these patterns to augment the training data with examples that specifically address these failure modes. Implement a more detailed error logging system to track the specific steps where the system fails.
*   **Targeted Data Collection:** Once failure modes are identified, collect more examples specifically targeting those areas. For example, if questions requiring numerical comparisons are often missed, gather more similar questions.
*   **Prompt Engineering:** If the questions analysis module is failing on a particular type of question, experiment with different prompts designed to improve its performance.
*   **Enhance Question Analysis for Constraints:** Improve the `analyze_question` function to explicitly identify and encode constraints within the question. For example, if the question asks about "rushing touchdowns," the analysis should flag "rushing" as a key constraint.
*   **Improve Answer Generation with Filtering:** Modify the `generate_answer` function to filter information based on the constraints identified in the question analysis. Ensure that only events matching the specified criteria are considered for counting.
*   **Refine Calculation and Verification:** Enhance the calculation logic to be more precise, and improve the `verify_answer` function to specifically check for correct counting of events based on question constraints. The system should explicitly verify that the count only includes events of the specified type.
*   **Incorporate Examples in Prompts:** When calling the LLM for analysis, retrieval, and generation, include a few shot examples in the prompt, showing how the model should handle counting and constraint-based questions. This can improve the LLM's ability to follow instructions for this particular dataset.
*   **
        

            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        

            TOP PERFORMING APPROACHES TO BUILD UPON:
            
TOP PERFORMING APPROACH #1:
Iteration: 1
Accuracy: 1.00
Approach Summary: The script uses a multi-stage LLM approach with chain-of-thought prompting to answer questions by identifying the question type, extracting relevant information, generating an answer, and then verifying the answer. The script uses the following functions: `main` orchestrates the entire process, `analyze_question` identifies the question type and keywords, `extract_relevant_passage` retrieves relevant text, `generate_answer` formulates an initial answer, `verify_answer` validates the answer, and `call_llm` interacts with the Gemini LLM, using system instructions and prompts to guide the LLM's reasoning at each step. The workflow starts with `main` calling `analyze_question`, followed by `extract_relevant_passage`, `generate_answer`, and `verify_answer`, each leveraging `call_llm` to interact with the LLM.

FULL SCRIPT TO REFINE:
```python
import os
import re
import math

def main(question):
    """
    Solve the question using a multi-stage LLM approach.
    This approach focuses on breaking down the problem into question type identification, 
    focused passage extraction, and direct answer generation with verification.
    """
    try:
        # Step 1: Identify question type and keywords
        question_analysis = analyze_question(question)
        if "Error" in question_analysis:
            return "Error analyzing question"

        # Step 2: Extract relevant passage using identified keywords
        relevant_passage = extract_relevant_passage(question, question_analysis)
        if "Error" in relevant_passage:
            return "Error extracting passage"

        # Step 3: Generate answer using extracted passage and question type
        answer = generate_answer(question, relevant_passage, question_analysis)
        if "Error" in answer:
            return "Error generating answer"

        # Step 4: Verify answer
        verified_answer = verify_answer(question, answer, relevant_passage)
        if "Error" in verified_answer:
            return "Error verifying answer"
        
        return verified_answer

    except Exception as e:
        return f"General Error: {str(e)}"

def analyze_question(question):
    """Analyzes the question to identify its type and keywords. Includes multiple examples."""
    system_instruction = "You are an expert at analyzing questions to determine their type and keywords."
    prompt = f"""
    Analyze the following question and identify its type (e.g., fact extraction, calculation, comparison) and keywords.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Analysis: {{"type": "fact extraction", "keywords": ["final touchdown", "caught"]}}

    Example 2:
    Question: How many running backs ran for a touchdown?
    Analysis: {{"type": "counting", "keywords": ["running backs", "touchdown"]}}
    
    Example 3:
    Question: Which player kicked the only field goal of the game?
    Analysis: {{"type": "fact extraction", "keywords": ["player", "field goal"]}}

    Question: {question}
    Analysis:
    """
    return call_llm(prompt, system_instruction)

def extract_relevant_passage(question, question_analysis):
    """Extracts the relevant passage from the question based on keywords. Includes multiple examples."""
    system_instruction = "You are an expert at extracting relevant passages from text."
    prompt = f"""
    Extract the relevant passage from the following text based on the question and keywords.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Keywords: {{"type": "fact extraction", "keywords": ["final touchdown", "caught"]}}
    Text: PASSAGE: After a tough loss at home, the Browns traveled to take on the Packers. ... The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    Passage: The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    
    Example 2:
    Question: How many running backs ran for a touchdown?
    Keywords: {{"type": "counting", "keywords": ["running backs", "touchdown"]}}
    Text: PASSAGE: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. The Lions would respond with kicker Jason Hanson getting a 53-yard field goal. The Titans would answer with Johnson getting a 58-yard TD run, along with DE Dave Ball returning an interception 15 yards for a touchdown. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.
    Passage: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.

    Example 3:
    Question: Which player kicked the only field goal of the game?
    Keywords: {{"type": "fact extraction", "keywords": ["player", "field goal"]}}
    Text: PASSAGE: Game SummaryComing off their Thanksgiving road win over the Falcons, the Colts went home for a Week 13 AFC South rematch with the Jacksonville Jaguars.  ... In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.
    Passage: In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.

    Question: {question}
    Keywords: {question_analysis}
    Text: {question}
    Passage:
    """
    return call_llm(prompt, system_instruction)

def generate_answer(question, relevant_passage, question_analysis):
    """Generates the answer based on the question, relevant passage, and question type. Includes multiple examples."""
    system_instruction = "You are an expert at generating answers to questions based on provided text."
    prompt = f"""
    Generate the answer to the question based on the relevant passage and question type.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Passage: The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    Answer: Jarrett Boykin

    Example 2:
    Question: How many running backs ran for a touchdown?
    Passage: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. The Lions would respond with kicker Jason Hanson getting a 53-yard field goal. The Titans would answer with Johnson getting a 58-yard TD run, along with DE Dave Ball returning an interception 15 yards for a touchdown. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.
    Answer: 2
    
    Example 3:
    Question: Which player kicked the only field goal of the game?
    Passage: In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.
    Answer: Josh Scobee

    Question: {question}
    Passage: {relevant_passage}
    Answer:
    """
    return call_llm(prompt, system_instruction)

def verify_answer(question, answer, relevant_passage):
    """Verifies the generated answer. Includes multiple examples."""
    system_instruction = "You are an expert at verifying answers to questions."
    prompt = f"""
    Verify the following answer to the question based on the relevant passage.  Return the answer if it is correct.  Return the correct answer if it is incorrect.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Answer: Jarrett Boykin
    Passage: The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    Verification: Jarrett Boykin
    
    Example 2:
    Question: How many running backs ran for a touchdown?
    Answer: 2
    Passage: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.
    Verification: 2

    Example 3:
    Question: Which player kicked the only field goal of the game?
    Answer: Josh Scobee
    Passage: In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.
    Verification: Josh Scobee

    Question: {question}
    Answer: {answer}
    Passage: {relevant_passage}
    Verification:
    """
    return call_llm(prompt, system_instruction)

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
```

TOP PERFORMING APPROACH #2:
Iteration: 2
Accuracy: 1.00
Approach Summary: The script uses a multi-stage LLM approach with example-driven reasoning to answer questions, incorporating question analysis, information extraction, answer generation, and verification. The problem is decomposed into four main steps: analyzing the question, extracting a relevant passage, generating an answer, and verifying the answer, each handled by a separate function. There are no agent roles indicated in the script. The script uses `analyze_question` to identify the question type and keywords, `extract_relevant_passage` to find the relevant information, `generate_answer` to form an answer, `verify_answer` to confirm its correctness, and `call_llm` to interface with the Gemini LLM. The overall workflow involves analyzing the question, extracting relevant information, generating an initial answer, verifying it for accuracy, and then returning the verified answer.

Key approach aspects (full code available for reference)

TOP PERFORMING APPROACH #3:
Iteration: 9
Accuracy: 1.00
Approach Summary: The script uses a multi-stage LLM approach, including chain-of-thought reasoning by decomposing the problem into question analysis, relevant passage extraction, answer generation, and answer verification. It uses the `call_llm` function to interact with the Gemini model with specific system instructions and prompts at each stage, effectively assigning different "expert" roles to the LLM. The workflow involves `analyze_question` to determine the question type and keywords, `extract_relevant_passage` to find relevant text, `generate_answer` to create an initial answer, and `verify_answer` to confirm the answer and perform calculations if needed.

Key approach aspects (full code available for reference)

            
FULL SCRIPT TO REFINE:
```python
import os
import re
import math

def main(question):
    """
    Solve the question using a multi-stage LLM approach.
    This approach focuses on breaking down the problem into question type identification, 
    focused passage extraction, and direct answer generation with verification.
    """
    try:
        # Step 1: Identify question type and keywords
        question_analysis = analyze_question(question)
        if "Error" in question_analysis:
            return "Error analyzing question"

        # Step 2: Extract relevant passage using identified keywords
        relevant_passage = extract_relevant_passage(question, question_analysis)
        if "Error" in relevant_passage:
            return "Error extracting passage"

        # Step 3: Generate answer using extracted passage and question type
        answer = generate_answer(question, relevant_passage, question_analysis)
        if "Error" in answer:
            return "Error generating answer"

        # Step 4: Verify answer
        verified_answer = verify_answer(question, answer, relevant_passage)
        if "Error" in verified_answer:
            return "Error verifying answer"
        
        return verified_answer

    except Exception as e:
        return f"General Error: {str(e)}"

def analyze_question(question):
    """Analyzes the question to identify its type and keywords. Includes multiple examples."""
    system_instruction = "You are an expert at analyzing questions to determine their type and keywords."
    prompt = f"""
    Analyze the following question and identify its type (e.g., fact extraction, calculation, comparison) and keywords.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Analysis: {{"type": "fact extraction", "keywords": ["final touchdown", "caught"]}}

    Example 2:
    Question: How many running backs ran for a touchdown?
    Analysis: {{"type": "counting", "keywords": ["running backs", "touchdown"]}}
    
    Example 3:
    Question: Which player kicked the only field goal of the game?
    Analysis: {{"type": "fact extraction", "keywords": ["player", "field goal"]}}

    Question: {question}
    Analysis:
    """
    return call_llm(prompt, system_instruction)

def extract_relevant_passage(question, question_analysis):
    """Extracts the relevant passage from the question based on keywords. Includes multiple examples."""
    system_instruction = "You are an expert at extracting relevant passages from text."
    prompt = f"""
    Extract the relevant passage from the following text based on the question and keywords.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Keywords: {{"type": "fact extraction", "keywords": ["final touchdown", "caught"]}}
    Text: PASSAGE: After a tough loss at home, the Browns traveled to take on the Packers. ... The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    Passage: The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    
    Example 2:
    Question: How many running backs ran for a touchdown?
    Keywords: {{"type": "counting", "keywords": ["running backs", "touchdown"]}}
    Text: PASSAGE: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. The Lions would respond with kicker Jason Hanson getting a 53-yard field goal. The Titans would answer with Johnson getting a 58-yard TD run, along with DE Dave Ball returning an interception 15 yards for a touchdown. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.
    Passage: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.

    Example 3:
    Question: Which player kicked the only field goal of the game?
    Keywords: {{"type": "fact extraction", "keywords": ["player", "field goal"]}}
    Text: PASSAGE: Game SummaryComing off their Thanksgiving road win over the Falcons, the Colts went home for a Week 13 AFC South rematch with the Jacksonville Jaguars.  ... In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.
    Passage: In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.

    Question: {question}
    Keywords: {question_analysis}
    Text: {question}
    Passage:
    """
    return call_llm(prompt, system_instruction)

def generate_answer(question, relevant_passage, question_analysis):
    """Generates the answer based on the question, relevant passage, and question type. Includes multiple examples."""
    system_instruction = "You are an expert at generating answers to questions based on provided text."
    prompt = f"""
    Generate the answer to the question based on the relevant passage and question type.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Passage: The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    Answer: Jarrett Boykin

    Example 2:
    Question: How many running backs ran for a touchdown?
    Passage: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. The Lions would respond with kicker Jason Hanson getting a 53-yard field goal. The Titans would answer with Johnson getting a 58-yard TD run, along with DE Dave Ball returning an interception 15 yards for a touchdown. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.
    Answer: 2
    
    Example 3:
    Question: Which player kicked the only field goal of the game?
    Passage: In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.
    Answer: Josh Scobee

    Question: {question}
    Passage: {relevant_passage}
    Answer:
    """
    return call_llm(prompt, system_instruction)

def verify_answer(question, answer, relevant_passage):
    """Verifies the generated answer. Includes multiple examples."""
    system_instruction = "You are an expert at verifying answers to questions."
    prompt = f"""
    Verify the following answer to the question based on the relevant passage.  Return the answer if it is correct.  Return the correct answer if it is incorrect.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Answer: Jarrett Boykin
    Passage: The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    Verification: Jarrett Boykin
    
    Example 2:
    Question: How many running backs ran for a touchdown?
    Answer: 2
    Passage: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.
    Verification: 2

    Example 3:
    Question: Which player kicked the only field goal of the game?
    Answer: Josh Scobee
    Passage: In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.
    Verification: Josh Scobee

    Question: {question}
    Answer: {answer}
    Passage: {relevant_passage}
    Verification:
    """
    return call_llm(prompt, system_instruction)

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
```

            PREVIOUSLY ATTEMPTED VARIATIONS:
            

            EXPLOITATION GUIDANCE:
            1. Review the error patterns, targeted improvements, and accumulated learnings carefully
            2. CRITICAL: Break down the problem into distinct reasoning steps before modifying code
            3. CRITICAL: Analyze the best scripts to identify which components are working well and which are failing. Focus your improvements on the weak points while preserving successful components.
            4. Maintain the core successful elements of the best approaches
            5. Consider how you can combine strengths from multiple top-performing approaches
            6. CRITICAL: Add EMBEDDED EXAMPLES to EVERY LLM prompt that illustrate:
               - Sample input that resembles the dataset
               - Step-by-step reasoning through the example
               - Properly formatted output
            7. Focus on fixing specific issues identified in previous error analyses. Create an explicit HYPOTHESIS for each targeted improvement, as well as a way to verify if it's successful.
            8. Enhance chain-of-thought reasoning and verification steps. Verification steps should be added to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
            9. Apply the key insights from ACCUMULATED LEARNINGS to enhance the approach
            10. Pay SPECIAL ATTENTION to the weaknesses and improvement suggestions from the capability assessment

            IMPROVEMENT STRATEGY:
            Analyze why the top approaches succeeded where others failed. Identify the key differentiators and strengthen them further.

            SYSTEMATIC ENHANCEMENT APPROACH:
            1. First, identify which specific function or component is underperforming based on error analysis
            2. Examine how error cases differ from successful cases
            3. For each identified weakness, implement a targeted enhancement
            4. Add additional verification steps around modified components
            5. Consider how components interact - ensure improvements don't break successful parts

            Consider enhancing the script with one or more of these patterns:
            - Repeated validation with feedback loops
            - Multi-perspective analysis with synthesis
            - Dynamic input-dependent routing
            - Hybrid approaches combining LLM with deterministic functions
            - Best-of-n solution generation and selection
            - ReAct pattern for interactive reasoning and action
            - If it is unknown how successful a processing state or part of the pipeline is, include verification steps to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
            - Answer checkers to validate the final answer against the problem statement. If the answer is incorrect, the checker can send the answer back to an earlier part of the system for refinement with feedback

            Here's how to call the Gemini API. Use this example without modification and don't invent configuration options:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

            Since this is an EXPLOITATION phase:
            - Build upon what's working well in the best approaches
            - Consider creative combinations of successful techniques from different scripts
            - Make TARGETED improvements to address specific error patterns
            - For EACH key LLM prompt, include a relevant example with:
              * Sample input similar to the dataset
              * Expected reasoning steps
              * Desired output format
            - Apply the knowledge from our accumulated learnings
            - Significantly enhance the script to address weaknesses identified in the capability assessment

            CRITICAL REQUIREMENTS:
            1. The script MUST properly handle all string literals - be extremely careful with quotes and triple quotes
            2. The script MUST NOT exceed 150 lines of code to prevent truncation
            3. Include detailed comments explaining your improvements
            4. EVERY SINGLE LLM PROMPT must include at least one embedded example showing:
               - Sample input with reasoning
               - Desired output format
            5. Make proper use of error handling
            6. Implement robust capabilities to address the specific weaknesses identified in the capability assessment
            7. Do NOT use json.loads() in the LLM calls to process input data. JSON formatting is good to use to structure information as inputs and outputs, but attempting to have functions process JSON data explicitly with strict built-in functionality is error prone due to formatting issues and additional text that appears as documentation, reasoning, or comments. When passing data into another LLM call, you can read it as plain text rather than trying to load it in strict json format, is the better approach.

            Return a COMPLETE, RUNNABLE Python script that:
            1. Has a main function that takes a question string as input and returns the answer string
            2. Makes multiple LLM calls for different reasoning steps
            3. Has proper error handling for API calls
            4. Includes embedded examples in EVERY LLM prompt
            5. Is COMPLETE - no missing code, no "..." placeholders
            6. Closes all string literals properly

            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            