
            You are improving a Python script that solves problems from a dataset.
            Your goal is to REFINE and ENHANCE the best performing approaches by combining their strengths and addressing specific weaknesses identified in error analysis.

            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "PASSAGE: The Ravens were facing the Tampa Bay Buccaneers, a team with an identical 7-3 record. However, the Ravens were considered favorites going into the game, given that they had never lost at home during the season, and all Tampa Bay wins were against teams with losing records. Still, they did not take this one lightly. After a scoreless first quarter, the Ravens scored first with a field goal. Tampa Bay later tied it, but Baltimore scored two touchdowns, including one on a 65-yard pass from Todd Heap, to have a 17-3 halftime lead. During a scoreless third quarter, a would be long TD-run from Ray Rice was negated by a controversial call of a penalty on Anquan Boldin. Though the Ravens would never score anymore in the game, they managed to hold off Tampa Bay and allow just one touchdown, enough to prevent their lead from being blown.\n\nQUESTION: How many points were scored in the first half?",
    "answer": "20"
  },
  {
    "id": 1,
    "question": "PASSAGE: The Steelers traveled to Chicago to take on the Bears. In the first quarter, the Bears scored first when Jordan Howard ran for a 3-yard touchdown to take a 7-0 lead for the only score of the quarter. In the second quarter, the Steelers managed to tie it up at 7-7 when Ben Roethlisberger found Antonio Brown on a 7-yard pass. The Bears moved ahead by double digits later on when Mike Glennon found Adam Shaheen on a 2-yard pass to make it 14-7. This would be followed up by Connor Barth's 24-yard field goal to make it 17-7 at halftime. After the break, the Steelers got back to work and came within 3 as Le'Veon Bell ran for a 1-yard touchdown to make it 17-14. They would tie the game up in the fourth quarter when Chris Boswell nailed a 32-yard field goal. The Steelers then tried their hand at coming back for the win later on in the quarter, but Roethlisberger was sacked in Bears territory, sending the game into overtime. In overtime, the Bears got the ball. They would win it by way of Howard's 19-yard run for a touchdown and the final score 23-17. With the loss and 9-game regular season winning streak snapped, the Steelers dropped to 2-1 and with the Ravens' loss to the Jaguars moved into a tie on top of the AFC North. The loss was the Steelers' third straight against the Bears, and dropped the Steelers to a record of 1-12 all-time against the Bears in Chicago.\n\nQUESTION: How many is the difference in the yards of the TD run by Howard and the yards of the field goal made by Barth?",
    "answer": "21"
  },
  {
    "id": 2,
    "question": "PASSAGE: Coming off their home win over the Ravens, the Dolphins flew to Gillette Stadium for a Week 16 AFC East rematch with the undefeated New England Patriots.  In the first quarter, Miami trailed early as Patriots QB Tom Brady completed an 11-yard TD pass to WR Randy Moss for the only score of the period.  In the second quarter, New England pulled away with Brady & Moss hooked up with each other again on a 1-yard TD pass, RB Laurence Maroney getting a 59-yard TD run, and Brady completing a 48-yard TD pass to WR Jabar Gaffney.  In the third quarter, the Dolphins would get on the board as QB Cleo Lemon completed a 21-yard TD pass to WR Greg Camarillo.  However, the Patriots' defense took over for the rest of the game.\n\nQUESTION: How many yards shorter was Tom Brady's second touchdown pass compared to his first?",
    "answer": "10"
  }
]

            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 6
        - Current explore/exploit balance: 25/75
        - Best accuracy achieved: 1.00 (iteration 1)

        APPROACH HISTORY (last 6 iterations):
        [
  {
    "iteration": 0,
    "strategy": "Exploration",
    "accuracy": 0.3333333333333333,
    "approach": "This script uses a chain-of-thought approach, leveraging LLMs to answer questions by first determining the question type, then extracting relevant information, generating an answer, and finally verifying the answer. Each step uses a specific LLM agent role (e.g., question type classifier, information extractor) defined by system instructions to decompose the problem into manageable parts. The functions used are `main` (orchestrates the workflow), `determine_question_type`, `extract_relevant_info`, `generate_answer`, `verify_answer` (each leveraging `call_llm` to interact with the Gemini LLM). The overall workflow is sequential, passing the output of each function as input to the next, with error handling at each stage."
  },
  {
    "iteration": 1,
    "strategy": "Exploitation",
    "accuracy": 1.0,
    "approach": "The script uses a chain-of-thought approach to answer questions by first determining the question type, then extracting relevant information, and finally generating the answer using the Gemini LLM. The problem is decomposed into three functions: `determine_question_type`, `extract_relevant_info`, and `generate_answer`, each acting as an agent with a specific role. The `call_llm` function is used to interface with the Gemini model, with the main function orchestrating the calls. The workflow is: main -> determine_question_type -> extract_relevant_info -> generate_answer, with each step passing information to the next."
  },
  {
    "iteration": 2,
    "strategy": "Exploitation",
    "accuracy": 1.0,
    "approach": "The script uses a chain-of-thought approach to answer questions by first determining the question type, then extracting relevant information, and finally generating the answer. Each step uses the `call_llm` function to interact with the Gemini LLM, acting as a specialized agent for question type classification, information extraction, and answer generation respectively. The `main` function orchestrates the workflow: `determine_question_type` identifies the question type, `extract_relevant_info` extracts needed information based on the question and its type, and `generate_answer` formulates the final answer. Error handling is incorporated at each step, returning an error message if a step fails."
  },
  {
    "iteration": 3,
    "strategy": "Exploitation",
    "accuracy": 0.6,
    "approach": "The script uses a decomposed approach to answer questions by first determining the question type, then extracting relevant information, and finally generating the answer, leveraging the Gemini LLM at each step. It uses a chain-of-thought prompting technique by providing examples to guide the LLM's responses for each subtask. The script employs different \"expert\" roles (question type classifier, information extractor, and answer generator) implemented via system instructions passed to the LLM. The functions used are `main` (orchestrates the process), `determine_question_type` (classifies question type), `extract_relevant_info` (extracts relevant information), `generate_answer` (generates the final answer), and `call_llm` (communicates with the Gemini API); with `main` calling the other functions sequentially to solve the task."
  },
  {
    "iteration": 4,
    "strategy": "Exploitation",
    "accuracy": 0.8,
    "approach": "The script uses a decompose-and-generate approach to answer questions by classifying the question type, extracting relevant information, and then generating the final answer using the Gemini LLM. It uses in-context learning with examples to guide the LLM in each step. There are three distinct agent roles: a question type classifier, an information extractor, and an answer generator.\n\nThe `main` function orchestrates the process by calling `determine_question_type`, `extract_relevant_info`, and `generate_answer` sequentially; these functions formulate specific prompts for the LLM. The `call_llm` function is used to interact with the Gemini LLM, sending prompts and receiving responses."
  },
  {
    "iteration": 5,
    "strategy": "Exploitation",
    "accuracy": 0.4,
    "approach": "The script uses a chain-of-thought approach to answer questions by first determining the question type, then extracting relevant information, and finally generating the answer, using the `gemini-2.0-flash` model. The problem is decomposed into three steps: question type determination, information extraction, and answer generation. The agents used are \"expert at classifying question types\", \"expert at extracting relevant information\", and \"expert at generating correct answers.\"\n\nThe main function orchestrates the process, calling `determine_question_type`, `extract_relevant_info`, and `generate_answer` sequentially. The `call_llm` function is used within the other functions to interact with the Gemini API. The workflow starts with the `main` function receiving a question, which is then passed to `determine_question_type` to classify the question. The question and its type are passed to `extract_relevant_info` which returns the relevant information from the passage. Finally, the question, its type, and the extracted information are passed to `generate_answer` which outputs the answer to the question."
  }
]

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 3 iterations):
        [
  {
    "iteration": 0,
    "issue": "The primary issue is the system's **failure to transition from identifying the required task to *actually executing* that task and generating a definitive answer.** The \"Verify that...\" pattern is a symptom of this core problem. It suggests the system is stuck in a planning/verification loop without proceeding to execution."
  },
  {
    "iteration": 1,
    "issue": "Since there are no error cases, there is no single most critical problem to fix."
  },
  {
    "iteration": 2,
    "issue": "The lack of error data makes it impossible to determine the most critical problem to fix. Further action is required to provide error cases."
  },
  {
    "iteration": 3,
    "issue": "The single most critical problem is **inaccurate and unreliable information extraction** from the provided text passages. The system frequently fails to identify and retrieve the specific numerical values, entities, and event details required to answer the questions correctly. This failure cascades into incorrect calculations and faulty reasoning, ultimately leading to wrong answers."
  },
  {
    "iteration": 4,
    "issue": "The primary issue is **inaccurate temporal reasoning and calculation, specifically the ability to precisely determine the difference in months between two dates mentioned in the passage.** The system is unable to reliably extract and compare dates, and then perform the necessary calculation to find the difference."
  },
  {
    "iteration": 5,
    "issue": "The most critical problem is the **inaccurate and indiscriminate information extraction process.** The system needs a much more sophisticated method of identifying the *specific* pieces of information required to answer the question, avoiding extraneous data. This requires improved semantic understanding of the question and the passage."
  }
]

        TARGETED IMPROVEMENTS:
        []
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity ✓
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity ✓
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity ✓
    4. All operations have O(1) average time complexity ✓
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        

            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        ```
# Dataset-Specific Research Log: Question Answering on Sports/Statistical Texts

This document serves as a running log of our research and findings specific to the task of question answering on a dataset consisting of short sports reports or statistical summaries followed by fact-based questions.

## 1. DATASET PATTERNS & CHARACTERISTICS

*   **Passage Structure:** Each example consists of a passage of text followed by a question, separated by "\n\nQUESTION: ". The passages describe sports events, statistical summaries, or other fact-based scenarios.
*   **Question Types:**
    *   **Who:** Asking for a person who performed an action (e.g., scored a touchdown). Example: "Who kicked the field goal?"
    *   **How many:** Asking for a count of something (e.g., running backs). Example: "How many rushing touchdowns did he have?"
    *   **Calculation:** Asking for a result of a simple numerical operation (e.g., addition, subtraction, comparison). Example: "How many total yards did the team gain?"
    *   **Comparative Questions Requiring Numerical Extraction and Comparison:** Questions involving comparing two entities based on a numerical attribute mentioned in the passage (e.g., "Which star has a smaller mass..."). Example: "Which team had more passing yards, the Bears or the Packers?"
    *   **"Which Player" Questions Based on Specific Actions:** Asking "which player" performed a specific action during a game. Example: "Which player threw the touchdown pass?"
    *   **Date Related Questions:** Asking about dates of events
    *   **Value identification:** Questions that ask for exact quantity of something ("How many total points were scored...")
    *   **Reason Identification:** Questions that ask for the "why" behind a specific event in the text.
    *   **Temporal Reasoning & Numeric Anchoring:** A significant portion of questions requires temporal reasoning (e.g., "How many months did...") often involving calculations based on dates and durations mentioned within the passage. Passages frequently embed relevant numeric values (years, counts, scores) close to relevant entity mentions.
    *   **Event Sequencing and Ordering:** A considerable number of questions require determining the order of events described in the passage. These questions often present two events and ask which occurred earlier or later.
*   **Question Diversity:** Questions are diverse in terms of what information they require (dates, names, quantities, comparisons, reasons).
*   **Numerical Reasoning:** Many questions require numerical reasoning, such as addition, subtraction, comparison, or counting. Questions often require numerical reasoning or extraction of specific details from the passage. Examples include calculating differences ("How many yards was the difference...") or identifying specific values ("How many total points were scored...").
*   **Answer Types:** Answers are short, usually a person's name, a number, or a short phrase directly derived from the passage. Answers are generally not ambiguous. Answers are fact-based and require extracting specific pieces of information from the passage. The questions aren't open-ended or opinion-based.
*   **Passage Length Variability:** Passages vary significantly in length and complexity, from simple scenarios to entire game summaries. Longer passages and more complex questions (requiring multi-step reasoning or implicit relationship understanding) can lead to failures in either the extraction or generation stages.
*   **Domain Knowledge:** Basic understanding of sports (teams, scoring) is helpful but not strictly required. All information needed is in the passage. The dataset heavily features summaries of sports games (American football in the examples).
*   **Implicit Relationship Understanding:** Some questions require understanding implicit relationships or performing simple calculations (e.g., "How many years after...").
*   **Complex Passages with Multiple Facts:** Passages tend to contain a high density of factual information, requiring accurate parsing and filtering to identify the specific details relevant to the question.
*   **Temporal Reasoning Focus:** A notable portion of the questions require reasoning about time, specifically calculating the difference between dates or identifying chronological order of events described in the passage.
*   **Named Entity Dependency:** The questions heavily rely on correctly identifying and associating named entities (people, organizations, locations) mentioned within the passage. The correct answer often involves identifying relationships between these entities.
*   **Mixed Question Types:** The dataset contains a mix of question types, including temporal reasoning, entity relationship queries, and fact retrieval.
*   **Sports Game Summaries & Calculation/Comparison Questions:** The dataset heavily features summaries of sports games (American football in the examples). A common question type involves calculating or comparing numerical values (points, yards) mentioned within these summaries. The questions often require extracting *specific* numbers and performing a calculation.
*   **Varied Numerical Information:** The numerical information isn't always presented in the same way. For example, it might be explicitly stated ("3-yard touchdown") or implied by game scores ("7-0 lead").

## 2. EFFECTIVE TASK-SPECIFIC STRATEGIES

*   **Chain-of-Thought Decomposition:** Decomposing the problem into smaller, well-defined functions (`determine_question_type`, `extract_relevant_info`, `generate_answer`) is generally effective. This modularity likely helps in focusing the LLM's reasoning at each step.
*   **LLM-Driven Modularity:** Using the LLM for each stage (question type determination, information extraction, and answer generation), rather than hard-coded rules, provides the flexibility needed to handle the diversity of question types.
*   **Directly Tie Extraction to Question Type:** Tailor the information extraction process to the specific question type identified. If it's a comparison question, the extraction should be geared towards finding the specific entities and their attributes to be compared. If the question requires a reason, the extraction should focus on extracting reasons from the text. This improves efficiency and accuracy.
*   **Specialized Agents for CoT:** The Chain-of-Thought (CoT) approach, using specialized agents for question type classification, information extraction, and answer generation, is effective when it works correctly. This breaks down the complex task into smaller, manageable steps. However, reliability needs to be assessed with more data.
*   **Chain-of-Thought Prompting with Examples:** The CoT structure with explicit examples seems promising, likely helping ground the LLM in the specific task by guiding it towards mimicking the format and content of the examples.
*   *The current results don't reveal any specific working strategies due to the low accuracy in iteration 4.* However, *in theory*, decomposing the problem into question type classification, information extraction, and answer generation should be a good approach for question answering over long documents.
*   **Naive Chain-of-Thought Insufficiency:** A simple, multi-stage chain-of-thought approach, while conceptually sound, is insufficient for this task. The "expert" agents, as implemented, lack the precision required for information extraction and reasoning on this type of data. The initial hypothesis that breaking down the problem into question type classification, information extraction, and answer generation would yield good results is rejected. (Iteration 5)

## 3. COMMON FAILURE MODES ON THIS DATASET

*   **Difficulty Locating Relevant Information:** Especially for longer passages, finding the specific sentence or phrase containing the answer can be challenging. Longer passages and more complex questions (requiring multi-step reasoning or implicit relationship understanding) can lead to failures in either the extraction or generation stages.
*   **Incorrect Entity Identification:** Misidentifying the entities (players, teams) referred to in the question within the passage.
*   **Numerical Calculation Errors:** Performing the wrong numerical calculation or misinterpreting the wording of the question leading to incorrect arithmetic.
*   **Stuck in Verification Loop (Generic Answer):** The system gets stuck in a planning or verification stage instead of executing the task, producing canned responses like "Verify that it makes sense to..." or "Verify that the required information is known and possible". This indicates a breakdown between task identification and task execution. This was observed with a chain-of-thought approach using a `verify_answer` function. Example: Question asked for the number of touchdowns, but the system got stuck verifying the *possibility* of finding that information.
*   **Inability to Extract and Compare Numerical Values:** When presented with comparative questions, the system fails to extract the relevant numerical values from the passage and perform the necessary comparison, instead getting stuck on "verifying" that such a comparison is possible. Example: Question asking which team had more yards, and the system failed to extract the yardage numbers for each team.
*   **Failure to Identify Specific Actors in Complex Scenarios:** In questions asking about a specific player's action within a game summary, the system gets stuck on "verifying" the identification of the player instead of extracting the name associated with the event from the passage.
*   **Script Errors:** NameError: name 'question' is not defined. This occurred during script repair. The variable 'question' was not defined in the scope where it was being used (specifically in the `generate_answer` function). This highlights the importance of careful variable scoping and passing in modular code.
*   **Lack of Error Information:** The primary failure mode currently is the lack of error data. Without knowing *why* the system fails, it's impossible to pinpoint specific weaknesses. We don't know if it's failing to classify question types correctly, extract the right information, or generate the answer properly. This makes targeted improvements impossible.
*   **Potential sensitivity to passage and question complexity:** It's likely that longer passages and more complex questions (requiring multi-step reasoning or implicit relationship understanding) could lead to failures in either the extraction or generation stages.
*   **Incorrect Temporal Calculation:** Failing to accurately extract and process temporal information. Example: The system incorrectly calculated the number of months between February and December, latching onto the numbers 12 and 2 but failing to apply the correct logic and adding an extraneous "+ 1".
*   **Misinterpreting Point Differentials:** Failing to extract scores and calculate the difference. Example: The LLM extracted numbers relating to points but failed to identify the point differential correctly, indicating an error in understanding the context and intended calculation.
*   **Erroneous Event Sequencing:** Difficulty in identifying the order of events and their relative timing based on the passage. Example: The system struggles to correctly identify the sequence based on the passage, leading to the selection of the wrong event.
*   **Temporal Calculation Errors:** The primary failure mode (observed in iteration 4) is the inability to accurately calculate time differences. The model struggles to extract the correct dates, convert them to a usable format, and then perform the subtraction to determine the month difference. *Example: Incorrectly calculating the number of months between the PC release of a video game and a later preview at the Target Bullseye Lounge.*
*   **Entity Confusion:** The model incorrectly identifies the relevant entities or confuses their relationships. *Example: In iteration 4, the model identified the wrong person who King Tancred seized power from.* This likely stems from issues in the information extraction step.
*   **Poor Numerical Extraction & Calculation (Iteration 5):** The system struggles to extract the *correct* numbers required for calculations. It extracts irrelevant numbers or misinterprets the relationships between them. *Example 1*: Instead of identifying the specific TD and field goal yardages to subtract, the system just pulls out other numbers and performs an incorrect subtraction. *Example 3*: The system incorrectly sums unrelated yardage values from multiple plays instead of identifying the initial points scored.
*   **Inadequate Temporal Reasoning (Iteration 5):** The system seems to have a weak understanding of temporal relations expressed in the text. *Example 2*: It fails to correctly order the events related to the march on Tehran and the dynasty's corruption. This suggests the need for a deeper understanding of chronological cues and event dependencies.
*   **Indiscriminate Information Extraction (Iteration 5):** The root cause is the "expert at extracting relevant information" agent isn't precise enough. It extracts too much information, and the subsequent steps are unable to filter and use only the necessary parts.

## 4. EXPERIMENT LOG & FINDINGS

*   **2025-05-17 12:44:34:** Initial Dataset Analysis: Comprehensive breakdown of data characteristics, challenges, potential approaches, creative insights, and implementation recommendations. Key recommendations included: (1) Keyword Matching, (2) Sentence Similarity, (3) Information Extraction with Rules, (4) LLM-based Question Answering. Also included a sample prompt structure for text based techniques.
*   **2025-05-17 12:44:44:** SCRIPT ERROR ENCOUNTERED: NameError: name 'question' is not defined. This occurred during script repair in the `generate_answer` function. The variable 'question' was not properly passed or defined within that function's scope.
*   **Iteration 0 Findings:** Chain-of-thought decomposition into question type classification, information extraction, answer generation, and verification was *ineffective*. The sequential chain, as implemented, led to premature "verification" steps that prevented the system from generating concrete answers. The system exhibited an over-reliance on verification, hindering progress. Accuracy was very low.
*   **Iteration 1 Findings:** Chain-of-thought approach (determine type, extract info, generate answer) achieved 100% accuracy, suggesting it is well-suited for this dataset. The modularity likely helps in focusing the LLM's reasoning at each step. Removing the verification step did not negatively impact performance, suggesting it was unnecessary overhead. This confirms the hypothesis that a chain-of-thought approach, leveraging the Gemini LLM for question type determination, information extraction, and answer generation, is highly effective for this dataset.
*   **Iteration 2 Findings:** CoT with specialized agents *can* achieve perfect accuracy (1.00) on a small subset, suggesting the underlying strategy has potential. However, without error data, we cannot determine how robust this approach is. The 1.00 accuracy might be a fluke on a small, easy subset of the data. Reliability is unknown.
*   **Iteration 3 Findings:** The exploitation strategy using a decomposed approach and chain-of-thought prompting achieved only moderate accuracy (0.60). This suggests that while the overall structure is promising, the information extraction and reasoning capabilities need significant improvement. The hypothesis that a structured, multi-step approach would reliably solve these questions was not confirmed.
*   **Iteration 4 Findings:** The decompose-and-generate approach, as implemented, is not yet effective for this dataset. The accuracy of 0.80 indicates that at least one of the sub-tasks (question type classification, information extraction, answer generation) is introducing errors. The current in-context learning examples are insufficient to guide the LLM in performing the required temporal reasoning and complex entity relationship extraction. The LLM is either misinterpreting the examples or failing to generalize from them effectively.
*   **Iteration 5 Findings:** A naive chain-of-thought implementation, without more precise extraction and reasoning, performs poorly on this dataset. The accuracy was low, indicating significant issues with information extraction and temporal reasoning.

## 5. NEXT RESEARCH DIRECTIONS

*   **Prioritize Error Logging and Analysis:** The *most critical* next step is to implement robust error logging that captures the following for each failure:
    *   The original question and passage.
    *   The predicted question type.
    *   The extracted information.
    *   The generated answer.
    *   The ground truth answer.
    *   The exact error message or exception that occurred.
*   **Analyze Error Data:** Once error data is available, analyze it to identify the most frequent failure modes (e.g., incorrect question type classification, poor information extraction for certain question types, hallucination during answer generation).
*   **Targeted Improvements:** Based on the error analysis, focus on improving the weakest components of the pipeline. This might involve:
    *   Providing more training examples for question type classification.
    *   Refining the information extraction prompts or logic.
    *   Improving the answer generation prompts to reduce hallucination.
*   **Introduce Complexity & Ambiguity:** Introduce more complex or ambiguous questions to test the limits of the current approach.
*   **Evaluate on Larger Dataset:** Evaluate the approach on a larger, more diverse dataset to ensure its robustness and generalizability.
*   **Prompt Engineering Exploration:** Explore the impact of different prompt engineering techniques for each step (determine type, extract info, generate answer) to potentially improve efficiency or reduce reliance on the LLM.
*   **Cost Metric Analysis:** Consider adding a cost metric (e.g., number of LLM calls) to evaluate the efficiency of the approach.
*   **Error Case Creation:** Create error cases in the dataset to evaluate the robustness of the approach. This could involve questions with missing information, ambiguous wording, or requiring deeper inference.
*   **Address Scope Issues:** Debug and resolve the scope issue that caused the NameError in the `generate_answer` function. Ensure that the `question` variable is properly passed and accessible within the function's scope.
*   **Prioritize Answer Generation:** Refocus the system on generating a *tentative* answer first, even if it's potentially incorrect. Subsequent steps can then refine or correct this answer. Shift the emphasis from verification to execution.
*   **Implement Numerical Reasoning Module:** For numerical comparison questions, incorporate a specific module designed for numerical extraction and comparison. This module should be triggered by the question type classifier and should handle the parsing and comparison of numbers found in the passage. This module should handle the parsing and comparison of numbers found in the passage.
*   **Test on Simpler Subsets:** Before scaling, test the adapted approach on smaller, simpler subsets of the data containing only one or two question types to isolate the effectiveness of the changes.
*   **Enhance Information Extraction Robustness:** Focus on improving the reliability of the `extract_relevant_info` function, with specific attention on numerical values and relationships, temporal phrases, and key entities. Incorporate techniques such as:
    *   **Fine-tuning a NER (Named Entity Recognition) model:** Train a model to identify specific entities (dates, locations, people, numerical values).
    *   **Regular Expression-based extraction:** Implement regular expressions to robustly extract numerical values and dates.
    *   **Contextual filtering:** Add logic to filter extracted information based on keywords in the question (e.g., if the question asks about "months," prioritize extracting date ranges).
*   **Improve Temporal Reasoning:** Implement specific modules or techniques to improve temporal reasoning. This could involve:
        *   Developing a robust date extraction component that can identify and normalize date mentions in the passage.
        *   Using a dedicated date calculation library or API to perform the month difference calculation.
        *   Providing more explicit examples of temporal reasoning in the in-context learning prompts.
    *   Explicitly demonstrate how to calculate time differences and order events in the chain-of-thought examples.
*   **Improve Entity Extraction and Linking:** Focus on improving the accuracy of the information extraction component, especially for named entities and their relationships. This could involve:
        *   Fine-tuning a named entity recognition (NER) model on a dataset relevant to the domain of the passages.
        *   Using a knowledge graph or entity linking service to resolve entity ambiguities.
        *   Adding more explicit examples of entity linking to the in-context learning prompts.
*   **Introduce Validation Steps:** Implement validation steps after information extraction to check the reasonableness and consistency of extracted values before passing them to the answer generation phase.
*   **Debug Sub-task Performance:** Break down the end-to-end accuracy by evaluating the performance of each sub-task independently (question type classification, information extraction, answer generation). This will help pinpoint the bottleneck in the pipeline.
*   **Increase In-Context Examples:** Experiment with increasing the number and diversity of in-context learning examples, especially for complex reasoning tasks like temporal calculation and entity relationship identification.
*   **Focus on Precise Information Extraction (Iteration 5):** The primary focus needs to be on improving the information extraction step. This likely requires a more sophisticated approach than a simple prompt. Consider these specific techniques:
    *   **Question-Aware Extraction:** Condition the information extraction on a *deeper* understanding of the question's requirements. The prompt for extraction should incorporate the question's intent (e.g., "Extract only the yards for touchdowns and field goals mentioned in the passage").
    *   **Named Entity Recognition (NER) with Type Constraints:** Use NER to identify numerical values, but *constrain* the entities based on the question type. For example, if the question asks about points, only consider NER results that are directly associated with scoring plays (touchdowns, field goals).
    *   **Relationship Extraction:** Implement a relationship extraction step to identify the *relationship* between the extracted entities and the question's targets. This could involve identifying the player, the type of play (run, pass, field goal), and the resulting score.

*   **Temporal Reasoning Module (Iteration 5):** For temporal reasoning questions, consider a dedicated module that explicitly identifies and orders events within the passage. This could involve techniques like temporal tagging or dependency parsing.
*   **Fine-tuning or Few-Shot Learning for Extraction (Iteration 5):** Fine-tune a model specifically for extracting relevant information from sports summaries or use a more robust few-shot learning strategy with more diverse examples to guide the extraction agent.
*   **Rethink Prompting (Iteration 5):** The current prompting strategy of labeling agents as "experts" may be insufficient. Experiment with more detailed and directive prompts that guide the extraction and reasoning processes more explicitly. Include more detailed examples in the prompt itself.
```
        

            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        

            TOP PERFORMING APPROACHES TO BUILD UPON:
            
TOP PERFORMING APPROACH #1:
Iteration: 1
Accuracy: 1.00
Approach Summary: The script uses a chain-of-thought approach to answer questions by first determining the question type, then extracting relevant information, and finally generating the answer using the Gemini LLM. The problem is decomposed into three functions: `determine_question_type`, `extract_relevant_info`, and `generate_answer`, each acting as an agent with a specific role. The `call_llm` function is used to interface with the Gemini model, with the main function orchestrating the calls. The workflow is: main -> determine_question_type -> extract_relevant_info -> generate_answer, with each step passing information to the next.

FULL SCRIPT TO REFINE:
```python
import os
import re
import math

def main(question):
    """
    This script solves questions based on a given passage by:
    1. Determining the question type with examples.
    2. Extracting the relevant information with examples.
    3. Generating the answer with examples.

    The 'verify_answer' step has been removed to avoid getting stuck in a verification loop.
    """

    # Step 1: Determine the question type
    question_type = determine_question_type(question)
    if "Error" in question_type:
        return question_type  # Return error message

    # Step 2: Extract relevant information from the passage
    extracted_info = extract_relevant_info(question, question_type)
    if "Error" in extracted_info:
        return extracted_info

    # Step 3: Generate the answer
    generated_answer = generate_answer(extracted_info, question_type, question)
    if "Error" in generated_answer:
        return generated_answer

    return generated_answer # Directly return the generated answer

def determine_question_type(question):
    """Determine the type of the question (numerical, identification, etc.) with examples."""
    system_instruction = "You are an expert at classifying question types."
    prompt = f"""
    Determine the type of question given the following examples. Return the type only.

    Example 1:
    Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
    Type: Numerical

    Example 2:
    Question: Who caught the final touchdown of the game?
    Type: Identification

    Example 3:
    Question: Which star has a smaller mass, Nu Phoenicis or Gliese 915?
    Type: Comparative

    Question: {question}
    Type:
    """
    try:
        question_type = call_llm(prompt, system_instruction)
        if not question_type:
            return "Error: Could not determine question type"
        return question_type
    except Exception as e:
        return f"Error: {str(e)}"

def extract_relevant_info(question, question_type):
    """Extract relevant information from the passage with examples, tailored to question type."""
    system_instruction = "You are an expert at extracting relevant information."
    prompt = f"""
    Extract relevant information from the passage based on the given question type.
    Return the extracted information as a plain text summary.

    Example 1:
    Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
    Type: Numerical
    Extracted Info: Chris Johnson's first touchdown yards, Jason Hanson's first field goal yards.

    Example 2:
    Question: Who caught the final touchdown of the game?
    Type: Identification
    Extracted Info: Player who caught the final touchdown.

    Example 3:
    Question: Which star has a smaller mass, Nu Phoenicis or Gliese 915?
    Type: Comparative
    Extracted Info: Mass of Nu Phoenicis, Mass of Gliese 915.

    Question: {question}
    Type: {question_type}
    Extracted Info:
    """
    try:
        extracted_info = call_llm(prompt, system_instruction)
        if not extracted_info:
            return "Error: Could not extract information."
        return extracted_info
    except Exception as e:
        return f"Error: {str(e)}"

def generate_answer(extracted_info, question_type, question):
    """Generate the answer based on extracted information and question type with examples."""
    system_instruction = "You are an expert at generating correct answers."
    prompt = f"""
    Generate an answer to the question based on the extracted information.

    Example 1:
    Extracted Info: Chris Johnson's first touchdown yards = 40, Jason Hanson's first field goal yards = 30.
    Question Type: Numerical
    Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
    Answer: 40 + 30 = 70 yards

    Example 2:
    Extracted Info: Player who caught the final touchdown = Mark Clayton
    Question Type: Identification
    Question: Who caught the final touchdown of the game?
    Answer: Mark Clayton

    Example 3:
    Extracted Info: Mass of Nu Phoenicis = 1.2 solar masses, Mass of Gliese 915 = 0.85 solar masses.
    Question Type: Comparative
    Question: Which star has a smaller mass, Nu Phoenicis or Gliese 915?
    Answer: Gliese 915

    Extracted Info: {extracted_info}
    Question Type: {question_type}
    Question: {question}
    Answer:
    """
    try:
        answer = call_llm(prompt, system_instruction)
        if not answer:
            return "Error: Could not generate answer."
        return answer
    except Exception as e:
        return f"Error: {str(e)}"

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
```

TOP PERFORMING APPROACH #2:
Iteration: 2
Accuracy: 1.00
Approach Summary: The script uses a chain-of-thought approach to answer questions by first determining the question type, then extracting relevant information, and finally generating the answer. Each step uses the `call_llm` function to interact with the Gemini LLM, acting as a specialized agent for question type classification, information extraction, and answer generation respectively. The `main` function orchestrates the workflow: `determine_question_type` identifies the question type, `extract_relevant_info` extracts needed information based on the question and its type, and `generate_answer` formulates the final answer. Error handling is incorporated at each step, returning an error message if a step fails.

Key approach aspects (full code available for reference)

TOP PERFORMING APPROACH #3:
Iteration: 4
Accuracy: 0.80
Approach Summary: The script uses a decompose-and-generate approach to answer questions by classifying the question type, extracting relevant information, and then generating the final answer using the Gemini LLM. It uses in-context learning with examples to guide the LLM in each step. There are three distinct agent roles: a question type classifier, an information extractor, and an answer generator.

The `main` function orchestrates the process by calling `determine_question_type`, `extract_relevant_info`, and `generate_answer` sequentially; these functions formulate specific prompts for the LLM. The `call_llm` function is used to interact with the Gemini LLM, sending prompts and receiving responses.

Key approach aspects (full code available for reference)

            
FULL SCRIPT TO REFINE:
```python
import os
import re
import math

def main(question):
    """
    This script solves questions based on a given passage by:
    1. Determining the question type with examples.
    2. Extracting the relevant information with examples.
    3. Generating the answer with examples.

    The 'verify_answer' step has been removed to avoid getting stuck in a verification loop.
    """

    # Step 1: Determine the question type
    question_type = determine_question_type(question)
    if "Error" in question_type:
        return question_type  # Return error message

    # Step 2: Extract relevant information from the passage
    extracted_info = extract_relevant_info(question, question_type)
    if "Error" in extracted_info:
        return extracted_info

    # Step 3: Generate the answer
    generated_answer = generate_answer(extracted_info, question_type, question)
    if "Error" in generated_answer:
        return generated_answer

    return generated_answer # Directly return the generated answer

def determine_question_type(question):
    """Determine the type of the question (numerical, identification, etc.) with examples."""
    system_instruction = "You are an expert at classifying question types."
    prompt = f"""
    Determine the type of question given the following examples. Return the type only.

    Example 1:
    Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
    Type: Numerical

    Example 2:
    Question: Who caught the final touchdown of the game?
    Type: Identification

    Example 3:
    Question: Which star has a smaller mass, Nu Phoenicis or Gliese 915?
    Type: Comparative

    Question: {question}
    Type:
    """
    try:
        question_type = call_llm(prompt, system_instruction)
        if not question_type:
            return "Error: Could not determine question type"
        return question_type
    except Exception as e:
        return f"Error: {str(e)}"

def extract_relevant_info(question, question_type):
    """Extract relevant information from the passage with examples, tailored to question type."""
    system_instruction = "You are an expert at extracting relevant information."
    prompt = f"""
    Extract relevant information from the passage based on the given question type.
    Return the extracted information as a plain text summary.

    Example 1:
    Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
    Type: Numerical
    Extracted Info: Chris Johnson's first touchdown yards, Jason Hanson's first field goal yards.

    Example 2:
    Question: Who caught the final touchdown of the game?
    Type: Identification
    Extracted Info: Player who caught the final touchdown.

    Example 3:
    Question: Which star has a smaller mass, Nu Phoenicis or Gliese 915?
    Type: Comparative
    Extracted Info: Mass of Nu Phoenicis, Mass of Gliese 915.

    Question: {question}
    Type: {question_type}
    Extracted Info:
    """
    try:
        extracted_info = call_llm(prompt, system_instruction)
        if not extracted_info:
            return "Error: Could not extract information."
        return extracted_info
    except Exception as e:
        return f"Error: {str(e)}"

def generate_answer(extracted_info, question_type, question):
    """Generate the answer based on extracted information and question type with examples."""
    system_instruction = "You are an expert at generating correct answers."
    prompt = f"""
    Generate an answer to the question based on the extracted information.

    Example 1:
    Extracted Info: Chris Johnson's first touchdown yards = 40, Jason Hanson's first field goal yards = 30.
    Question Type: Numerical
    Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
    Answer: 40 + 30 = 70 yards

    Example 2:
    Extracted Info: Player who caught the final touchdown = Mark Clayton
    Question Type: Identification
    Question: Who caught the final touchdown of the game?
    Answer: Mark Clayton

    Example 3:
    Extracted Info: Mass of Nu Phoenicis = 1.2 solar masses, Mass of Gliese 915 = 0.85 solar masses.
    Question Type: Comparative
    Question: Which star has a smaller mass, Nu Phoenicis or Gliese 915?
    Answer: Gliese 915

    Extracted Info: {extracted_info}
    Question Type: {question_type}
    Question: {question}
    Answer:
    """
    try:
        answer = call_llm(prompt, system_instruction)
        if not answer:
            return "Error: Could not generate answer."
        return answer
    except Exception as e:
        return f"Error: {str(e)}"

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
```

            PREVIOUSLY ATTEMPTED VARIATIONS:
            

            EXPLOITATION GUIDANCE:
            1. Review the error patterns, targeted improvements, and accumulated learnings carefully
            2. CRITICAL: Break down the problem into distinct reasoning steps before modifying code
            3. CRITICAL: Analyze the best scripts to identify which components are working well and which are failing. Focus your improvements on the weak points while preserving successful components.
            4. Maintain the core successful elements of the best approaches
            5. Consider how you can combine strengths from multiple top-performing approaches
            6. CRITICAL: Add EMBEDDED EXAMPLES to EVERY LLM prompt that illustrate:
               - Sample input that resembles the dataset
               - Step-by-step reasoning through the example
               - Properly formatted output
            7. Focus on fixing specific issues identified in previous error analyses. Create an explicit HYPOTHESIS for each targeted improvement, as well as a way to verify if it's successful.
            8. Enhance chain-of-thought reasoning and verification steps. Verification steps should be added to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
            9. Apply the key insights from ACCUMULATED LEARNINGS to enhance the approach
            10. Pay SPECIAL ATTENTION to the weaknesses and improvement suggestions from the capability assessment

            IMPROVEMENT STRATEGY:
            Analyze why the top approaches succeeded where others failed. Identify the key differentiators and strengthen them further.

            SYSTEMATIC ENHANCEMENT APPROACH:
            1. First, identify which specific function or component is underperforming based on error analysis
            2. Examine how error cases differ from successful cases
            3. For each identified weakness, implement a targeted enhancement
            4. Add additional verification steps around modified components
            5. Consider how components interact - ensure improvements don't break successful parts

            Consider enhancing the script with one or more of these patterns:
            - Repeated validation with feedback loops
            - Multi-perspective analysis with synthesis
            - Dynamic input-dependent routing
            - Hybrid approaches combining LLM with deterministic functions
            - Best-of-n solution generation and selection
            - ReAct pattern for interactive reasoning and action
            - If it is unknown how successful a processing state or part of the pipeline is, include verification steps to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
            - Answer checkers to validate the final answer against the problem statement. If the answer is incorrect, the checker can send the answer back to an earlier part of the system for refinement with feedback

            Here's how to call the Gemini API. Use this example without modification and don't invent configuration options:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

            Since this is an EXPLOITATION phase:
            - Build upon what's working well in the best approaches
            - Consider creative combinations of successful techniques from different scripts
            - Make TARGETED improvements to address specific error patterns
            - For EACH key LLM prompt, include a relevant example with:
              * Sample input similar to the dataset
              * Expected reasoning steps
              * Desired output format
            - Apply the knowledge from our accumulated learnings
            - Significantly enhance the script to address weaknesses identified in the capability assessment

            CRITICAL REQUIREMENTS:
            1. The script MUST properly handle all string literals - be extremely careful with quotes and triple quotes
            2. The script MUST NOT exceed 150 lines of code to prevent truncation
            3. Include detailed comments explaining your improvements
            4. EVERY SINGLE LLM PROMPT must include at least one embedded example showing:
               - Sample input with reasoning
               - Desired output format
            5. Make proper use of error handling
            6. Implement robust capabilities to address the specific weaknesses identified in the capability assessment
            7. Do NOT use json.loads() in the LLM calls to process input data. JSON formatting is good to use to structure information as inputs and outputs, but attempting to have functions process JSON data explicitly with strict built-in functionality is error prone due to formatting issues and additional text that appears as documentation, reasoning, or comments. When passing data into another LLM call, you can read it as plain text rather than trying to load it in strict json format, is the better approach.

            Return a COMPLETE, RUNNABLE Python script that:
            1. Has a main function that takes a question string as input and returns the answer string
            2. Makes multiple LLM calls for different reasoning steps
            3. Has proper error handling for API calls
            4. Includes embedded examples in EVERY LLM prompt
            5. Is COMPLETE - no missing code, no "..." placeholders
            6. Closes all string literals properly

            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            