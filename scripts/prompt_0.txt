
            You are developing a Python script to solve problems using LLM reasoning capabilities.
            You are in the EXPLORATION PHASE. You must generate a NEW approach that's different from previous approaches but informed by their successes and failures. With this approach, you will have a specific NEW HYPOTHESIS or variable you are trying to test. Your goal is to see if this new approach works, and you must add verification and validation steps to deduce if this new change is helpful. You may also test RADICAL NEW APPROACHES that are substantially different from previous approaches. 
            
            You should try NEW THINGS:
            
            Break down the problem into smaller pieces
            Think CREATIVELY about how to solve your problem if other approaches aren't working
            Transform data into different formats to see if it helps

            # YOUR TASK
            You are deeply familiar with prompting techniques and the agent works from the literature. 
            Your goal is to maximize the specified performance metrics by proposing interestingly new agents.
            Observe the past discovered agents and scripts carefully and think about what insights, lessons, or stepping stones can be learned from them.
            Be creative when thinking about the next interesting agent to try. You are encouraged to draw inspiration from related agent papers or academic papers from other research areas.
            Use the knowledge from the archive and inspiration from academic literature to propose the next interesting agentic system design.
            THINK OUTSIDE THE BOX.
            

            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "PASSAGE: After a tough loss at home, the Browns traveled to take on the Packers.  In the first quarter, it was all Packers as Aaron Rodgers found Jermichael Finley on a 10-yard pass taking a 7-0 lead followed up by Eddie Lacy running for a 1-yard touchdown for a 14-0 lead.  In the second quarter, the Browns got on the board as Billy Cundiff kicked a 46-yard field goal 14-3 game.  Though the Packers moved ahead by 2 touchdowns at halftime when Mason Crosby nailed a 26-yard field goal for a 17-3 game.  In the third quarter, the Browns would score the only points when Cundiff kicked another field goal this time from 44 yards out for a 17-6 game.  In the fourth quarter, the Packers went back to work as Rodgers found Jordy Nelson on a 1-yard pass for a 24-6 game.  The Browns came within 11 again as Brandon Weeden found Jordan Cameron on a 2-yard pass for a 24-13 game.  The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.\n\nQUESTION: Who caught the final touchdown of the game?",
    "answer": "Jarrett Boykin"
  },
  {
    "id": 1,
    "question": "PASSAGE: (2012 Tennessee Titans season)Despite Chris Johnson breaking out with his first 100+ yard rushing game for the Titans this season, Tennessee was unable to contain a dynamic Texans team, who recorded two touchdowns on defense en route to blowout the former Oilers 38-14.  With the win, the Texans improved to 4-0 for the first time in franchise history. On the opening drive of the game, Matt Schuab threw an 11-yard touchdown to make it 7-0. 2 drives later, Arian Foster scored a touchdown of 4 yards to make it 14-0 at the end of the first. Tennessee got going in the second with a 19-yard touchdown making it 14-7. That was the score at the half. In the third Matt Hasselbeck dropped back but was picked of by Danieal Manning and returned for a touchdown making it 21-7. On Houston's next drive, Schuab threw a 28-yard touchdown to Owen Daniels to make it 28-7 at the end of the third. In the fourth Tennessee fumbled giving it to Houston who took a 31-7 lead. Tennessee then threw an interception returned 63 yards for a touchdown. It was 38-7. Tennessee scored late in the game to make it 38-14 and that was the final score.\n\nQUESTION: How many running backs ran for a touchdown?",
    "answer": "1"
  },
  {
    "id": 2,
    "question": "PASSAGE: Hoping to rebound from their first loss of the season, the Titans flew to Ford Field for a Week 13 Thanksgiving game against the winless Detroit Lions. In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. The Lions would respond with kicker Jason Hanson getting a 53-yard field goal. The Titans would answer with Johnson getting a 58-yard TD run, along with DE Dave Ball returning an interception 15 yards for a touchdown. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run. Detroit would reply with QB Daunte Culpepper completing a 2-yard TD pass to TE Michael Gaines. In the third quarter, kicker Rob Bironas would kick field goals of 41 and 49 yards to put the Titans up by 31, and in the fourth quarter, he would nail field goals of 45 and 43 yards to put the Titans up for good. With the win, Tennessee not only improved to 11-1 on the season, but the Titans were the only AFC South team to defeat all four NFC North teams in interconference play. Center Kevin Mawae, DT Albert Haynesworth, and Running Backs Johnson & White were given CBS's All-Iron Award for their performance.\n\nQUESTION: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?",
    "answer": "59"
  },
  {
    "id": 3,
    "question": "PASSAGE: Still looking for a win the 49ers played on home ground where they played their state rival, the Oakland Raiders. In the first quarter, the 49ers trailed early as kicker Sebastian Janikowski got a 27-yard field goal. Then he made a 24-yard field goal in the 2nd quarter. The 49ers replied with kicker Joe Nedney making a 25-yard field goal. They took the lead in the third quarter with QB Alex Smith making a 32-yard TD pass to WR Michael Crabtree. The Raiders cut the lead with Janikowski making a 40-yard field goal. The 49ers pulled away after Smith found TE Vernon Davis on a 17-yard TD pass.\n\nQUESTION: How many yards longer was Sebastian Janikowski's first field goal compared to his second?",
    "answer": "3"
  },
  {
    "id": 4,
    "question": "PASSAGE: As of 2012, Singapore total fertility rate (TFR) is 1.20 children born per woman, which represents a sub-replacement fertility rate and is one of the lowest in the world. Ethnic Chinese had a ferlility of 1.07 in 2004 (1.65 in 1990), while Malays had a TFR of 2.10 (2.69 in 1990). Both figures declined further in 2006. TFR for Indians was 1.30 in 2004 and 1.89 in 1990.  The Singapore government has launched several highly publicised attempts to raise the fertility rate and increase awareness of the negative effects of an aging population, the elderly (65 and above) had constituted 9.9% of its population in 2012; this proportion is still significantly lower than that of many other developed nations, such as the United States (12%) and Japan (21.2%) .\n\nQUESTION: How many more TFR did Malays have in 2004 compared to Ethnic Chinese TFR in 2004?",
    "answer": "1.03"
  }
]

            HISTORICAL CONTEXT:
            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 0
        - Current explore/exploit balance: 60/40
        - Best accuracy achieved: None

        APPROACH HISTORY (last 0 iterations):
        []

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 0 iterations):
        []

        TARGETED IMPROVEMENTS:
        []
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity ✓
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity ✓
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity ✓
    4. All operations have O(1) average time complexity ✓
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        

            PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS). YOUR APPROACH MUST BE SUBSTANTIVELY DIFFERENT THAN THESE:
            

            LEARNINGS FROM PREVIOUS ITERATIONS:
            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        === INITIAL DATASET ANALYSIS [2025-05-17 13:35:33] ===

    Okay, I've analyzed the provided dataset examples and have identified the following characteristics, challenges, potential approaches, and implementation recommendations.

## DATASET CHARACTERISTICS

*   **Patterns in Questions:**
    *   All questions are presented in a standard format: `"PASSAGE:\n[passage text]\n\nQUESTION: [question text]"`.
    *   The questions are fact-based and require extracting and processing information directly from the passage.
    *   Many questions involve numerical reasoning and comparison (e.g., "How many yards longer...", "How many running backs...").
    *   Some questions ask for specific entities (e.g., "Who caught the final touchdown...").
    *   The questions assume the user has access to and can understand the provided passage.
    *   There's a clear focus on sports statistics and events, although one example deals with demographics.

*   **Patterns in Answers:**
    *   Answers are typically short phrases, numbers, or names.
    *   Answers are always explicitly stated or directly derivable from the passage.
    *   The answers are case-sensitive in some instances.
    *   The answers don't seem to require external knowledge beyond the provided passage.

*   **Structure and Format:**
    *   Input: JSON format containing a `passage` (string) and a `question` (string).
    *   Output: Expected to be a string representing the answer.

*   **Domain Knowledge:**
    *   Basic understanding of sports terminology (football positions, scoring, etc.) is required for the sports-related examples.
    *   General reading comprehension is crucial.
    *   For the demographic example, basic understanding of fertility rates is needed.

*   **Question Types:**
    *   Entity Extraction: Identifying specific players or entities mentioned in the text.
    *   Numerical Comparison: Comparing numerical values (yards, scores, TFR) to determine differences.
    *   Counting: Counting the number of occurrences of an event or entity.

*   **Reasoning Types:**
    *   **Direct Extraction:** Finding the answer directly stated in the passage.
    *   **Simple Arithmetic:** Performing basic calculations (addition, subtraction) using numbers from the passage.
    *   **Logical Deduction:**  Combining information from different parts of the passage to arrive at the answer.

## DATA CHALLENGES

*   **Difficulty:**
    *   The passages can be lengthy, requiring careful reading and attention to detail.
    *   Questions may require combining information scattered throughout the passage.
    *   Numerical reasoning can be error-prone if not handled carefully.
    *   The structure and syntax of the passages can be complex, making it difficult to reliably extract relevant information.

*   **Edge Cases/Complexities:**
    *   Passages with ambiguous or contradictory information.
    *   Questions that require more complex arithmetic operations (e.g., multiplication, division).
    *   Questions with implicit rather than explicit answers, requiring deeper inference.
    *   Handling of units (yards, points, etc.) consistently.

*   **Reasoning Required:**
    *   **Textual Understanding:** Accurately interpreting the meaning of the passage.
    *   **Information Retrieval:** Identifying the specific sentences or phrases that contain the answer.
    *   **Numerical Reasoning:** Performing the necessary calculations to arrive at the correct answer.
    *   **Contextual Awareness:** Using the context of the passage to resolve ambiguities or make inferences.

## POTENTIAL APPROACHES

*   **Solution Strategies:**
    1.  **Keyword-Based Retrieval:** Identify keywords from the question and search for those keywords in the passage. This helps narrow down the relevant sections of the text.
    2.  **LLM-Based Extraction & Reasoning:** Use a large language model (LLM) to directly answer the question based on the passage.  Prompting is key here.
    3.  **Hybrid Approach:** Combine keyword retrieval with LLM reasoning. Use keywords to identify relevant sections, then use the LLM to extract the answer from those sections.

*   **Decomposition:**
    1.  **Question Analysis:** Determine the type of question (e.g., entity extraction, numerical comparison).
    2.  **Passage Filtering:** Identify relevant sections of the passage based on keywords from the question.
    3.  **Information Extraction:** Extract the specific information needed to answer the question from the filtered passage.
    4.  **Answer Generation:**  Generate the final answer based on the extracted information.

*   **Validation Techniques:**
    *   **Consistency Checks:** Verify that the extracted information is consistent with the rest of the passage.
    *   **Unit Analysis:** Ensure that the units are correct and consistent.
    *   **Numerical Validation:** Perform calculations independently to verify the LLM's results.
    *   **Fact Verification:** If possible, compare the answer to external knowledge sources to ensure its accuracy.

*   **Handling Edge Cases:**
    *   **Ambiguous Passages:** Use LLM to identify and flag ambiguous passages.
    *   **Missing Information:** If the answer cannot be found in the passage, return "Not Found" or a similar indicator.
    *   **Complex Calculations:** Break down complex calculations into smaller, more manageable steps.

## CREATIVE INSIGHTS

*   **Non-Obvious Patterns:** The chronological order of events is often important. Focusing on time-related keywords (e.g., "final," "first," "later") can help narrow the search.
*   **Unique Perspectives:** Frame the problem as a summarization task. Ask the LLM to summarize the passage focusing on the information needed to answer the question.
*   **Analogies:** This problem is similar to a search engine that needs to find the most relevant information for a given query.

## IMPLEMENTATION RECOMMENDATIONS

*   **Verification Steps:**
    1.  **Keyword Matching:** Ensure that the keywords extracted from the question are actually present in the identified relevant sections of the passage.
    2.  **Numerical Range Checks:** If the question asks for a number within a specific range, verify that the extracted number falls within that range.

*   **Intermediate Steps/Representations:**
    1.  **Extracted Facts:** Create a list of key facts extracted from the passage, such as player names, scores, and yardage.  Represent these as simple dictionaries/objects.
    2.  **Relevant Sentences:** Store the sentences that contain the extracted facts for later use.

*   **Effective Text Techniques:**

    *   **Prompt Engineering:** Craft prompts carefully to leverage the LLM's reasoning abilities. For example:

        ```python
        def generate_prompt(passage, question):
            return f"""
            Passage:
            {passage}

            Question: {question}

            Answer:
            """
        ```

    *   **Few-Shot Learning:** Provide the LLM with a few examples of question-answer pairs to guide its reasoning.

    *   **Chain-of-Thought Prompting:** Encourage the LLM to explain its reasoning process step-by-step. This can improve accuracy and help identify errors.
        ```python
            return f"""
            Passage:
            {passage}

            Question: {question}

            Let's think step by step. First,... Then... Finally, the answer is:
            """
        ```

    *   **Zero-Shot Reasoning:** Instead of relying on complex code, directly ask the LLM to answer the question based on the passage, using a well-crafted prompt. This approach is often surprisingly effective.

    *   **Text Splitting:** If passages are too long, split them into smaller chunks and process each chunk separately. Be mindful of context loss.

    *   **Post-processing:** Clean up the LLM's output to ensure that it is in the desired format. For example, remove any extra text or formatting that is not part of the answer.  Regular expressions might be helpful here, but keep them simple.

The key is to use the LLM's inherent capabilities to understand the text, extract information, and reason about the answer. Avoid complex programmatic manipulation of the text unless absolutely necessary. Start with simple prompts and gradually increase complexity as needed.


    === END INITIAL DATASET ANALYSIS ===
        

            CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        

            EXPLORATION GUIDANCE:
            1. Review the historical approaches, error patterns, and accumulated learnings carefully
            2. Review the FULL CODE of previous scripts to understand what has already been tried
            3. Design a new approach that is DISTINCTLY DIFFERENT from previous attempts. This approach should have a specific NEW HYPOTHESIS or variable you are trying to test. 
            4. CRITICAL: Include EMBEDDED EXAMPLES directly within your LLM prompts
            5. For each key function, show a complete worked example, or include multiple examples, including:
               - Input example that resembles the dataset
               - Step-by-step reasoning through the example
               - Properly formatted output
            6. Apply the insights from the ACCUMULATED LEARNINGS section to avoid repeating past mistakes
            7. Pay SPECIAL ATTENTION to the weaknesses and improvement suggestions from the capability assessment
            8. Consider implementing one or more of these LLM usage patterns:
               - Repeated validation with feedback loops
               - Multi-perspective analysis with synthesis
               - Dynamic input-dependent routing with an orchestrator
               - Hybrid approaches combining LLM with deterministic functions
               - Best-of-n solution generation and selection
               - ReAct pattern for interactive reasoning and action
               - If it is unknown how successful a processing state or part of the pipeline is, include verification steps to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
               - Answer checkers to validate the final answer against the problem statement. If the answer is incorrect, the checker can send the answer back to an earlier part of the system for for refinement with feedback

            Here's how to call the Gemini API. Use this example without modification and don't invent configuration options:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

            Since this is an EXPLORATION phase:
            - Try a fundamentally different approach to reasoning about the problem. Test a NEW HYPOTHESIS or variable, and add verification steps to deduce if this new change is helpful.
            - THIS IS KEY: Break down the problem into new, distinct reasoning steps based on past performance before you start coding
            - For EACH key LLM prompt, include a relevant example with:
              * Sample input similar to the dataset
              * Expected reasoning steps
              * Desired output format
            - Apply a verifier call to different parts of the pipeline in order to understand what parts of the pipeline of calls is successful and where the system is breaking
            - Pay special attention to addressing the primary issues from previous iterations
            - Ensure your new approach addresses the weaknesses identified in the capability assessment

            CRITICAL REQUIREMENTS:
            1. The script MUST properly handle all string literals - be extremely careful with quotes and triple quotes
            2. The script MUST NOT exceed 150 lines of code to prevent truncation
            3. Include detailed comments explaining your reasoning approach
            4. EVERY SINGLE LLM PROMPT must include at least one embedded example showing:
               - Sample input with reasoning
               - Desired output format
            5. Make proper use of error handling
            6. Implement robust capabilities to address the specific weaknesses identified in the capability assessment
            7. Do NOT use json.loads() in the LLM calls to process input data. JSON formatting is good to use to structure information as inputs and outputs, but attempting to have functions process JSON data explicitly with strict built-in functionality is error prone due to formatting issues and additional text that appears as documentation, reasoning, or comments. When passing data into another LLM call, you can read it as plain text rather than trying to load it in strict json format, is the better approach.

            Return a COMPLETE, RUNNABLE Python script that:
            1. Has a main function that takes a question string as input and returns the answer string
            2. Makes multiple LLM calls for different reasoning steps
            3. Has proper error handling for API calls
            4. Includes embedded examples in EVERY LLM prompt
            5. Is COMPLETE - no missing code, no "..." placeholders
            6. Closes all string literals properly

            This should be FUNDAMENTALLY DIFFERENT from all previous approaches. Do not reuse the same overall structure.

            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            