
            You are developing a Python script to solve problems using LLM reasoning capabilities.
            You are in the EXPLORATION PHASE. You must generate a NEW approach that's different from previous approaches but informed by their successes and failures. With this approach, you will have a specific NEW HYPOTHESIS or variable you are trying to test. Your goal is to see if this new approach works, and you must add verification and validation steps to deduce if this new change is helpful. You may also test RADICAL NEW APPROACHES that are substantially different from previous approaches. 
            
            You should try NEW THINGS:
            
            Break down the problem into smaller pieces
            Think CREATIVELY about how to solve your problem if other approaches aren't working
            Transform data into different formats to see if it helps

            # YOUR TASK
            You are deeply familiar with prompting techniques and the agent works from the literature. 
            Your goal is to maximize the specified performance metrics by proposing interestingly new agents.
            Observe the past discovered agents and scripts carefully and think about what insights, lessons, or stepping stones can be learned from them.
            Be creative when thinking about the next interesting agent to try. You are encouraged to draw inspiration from related agent papers or academic papers from other research areas.
            Use the knowledge from the archive and inspiration from academic literature to propose the next interesting agentic system design.
            THINK OUTSIDE THE BOX.
            

            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "What is the name of the sole survivor of the Saha Airlines 2019 Boeing 707 crash?",
    "answer": "Farshad Mahdavinejad"
  },
  {
    "id": 1,
    "question": "Who was awarded the Oceanography Society's Jerlov Award in 2018?",
    "answer": "Annick Bricaud"
  },
  {
    "id": 2,
    "question": "What are the first names and surnames of the figure skaters who came 21st in the ice dance category at the 2022 Winter Olympics in Beijing?",
    "answer": "Katharina M\u00fcller and Tim Dieck"
  }
]

            HISTORICAL CONTEXT:
            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 22
        - Current explore/exploit balance: 80/20
        - Best accuracy achieved: 0.33 (iteration 1)

        APPROACH HISTORY (last 10 iterations):
        [
  {
    "iteration": 12,
    "strategy": "Exploration",
    "accuracy": 0.3333333333333333,
    "approach": "The script implements a knowledge-base selection and targeted fact verification approach to answer factual questions using the Gemini LLM. It decomposes the problem into knowledge base selection, targeted query generation, information retrieval (simulated), answer extraction, and fact verification. Several LLM agent roles are used, including a knowledge base selector, query generator, search engine, answer extractor, and fact verifier. The script uses `call_llm` to interact with the Gemini LLM, passing different prompts and system instructions for each step. The overall workflow involves selecting a knowledge base, generating a targeted query, simulating information retrieval from the selected knowledge base, extracting a potential answer, and verifying the answer's validity against the knowledge base before returning the result."
  },
  {
    "iteration": 13,
    "strategy": "Exploration",
    "accuracy": 0.0,
    "approach": "The script uses LLMs to answer questions via question decomposition, fact extraction and ranking, direct answer synthesis, and verification. The problem is broken down into sub-questions, candidate facts are extracted for each, the facts are synthesized into an answer, and the answer is verified for accuracy. There are 4 agent roles: an expert at breaking down questions, an expert in extracting facts and their relevance, an expert at synthesizing accurate answers, and a validator of answer accuracy. The function `call_llm` is used to interface with the Gemini LLM. `main` first decomposes the initial question by calling `call_llm`, then extracts candidate facts by calling `call_llm`, synthesizes an answer by calling `call_llm`, and finally verifies the answer by calling `call_llm`."
  },
  {
    "iteration": 14,
    "strategy": "Exploration",
    "accuracy": 0.3333333333333333,
    "approach": "The script uses an iterative context expansion and answer ranking approach to answer questions. It decomposes the problem into contextualization, iterative expansion/extraction, answer ranking, and validation steps, leveraging the LLM in different roles such as context provider, question expander/answer extractor, answer ranker, and validator. The functions used are `call_llm` for LLM interaction and `main` to orchestrate the answering process, with `main` calling `call_llm` multiple times to refine the context and evaluate potential answers. The overall workflow involves initially contextualizing the question, iteratively expanding on that context to extract potential answers, ranking these answers based on the original question and expanded context, and finally validating the top-ranked answer."
  },
  {
    "iteration": 15,
    "strategy": "Exploration",
    "accuracy": 0.3333333333333333,
    "approach": "The script uses LLMs to answer questions by decomposing them into multiple sub-questions, extracting answers with confidence scores, and aggregating those answers. It employs a \"Multi-faceted Question Decomposition and Answer Aggregation with Confidence Scoring\" approach. The script uses the `call_llm` function to generate sub-questions, extract answers, synthesize a final answer, and verify the answer, each time providing a distinct system instruction to give the LLM a specific agent role (question decomposer, answer extractor, information synthesizer, answer accuracy validator). The workflow starts by decomposing the original question, extracting the answers to the sub-questions, aggregating these answers, and finally validating the final answer."
  },
  {
    "iteration": 16,
    "strategy": "Exploitation",
    "accuracy": 0.0,
    "approach": "The script implements a question-answering system using the Gemini LLM and a series of agent roles to decompose the problem. It uses chain-of-thought prompting by sequentially extracting information, generating a search query, retrieving information, extracting the answer, and validating it. The agent roles include information extractor, search query generator, search engine, answer extraction expert, and answer validator.\n\nThe functions used are `call_llm` (interacts with the LLM), `extract_information` (extracts entities and constraints), `generate_search_query` (creates search query), `extract_answer` (extracts answer from search results), `validate_answer` (validates the answer). The `main` function orchestrates the process by calling these functions in sequence to answer a given question. The overall workflow involves processing the question through these stages to arrive at a validated answer."
  },
  {
    "iteration": 17,
    "strategy": "Exploration",
    "accuracy": 0.3333333333333333,
    "approach": "The script answers factual questions using structured decomposition, adaptive querying, and fact verification, leveraging the Gemini LLM. The problem is decomposed into identifying entities, attributes, and constraints. The agent roles include an expert in question decomposition, adaptive query generation, information retrieval, concise answer extraction and a strict fact verifier. The overall workflow involves `call_llm` being used with different system instructions to decompose the question, generate search queries, retrieve information, extract an answer, and verify the answer's validity against the original question. `main` orchestrates these steps, returning the answer if validated or indicating failure."
  },
  {
    "iteration": 18,
    "strategy": "Exploration",
    "accuracy": 0.3333333333333333,
    "approach": "The script uses an LLM-driven knowledge graph traversal approach to answer factual questions. It decomposes the problem into simulated web search, knowledge graph construction, graph traversal, and answer validation, each performed by the LLM acting in a specific role. The `call_llm` function is the core function that interacts with the Gemini model, taking prompts and system instructions as input. The `main` function orchestrates the process: it first generates a search query from the question, then it builds a knowledge graph, traverses it, and validates the answer, using the LLM at each stage."
  },
  {
    "iteration": 19,
    "strategy": "Exploration",
    "accuracy": 0.0,
    "approach": "The script employs an iterative question expansion and answer distillation technique with dual verification to answer factual questions. It decomposes the problem into question expansion, answer extraction, internal consistency verification, and external plausibility verification, using `call_llm` to interact with the Gemini LLM for each step. The agent roles are defined via system instructions, such as \"expert at expanding questions\". The overall workflow involves iteratively expanding the question and extracting potential answers, verifying them internally, and then verifying the final answer externally for plausibility, returning the answer if both verifications pass.\n`call_llm` is used to call the Gemini LLM with different prompts and system instructions. `main` calls `call_llm` multiple times within a loop to iteratively refine the question and answer."
  },
  {
    "iteration": 20,
    "strategy": "Exploration",
    "accuracy": 0.0,
    "approach": "This script addresses factual questions using an LLM-driven approach that includes question transformation, knowledge base retrieval (simulated), and a verification loop. The problem is decomposed into transforming the initial question into a more specific query, simulating a knowledge base search based on the transformed query, and then verifying if the retrieved information accurately answers the original question. Three agents are involved: a question transformation expert, a knowledge base, and an expert validator. The functions used are `call_llm` to interact with the LLM with `main` calling `call_llm` three times sequentially for question transformation, knowledge base retrieval, and verification. The overall workflow transforms the question, retrieves information, and validates the answer."
  },
  {
    "iteration": 21,
    "strategy": "Exploitation",
    "accuracy": 0.0,
    "approach": "The script implements a retrieval-augmented generation approach to answer questions using an LLM. It decomposes the problem into information extraction, search query generation, search (simulated with an LLM), answer extraction with confidence scoring, and answer validation. The agent roles are information extractor, search query generator, search engine, answer extraction expert, and answer validator. The main function orchestrates the process, calling `extract_information` to get entities and constraints, `generate_search_query` to create a search query, `call_llm` to simulate search, `extract_answer` to find the answer and confidence, and `validate_answer` to check the correctness of the response, returning the extracted answer if valid, otherwise an error message."
  }
]

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 3 iterations):
        [
  {
    "iteration": 12,
    "issue": "The most critical problem is the **lack of a robust mechanism for precise information extraction from the retrieved (simulated) search results.** The system can identify relevant information sources, but it cannot consistently extract the specific answer."
  },
  {
    "iteration": 13,
    "issue": "The most critical problem is the **inaccurate retrieval and ranking of relevant facts, coupled with a lack of robust fact verification, leading to the propagation of misinformation and confident, yet incorrect, answers.**"
  },
  {
    "iteration": 14,
    "issue": "The primary issue is the system's susceptibility to contextual misdirection, combined with a weak ability to extract specific numerical answers. It fails to differentiate between relevant and irrelevant contextual details, and its extraction process for numerical values is unreliable, leading to the \"Could not be validated\" error."
  },
  {
    "iteration": 15,
    "issue": "The primary issue is the system's inability to accurately retrieve and synthesize factual knowledge from external sources and integrate it effectively through reasoning over generated sub-questions to produce a precise final answer. It also has issues with generating sensible prompts for the sub-questions."
  },
  {
    "iteration": 16,
    "issue": "The primary issue is a **deficient answer validation mechanism**. The current validation process seems to rely on superficial matching or lacks the capacity to deeply analyze the extracted answer in relation to the original question and constraints. This leads to false positives, where incorrect answers are deemed correct, preventing the system from engaging in error correction or further refinement."
  },
  {
    "iteration": 17,
    "issue": "The most critical problem is the system's **reliance on internal knowledge and its lack of a robust mechanism for information retrieval and integration from external sources**. This is compounded by the system's limited capability to handle numerical values and semantic equivalence accurately."
  },
  {
    "iteration": 18,
    "issue": "The primary issue is the **inability to perform reasoning and inference** beyond direct lookups in the knowledge graph. This limitation restricts the system to only answering questions where the exact answer is explicitly stated, rather than implied or requiring any further processing."
  },
  {
    "iteration": 19,
    "issue": "The primary issue is **inaccurate and imprecise knowledge retrieval coupled with poor answer formulation.** The system appears to struggle with identifying the *exact* piece of information needed to answer the question and struggles to formulate a response that contains ONLY that information. This might be caused by the system failing to filter based on the question context."
  },
  {
    "iteration": 20,
    "issue": "The single most critical problem is **incorrect answer selection due to a failure to adequately rank or filter potential answers based on contextual information and temporal constraints**. The system is retrieving *some* relevant information but failing to pinpoint the *most accurate* answer within that set."
  },
  {
    "iteration": 21,
    "issue": "The most critical problem is the **overly trusting nature of the solution verification process**, which leads to the validation of incorrect information extracted from search results. This is coupled with a **lack of critical assessment of the search results**."
  }
]

        TARGETED IMPROVEMENTS:
        [
  "Employ techniques such as abductive reasoning to infer missing information based on the constraints.",
  "Use structured data more effectively:** If the underlying knowledge base contains structured data (e.g., tables, knowledge graphs), leverage this structure to improve the accuracy of knowledge retrieval. This involves designing queries that target specific fields and relationships within the structured data.",
  "The source reliability for the candidate answer.",
  "Implement a \"confidence score\" for answers:** The system could assign a confidence score to each potential answer based on the quality of the evidence and the certainty of the information. If the confidence score is below a threshold, the system should refrain from providing an answer. This would address cases like sample 2 where the system returns \"No answer\" when it isn't certain.",
  "The presence of relevant keywords and entities.",
  "Improve question understanding with better NLP techniques:** Enhance the model's ability to understand the question's intent and the type of answer required (e.g., \"specific year,\" \"name of person,\" etc.). This could involve using more sophisticated NLP techniques like dependency parsing or semantic role labeling.",
  "Implement a more robust knowledge filtering mechanism:** After retrieving information, add a filtering step that removes irrelevant details based on the question's focus. This might involve analyzing the question's keywords and using them to filter the retrieved knowledge.",
  "Semantic similarity between the answer and the question.",
  "The fulfillment of any explicit or implicit constraints (e.g., temporal constraints, specific dates).",
  "Fine-tune the answer generation process:** Implement a post-processing step to ensure the answer is concise and directly addresses the question. This step could involve identifying and removing redundant or irrelevant phrases."
]
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity ✓
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity ✓
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity ✓
    4. All operations have O(1) average time complexity ✓
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        

            PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS). YOUR APPROACH MUST BE SUBSTANTIVELY DIFFERENT THAN THESE:
            
PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS):

=== SCRIPT FROM ITERATION 21 (Exploitation, ACCURACY: 0.00) ===
Approach: The script implements a retrieval-augmented generation approach to answer questions using an LLM. It decomposes the problem into information extraction, search query generation, search (simulated with an LLM), answer extraction with confidence scoring, and answer validation. The agent roles are information extractor, search query generator, search engine, answer extraction expert, and answer validator. The main function orchestrates the process, calling `extract_information` to get entities and constraints, `generate_search_query` to create a search query, `call_llm` to simulate search, `extract_answer` to find the answer and confidence, and `validate_answer` to check the correctness of the response, returning the extracted answer if valid, otherwise an error message.

```python
import os
import re
import math

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response."""
    try:
        from google import genai
        from google.genai import types

        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def extract_information(question):
    """Extract key information from the question."""
    system_instruction = "You are an expert information extractor focusing on entities, constraints, and temporal context."
    prompt = f"""
    Extract the key entities, constraints, and temporal context from the following question.

    Example 1:
    Question: What is the capital of the country where the Great Barrier Reef is located?
    Entities: Great Barrier Reef
    Constraints: Location is a country, seeking its capital
    Temporal Context: None

    Example 2:
    Question: How many corners did Barcelona take in the Champions League semi-final match between Barcelona and Milan on April 27, 2006?
    Entities: Barcelona, Champions League, Milan
    Constraints: Corners taken by Barcelona, in that specific match
    Temporal Context: April 27, 2006

    Example 3:
    Question: Who won the Eddington Medal in 1993?
    Entities: Eddington Medal
    Constraints: Seeking the winner
    Temporal Context: 1993

    Question: {question}
    Entities, Constraints, and Temporal Context:
    """
    return call_llm(prompt, system_instruction)

def generate_search_query(question, extracted_info):
    """Generate a search query."""
    system_instruction = "You are a search query generator, focusing on precision and temporal relevance."
    prompt = f"""
    Generate a search query to answer the question, using the extracted information.

    Example 1:
    Question: What is the capital of Australia?
    Extracted Info: Australia, capital, None
    Search Query: "capital of Australia"

    Example 2:
    Question: How many corners did Barcelona take in the Champions League semi-final match between Barcelona and Milan on April 27, 2006?
    Extracted Info: Barcelona, Champions League, Milan, corners, April 27, 2006
    Search Query: "Barcelona Milan Champions League April 27 2006 corner kicks"

    Example 3:
    Question: Who won the Eddington Medal in 1993?
    Extracted Info: Eddington Medal, winner, 1993
    Search Query: "Eddington Medal winner 1993"

    Question: {question}
    Extracted Info: {extracted_info}
    Search Query:
    """
    return call_llm(prompt, system_instruction)

def extract_answer(question, search_results):
    """Extract the answer with a confidence score."""
    system_instruction = "You are an answer extraction expert, focusing on precision and factual correctness."
    prompt = f"""
    Extract the answer to the question from the search results and provide a confidence score (1-10).

    Example 1:
    Question: What is the capital of Australia?
    Search Results: Canberra is the capital city of Australia.
    Answer: Canberra (Confidence: 10)

    Example 2:
    Question: How many corners did Barcelona take in the Champions League semi-final match between Barcelona and Milan on April 27, 2006?
    Search Results: Barcelona took 3 corners in the match.
    Answer: 3 (Confidence: 10)

    Example 3:
    Question: Who won the Eddington Medal in 1993?
    Search Results: Leon Mestel won the Eddington Medal in 1993.
    Answer: Leon Mestel (Confidence: 10)

    Question: {question}
    Search Results: {search_results}
    Answer:
    """
    return call_llm(prompt, system_instruction)

def validate_answer(question, answer):
    """Validate if the extracted answer is correct."""
    system_instruction = "You are a strict answer validator, focusing on factual correctness and temporal accuracy."
    prompt = f"""
    Validate if the extracted answer is correct and satisfies the question's requirements, including temporal context. Provide a detailed explanation.

    Example 1:
    Question: What is the capital of Australia?
    Answer: Canberra (Confidence: 10)
    Validation: VALID - The answer is correct. Australia's capital is Canberra.

    Example 2:
    Question: How many corners did Barcelona take in the Champions League semi-final match between Barcelona and Milan on April 27, 2006?
    Answer: 3 (Confidence: 10)
    Validation: VALID - The answer is correct. Barcelona took 3 corners.

    Example 3:
    Question: Who won the Eddington Medal in 1993?
    Answer: Leon Mestel (Confidence: 10)
    Validation: VALID - The answer is correct. Leon Mestel won the Eddington Medal in 1993.

    Question: {question}
    Answer: {answer}
    Validation:
    """
    return call_llm(prompt, system_instruction)

def main(question):
    """Main function."""
    try:
        # Extract information
        extracted_info = extract_information(question)
        print(f"Extracted Info: {extracted_info}")

        # Generate search query
        search_query = generate_search_query(question, extracted_info)
        print(f"Search Query: {search_query}")

        # Simulate information retrieval
        search_results = call_llm(search_query, "You are a helpful search engine that provides concise, factual information.")
        print(f"Search Results: {search_results}")

        # Extract answer
        extracted_answer_raw = extract_answer(question, search_results)
        print(f"Extracted Answer (raw): {extracted_answer_raw}")
        
        # Split out answer and confidence score
        try:
            extracted_answer = extracted_answer_raw.split('(Confidence:')[0].strip()
            confidence = int(extracted_answer_raw.split('(Confidence:')[1].replace(')','').strip())
        except:
            extracted_answer = extracted_answer_raw
            confidence = 5

        # Validate answer
        validation_result = validate_answer(question, extracted_answer)
        print(f"Validation Result: {validation_result}")

        if "VALID" in validation_result:
            return extracted_answer
        else:
            return "Could not be validated."
    except Exception as e:
        print(f"Error: {str(e)}")
        return f"Error: {str(e)}"
```

=== SCRIPT FROM ITERATION 20 (Exploration, ACCURACY: 0.00) ===
Approach: This script addresses factual questions using an LLM-driven approach that includes question transformation, knowledge base retrieval (simulated), and a verification loop. The problem is decomposed into transforming the initial question into a more specific query, simulating a knowledge base search based on the transformed query, and then verifying if the retrieved information accurately answers the original question. Three agents are involved: a question transformation expert, a knowledge base, and an expert validator. The functions used are `call_llm` to interact with the LLM with `main` calling `call_llm` three times sequentially for question transformation, knowledge base retrieval, and verification. The overall workflow transforms the question, retrieves information, and validates the answer.

```python
import os
import re
import math

# New Approach: Question Transformation and Knowledge Base Retrieval with Verification Loop
# Hypothesis: Transforming the question into a more specific query and using a verification loop after knowledge base retrieval will improve accuracy.
# This approach tests whether more specific questions and verification steps improves performance.

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question, max_attempts=3):
    """Solve factual questions using Question Transformation and Knowledge Base Retrieval with Verification Loop."""

    # Step 1: Question Transformation (with examples)
    transformation_prompt = f"""
    Transform the question into a more specific and targeted query for a knowledge base.

    Example 1:
    Original Question: What is the capital of Australia?
    Transformed Query: Return the capital city of the country Australia.

    Example 2:
    Original Question: Who choreographed Issey Miyake's produced “Aomori University Men’s Rhythmic Gymnastics Team” performance?
    Transformed Query: Give the name of the choreographer for the "Aomori University Men’s Rhythmic Gymnastics Team" performance produced by Issey Miyake.

    Original Question: {question}
    Transformed Query:
    """
    transformed_query = call_llm(transformation_prompt, system_instruction="You are an expert at question transformation.").strip()

    # Step 2: Knowledge Base Retrieval (simulated)
    knowledge_base_prompt = f"""
    Simulate retrieving information from a knowledge base based on the transformed query.

    Example:
    Query: Return the capital city of the country Australia.
    Knowledge Base Results: Canberra is the capital city of Australia.

    Query: {transformed_query}
    Knowledge Base Results:
    """
    knowledge_base_results = call_llm(knowledge_base_prompt, system_instruction="You are a helpful knowledge base.").strip()

    # Step 3: Verification Loop (with examples)
    answer = "Could not be validated." # Initialize
    for attempt in range(max_attempts):
        verification_prompt = f"""
        Verify if the knowledge base results provide a direct and accurate answer to the original question. If the extracted data contains the information sought in the original question, respond with the correct answer from the knowlege base results. If there isn't enough information, respond with 'insufficient information'.

        Example 1:
        Original Question: What is the capital of Australia?
        Knowledge Base Results: Canberra is the capital city of Australia.
        Answer: Canberra

        Example 2:
        Original Question: What is the wingspan of Eugnosta misella in millimeters?
        Knowledge Base Results: The wingspan of Eugnosta misella is 9-11 mm.
        Answer: 9-11 mm

        Original Question: {question}
        Knowledge Base Results: {knowledge_base_results}
        Answer:
        """
        answer = call_llm(verification_prompt, system_instruction="You are an expert validator.").strip()
        if answer != "insufficient information":
            break

    return answer
```

=== SCRIPT FROM ITERATION 19 (Exploration, ACCURACY: 0.00) ===
Approach: The script employs an iterative question expansion and answer distillation technique with dual verification to answer factual questions. It decomposes the problem into question expansion, answer extraction, internal consistency verification, and external plausibility verification, using `call_llm` to interact with the Gemini LLM for each step. The agent roles are defined via system instructions, such as "expert at expanding questions". The overall workflow involves iteratively expanding the question and extracting potential answers, verifying them internally, and then verifying the final answer externally for plausibility, returning the answer if both verifications pass.
`call_llm` is used to call the Gemini LLM with different prompts and system instructions. `main` calls `call_llm` multiple times within a loop to iteratively refine the question and answer.

```python
import os
import re
import math

# New Approach: Iterative Question Expansion and Answer Distillation with Dual Verification
# Hypothesis: By iteratively expanding the question with related context and distilling potential answers using a dual verification process (internal consistency and external plausibility), we can improve accuracy in factual question answering.
# This approach will test whether iteratively layering context helps refine the answer and if dual verification reduces errors.

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question, max_iterations=3):
    """Solve factual questions using iterative question expansion and answer distillation with dual verification."""

    expanded_question = question
    potential_answer = "No answer"

    for i in range(max_iterations):
        # Step 1: Iterative Question Expansion
        expansion_prompt = f"""
        Expand the question with related context and information.

        Example:
        Question: What is the capital of Australia?
        Expanded Question: What is the capital of the country Australia, and what is its significance?

        Question: {expanded_question}
        Expanded Question:
        """
        expanded_question = call_llm(expansion_prompt, system_instruction="You are an expert at expanding questions.").strip()

        # Step 2: Answer Extraction
        extraction_prompt = f"""
        Extract a concise answer from the expanded question.

        Example:
        Question: What is the capital of the country Australia, and what is its significance?
        Answer: Canberra

        Question: {expanded_question}
        Answer:
        """
        potential_answer = call_llm(extraction_prompt, system_instruction="You are an expert at concise answer extraction.").strip()

        # Step 3: Internal Consistency Verification
        internal_verification_prompt = f"""
        Verify if the answer is consistent with the information present in the expanded question.

        Example:
        Question: What is the capital of the country Australia, and what is its significance?
        Answer: Canberra
        Verification: Consistent - Canberra is mentioned as the capital in the question.

        Question: {expanded_question}
        Answer: {potential_answer}
        Verification:
        """
        internal_verification = call_llm(internal_verification_prompt, system_instruction="You are an expert at verifying internal consistency.").strip()

        if "Inconsistent" in internal_verification:
            potential_answer = "No answer"  # Reset if inconsistent

    # Step 4: External Plausibility Verification
    external_verification_prompt = f"""
    Verify if the potential answer is a plausible and accurate answer to the original question.

    Example:
    Question: What is the capital of Australia?
    Answer: Canberra
    Verification: Plausible - Canberra is widely known as the capital of Australia.

    Question: {question}
    Answer: {potential_answer}
    Verification:
    """
    external_verification = call_llm(external_verification_prompt, system_instruction="You are an expert at verifying plausibility.").strip()

    if "Plausible" in external_verification:
        return potential_answer
    else:
        return "Could not be validated."
```

=== SCRIPT FROM ITERATION 18 (Exploration, ACCURACY: 0.33) ===
Approach: The script uses an LLM-driven knowledge graph traversal approach to answer factual questions. It decomposes the problem into simulated web search, knowledge graph construction, graph traversal, and answer validation, each performed by the LLM acting in a specific role. The `call_llm` function is the core function that interacts with the Gemini model, taking prompts and system instructions as input. The `main` function orchestrates the process: it first generates a search query from the question, then it builds a knowledge graph, traverses it, and validates the answer, using the LLM at each stage.

```python
import os
import re
import math

# New Approach: Knowledge Graph Traversal with LLM-Guided Relation Extraction and Focused Answer Validation
# Hypothesis: Constructing a simplified in-memory knowledge graph from the question and LLM-simulated search results, then traversing it with LLM guidance to extract the answer, will improve accuracy by enabling structured reasoning.
# This approach combines information extraction and structured reasoning. It tests whether a structured intermediate representation improves performance.

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question, max_attempts=3):
    """Solve factual questions using Knowledge Graph Traversal with LLM-Guided Relation Extraction."""

    # Step 1: Simulated Search and Initial Information Extraction
    search_query = call_llm(f"Generate a concise search query for the question: {question}", system_instruction="You are an expert search query generator.")
    search_results = call_llm(f"Simulated web search results for: {search_query}. Focus on concise and relevant results.", "You are a helpful search engine.")

    # Step 2: Knowledge Graph Construction (Simplified, In-Memory)
    kg_construction_prompt = f"""
    Extract entities and relationships from the question and search results to build a simplified knowledge graph.

    Example:
    Question: What is the capital of Australia?
    Search Results: Canberra is the capital of Australia.
    Knowledge Graph:
    {{
        "Australia": {{"relation": "capital", "target": "Canberra"}}
    }}

    Question: {question}
    Search Results: {search_results}
    Knowledge Graph:
    """
    knowledge_graph_str = call_llm(kg_construction_prompt, system_instruction="You are an expert knowledge graph builder.")
    print(f"Initial Knowledge Graph:{knowledge_graph_str}")

    # Step 3: Knowledge Graph Traversal (LLM-Guided)
    traversal_prompt = f"""
    Traverse the knowledge graph to find the answer to the question. Follow relationships to reach the target information.

    Example:
    Question: What is the capital of Australia?
    Knowledge Graph:
    {{
        "Australia": {{"relation": "capital", "target": "Canberra"}}
    }}
    Answer: Canberra

    Question: {question}
    Knowledge Graph: {knowledge_graph_str}
    Answer:
    """
    answer = call_llm(traversal_prompt, system_instruction="You are an expert knowledge graph traversal agent.")
    print(f"Answer after Traversal:{answer}")

    # Step 4: Focused Answer Validation
    validation_prompt = f"""
    Validate if the answer is a correct and complete response to the question, given the knowledge graph and original information.

    Example:
    Question: What is the capital of Australia?
    Answer: Canberra
    Validation: VALID - Canberra is the capital of Australia.

    Question: {question}
    Answer: {answer}
    Validation:
    """
    validation_result = call_llm(validation_prompt, system_instruction="You are a strict answer validator.")

    if "VALID" in validation_result:
        return answer
    else:
        return "Could not be validated."
```

=== SCRIPT FROM ITERATION 17 (Exploration, ACCURACY: 0.33) ===
Approach: The script answers factual questions using structured decomposition, adaptive querying, and fact verification, leveraging the Gemini LLM. The problem is decomposed into identifying entities, attributes, and constraints. The agent roles include an expert in question decomposition, adaptive query generation, information retrieval, concise answer extraction and a strict fact verifier. The overall workflow involves `call_llm` being used with different system instructions to decompose the question, generate search queries, retrieve information, extract an answer, and verify the answer's validity against the original question. `main` orchestrates these steps, returning the answer if validated or indicating failure.

```python
import os
import re
import math

# New Approach: Structured Decomposition and Fact Verification with Adaptive Querying
# Hypothesis: Decomposing the question into structured components, using those components to generate adaptive search queries, and then verifying extracted facts against the original question structure will improve accuracy.
# This approach combines structured analysis with flexible querying.

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question, max_attempts=3):
    """Solve factual questions using Structured Decomposition and Fact Verification with Adaptive Querying."""

    # Step 1: Structured Question Decomposition
    decomposition_prompt = f"""
    Decompose the question into these structured components:
    - Entities: The key objects or concepts in the question.
    - Attributes: The specific properties or characteristics being asked about.
    - Constraints: Any limitations or conditions that must be met.

    Example:
    Question: What is the capital of the country where the Great Barrier Reef is located?
    Decomposition:
    {{
        "Entities": ["Great Barrier Reef"],
        "Attributes": ["capital"],
        "Constraints": ["country"]
    }}

    Question: {question}
    Decomposition:
    """
    decomposition_result = call_llm(decomposition_prompt, system_instruction="You are an expert at structured question decomposition.")

    # Step 2: Adaptive Query Generation
    query_prompt = f"""
    Generate a search query based on the decomposed question. Adapt the query to include the entities, attributes, and constraints.

    Example:
    Decomposition:
    {{
        "Entities": ["Great Barrier Reef"],
        "Attributes": ["capital"],
        "Constraints": ["country"]
    }}
    Query: "capital of country with Great Barrier Reef"

    Decomposition: {decomposition_result}
    Query:
    """
    search_query = call_llm(query_prompt, system_instruction="You are an expert at adaptive query generation.")

    # Step 3: Information Retrieval (simulated)
    search_results = call_llm(f"Simulated web search results for: {search_query}. Focus on concise and relevant results.", "You are a helpful search engine.")

    # Step 4: Fact Extraction
    extraction_prompt = f"""
    Extract a concise answer from the search results.

    Example:
    Search Results: Canberra is the capital of Australia.
    Answer: Canberra

    Search Results: {search_results}
    Answer:
    """
    extracted_answer = call_llm(extraction_prompt, system_instruction="You are an expert at concise answer extraction.")

    # Step 5: Fact Verification
    verification_prompt = f"""
    Verify that the extracted answer is a correct and complete answer to the original question, taking into account the original question's entities, attributes, and constraints from the decomposed question.

    Example:
    Question: What is the capital of the country where the Great Barrier Reef is located?
    Answer: Canberra
    Verification: VALID - Canberra is the capital of Australia, where the Great Barrier Reef is located.

    Question: {question}
    Answer: {extracted_answer}
    Verification:
    """

    verification_result = call_llm(verification_prompt, system_instruction="You are a strict fact verifier.")
    print(f"Verification Result: {verification_result}") #Print to see if the last step validated and why
    
    if "VALID" in verification_result:
        return extracted_answer
    else:
        return "Could not be validated."
```


            LEARNINGS FROM PREVIOUS ITERATIONS:
            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        Okay, here's the updated and synthesized version of our learnings, focusing specifically on the given dataset and task. This document will serve as our evolving research log. Given the token limit, I've focused on merging the new learnings concisely and efficiently.

## 1. DATASET PATTERNS & CHARACTERISTICS

*   **Factual and Specific Questions:** Questions are factual and seek specific pieces of information across a wide range of topics.
*   **Emphasis on Proper Nouns and Named Entities:** Questions frequently contain proper nouns and specific dates, making them amenable to information retrieval. Answers are often short phrases, often consisting of one or a few proper nouns (names of people or things). Examples: "Who was the first cardiologist in Kashmir?" "Who murdered the supervillain Monsieur Mallah?". Need to handle generational titles correctly (e.g., "James Vernon the Younger"). Many questions center around named entities like people, places, or creative works.
*   **Explicit Named Entities:** Questions frequently focus on named entities like people, organizations (universities), and biological classifications (genus, species). The questions directly ask *for the name of* these things.
*   **Temporal Specificity:** Questions often require precise temporal information (month, year, range of years), with many questions requiring events within a specific year or even date (e.g., "February 27, 2022"). This is a key characteristic. Example from Iteration 11: "day, month, and year".
*   **Extraction of Specific Dates/Names From Historical Contexts:** The questions often require extracting specific dates or names from historical contexts ("month, day, and year of philosopher Edmund Burke's last day in office").
*   **"Which" Questions Targeting Specific Individuals/Entities with Constraints:** Several questions begin with "Which" and seek a specific individual or entity associated with a particular role or accomplishment, often including specific constraints related to entities, timeframes, or categories (e.g., "Which architect was tasked with finishing the chapel...?"). The constraints are crucial to identifying the correct answer.
*   **"Specific Instance" Questions:** The questions frequently ask for the specific instance of a general category ("Which *specific* ocean was the asteroid 224 Oceana named after?"). This requires high precision.
*   **Complex Noun Phrases:** The questions often include complex noun phrases that require accurate parsing and understanding (e.g., "Aomori University Men’s Rhythmic Gymnastics Team").
*   **Question Types:** Primarily fact-retrieval questions asking for specific details about people, places, or things ("Who...", "What...", "Name of...", "Which...").
*   **Need for Precise Factual Recall:** Questions require precise factual recall, often involving dates, numbers, or names (e.g., "Champions League semi-final match between Barcelona and Milan on April 27, 2006", "What month and year did Apple release the Hey Siri feature?"). Examples from Iteration 11: resolution in pixels, dates of events.
*   **Contextual Specificity:** Questions often contain specific contextual details to narrow the scope of the answer. The "Dragon Age" question requires understanding of character relationships and in-game possibilities. Complex event queries require integrating information from multiple sources (e.g., sports matches, product announcements).
*   **Information Extraction Task:** The dataset tests the ability to extract *very specific* pieces of information from potentially larger contexts. It's not simply about broad topic understanding.
*   **"Who" questions requiring specific names:** Many questions (e.g., "Who was the first cardiologist in Kashmir?") require identifying specific individuals, making accurate name retrieval crucial. A significant portion of questions targets **specific names, titles, or locations.** The answers often involve precise string matching. Example from Iteration 21: figure skater names.
*   **Questions requiring temporal context:** Some questions involve temporal qualifiers. This demands understanding of historical or fictional timelines to extract correct information. Questions frequently involve ordinal numbers (e.g., 21st) or specific years, which should be critical signals.
*   **Questions expecting definitive answers:** The questions expect precise answers (specific dates, names), not general descriptions or related concepts.
*   **Complex Factual Questions:** The dataset contains complex factual questions that often require multi-hop reasoning or accessing multiple pieces of information. Questions often involve multiple entities or relationships (e.g., "the younger daughter of Mehbooba Mufti"). This requires the system to correctly identify and relate these elements to retrieve the final answer.
*   **Multi-part Answers Common:** Several questions require multi-part answers, such as specifying both the award and the category (e.g., "Which award and in which category..."). This necessitates the system to identify and extract multiple pieces of information that are linked together.
*   **Entity Specificity:** Many questions revolve around specific entities like people (e.g., "Anita Sandoval"), projects ("Project Firebreak"), awards ("ISCB Accomplishment by a Senior Scientist Award"), and products ("Horizon Zero Dawn"). The correctness depends heavily on identifying these entities accurately.
*   **Temporal Reasoning:** Some questions require temporal reasoning, understanding dates, and relating events to specific time periods.
*   **Indirect Relationships:** Questions often involve indirect relationships. The answer isn't directly stated but needs to be inferred from the provided information.
*   **Complex Question Structure:** The questions are fact-retrieval based but require understanding relationships and potentially hierarchical information.
*   **Context Dependency:** The answers are not directly present in the question itself; they require accessing and processing external information (simulated knowledge base) and connecting it to the question terms.
*   **Assumed Prior Knowledge and Lack of Context:** The questions often assume background knowledge not explicitly stated in the question itself. For example, the IJCAI award question assumes the user knows what IJCAI stands for and the specific award being referenced (Iteration 15).
*   **Varied Answer Types:** The expected answers encompass different data types, including strings (names, academy titles) and numerical values (years), which necessitate a flexible validation approach.
*   **Multi-hop Reasoning:** The dataset features questions that often require **multi-hop reasoning** and synthesis of information from multiple sources.
*   **Broad Subject Matter:** The questions are varied in subject matter, ranging from history and geography to academia and linguistics, politics, zoology, and performing arts. This requires a broad knowledge base and adaptable information retrieval.
*   **Fact-Seeking and Precise Information:** The questions are fact-seeking, requiring precise information retrieval (dates, ages, publication details).
*   **Name Disambiguation:** Many questions revolve around specific individuals, requiring accurate identification and disambiguation (e.g., Satyanarayan Gangaram Pitroda). The system needs to differentiate between similar names. Questions frequently involve ordinal numbers (e.g., 21st) or specific years, which should be critical signals.
*   **Complex Attribute Retrieval:** The task involves retrieving specific attributes (e.g., award received, position held, professorship duration) associated with entities.
*   **Fact-based, Detail-Oriented Questions:** The questions require precise factual recall, often related to dates, names, or specific versions/patches. Even slight inaccuracies are penalized.
*   **Subject Matter Specificity:** The questions span diverse domains (history, video games, academia), requiring a broad base of knowledge or the ability to quickly acquire it. The "Terraria" example highlights the need for specialized knowledge. Example from Iteration 11: technical or specialized terminology.
*   **Implicit Context:** Some questions, like the Valiant example, require understanding implicit context or relationships. Simply extracting keywords is insufficient.
*   **Numerical Data:** The dataset contains fact-based questions requiring precise answers, often involving numerical data (e.g., resolution in pixels).
*   **Niche Knowledge Domain:** The questions cover very specific areas (video game mechanics, advanced mathematics, historical events in specific fields).
*   **Complex Sentence Structure:** The questions can have complex sentence structures involving conditions, mathematical notation and specific terminology.
*   **Potential for Contradictory Information:** Search results might contain contradictory information (e.g., stating "no survivors" versus a specific name).

## 2. EFFECTIVE TASK-SPECIFIC STRATEGIES

*   **Initial Attempts Ineffective:** The initial strategy of using an LLM for iterative refinement with a validation step resulted in 0% accuracy, indicating the need for significant improvements.
*   **Current Accuracy Very Low:** With a maximum accuracy of 0.33 in experiments 1, 4, 17 and 18 and 0.00 in all other experiments, no current strategies can be considered effectively "working". The underlying framework shows potential but requires significant refinement.
*   **RAG Framework Insufficient:** The initial hypothesis that a basic RAG framework with Gemini is sufficient for this task is rejected.
*   **Fact Verification Approach Needs Active Information Seeking**: The fact verification approach with multi-source integration shows promise, since it decomposes the problem into generating multiple search queries, simulating context retrieval from these queries while verifying their relevance, extracting answers from each context, synthesizing a final answer, and validating that final answer. However, this strategy needs an active information-seeking loop.
*   Decomposing the task into sub-tasks such as information extraction, search query generation, answer extraction, and validation seems like a reasonable approach and may provide a good starting point if further refined.
*   **Concept Expansion is Insufficient:** The approach's reliance on concept expansion alone isn't sufficient to guarantee accurate answer extraction (Experiment 10). While expanding concepts may broaden the search space, it doesn't ensure that the specific piece of information required by the question will be found and correctly extracted.
*   **NONE:** The accuracy in most experiments is 0.00, suggesting no part of the current strategy is effective.
*   **Knowledge Base Selection (Iteration 12):** The knowledge base selection seems to be functioning reasonably well.
*   **Decomposition Potential (Iteration 12):** Using LLMs to decompose the problem into KB selection, query generation, answer extraction, and fact verification shows potential as a framework.
*   **Decomposition Isn't Always Helpful (Iteration 15):** Decomposing the question into sub-questions without a robust mechanism for ensuring that the sub-questions are helpful and comprehensively answered can lead to failures. LLM confidence scores are not reliable.
*   **Structured decomposition, adaptive querying, and fact verification are insufficient** (Iteration 17)
*   **LLM-driven knowledge graph traversal alone is insufficient** for questions requiring anything beyond direct fact retrieval (Iteration 18).
*   **Question transformation and a verification loop in isolation are ineffective.** (Iteration 20)

## 3. COMMON FAILURE MODES ON THIS DATASET

*   **Insufficient Answer Granularity:** The system often returns answers that are correct but lack the precision required. E.g., answering "2023" when the expected answer was "Sep 07, 2023." The LLM needs stronger guidance to prioritize granularity. Disregarding or misinterpreting temporal context leads to incorrect answers.
*   **Inability to Extract Precise Information:** The system often fails to extract the *exact* piece of information required to answer the question. For example, when asked about the years of service, it returns a description of the service instead of the specific years (Iteration 19).
*   **Inability to Perform Temporal Reasoning:** The system fails when a question requires comparing or understanding dates. Example: The Edmund Burke question requires knowing the difference between August 1783 and 8 January 1784, implying reasoning about a timeframe.
*   **Lack of Granularity and Specificity:** The system provides overly general answers when specific details are needed. Example: The Oceana question where "the ocean" was returned instead of "Pacific Ocean." This suggests an issue in answer validation or knowledge graph granularity. The system isn't able to effectively differentiate between broader and more specific concepts.
*   **Incorrect Granularity:** The system sometimes provides answers that are either too broad or too narrow. For example, when asked about the digits of forefeet, it includes an extra digit not specified in the ground truth (Iteration 19).
*   **Incorrect Entity Association:** The system retrieves information related to the correct event but attributes it to the wrong entity. Example: Identifying a different winner of the Eddington Medal (George W. Wetherill instead of Leon Mestel). The knowledge base retrieval must accurately resolve entities and differentiate between similar names or related events. Example from Iteration 21: extracting "Collin Roesler" instead of "Annick Bricaud" for the Jerlov Award question.
*   **Inference Limitations:** Questions requiring inference based on the knowledge graph structure consistently fail. The system can only look up directly stated facts and struggles with multi-hop reasoning or combining information.
*   **Overly Strict Validation:** The primary failure mode is the "Could not be validated" response. This suggests the validation logic is too rigid and relies too heavily on keyword matching, failing to account for semantic similarity or logical equivalence.
*   **Validation Sensitivity:** The primary failure stems from over-sensitive validation logic. The system frequently reports "Could not be validated" even when the answer *might* be present but not in the exact expected format. The validation mechanism struggles with answers that convey the same information in different ways. It is limited by it's ability to see that the system output and golden answer are linked.
*   **Lack of Contextual Understanding During Validation:** The validator likely struggles to relate the retrieved information to the specific constraints in the question. Contextual misinterpretation occurs in event queries.
*   **Dependency on Perfect Information Retrieval:** The system's reliance on simulated retrieval means a slightly off search query can lead to no relevant information, and thus, a validation failure even if the LLM could have reasoned with broader knowledge.
*   **Inability to resolve conflicting information.** The system needs a more robust method for selecting the correct answer when multiple possibilities are presented in the search context. Example: When the search result contains contradictory information (e.g., stating "no survivors" versus a specific name), the system *doesn't detect the contradiction* and may select the wrong information.
*   **Failure to identify the correct entity in search results.** The system struggles with identifying the correct entities when multiple entities might fit the search query. The system doesn't appear to be able to accurately resolve ambiguous search results.
*   **Poor Simulated Search Results:** The low accuracy reinforces the possibility that the simulated search results are not accurate or relevant enough, hindering both information extraction and validation.
*   **Inaccurate Entity Resolution:** The "James Vernon the Younger" example highlights the need to handle generational titles correctly.
*   **Knowledge Graph Misidentification/Hallucination:** The "ISCB Accomplishment by a Senior Scientist Award" example showcases a failure to retrieve the correct information from the knowledge source.
*   **Information Extraction Bottleneck:** The primary failure mode is the inability of the system to extract the correct answer from the provided context. The system struggles to translate expanded concepts into precise, factual answers. For example, it might understand that Santo Domingo is a settlement but fail to identify the specific founding year (1778) (Experiment 10).
*   **Insufficient Context Detection Failure:** The agent struggles when the initially retrieved context (simulated or otherwise) lacks the information required to answer the question.
*   **Passive Behavior Regarding Missing Information:** The system's most significant failure mode is its inability to actively address information gaps.
*   **Lack of Numerical Precision:** The James Vernon example demonstrates a failure in matching numerical answers.
*   **Insufficient validation leads to incorrect answers:** As the primary identified issue is a lack of a robust validation mechanism. The validation process is overly trusting and fails to catch incorrect extractions. Example: The validation process then *fails to flag* an incorrect answer, leading to its acceptance
*   **Inability to handle ambiguity and semantic equivalence.**
*   **Poor Query Generation:** The generated search queries consistently fail to retrieve the necessary information.
*   **Incorrect Entity Attribution:** The system incorrectly attributes information to the wrong entity. This highlights a failure in entity recognition and linking.
*   **Temporal Reasoning Errors:** The system struggles with temporal reasoning, such as determining the start and end years of a professorship.
*   **Inability to Handle Partial Information:** Even when the system extracts *some* relevant information, it fails if it cannot find *all* the required pieces.
*   **Ineffective Search Query Formulation:** The core failure stems from the inability to generate search queries that retrieve relevant information.
*   **Lack of Domain Specificity in Search:** The search queries appear to lack the domain-specific nuance required.
*   **Failure to Handle Implicit Context:** The Valiant example in Experiment 9 illustrates that the search strategy cannot infer the necessary contextual information to formulate an effective search query.
*   **Answer Extraction Failure:** A common failure is returning "No Answer" even when related concepts are identified. This points to a weakness in the answer extraction stage.
*   **Lack of Information Retrieval Linkage:** Even when expanded concepts are relevant, the system fails to connect them to the information needed to answer the question.
*   **Incorrect Precision:** The system struggles with providing answers that match the precise detail required by the dataset (e.g., resolution in pixels) (Iteration 11).
*   **Inability to Extract Specific Temporal Data:** The system failed to extract the exact date of the Hubble Telescope incident, instead providing general information about anomalies. This often happens if the context is murky or if multiple dates are present (Iteration 11).
*   **Insufficient Fact Verification:** The system doesn't effectively cross-reference information from multiple sources to validate the accuracy of its answers (Iteration 11).
*   **Inability to extract exact dates/years (Iteration 12):** The primary failure is the inability to pinpoint the specific year or date from the simulated search results. The LLM often provides a range or an approximation (e.g., "likely 2005 or later") instead of the exact answer.
*   **Insufficient precision in information extraction (Iteration 12):** Even when relevant information is present in the "retrieved" text, the system fails to extract the specific answer.
*   **Reliance on Simulated Results (Iteration 12):** Since the search results are simulated, any errors in the simulation are passed down the chain.
*   **Incorrect Fact Retrieval and Ranking (Iteration 13):** LLM retrieves/ranks incorrect facts. Information retrieval and fact prioritization need improvement.
*   **Inability to Find Relevant Facts (Iteration 13):** LLM fails to find the correct answer, indicating limitations in search strategy or knowledge base.
*   **Synthesis of Incorrect Information (Iteration 13):** The synthesis stage does not correct misinformation provided in the extracted facts.
*   **Lack of Robust Verification (Iteration 13):** Fact verification fails, as incorrect answers are confidently presented.
*   **Contextual Misdirection (Location) (Iteration 14):** The system incorrectly identified "Winterthur" as the location. The system needs improved filtering of relevant vs. irrelevant location information.
*   **Numerical Extraction Failure (Version Number) (Iteration 14):** The system failed to extract the version number "1.1" related to the Terraria patch.
*   **General Numerical Extraction (Iteration 14):** The system demonstrates a weakness in reliably extracting specific numerical values from text.
*   **Unit Conversion and Precision (Iteration 15):** The system failed to provide the correct wingspan for *Eugnosta misella* because it either retrieved the wrong information or failed to convert the units correctly.
*   **Incomplete Sub-question Answering (Iteration 15):** The system failed to answer the IJCAI award question because the LLM agent was unable to provide the sub-questions with adequate information to retrieve the answer.
*   **Inability to Disambiguate Related but Incorrect Information:** The "Avatar" question demonstrates the system's failure to differentiate between related but incorrect information.
*   **Insufficient Error Handling when Search Fails:** In the "college" question, when the search fails to return the correct answer, the system defaults to "I cannot answer the question."
*   **Failure to Retrieve Precise Numerical Data:** The system struggles to accurately retrieve and report numerical values.
*   **Insufficient Information Retrieval for Niche Topics:** The system fails when questions require information from specialized or less readily available sources.
*   **Inability to Verify Semantic Equivalence with Numerical Values:** The "explanation" field in the failed examples indicates a specific problem with semantic equivalence in the context of numerical values.
*   **"No Answer" Response:** The system sometimes defaults to "No answer" even when a correct answer exists within its knowledge base (or could be found). This indicates an issue with confidence or a failure to properly search and retrieve relevant information (Iteration 19).

## 4. EXPERIMENT LOG & FINDINGS

*   **Experiment 0 (Initial Exploration):**
    *   **Strategy:** LLM-based iterative refinement with validation.
    *   **Result:** 0% accuracy.
    *   **Finding:** The initial configuration is inadequate.
*   **Experiment 1 (Agent-based LLM Approach):**
    *   **Strategy:** Basic framework of information extraction, query generation, simulated search, answer extraction, and validation.
    *   **Result:** 0.33 accuracy.
    *   **Finding:** The agent-based LLM approach, in its current form, struggles with reasoning and information synthesis from (simulated) search results. This is the highest accuracy achieved so far.
*   **Experiment 2:**
    *   **Strategy:** Multi-stage reasoning with explicit instructions for entity extraction, knowledge lookup, synthesis, and validation.
    *   **Result:** 0.00 accuracy.
    *   **Finding:** The experiment confirmed the reliance of LLMs on external knowledge sources when answering factual questions.
*   **Experiment 3:**
    *   **Strategy:** RAG (Retrieval Augmented Generation) framework with Gemini model. Multiple agents for query generation, information extraction, and verification with explicit source citation.
    *   **Result:** 0.00 accuracy.
    *   **Finding:** The initial hypothesis that a basic RAG framework with Gemini is sufficient for this task is rejected. Information extraction, specifically, is a critical failure point.
*   **Experiment 4:**
    *   **Strategy:** Fact verification with multi-source integration. Decomposed the problem into generating multiple search queries, simulating context retrieval, extracting answers, synthesizing a final answer, and validating that final answer.
    *   **Result:** 0.33 accuracy.
    *   **Finding:** The simulation of context retrieval proves insufficient. The system cannot passively rely on initial context.
*   **Experiment 5:**
    *   **Strategy:** Iterative refinement and extraction, leveraging LLMs for query formulation, information retrieval, and answer extraction, followed by stringent validation.
    *   **Result:** 0% accuracy.
    *   **Finding:** The initial hypothesis that the LLM-driven iterative refinement and extraction approach would be effective for this factual question answering task is rejected.
*   **Experiment 6:**
    *   **Strategy:** Multi-source fact verification with an attempt to improve validation
    *   **Result:** 0.00 accuracy.
    *   **Finding:** The experiment rejects the hypothesis that a decomposed approach using multiple sources with enhanced validation provides sufficient accuracy on this task.
*   **Experiment 7:**
    *   **Strategy:** Exploration strategy, relying on LLM-based decomposition and targeted retrieval with the *existing* prompt engineering.
    *   **Result:** 0% accuracy.
    *   **Finding:** The current exploration strategy is **unsuccessful** for this dataset.
*   **Experiment 8:**
    *   **Strategy:** chain-of-thought approach, with expert agents and confidence scoring
    *   **Result:** 0% accuracy
    *   **Finding:** Hypothesis Rejected: The hypothesis that a chain-of-thought approach, with expert agents and confidence scoring, would lead to accurate question answering on this dataset was rejected.
*   **Experiment 9:**
    *   **Strategy:** Transforming the original question into sub-questions, each targeting a specific piece of information required for the final answer.
    *   **Result:** 0% accuracy.
    *   **Finding:** Question transformation alone is insufficient if the search queries generated are inadequate.
*   **Experiment 10:**
    *   **Strategy:** Expand Concepts for Information Retrieval. Implemented concept expansion to broaden the search space and improve recall.
    *   **Result:** 0% accuracy.
    *   **Finding:** The approach's reliance on concept expansion alone isn't sufficient to guarantee accurate answer extraction.
*   **Experiment 11:**
    *   **Strategy:** Multi-agent approach, similar to previous experiments, focused on query generation, information retrieval, answer extraction, and validation.
    *   **Result:** 0% accuracy.
    *   **Finding:** The multi-agent approach, as implemented, is insufficient for accurately answering complex, fact-based questions requiring precise information and temporal awareness. Simulated search is not sufficient for this task.
*   **Experiment 12:**
    *   **Strategy:** Exploration strategy, relying on LLM-based decomposition and targeted retrieval with the *existing* prompt engineering.
    *   **Result:** 0% accuracy.
    *   **Finding:** The exploration strategy highlighted the critical bottleneck: **precise information extraction**.
*   **Experiment 13:**
    *   **Strategy:** Question decomposition into sub-questions, followed by LLM fact extraction, synthesis, and verification.
    *   **Result:** 0% accuracy.
    *   **Finding:** Question decomposition alone is insufficient. LLM fact retrieval fails, leading to incorrect syntheses and highlighting the need for external verification and robust fact retrieval mechanisms.
*   **Experiment 14:**
    *   **Strategy:** Iterative context expansion
    *   **Result:** 0.00 accuracy
    *   **Finding:** Iterative context expansion alone does not guarantee accurate answers. The system struggles to filter relevant context and extract precise answers, particularly numerical ones. The current validation method is ineffective.
*   **Experiment 15:**
    *   **Strategy:** Question Decomposition with Sub-question Validation. Explored the effectiveness of decomposing the original question into sub-questions, but with an added focus on validating the relevance and informativeness of the generated sub-questions before proceeding to answer extraction.
    *   **Result:** 0% accuracy
    *   **Finding:** The experiment suggests that simply decomposing the question into sub-questions without a robust mechanism for ensuring that the sub-questions are helpful and comprehensively answered can lead to failures. The overhead of managing sub-questions might outweigh the benefits.
*   **Experiment 16:**
    *   **Strategy:** Sequential chain-of-thought with specific agent roles.
    *   **Result:** 0% accuracy.
    *   **Finding:** The initial hypothesis that a sequential chain-of-thought approach with specific agent roles would provide acceptable accuracy in this dataset has been rejected, given the 0.00 accuracy score. Current LLM interaction is inadequate to produce correct answers.
*   **Experiment 17:**
    *   **Strategy:** Structured decomposition, adaptive querying, and fact verification.
    *   **Result:** 0.33 accuracy
    *   **Finding:** Structured decomposition, adaptive querying, and fact verification was not supported by the results. The system needs a far more effective and reliable mechanism for external information retrieval and integration to handle these questions.
*   **Experiment 18:**
    *   **Strategy:** Simplified in-memory knowledge graph construction and LLM-guided traversal.
    *   **Result:** 0.33 accuracy
    *   **Finding:** The hypothesis that constructing a simplified in-memory knowledge graph and traversing it with LLM guidance will improve accuracy is rejected. The LLM's reasoning capabilities within the current framework are limited. LLM-driven knowledge graph traversal alone is insufficient for questions requiring anything beyond direct fact retrieval.
*   **Experiment 19:**
    *   **Strategy:** Iterative Question Expansion & Dual Verification
    *   **Result:** 0.00 accuracy
    *   **Finding:** The hypothesis that iteratively expanding the question and using dual verification would improve accuracy was rejected. The results suggest that the current implementation of these techniques is not effective for this dataset, potentially due to the complexity of the questions and the need for precise information retrieval.
*   **Experiment 20:**
    *   **Strategy:** Question transformation and verification loop.
    *   **Result:** 0.00 accuracy
    *   **Finding:** The initial hypothesis was that question transformation and a verification loop would improve accuracy. The implemented approach, in its current form, is ineffective for this dataset.
*   **Experiment 21:**
    *   **Strategy:** RAG approach with information extraction, search simulation, answer extraction, and validation components.
    *   **Result:** 0.00 accuracy.
    *   **Finding:** The hypothesis that the current RAG approach could accurately answer fact-retrieval questions from this dataset was rejected. Answer validation is a critical weakness.
*   **Script Repair Errors:**
    *   Multiple errors during script repair attempts highlight issues with validation logic, missing attributes, index errors, and module configurations. (See original document for full list).

## 5. NEXT RESEARCH DIRECTIONS

*   **Strengthen Answer Validation:** Implement more robust checks. Focus on name matching using string similarity metrics (e.g., Levenshtein distance) to detect minor variations or possible misspellings. Ensure the extracted name is actually associated with relevant context (award and year, if applicable).
*   **Improve Search Result Assessment:** Implement a step to assess the "quality" or "relevance" of the simulated search results *before* extracting the answer. If the results seem contradictory or low quality, trigger a re-query or flag the question for human review.
*   **Enhanced Information Extraction:** Improve the `extract_information` function to accurately identify entities and constraints from the question. Pay special attention to ordinal numbers, years, and potential name variations. Consider incorporating external knowledge bases or APIs (if permitted) to help resolve name ambiguities or verify facts.
*   **Enhance Temporal Reasoning:** Implement a mechanism for explicitly encoding and reasoning about temporal information. This could involve adding temporal constraints to the transformed query or using a dedicated temporal reasoning module.
*   **Improve Entity Linking:** Use a more robust entity linking system to accurately resolve and disambiguate entities mentioned in the question. This could involve incorporating a pre-trained entity linking model or improving the entity linking prompts.
*   **Refine Verification Prompts:** The verification prompts need to be far more precise and explicitly check for temporal accuracy, correct entity association, and contextual relevance. The prompts should specifically instruct the LLM to compare and contrast the retrieved information with the original question's context. Consider providing examples of what constitutes a "semantically equivalent" vs. "incorrect" answer.
*   **Consider Reranking Answers:** If the knowledge base retrieves multiple potentially relevant answers, implement a re-ranking mechanism that prioritizes answers based on a confidence score that considers factors such as temporal proximity, entity match accuracy, and contextual relevance.
*   **Fine-tune for Granularity:** If possible, fine-tune the LLM on examples that emphasize the importance of answer granularity, rewarding responses that provide the most specific and detailed information.
*   **Improve Information Extraction Accuracy:**
    *   **Fine-tune the LLM:** Fine-tune the LLM on a dataset of similar question-answer pairs to improve its ability to extract specific information.
    *   **Implement Targeted Filtering:** Develop techniques for filtering the information extracted by the LLM to only include the specific answer requested by the question. Techniques might include span prediction.
*   **Refine Answer Formulation:**
    *   **Constrain Answer Format:** Use prompt engineering or post-processing to constrain the output format to match the expected answer format.
    *   **Improve Semantic Similarity Evaluation:** Implement more robust methods for evaluating semantic similarity between the generated answer and the ground truth.
*   **Address "No Answer" Responses:**
    *   **Implement a Confidence Threshold:** Only return an answer if the system reaches a certain confidence threshold.
    *   **Improve Search and Retrieval:** Refine the search and retrieval mechanisms to ensure that the system can reliably find relevant information for each question. Consider implementing a retrieval augmented generation approach using a vector database created from a high quality knowledge base.
*   **Increase Few-Shot Examples:** Provide more few-shot examples to better guide the LLM to return the correct answers.
*   **Implement Temporal Reasoning:** Adapt the system to better handle dates and time-based reasoning. This could involve incorporating date parsing libraries or modifying the knowledge graph to represent temporal relationships explicitly.
*   **Improve Specificity in Knowledge Graph and Validation:** Enhance the knowledge graph construction to ensure finer-grained distinctions between concepts. Refine the answer validation stage to prioritize precise, specific answers over general ones. Add constraints on the type of answer expected (e.g., "must be a specific ocean name").
*   **Incorporate Explicit Reasoning Steps:** Introduce explicit reasoning steps within the graph traversal process. Instead of relying solely on the LLM, consider adding modules or functions that perform specific reasoning tasks (e.g., pathfinding algorithms, logical inference rules).
*   **Refine Node/Edge Labels:** The LLM struggles with implicit relationship understanding. Ensure that edge labels in the KG are precise and reflect the actual relationship.
*   **Enhance Answer Validation:** Implement a more sophisticated answer validation mechanism that considers semantic similarity, logical reasoning, and contextual understanding. This could involve using techniques like entailment checking or knowledge graph reasoning.
*   **Improve Search Query Generation:** Improve the search query generation logic to prioritize queries that target the specific constraints outlined in the question. Focus on creating queries that are more likely to retrieve relevant and precise information.
*   **Implement Multi-Part Answer Extraction:** Develop specialized routines that can extract and link multiple pieces of information to form a complete answer. This is crucial for questions requiring combined answers, such as the award and category.
*   **Develop Fallback Strategies:** Implement fallback strategies to handle cases where the initial search fails or the extracted answer is deemed incorrect. This could involve reformulating the search query, consulting a different knowledge source, or attempting a more direct approach to answering the question.
*   **Fine-tune Agent Roles:** Re-evaluate and fine-tune the roles of each agent to ensure they are effectively contributing to the overall process. This may involve adjusting the prompts, adding new roles, or modifying the interaction between agents.
*   **Modify LLM prompt strategy:** Current prompt strategy is inadequate and is not providing high accuracy. Experiment with zero shot or few shot learning, as current strategy is resulting in low accuracy.
*   **Implement a Unit Verification and Conversion Module (Iteration 15):** A post-processing step should be added to verify and convert units, ensuring the final answer is in the requested unit.
*   **Improve Sub-question Generation and Validation (Iteration 15):** Focus on strategies for generating more targeted and informative sub-questions. Implement a mechanism to assess the *quality* and *relevance* of sub-questions before passing them to the answer extraction stage.
*   **Experiment with Direct Fact Retrieval Baselines (Iteration 15):** Compare the performance of the decomposition approach against a baseline that directly queries a knowledge base or search engine without decomposition.
*   **Focus on More Robust Knowledge Integration (Iteration 15):** Explore alternative methods for integrating information from sub-questions, such as using a graph-based approach or a more sophisticated reasoning engine.
*   **Improve Numerical Answer Extraction (Iteration 14):** Implement a more robust numerical extraction method that can reliably identify and isolate numerical answers, potentially using regular expressions or dedicated numerical parsing libraries.
*   **Enhance Contextual Filtering (Location) (Iteration 14):** Develop a method to explicitly filter location information based on relevance to the specific event or creation mentioned in the question.
*   **Refine Validation Criteria (Iteration 14):** Relax the validation criteria to allow for minor variations in phrasing or formatting of numerical answers.
*   **Focused Few-Shot Examples (Numerical) (Iteration 14):** Provide few-shot examples that specifically demonstrate how to extract and validate numerical answers from context.
*   **Refine the answer extraction prompt (Iteration 12):** Design a more specific prompt for the answer extraction agent that emphasizes the need for a precise date or year.
*   **Implement a date/year validation mechanism (Iteration 12):** After the answer extraction step, add a rule-based validation mechanism to check if the extracted answer is a valid date/year format.
*   **Improve simulation realism (Iteration 12):** The simulated retrieval step should be designed to more closely mimic how a real-world search engine would return results.
*   **Fine-tune the LLM on date extraction tasks (Iteration 12):** If feasible, fine-tune the LLM on a dataset of question-answer pairs specifically focused on extracting dates and years from text.
*   **Implement Robust Fact Verification:** Integrate a mechanism to compare information from multiple simulated sources, prioritize credible sources, and flag conflicting information.
*   **Improve Search Query Generation:** This is the highest priority. Experiment with strategies such as:
    *   **Domain-Specific Query Templates:** Create templates that automatically add domain-specific terms to search queries.
    *   **Iterative Query Refinement:** Implement a feedback loop where the LLM analyzes the initial search results and refines the query.
    *   **Multi-Query Approach:** Generate multiple search queries for each sub-question, varying the keywords and phrasing.
    *   **Adding search diversity:** Enforce the generation of queries
        

            CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        

            EXPLORATION GUIDANCE:
            1. Review the historical approaches, error patterns, and accumulated learnings carefully
            2. Review the FULL CODE of previous scripts to understand what has already been tried
            3. Design a new approach that is DISTINCTLY DIFFERENT from previous attempts. This approach should have a specific NEW HYPOTHESIS or variable you are trying to test. 
            4. CRITICAL: Include EMBEDDED EXAMPLES directly within your LLM prompts
            5. For each key function, show a complete worked example, or include multiple examples, including:
               - Input example that resembles the dataset
               - Step-by-step reasoning through the example
               - Properly formatted output
            6. Apply the insights from the ACCUMULATED LEARNINGS section to avoid repeating past mistakes
            7. Pay SPECIAL ATTENTION to the weaknesses and improvement suggestions from the capability assessment
            8. Consider implementing one or more of these LLM usage patterns:
               - Repeated validation with feedback loops
               - Multi-perspective analysis with synthesis
               - Dynamic input-dependent routing with an orchestrator
               - Hybrid approaches combining LLM with deterministic functions
               - Best-of-n solution generation and selection
               - ReAct pattern for interactive reasoning and action
               - If it is unknown how successful a processing state or part of the pipeline is, include verification steps to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
               - Answer checkers to validate the final answer against the problem statement. If the answer is incorrect, the checker can send the answer back to an earlier part of the system for for refinement with feedback

            Here's how to call the Gemini API. Use this example without modification and don't invent configuration options:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

            Since this is an EXPLORATION phase:
            - Try a fundamentally different approach to reasoning about the problem. Test a NEW HYPOTHESIS or variable, and add verification steps to deduce if this new change is helpful.
            - THIS IS KEY: Break down the problem into new, distinct reasoning steps based on past performance before you start coding
            - For EACH key LLM prompt, include a relevant example with:
              * Sample input similar to the dataset
              * Expected reasoning steps
              * Desired output format
            - Apply a verifier call to different parts of the pipeline in order to understand what parts of the pipeline of calls is successful and where the system is breaking
            - Pay special attention to addressing the primary issues from previous iterations
            - Ensure your new approach addresses the weaknesses identified in the capability assessment

            CRITICAL REQUIREMENTS:
            1. The script MUST properly handle all string literals - be extremely careful with quotes and triple quotes
            2. The script MUST NOT exceed 150 lines of code to prevent truncation
            3. Include detailed comments explaining your reasoning approach
            4. EVERY SINGLE LLM PROMPT must include at least one embedded example showing:
               - Sample input with reasoning
               - Desired output format
            5. Make proper use of error handling
            6. Implement robust capabilities to address the specific weaknesses identified in the capability assessment
            7. Do NOT use json.loads() in the LLM calls to process input data. JSON formatting is good to use to structure information as inputs and outputs, but attempting to have functions process JSON data explicitly with strict built-in functionality is error prone due to formatting issues and additional text that appears as documentation, reasoning, or comments. When passing data into another LLM call, you can read it as plain text rather than trying to load it in strict json format, is the better approach.

            Return a COMPLETE, RUNNABLE Python script that:
            1. Has a main function that takes a question string as input and returns the answer string
            2. Makes multiple LLM calls for different reasoning steps
            3. Has proper error handling for API calls
            4. Includes embedded examples in EVERY LLM prompt
            5. Is COMPLETE - no missing code, no "..." placeholders
            6. Closes all string literals properly

            This should be FUNDAMENTALLY DIFFERENT from all previous approaches. Do not reuse the same overall structure.

            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            