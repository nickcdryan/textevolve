
            You are developing a Python script to solve problems using LLM reasoning capabilities.
            You are in the EXPLORATION PHASE. You must generate a NEW approach that's different from previous approaches but informed by their successes and failures. With this approach, you will have a specific NEW HYPOTHESIS or variable you are trying to test. Your goal is to see if this new approach works, and you must add verification and validation steps to deduce if this new change is helpful. You may also test RADICAL NEW APPROACHES that are substantially different from previous approaches. 
            
            You should try NEW THINGS:
            
            Break down the problem into smaller pieces
            Think CREATIVELY about how to solve your problem if other approaches aren't working
            Transform data into different formats to see if it helps

            # YOUR TASK
            You are deeply familiar with prompting techniques and the agent works from the literature. 
            Your goal is to maximize the specified performance metrics by proposing interestingly new agents.
            Observe the past discovered agents and scripts carefully and think about what insights, lessons, or stepping stones can be learned from them.
            Be creative when thinking about the next interesting agent to try. You are encouraged to draw inspiration from related agent papers or academic papers from other research areas.
            Use the knowledge from the archive and inspiration from academic literature to propose the next interesting agentic system design.
            THINK OUTSIDE THE BOX.
            

            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "Multi-hop reasoning task:\n\nQuestion: What instrument does Duff McKagan play on Macy Gray's single, Kissed It?\n\nSupporting Documents:\n=== Document 1: The Very Best of Macy Gray ===\nThe Very Best of Macy Gray is the first greatest hits album by American singer and songwriter Macy Gray, released on August 30, 2004 by Epic Records. It contains all singles from Gray's first three studio albums, as well as two previously unreleased tracks (the single \"Love Is Gonna Get You\" and a cover of Aerosmith's 1975 song \"Walk This Way\"), three album tracks, three remixes, and the single \"Demons\", a collaboration with Fatboy Slim from his 2000 album \"Halfway Between the Gutter and the Stars\". The album peaked at number 36 on the UK Albums Chart and charted moderately in other European countries. \n\n=== Document 2: Behind the Player: Duff McKagan ===\nBehind The Player: Duff McKagan is an Interactive Music Video featuring Guns N' Roses and Velvet Revolver bassist Duff McKagan \n\n=== Document 3: Kissed It ===\n\"Kissed It\" is a song by the American soul singer Macy Gray. It is the second US single from her fifth album \"The Sellout\". The song was released digitally on May 24, 2010 in the United States and features the musicians of Velvet Revolver and Guns N' Roses, Slash, Duff McKagan and Matt Sorum. In September 2010, the song peaked on the Italian Airplay Chart at number 62. \n\n=== Document 4: Loaded (band) ===\nLoaded (also known as Duff McKagan's Loaded) is an American rock band from Seattle, Washington, formed in 1999. Since 2001, the band's line-up has included vocalist and rhythm guitarist Duff McKagan (Velvet Revolver and Guns N' Roses), lead guitarist Mike Squires (formerly of Nevada Bachelors and Alien Crime Syndicate) and bassist Jeff Rouse (formerly of Alien Crime Syndicate, Sirens Sister, and Vendetta Red). Since 2009, Isaac Carpenter (formerly of Loudermilk, Gosling, and The Exies) has been the band's drummer, replacing Geoff Reading (formerly of New American Shame and Green Apple Quick Step). \n\n=== Document 5: Velvet Revolver ===\nVelvet Revolver was an American hard rock supergroup consisting of former Guns N' Roses members Slash (lead guitar), Duff McKagan (bass, backing vocals), and Matt Sorum (drums, backing vocals), alongside Dave Kushner (rhythm guitar) formerly of punk band Wasted Youth and Scott Weiland formerly of Stone Temple Pilots. Weiland left the band to rejoin Stone Temple Pilots in 2008. \n\n=== Document 6: Seattlehead ===\n\"Seattlehead\" (also typeset Seattle Head) is a song written by American musician Duff McKagan more popularly known as a song by McKagan's band Loaded, from the album \"Dark Days\", but has also featured on earlier releases by Neurotic Outsiders as well as McKagan's unreleased solo album \"Beautiful Disease\". \n\n=== Document 7: Loaded discography ===\nLoaded (also known as Duff McKagan's Loaded) is an American hard rock band from Seattle, Washington, formed in 1999. Since 2001, the band's line-up has included vocalist and rhythm guitarist Duff McKagan (Velvet Revolver and formerly of Guns N' Roses), lead guitarist Mike Squires (formerly of Nevada Bachelors and Alien Crime Syndicate) and bassist Jeff Rouse (formerly of Alien Crime Syndicate, Sirens Sister and Vendetta Red). Since 2009, Isaac Carpenter (formerly of Loudermilk, Gosling and The Exies) has been the band's drummer replacing Geoff Reading (formerly of New American Shame and Green Apple Quick Step). The band has released 3 studio albums, 1 live album, 1 extended play, 4 singles and 4 music videos. \n\n=== Document 8: Beautiful Disease ===\nBeautiful Disease was to be the second solo album released by then ex-Guns N' Roses's bassist Duff McKagan in 1999. However, it was shelved after a merger between McKagan's parent label Polygram and Universal. \n\n=== Document 9: Demons (Fatboy Slim song) ===\n\"Demons\" is a song by English big beat musician Fatboy Slim, featuring Grammy Award-winning American R&B-soul singer Macy Gray. The song was released as a single from Slim's 2000 album \"Halfway Between the Gutter and the Stars\", and later appeared on Gray's 2004 greatest hits compilation \"The Very Best of Macy Gray\" as well as Slim's 2006 greatest hits compilation \"The Greatest Hits - Why Try Harder\". It contains elements of Bill Withers' 1973 song \"I Can't Write Left-Handed\". The gospel group The Blind Boys of Alabama covered the song on their 2005 album \"Atom Bomb\". Recently, the song was featured in the Netflix series Sense8. \n\n=== Document 10: Neurotic Outsiders ===\nNeurotic Outsiders was a supergroup founded in 1995, consisting of Steve Jones of the Sex Pistols, Matt Sorum and Duff McKagan of Guns N' Roses, and John Taylor of Duran Duran. The first line-up featured Billy Idol and Steve Stevens (together with McKagan and Sorum), but they were soon replaced by Jones and Taylor. The group was originally called Neurotic Boy Outsiders. \n\n\nProvide your answer based on the information in the supporting documents.",
    "answer": "bass"
  },
  {
    "id": 1,
    "question": "Multi-hop reasoning task:\n\nQuestion: Which American popular music and country music singer recorded J. D. Souther song \n\nSupporting Documents:\n=== Document 1: Linda Ronstadt ===\nLinda Maria Ronstadt (born July 15, 1946) is an American popular music and country music singer. She has earned 11 Grammy Awards, three American Music Awards, two Academy of Country Music awards, an Emmy Award, and an ALMA Award, and many of her albums have been certified gold, platinum or multiplatinum in the United States and internationally. She has also earned nominations for a Tony Award and a Golden Globe award. She was inducted into the Rock and Roll Hall of Fame in April 2014. On July 28, 2014, she was awarded the National Medal of Arts and Humanities. \n\n=== Document 2: They Call the Wind Maria ===\n\"They Call the Wind Maria\" is an American popular song with lyrics written by Alan J. Lerner and music by Frederick Loewe for their 1951 Broadway musical, \"Paint Your Wagon\", which is set in the California Gold Rush. Rufus Smith originally sang the song on Broadway, and Joseph Leader was the original singer in London's West End. It quickly became a runaway hit, and during the Korean War, the song was among the \"popular music listened to by the troops\". Vaughan Monroe and his Orchestra recorded the song in 1951, and it was among the \"popular hit singles at the record stores\" that year. It has since become a standard, performed by many notable singers across several genres of popular music. A striking feature of the song in the original orchestration (also used in many cover versions), is a driving, staccato rhythm, played on the string instruments, that evokes a sense of restless motion. \n\n=== Document 3: Eddy Arnold ===\nRichard Edward \"Eddy\" Arnold (May 15, 1918 \u2013 May 8, 2008) was an American country music singer who performed for six decades. He was a Nashville sound (country/popular music) innovator of the late 1950s, and scored 147 songs on the \"Billboard\" country music charts, second only to George Jones. He sold more than 85 million records. A member of the Grand Ole Opry (beginning 1943) and the Country Music Hall of Fame (beginning 1966), Arnold ranked 22nd on Country Music Television's 2003 list of \"The 40 Greatest Men of Country Music.\" \n\n=== Document 4: Albert Campbell (singer) ===\nAlbert Charles Campbell (August 19, 1872 \u2013 January 25, 1947) was an American popular music singer who recorded between the late 1890s and the 1920s. He was best known for his many duo recordings with Henry Burr, and as a member of the Peerless Quartet and other vocal groups, but also recorded successfully as a solo singer both under his own name and under various pseudonyms including Frank Howard. \n\n=== Document 5: J. D. Souther ===\nJohn David Souther, known professionally as J.D. Souther (born November 2, 1945) is an American singer and songwriter. He has written and co-written songs recorded by Linda Ronstadt and the Eagles. \n\n=== Document 6: Riddles in the Sand ===\nRiddles in the Sand is the thirteenth studio album by American popular music singer-songwriter Jimmy Buffett. It was released in September 1984 as MCA 5512 and was produced by noted country music producer Jimmy Bowen and represented a concerted shift toward a more country sound by Buffett. He appeared on the album's cover in typical country singer garb and promoted the album at Fan Fair country music festival in Nashville, Tennessee. The album was originally to have been titled \"Gulf and Western Music\" reflecting the fusion of musical styles seen in much of Buffett's music often called gulf and western music. In the album's liner notes, Jim Harrison says, \"This album has a musical range expanding in an arc from Bob Wills to Bob Marley with the Gulf somehow always there.\" \n\n=== Document 7: One Particular Harbour ===\nOne Particular Harbour is the twelfth studio album by American popular music singer-songwriter Jimmy Buffett. It was released in September 1983 as MCA 5447 and was produced by Buffett and Michael Utley. It was Buffett's first involvement producing an album. Stars On The Water was written by and a minor hit for country music songsmith Rodney Crowell and also covered by Texan country music singer George Strait on his 2001 album, \"The Road Less Traveled\". \n\n=== Document 8: Andy Williams ===\nHoward Andrew Williams (December 3, 1927\u00a0\u2013 September 25, 2012) was an American popular music singer. He recorded 44 albums in his career, 15 of which have been gold-certified and three of which have been platinum-certified. He was also nominated for six Grammy Awards. He hosted \"The Andy Williams Show\", a television variety show, from 1962 to 1971, and numerous TV specials. \"The Andy Williams Show\" garnered three Emmy awards. The Moon River Theatre in Branson, Missouri, is named after the song he is most known for singing\u2014Johnny Mercer and Henry Mancini's \"Moon River\". He sold more than 100 million records worldwide, including 10.5 million certified units in the United States. \n\n=== Document 9: Jimmie Rodgers (pop singer) ===\nJames Frederick Rodgers (born September 18, 1933, Camas, Washington) is an American popular music singer. Rodgers had a brief run of mainstream popularity in the late 1950s with a string of crossover singles that ranked highly on the \"Billboard Pop Singles\", \"Hot Country and Western Sides\" and \"Hot Rhythm and Blues Sides\" charts; in the 1960s, Rodgers had more modest successes with adult contemporary music. \n\n=== Document 10: The Delmore Brothers ===\nAlton Delmore (December 25, 1908 \u2013 June 8, 1964) and Rabon Delmore (December 3, 1916 \u2013 December 4, 1952), billed as The Delmore Brothers, were country music pioneers and stars of the Grand Ole Opry in the 1930s. The Delmore Brothers, together with other brother duos such as the Louvin Brothers, the Blue Sky Boys, the Monroe Brothers (Birch, Charlie and Bill Monroe), the McGee Brothers, and The Stanley Brothers, had a profound impact on the history of country music and American popular music. \n\n\nProvide your answer based on the information in the supporting documents.",
    "answer": "Linda Maria Ronstadt"
  },
  {
    "id": 2,
    "question": "Multi-hop reasoning task:\n\nQuestion: In which six Western European territories have Celtic languages or cultural traits survived?\n\nSupporting Documents:\n=== Document 1: Zeitschrift f\u00fcr celtische Philologie ===\nThe Zeitschrift f\u00fcr celtische Philologie is an academic journal of Celtic studies, which was established in 1897 by the German scholars Kuno Meyer and Ludwig Christian Stern. It was the first journal devoted exclusively to Celtic languages and literature and the oldest significant journal of Celtic studies still in existence today. The emphasis is on (early) Irish language and literature and Continental Celtic languages, but other aspects of Celtic philology and literature (including modern literature) also receive attention. \n\n=== Document 2: Insular Celtic languages ===\nInsular Celtic languages are a group of Celtic languages that originated in Britain and Ireland, in contrast to the Continental Celtic languages of mainland Europe and Anatolia. All surviving Celtic languages are from the Insular Celtic group, including that which is now spoken in Continental Europe; the Continental Celtic languages are extinct. The six Insular Celtic languages of modern times can be divided into: \n\n=== Document 3: Alexei Kondratiev ===\nAlexei Kondratiev (1949\u20132010) was an American author, linguist, and teacher of Celtic languages, folklore and culture. He taught the Irish language and Celtic history at the Irish Arts Center in Manhattan, New York from 1985 until his death on May 28, 2010. Nine editions of his book, \"The Apple Branch\", were published in English and Spanish between 1998 and 2004. At various times, he taught all six of the living Celtic languages. \n\n=== Document 4: Celtic studies ===\nCeltic studies or Celtology is the academic discipline occupied with the study of any sort of cultural output relating to the Celtic people. This ranges from linguistics, literature and art history, archaeology and history, the focus lying on the study of the various Celtic languages, living and extinct. The primary areas of focus are the six Celtic languages currently in use: Irish, Scottish Gaelic, Manx, Welsh, Cornish, and Breton. \n\n=== Document 5: Continental Celtic languages ===\nThe Continental Celtic languages are the Celtic languages, now extinct, that were spoken on the continent of Europe, as distinguished from the Insular Celtic languages of the British Isles and Brittany. \"Continental Celtic\" is a geographic, not a linguistic, grouping of the ancient Celtic languages. The Continental Celtic languages were spoken by the people known to Roman and Greek writers as \"Keltoi\", \"Celtae\", \"Galli\" and \"Galatae\". These languages were spoken in an arc stretching across from Iberia in the west to the Balkans and Anatolia in the east. \n\n=== Document 6: Celtic nations ===\nThe Celtic nations are territories in western Europe where Celtic languages or cultural traits have survived. The term \"nation\" is used in its original sense to mean a people who share a common identity and culture and are identified with a traditional territory. \n\n=== Document 7: Journal of Celtic Linguistics ===\nThe Journal of Celtic Linguistics is a peer-reviewed annual academic journal established in 1992 with the goal of encouraging and publishing original linguistic research in the Celtic languages. The journal is published by the University of Wales Press, but has specialist editors in all six Celtic languages. The current editor-in-chief, since volume 16, is Simon Rodway (Aberystwyth University), who replaced Graham Isaac (National University of Ireland, Galway). \n\n=== Document 8: Amazonian languages ===\nAmazonian languages is the term used to refer to the indigenous languages of \"Greater Amazonia.\" This area is significantly larger than the Amazon and extends from the Atlantic coast all the way to the Andes, while its southern border is usually said to be the Paran\u00e1. The region is inhabited by societies that share many cultural traits but whose languages are characterized by great diversity. There are about 330 extant languages in Greater Amazonia, almost half of which have fewer than 500 speakers. Meanwhile, only Guajiro has a six-digit number of speakers (about 300,000). Of the 330 total languages, about fifty are isolates, while the remaining ones belong to about 25 different families. Most of the posited families have few members. It is this distribution of many small and historically unrelated speech communities that makes Amazonia one of the most linguistically diverse regions in the world. The precise reasons for this unusual diversity have not yet been conclusively determined, but it is noteworthy that Amazonian languages seem to have had fewer than 10,000 native speakers even before the invasion of European colonists wrought havoc on the societies by which they were spoken. Despite the large-scale diversity, the long-term contact among many of the languages of Greater Amazonia has created similarities between many neighboring languages that are not genetically related. The small tribes can speak English but that would be used as one of their secondary languages \n\n=== Document 9: Celtic art ===\nCeltic art is associated with the peoples known as Celts; those who spoke the Celtic languages in Europe from pre-history through to the modern period, as well as the art of ancient peoples whose language is uncertain, but have cultural and stylistic similarities with speakers of Celtic languages. \n\n=== Document 10: Pan Celtic Festival ===\nThe Pan Celtic Festival (Irish: \"F\u00e9ile Pan Cheilteach\" ) is a Celtic-language music festival held annually in the week following Easter, since its inauguration in 1971. The first Pan Celtic Festival took place in Killarney, County Kerry, Ireland. Its aim is to promote the modern Celtic languages and cultures and artists from all six Celtic nations: Brittany, Cornwall, Ireland, Isle of Man, Scotland and Wales. \n\n\nProvide your answer based on the information in the supporting documents.",
    "answer": "Brittany, Cornwall, Ireland, Isle of Man, Scotland and Wales."
  }
]

            HISTORICAL CONTEXT:
            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 3
        - Current explore/exploit balance: 70/10
        - Best accuracy achieved: 0.80 (iteration 0)

        APPROACH HISTORY (last 3 iterations):
        [
  {
    "iteration": 0,
    "strategy": "baseline",
    "accuracy": 0.8,
    "approach": "Simple baseline script: Direct LLM call without sophisticated techniques"
  },
  {
    "iteration": 1,
    "strategy": "explore",
    "accuracy": 0.0,
    "approach": "The script addresses multi-hop reasoning by first summarizing each document and then reasoning across the summaries to answer a question. It employs a verification step to ensure the summaries retain relevant information, retrying summarization if the verification fails. The `main` function orchestrates the process, calling `summarize_document_with_verification` for each document and then `reason_across_summaries` to generate the final answer. `call_llm` is the core function that communicates with the Gemini LLM, while `summarize_document_with_verification` summarizes individual documents and verifies the summaries, and `reason_across_summaries` answers the final question based on the generated summaries."
  },
  {
    "iteration": 2,
    "strategy": "explore",
    "accuracy": 0.0,
    "approach": "The script implements a knowledge graph approach to answer questions using supporting documents by leveraging the Gemini LLM. It decomposes the problem into knowledge extraction and reasoning, assigning the LLM the roles of knowledge extraction expert and question answering system. The `extract_knowledge` function extracts entities and relationships, validates the format of the extracted knowledge, and returns the knowledge in JSON format, calling `call_llm` to generate and validate the knowledge, while `reason_over_knowledge` generates the final answer based on the extracted knowledge, using `call_llm` to generate the answer. The `main` function orchestrates the process by calling the other functions and iterating through the supporting documents to generate an answer."
  }
]

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 3 iterations):
        [
  {
    "iteration": 0,
    "issue": "The primary issue is the system's **inability to synthesize extracted information to provide precise and concise answers** that directly address the core question, even when relevant information is present in the documents."
  },
  {
    "iteration": 1,
    "issue": "The most critical problem is the `test_script_1.py` file calling the `main` function incorrectly. It is calling `answer = module.main(question)` when it should be calling `answer = module.main(question, supporting_documents)`."
  },
  {
    "iteration": 2,
    "issue": "The single most critical problem is the incorrect invocation of the `main()` function within the `test_script_2.py` script.  The `supporting_documents` argument is not being passed, leading to a `TypeError`."
  }
]

        TARGETED IMPROVEMENTS:
        []
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity ✓
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity ✓
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity ✓
    4. All operations have O(1) average time complexity ✓
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```\n\n```python\ndef solve_with_meta_programming(question):
            """
            Advanced: Script generates and executes its own code/prompts dynamically.
            The script acts as its own programmer and prompt engineer.
            """

            # Step 1: Analyze what approach is needed
            strategy_prompt = f"""
            For this problem: {question}

            What's the best approach?
            A) Generate Python code to calculate/process something
            B) Generate specialized LLM prompts for analysis  
            C) Use a hybrid approach with both code and LLM calls

            Explain your choice and what specific code or prompts I should generate.
            """


                analysis_system_prompt = """ 
                You are a problem analysis expert. You are a master of problem analysis and can 
                determine the best approach to solve a problem, understanding the strenghts and 
                weaknesses of LLMs for problem solving, when to delegate a more specific or problem 
                or subproblem to an additional LLM call, and when to write code to solve a problem.
            """
            strategy = call_llm(strategy_prompt, analysis_system_prompt)

            # Step 2: Generate and execute based on strategy
            if "###CODE_ONLY###" in strategy.lower():
                # Generate code dynamically
                code_gen_prompt = f"""
                Problem: {question}
                Strategy: {strategy}

                Write Python code to solve this problem. Include print statements for output.
                Return ONLY the Python code:
                """

                generated_code = call_llm(code_gen_prompt, "You are a Python programmer.")

                # Clean up code if wrapped in markdown
                import re
                code_match = re.search(r'```python\s*\n(.*?)\n```', generated_code, re.DOTALL)
                if code_match:
                    clean_code = code_match.group(1).strip()
                else:
                    clean_code = generated_code.strip()

                # Execute the generated code
                execution_result = execute_code(clean_code)

                # Interpret the execution result
                interpretation_prompt = f"""
                Original problem: {question}
                Generated code: {clean_code}
                Execution result: {execution_result}

                What is the final answer based on these results?
                """

                final_answer = call_llm(interpretation_prompt, "You are a solution interpreter.")
                return final_answer

            elif "###PROMPT_ONLY###" in strategy.lower():
                # Generate specialized prompts dynamically
                prompt_design = f"""
                For this problem: {question}
                Strategy: {strategy}

                Design the most effective prompt to solve this problem:
                """

                specialized_prompt = call_llm(prompt_design, "You are a prompt engineer.")

                # Use the generated prompt
                solution = call_llm(specialized_prompt, "You are an expert problem solver.")
                return solution

            else:  # Hybrid approach
                # Chain code and LLM calls dynamically
                current_result = question

                for step in range(3):
                    # Decide what to do at this step
                    step_decision = call_llm(f"""
                    Step {step + 1} of hybrid approach.
                    Current state: {current_result}

                    What should I do next?
                    - Generate and execute code
                    - Make an LLM analysis call
                    - Provide final answer

                    Choose one and explain exactly what to do.
                    """, "You are a workflow coordinator.")

                    if "final answer" in step_decision.lower():
                        return current_result
                    elif "code" in step_decision.lower():
                        # Generate code for this step
                        step_code_prompt = f"""
                        Based on this decision: {step_decision}
                        Current data: {current_result}

                        Write Python code to process this. Return only the code:
                        """
                        step_code = call_llm(step_code_prompt, "You are a Python programmer.")
                        code_result = execute_code(step_code)
                        current_result = f"Previous: {current_result}\nCode result: {code_result}"
                    else:
                        # Make LLM call for this step  
                        step_analysis = call_llm(f"Analyze this data: {current_result}\nBased on: {step_decision}", "You are an analyst.")
                        current_result = f"Previous: {current_result}\nAnalysis: {step_analysis}"

                return current_result\n```\n\n```python\ndef self_modifying_solver(problem):
            """
            A solver that rewrites its own approach based on intermediate results.
            Advanced meta-programming where the script evolves its strategy.
            """

            strategy = "direct_analysis"
            attempts = 0
            max_attempts = 3

            while attempts < max_attempts:
                attempts += 1

                if strategy == "direct_analysis":
                    # Try direct LLM analysis
                    result = call_llm(f"Solve this problem: {problem}", "You are an expert problem solver.")

                    # Evaluate if this worked
                    evaluation_prompt = f"""
                    Problem: {problem}
                    My attempt: {result}

                    Did this solve the problem correctly? If not, what approach should I try next?
                    Options: computational_approach, step_by_step_breakdown, code_generation
                    """

                    evaluation = call_llm(evaluation_prompt, "You are a solution evaluator.")

                    if "correct" in evaluation.lower() or "solved" in evaluation.lower():
                        return result
                    elif "computational" in evaluation.lower():
                        strategy = "computational_approach"
                    elif "step_by_step" in evaluation.lower():
                        strategy = "step_by_step_breakdown"  
                    else:
                        strategy = "code_generation"

                elif strategy == "computational_approach":
                    # Generate and execute computational code
                    comp_prompt = f"""
                    Problem: {problem}

                    Write Python code to solve this computationally. Include:
                    - Extract relevant numbers or data
                    - Perform calculations
                    - Print results clearly

                    Return only the Python code:
                    """

                    comp_code = call_llm(comp_prompt, "You are a computational programmer.")
                    comp_result = execute_code(comp_code)

                    # Interpret computational result
                    interpretation = call_llm(f"Problem: {problem}\nComputation result: {comp_result}\nFinal answer:", "You are an interpreter.")
                    return interpretation

                elif strategy == "step_by_step_breakdown":
                    # Generate step-by-step solution code
                    breakdown_prompt = f"""
                    Problem: {problem}

                    Write Python code that breaks this problem into steps and solves it methodically:
                    """

                    breakdown_code = call_llm(breakdown_prompt, "You are a systematic programmer.")
                    breakdown_result = execute_code(breakdown_code)

                    # Build final solution based on breakdown
                    final_solution = call_llm(f"Problem: {problem}\nStep-by-step result: {breakdown_result}\nFinal answer:", "You are a problem solver.")
                    return final_solution

                else:  # code_generation strategy
                    # Generate completely custom code for this problem
                    custom_prompt = f"""
                    Problem: {problem}

                    Write custom Python code specifically designed to solve this exact problem type:
                    """

                    custom_code = call_llm(custom_prompt, "You are a custom code generator.")
                    custom_result = execute_code(custom_code)

                    return f"Custom solution result: {custom_result}"

            return "Could not solve after multiple strategy attempts"\n```\n\n```python\ndef adaptive_chain_solver(question):
            """
            Chains multiple code generations and LLM calls adaptively.
            Each step decides what the next step should be.
            """

            current_data = question
            step_count = 0
            max_steps = 5

            while step_count < max_steps:
                step_count += 1

                # Decide what to do at this step
                decision_prompt = f"""
                Step {step_count}: Working with: {current_data}

                What should I do next to solve this problem?
                A) Generate and execute Python code to process/calculate something
                B) Generate a specialized LLM prompt for analysis
                C) I have enough information - provide final answer

                Choose A, B, or C and explain exactly what to do:
                """

                decision = call_llm(decision_prompt, "You are an adaptive workflow coordinator.")

                if "C)" in decision or "final answer" in decision.lower():
                    # Generate final answer
                    final_prompt = f"""
                    Original question: {question}
                    Current data/results: {current_data}

                    Based on all the processing done, what is the final answer?
                    """
                    return call_llm(final_prompt, "You are a solution synthesizer.")

                elif "A)" in decision or "code" in decision.lower():
                    # Generate and execute code
                    code_prompt = f"""
                    Current data: {current_data}
                    Decision: {decision}

                    Write Python code to process this data as suggested. Return only the code:
                    """

                    code = call_llm(code_prompt, "You are a Python programmer.")

                    # Execute and update current data
                    code_result = execute_code(code)
                    current_data = f"Step {step_count} result: {code_result}"

                else:  # Generate specialized LLM prompt
                    # Create specialized prompt
                    prompt_design = f"""
                    Current data: {current_data}
                    Decision: {decision}

                    Design a specialized prompt for this analysis:
                    """

                    specialized_prompt = call_llm(prompt_design, "You are a prompt engineer.")

                    # Use the specialized prompt
                    analysis_result = call_llm(specialized_prompt, "You are a specialized analyst.")
                    current_data = f"Step {step_count} analysis: {analysis_result}"

            return f"Final result after {max_steps} steps: {current_data}"\n```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        

            PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS). YOUR APPROACH MUST BE SUBSTANTIVELY DIFFERENT THAN THESE:
            
PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS):

=== SCRIPT FROM ITERATION 2 (explore, ACCURACY: 0.00) ===
Approach: The script implements a knowledge graph approach to answer questions using supporting documents by leveraging the Gemini LLM. It decomposes the problem into knowledge extraction and reasoning, assigning the LLM the roles of knowledge extraction expert and question answering system. The `extract_knowledge` function extracts entities and relationships, validates the format of the extracted knowledge, and returns the knowledge in JSON format, calling `call_llm` to generate and validate the knowledge, while `reason_over_knowledge` generates the final answer based on the extracted knowledge, using `call_llm` to generate the answer. The `main` function orchestrates the process by calling the other functions and iterating through the supporting documents to generate an answer.

```python
import os
import re

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question, supporting_documents):
    """
    This script tests a 'knowledge graph' approach.
    It extracts entities and relationships from documents, then uses these to answer the question.
    """

    # Hypothesis: Building a simple knowledge graph can help with multi-hop reasoning.

    # Step 1: Extract knowledge (entities and relationships) from each document.
    knowledge = []
    for i, doc in enumerate(supporting_documents):
        knowledge_extraction_result = extract_knowledge(doc, i, question)
        if knowledge_extraction_result.get("is_valid"):
            knowledge.extend(knowledge_extraction_result["knowledge"])
        else:
            print(f"Knowledge extraction failed for document {i}: {knowledge_extraction_result.get('validation_feedback')}")
            return f"Knowledge extraction failed for document {i}"

    # Step 2: Reason over the extracted knowledge to answer the question.
    answer = reason_over_knowledge(question, knowledge)
    return answer

def extract_knowledge(document, doc_id, question, max_attempts=3):
    """Extracts entities and relationships from a document."""
    system_instruction = "You are a knowledge extraction expert."

    for attempt in range(max_attempts):
        extraction_prompt = f"""
        Extract entities and relationships from this document relevant to the question: {question}.
        Output a list of dictionaries, where each dictionary represents a fact.

        Example:
        Document: "Oasis was formed in 1991 by Liam Gallagher and Noel Gallagher."
        Output:
        [
            {{"entity1": "Oasis", "relation": "formed", "entity2": "Liam Gallagher", "year": "1991"}},
            {{"entity1": "Oasis", "relation": "formed", "entity2": "Noel Gallagher", "year": "1991"}}
        ]

        Document: {document}
        Output:
        """

        extracted_knowledge = call_llm(extraction_prompt, system_instruction)

        # Step 3: Validation step - is the knowledge in the correct format?
        validation_prompt = f"""
        Verify that the extracted knowledge from document {doc_id} is valid and in the correct format.

        Extracted Knowledge: {extracted_knowledge}

        Respond with "VALID" if the format is correct, otherwise "INVALID: [reason]"

        Example 1:
        Extracted Knowledge: [{{"entity1": "Oasis", "relation": "formed", "entity2": "Liam Gallagher", "year": "1991"}} ]
        Validation: VALID

        Example 2:
        Extracted Knowledge: Oasis was formed by Liam.
        Validation: INVALID: Knowledge should be a list of dictionaries.
        """

        validation_result = call_llm(validation_prompt, system_instruction)

        if "VALID" in validation_result:
            try:
                #Attempt conversion to data to verify. The call_llm JSON output will be plain text, so we will try to convert it to code but if it fails, then it will go to except
                data = eval(extracted_knowledge)
                return {"is_valid": True, "knowledge": data}
            except:
                return {"is_valid": False, "validation_feedback": "Could not be converted using eval()"}
        else:
            print(f"Knowledge extraction validation failed for doc {doc_id}, attempt {attempt+1}: {validation_result}")
            if attempt < max_attempts - 1:
                continue
            else:
                return {"is_valid": False, "knowledge": [], "validation_feedback": validation_result}
    return {"is_valid": False, "knowledge": [], "validation_feedback": "Failed after multiple attempts."}

def reason_over_knowledge(question, knowledge):
    """Reasons over the extracted knowledge to answer the question."""
    system_instruction = "You are an expert question answering system."

    reasoning_prompt = f"""
    Based on this extracted knowledge, answer the question: {question}.

    Extracted Knowledge:
    {knowledge}

    Example:
    Question: Who founded Oasis?
    Extracted Knowledge:
    [
        {{"entity1": "Oasis", "relation": "formed", "entity2": "Liam Gallagher", "year": "1991"}},
        {{"entity1": "Oasis", "relation": "formed", "entity2": "Noel Gallagher", "year": "1991"}}
    ]
    Answer: Liam Gallagher and Noel Gallagher

    Question: {question}
    Answer:
    """

    answer = call_llm(reasoning_prompt, system_instruction)
    return answer
```

=== SCRIPT FROM ITERATION 1 (explore, ACCURACY: 0.00) ===
Approach: The script addresses multi-hop reasoning by first summarizing each document and then reasoning across the summaries to answer a question. It employs a verification step to ensure the summaries retain relevant information, retrying summarization if the verification fails. The `main` function orchestrates the process, calling `summarize_document_with_verification` for each document and then `reason_across_summaries` to generate the final answer. `call_llm` is the core function that communicates with the Gemini LLM, while `summarize_document_with_verification` summarizes individual documents and verifies the summaries, and `reason_across_summaries` answers the final question based on the generated summaries.

```python
import os

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response"""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question, supporting_documents):
    """
    This script attempts to address multi-hop reasoning by:
    1. Summarizing each document independently to reduce information overload, then verifying the summarization.
    2. Reasoning across the summaries to find the answer.
    """

    # Hypothesis: Summarizing documents before reasoning will improve accuracy. This explores a document reduction strategy.
    # Addressing previous errors: Information Overload, Inability to Connect Disparate Facts
    # Verification goal: Verify the document summarization is both concise and accurate.
    
    summaries = []
    for i, doc in enumerate(supporting_documents):
        summary_result = summarize_document_with_verification(doc, i, question)
        if summary_result.get("is_valid"):
            summaries.append(summary_result["summary"])
        else:
            print(f"Error summarizing document {i}: {summary_result.get('validation_feedback')}")
            return f"Error summarizing document {i}"
    
    # Now, reason across the summaries to answer the question
    answer = reason_across_summaries(question, summaries)
    return answer

def summarize_document_with_verification(document, doc_id, question, max_attempts=3):
    """Summarizes a document and verifies the summary."""
    system_instruction = "You are an expert summarizer who creates concise, accurate summaries."
    
    #Attempt summarization, then verify
    for attempt in range(max_attempts):
        summary_prompt = f"""
        Summarize this document, focusing on information relevant to this question: {question}.
        Be concise and retain all critical information related to the question.
        
        Document: {document}
        
        Example 1:
        Document: The capital of Australia is Canberra. Canberra is located in the Australian Capital Territory.
        Summary: The capital of Australia is Canberra, located in the Australian Capital Territory.
        
        Example 2:
        Document:  Tommy's Honour is a 2016 historical drama film depicting the lives and careers of, and the complex relationship between, the pioneering Scottish golfing champions Old Tom Morris and his son Young Tom Morris.
        Summary: Tommy's Honour is a 2016 film about Scottish golfers Old Tom Morris and his son.

        Summary:
        """
        
        summary = call_llm(summary_prompt, system_instruction)
        
        # Verify the summary - does it retain relevant information?
        verification_prompt = f"""
        Verify that this summary of document {doc_id} retains all information relevant to the question: {question}.
        If not, explain what is missing.
        
        Document: {document}
        Summary: {summary}
        
        Respond with "VALID" if the summary is valid, or "INVALID: [reason]" if not.

        Example 1:
        Document: The Prime Minister of the UK is Rishi Sunak, who assumed office in 2022.
        Summary: Rishi Sunak is the UK Prime Minister.
        Verification: VALID
        
        Example 2:
        Document: Tommy's Honour is a film about Old Tom Morris and his son. Jack Lowden starred in it.
        Summary: Tommy's Honour is a film about Old Tom Morris.
        Verification: INVALID: The summary is missing the fact that Jack Lowden starred in it.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)
        
        if "VALID" in verification_result:
            return {"is_valid": True, "summary": summary}
        else:
            print(f"Summary verification failed for doc {doc_id}, attempt {attempt+1}: {verification_result}")
            if attempt < max_attempts-1:
                continue
            else:
                return {"is_valid": False, "summary": summary, "validation_feedback": verification_result}
    return {"is_valid": False, "summary": "", "validation_feedback": "Failed to generate a valid summary after multiple attempts."}

def reason_across_summaries(question, summaries):
    """Reasons across the summaries to answer the question."""
    system_instruction = "You are an expert at answering questions based on summaries of documents."
    
    reasoning_prompt = f"""
    Based on these summaries, answer the question: {question}. Synthesize information from multiple summaries if needed.
    
    Summaries:
    {summaries}

    Example 1:
    Question: What is the capital of Australia?
    Summaries: ['The capital of Australia is Canberra.']
    Answer: Canberra

    Example 2:
    Question: Tommy's Honour was a film that starred who?
    Summaries: ["Tommy's Honour is a 2016 film about Scottish golfers Old Tom Morris and his son.", "Jack Lowden starred in War & Peace"]
    Answer: Jack Lowden

    Answer:
    """
    
    answer = call_llm(reasoning_prompt, system_instruction)
    return answer

# Example usage (replace with actual data and document splitting)
if __name__ == "__main__":
    question = "Tommy's Honour was a drama film that included the actor who found success with what 2016 BBC miniseries?"
    supporting_documents = [
        "Tommy's Honour is a 2016 historical drama film depicting the lives and careers of, and the complex relationship between, the pioneering Scottish golfing champions Old Tom Morris and his son Young Tom Morris. The film is directed by Jason Connery, and the father and son are portrayed by Peter Mullan and Jack Lowden. The film won Best Feature Film at the 2016 British Academy Scotland Awards.",
        "Jack Andrew Lowden (born 2 June 1990) is a Scottish stage, television, and film actor. Following a highly successful and award-winning four-year stage career, his first major international onscreen success was in the 2016 BBC miniseries War & Peace, which led to starring roles in feature films."
    ]

    answer = main(question, supporting_documents)
    print(f"Answer: {answer}")
```

=== SCRIPT FROM ITERATION 0 (baseline, ACCURACY: 0.80) ===
Approach: Simple baseline script: Direct LLM call without sophisticated techniques

```python
import os
from google import genai
from google.genai import types

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response"""
    try:
        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question):
    """
    Baseline script: Simple direct LLM call without sophisticated techniques.
    This establishes the baseline performance capability for this dataset.
    """
    system_instruction = "You are a helpful assistant. Answer the question directly and concisely based on the information provided."

    # Simple, direct call to LLM
    answer = call_llm(question, system_instruction)

    return answer
    
```


            LEARNINGS FROM PREVIOUS ITERATIONS:
            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        Okay, here's the updated and synthesized version of our learnings, focused on the specifics of this multi-hop reasoning dataset and task, organized into the requested sections:

**1. DATASET PATTERNS & CHARACTERISTICS**

*   **Explicit Multi-Hop Reasoning:** Questions are explicitly framed as multi-hop reasoning tasks, demanding information linkage across multiple documents. Single document retrieval is insufficient.
    *   *Example:* Questions require linking information about a person's career, location, and related events found in separate documents.
*   **Diverse Subject Matter:** Questions span various domains including history, geography, entertainment, and current events.
    *   *Example:* Questions range from historical figures to film trivia to geographical locations of organizations.
*   **Concise Factual Answers:** Answers are generally concise named entities, dates, or short phrases, directly responding to the question.
    *   *Example:* "1984", "Los Angeles", "Emilio Estevez".
*   **Answer Source Constraint:** Answers are *explicitly* within the provided supporting documents. The model should *not* require external knowledge. The core challenge is information retrieval and synthesis from provided texts.
*   **Information Overload:** "Supporting Documents" contain significant irrelevant information (noise). Effective models must filter and focus on relevant passages.
*   **Ambiguity:** Terms and entities can be ambiguous. Context is crucial for disambiguation.
*   **Synonymy and Paraphrasing:** Concepts are expressed differently in questions and supporting documents, requiring understanding of synonyms and paraphrases.
*   **Reasoning Depth Variation:** The number of inference "hops" to answer questions varies.
*   **Edge Cases Exist:**
    *   **Missing Information:** Documents *may not* contain the complete answer, even if relevant.
    *   **Contradictory Information:** Documents might contain conflicting information, requiring a resolution strategy.
    *   **Coreference Resolution:** Pronoun references must be resolved (e.g., "He" refers to whom?).
*   **Complex Multi-Hop Reasoning (Reinforced):** The dataset heavily relies on complex multi-hop reasoning. Answering a question often requires connecting information from multiple documents, sometimes in subtle ways.
    *   *Example:* Connecting "Emilio Estevez starred in Nightmares" with another document mentioning a film released in the same year.
*   **Information Synthesis Required (Reinforced):** Correct answers require synthesizing information rather than directly quoting a single document.
    *   *Example:* Combining facts, dates, names, and contexts to produce a derived answer.
*   **Real-World Knowledge Assumptions (Identified):** The questions often implicitly assume some basic real-world knowledge not explicitly in the documents.
    *   *Example:* Needing common-sense reasoning to understand the question's context even with supporting documents.
*   **Contextual Clues in Document Titles:** The document titles often provide crucial context for understanding the content of the document and its relevance to the question (e.g., "Oasis discography", "St. John's College, Belize").
*   **Varied Document Content:** The supporting documents encompass a wide range of formats, including discographies, lists of band members, historical context, and descriptions of albums and tours. This variety requires flexible information extraction.
*   **Entity Relationships Critical:** Questions often hinge on identifying relationships between entities mentioned across different documents.
    *   *Example:* Determining if a person played a specific instrument on a specific song.
*   **Passage Complexity Varies:** Supporting documents (passages of text) exhibit varying lengths and complexity. This requires robust handling of both concise and verbose information sources.

**2. EFFECTIVE TASK-SPECIFIC STRATEGIES**

*   *(Initial State: No strategies have been proven effective yet as the baseline.)*
*   *(Update: No strategies have been proven effective. Experiment 1 resulted in accuracy 0.00 due to a critical error in the test script.)*
*   *(Update: No strategies have been proven effective. Experiment 2 resulted in accuracy 0.00 due to a critical error in the test script.)*

**3. COMMON FAILURE MODES ON THIS DATASET**

*   **Information Overload:** The LLM struggles to sift through the volume of information provided in the supporting documents.
*   **Inability to Connect Disparate Facts:** The system fails when the answer requires linking information that isn't explicitly connected in the documents.
    *   *Example:* Failing to connect Emilio Estevez and Nightmares with another document mentioning a different film released in the same year.
*   **Poor Summarization and Extraction:** The system struggles to extract the specific requested detail, returning more general information.
    *   *Example:* In the "Eric S. Pistorius" example, failing to extract the *concept* of his work, instead returning a more general description of his specializations as an attorney.
*   **Lack of Temporal Reasoning:** Weakness in temporal reasoning; the system can't easily determine which events occurred in the same year.
    *   *Example:* Failure involving Emilio Estevez demonstrates a weakness in temporal reasoning; the system can't easily determine which events occurred in the same year without more sophisticated processing.
*   **Basic Information Extraction is Not Enough:** Simply extracting facts from documents is insufficient. The system must be able to reason *with* those facts.
*   **Incorrect Function Call (CRITICAL - Repeated):** Critical failure due to incorrect function call in the test script, preventing the LLM from accessing the supporting documents. This has occurred in *both* Experiment 1 and Experiment 2.
*   **Script Integration Failure:** Failure to correctly integrate supporting documents into the script's `main()` function prevents the LLM from even parsing or processing the document set. The `TypeError` indicates a fundamental flaw in how the script receives input data.

**4. EXPERIMENT LOG & FINDINGS**

*   **Experiment 0: Baseline LLM Call**
    *   *Description:* Direct call to the LLM with the question and supporting documents.
    *   *Accuracy:* 80%
    *   *Findings:* Baseline performance indicates that the task complexity exceeds the capabilities of a simple LLM call without additional reasoning or information retrieval techniques. Highlights the need for a more structured approach to reasoning, potentially involving intermediate steps to identify relevant entities, relationships, and temporal information.
    *   *Failure Mode Examples:*
        *   Inability to connect disparate facts across documents.
        *   Poor summarization and extraction of specific details.
        *   Lack of temporal reasoning.
*   **Experiment 1: Summarization and Reasoning Pipeline**
    *   *Description:* Attempted to use `summarize_document_with_verification` to summarize supporting documents, then `reason_across_summaries` to answer the question.
    *   *Accuracy:* 0.00
    *   *Findings:* The experiment failed due to a critical error in the test script (`test_script_1.py`). The `main` function was called with only the question, omitting the `supporting_documents` argument. This prevented the model from accessing the necessary information to answer the questions, leading to zero accuracy. This highlights the importance of rigorous testing and validation to ensure the correct flow of information.
    *   *Error Logs:*
        ```
        === SCRIPT ERROR ENCOUNTERED [2025-06-01 07:02:35] ===
        Error detected during script repair (attempt 1): main() missing 1 required positional argument: 'supporting_documents'
        Traceback (most recent call last):
          File "/home/runner/workspace/scripts/test_script_1.py", line 261, in <module>
            answer = module.main(question)
                     ^^^^^^^^^^^^^^^^^^^^^
        TypeError: main() missing 1 required positional argument: 'supporting_documents'
        === END SCRIPT ERROR ===
        ```
        *(Similar error logs for attempt 2 and 3)*
    *   *Root Cause Identified:* Incorrect function call `module.main(question)` instead of `module.main(question, supporting_documents)`.
*   **Experiment 2: Knowledge Graph Extraction and Reasoning**
    *   *Description:* Attempted to use a knowledge graph extraction approach followed by reasoning over the graph to answer the question.
    *   *Accuracy:* 0.00
    *   *Findings:* The experiment failed due to a critical error in the test script (`test_script_2.py`). The `main` function was called with only the question, omitting the `supporting_documents` argument. This prevented the model from accessing the necessary information to answer the questions, leading to zero accuracy. The architectural design itself (knowledge extraction followed by reasoning) remains a potentially promising direction, but is untested.
    *   *Error Logs:*
        ```
        === SCRIPT ERROR ENCOUNTERED [2025-06-01 07:03:31] ===
        Error detected during script repair (attempt 1): main() missing 1 required positional argument: 'supporting_documents'
        Traceback (most recent call last):
          File "/home/runner/workspace/scripts/test_script_2.py", line 261, in <module>
            answer = module.main(question)
                     ^^^^^^^^^^^^^^^^^^^^^
        TypeError: main() missing 1 required positional argument: 'supporting_documents'
        === END SCRIPT ERROR ===
        ```
        *(Similar error logs for attempt 2 and 3)*
    *   *Root Cause Identified:* Incorrect function call `module.main(question)` instead of `module.main(question, supporting_documents)`. This is a *repeat* of the error from Experiment 1.

**5. NEXT RESEARCH DIRECTIONS**

*   **Correct the Function Call (CRITICAL - RED ALERT):** Immediately fix the function call in `test_script_1.py` *and* `test_script_2.py` to `answer = module.main(question, supporting_documents)`. This is the most critical step to enable proper testing and move forward. The *same* error has blocked progress in two consecutive experiments.
*   **Establish Rigorous Testing Protocol:** Implement a more rigorous testing protocol *before* launching experiments. This should include unit tests to verify that function calls are correct and that all required arguments are passed.
*   **Verify Input Handling:** Once the function call is corrected, add input validation within the `main` function to ensure that the `supporting_documents` are received and properly formatted before proceeding with summarization and reasoning. This validation should include checks for:
    *   Presence of the `supporting_documents` argument.
    *   Correct data type of `supporting_documents` (e.g., list of strings).
    *   Non-empty `supporting_documents` list.
*   **Evaluate Summarization Quality:** After correcting the input in Experiment 1, analyze the summaries produced by `summarize_document_with_verification`. Determine if the verification process effectively retains relevant information from the original documents. If the summaries are consistently poor, refine the summarization prompts and verification criteria. Consider metrics to quantify the information retained in the summaries.
*   **Analyze Knowledge Graph Quality:** After correcting the input in Experiment 2, analyze the extracted knowledge graph. Assess the quality of the nodes (entities) and edges (relationships) extracted from the supporting documents. Refine the knowledge extraction prompts to improve the accuracy and completeness of the graph.
*   **Analyze Reasoning Chain:** Inspect the reasoning steps performed by `reason_across_summaries` (Experiment 1) or the reasoning performed on the Knowledge Graph (Experiment 2). Identify any logical gaps or incorrect inferences made by the model. Experiment with different prompting strategies and reasoning frameworks to improve the accuracy of the final answer. Focus on the ability of the reasoning chain to synthesize information from multiple summaries/graph nodes.
*   **Implement Document Ranking/Filtering:** Implement a mechanism to rank or filter documents based on their relevance to the question *before* feeding them to the LLM.
    *   *Potential Techniques:* Keyword matching, semantic similarity, named entity recognition.
*   **Introduce a Chain-of-Thought Prompting:** Structure the LLM prompt to encourage chain-of-thought reasoning.
    *   *Example Prompt Structure:* Ask the LLM to first identify relevant entities, then identify relevant relationships between those entities, and finally answer the question based on those relationships.
*   **Fine-tune LLM (if feasible):** If resources allow, consider fine-tuning the LLM on a subset of the dataset to improve its ability to perform multi-hop reasoning and information synthesis. This would require a carefully constructed training set with questions and corresponding "reasoning paths".
*   **Incorporate External Knowledge (Cautiously):** Consider incorporating external knowledge sources (e.g., a knowledge graph or a database of facts) to augment the information provided in the documents. However, be careful to avoid introducing irrelevant or contradictory information.
*   **Implement a Temporal Reasoning Module:** Specifically address the temporal reasoning challenges by incorporating a module that can reason about dates, time intervals, and the order of events. This module could be a rule-based system or a machine learning model trained on temporal reasoning tasks.
        

            CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        

            EXPLORATION GUIDANCE:
            1. Review the historical approaches, error patterns, and accumulated learnings carefully
            2. Review the FULL CODE of previous scripts to understand what has already been tried
            3. Design a new approach that is DISTINCTLY DIFFERENT from previous attempts. This approach should have a specific NEW HYPOTHESIS or variable you are trying to test. 
            4. CRITICAL: Include EMBEDDED EXAMPLES directly within your LLM prompts
            5. For each key function, show a complete worked example, or include multiple examples, including:
               - Input example that resembles the dataset
               - Step-by-step reasoning through the example
               - Properly formatted output
            6. Apply the insights from the ACCUMULATED LEARNINGS section to avoid repeating past mistakes
            7. Pay SPECIAL ATTENTION to the weaknesses and improvement suggestions from the capability assessment
            8. Consider implementing one or more of these LLM usage patterns:
               - Repeated validation with feedback loops
               - Multi-perspective analysis with synthesis
               - Dynamic input-dependent routing with an orchestrator
               - Hybrid approaches combining LLM with deterministic functions
               - Best-of-n solution generation and selection
               - ReAct pattern for interactive reasoning and action
               - If it is unknown how successful a processing state or part of the pipeline is, include verification steps to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
               - Answer checkers to validate the final answer against the problem statement. If the answer is incorrect, the checker can send the answer back to an earlier part of the system for for refinement with feedback

            Here's how to call the Gemini API. Use this example without modification and don't invent configuration options:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

            Since this is an EXPLORATION phase:
            - Try a fundamentally different approach to reasoning about the problem. Test a NEW HYPOTHESIS or variable, and add verification steps to deduce if this new change is helpful.
            - THIS IS KEY: Break down the problem into new, distinct reasoning steps based on past performance before you start coding
            - For EACH key LLM prompt, include a relevant example with:
              * Sample input similar to the dataset
              * Expected reasoning steps
              * Desired output format
            - Apply a verifier call to different parts of the pipeline in order to understand what parts of the pipeline of calls is successful and where the system is breaking
            - Pay special attention to addressing the primary issues from previous iterations
            - Ensure your new approach addresses the weaknesses identified in the capability assessment

            CRITICAL REQUIREMENTS:
            1. The script MUST properly handle all string literals - be extremely careful with quotes and triple quotes
            2. The script MUST NOT exceed 150 lines of code to prevent truncation
            3. Include detailed comments explaining your reasoning approach
            4. EVERY SINGLE LLM PROMPT must include at least one embedded example showing:
               - Sample input with reasoning
               - Desired output format
            5. Make proper use of error handling
            6. Implement robust capabilities to address the specific weaknesses identified in the capability assessment
            7. Do NOT use json.loads() in the LLM calls to process input data. JSON formatting is good to use to structure information as inputs and outputs, but attempting to have functions process JSON data explicitly with strict built-in functionality is error prone due to formatting issues and additional text that appears as documentation, reasoning, or comments. When passing data into another LLM call, you can read it as plain text rather than trying to load it in strict json format, is the better approach.

            Return a COMPLETE, RUNNABLE Python script that:
            1. Has a main function that takes a question string as input and returns the answer string
            2. Makes multiple LLM calls for different reasoning steps
            3. Has proper error handling for API calls
            4. Includes embedded examples in EVERY LLM prompt
            5. Is COMPLETE - no missing code, no "..." placeholders
            6. Closes all string literals properly

            This should be FUNDAMENTALLY DIFFERENT from all previous approaches. Do not reuse the same overall structure.

            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            