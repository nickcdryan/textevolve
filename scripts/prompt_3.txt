
            You are improving a Python script that solves problems from a dataset.
            Your goal is to REFINE and ENHANCE the best performing approaches by combining their strengths and addressing specific weaknesses identified in error analysis.

            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "PASSAGE: Hoping to rebound from their first loss of the season, the Broncos returned home for an AFC West divisional rematch with the Kansas City Chiefs. After Peyton Manning became the NFL's all-time leader in regular season passing yardage, the game turned sour for the Broncos. Following a Manning interception, the Chiefs capitalized, with a 4-yard touchdown run by running back Charcandrick West. The Broncos' offense went three-and-out on their next two possessions, and the Chiefs increased their lead to 10-0, with a 48-yard field goal by placekicker Cairo Santos. The Chiefs increased their lead to 19-0 at halftime, with three more field goals by Santos &#8212; from 49, 34 and 33 yards out. By halftime, Manning had thrown three interceptions and the Broncos' offense had earned only one first down. The Broncos went three-and-out on their first possession of the second half, and a 50-yarder field goal by Santos increased the Chiefs' lead to 22-0. After Manning threw his fourth interception of the game on the Broncos' next possession, he was pulled and replaced by backup quarterback Brock Osweiler for the remainder of the game. Osweiler drove the Broncos' into the red zone early in the fourth quarter, but was intercepted by Chiefs' safety Eric Berry. Two plays later, the Chiefs increased their lead to 29-0, when quarterback Alex Smith connected with West on an 80-yard touchdown pass. The Broncos' finally got on the scoreboard with 5:31 remaining in the game, with running back Ronnie Hillman rushing for a 1-yard touchdown (two-point conversion attempt unsuccessful), followed by a 7-yard touchdown pass from Osweiler to wide receiver Andre Caldwell, but the Chiefs' lead was too much for the Broncos to overcome. Peyton Manning finished the day with the first 0.0 passer rating of his career.\n\nQUESTION: How many yards longer was the longest touchdown pass than the longest field goal?",
    "answer": "30"
  },
  {
    "id": 1,
    "question": "PASSAGE: Coming off their road win over the 49ers, the Titans went home, donned their Houston Oilers throwbacks, and played a Week 10 AFL Legacy game with the Buffalo Bills.  Tennessee would trail early in the first quarter as Bills running back Fred Jackson threw a 27-yard touchdown pass to wide receiver Lee Evans.  The Titans would respond as running back Chris Johnson got a 28-yard touchdown run, followed by quarterback Vince Young hooking up with wide receiver Nate Washington on a 14-yard touchdown pass.  In the second quarter, Tennessee would increase their lead as kicker Rob Bironas booted a 38-yard field goal.  Buffalo would end the half with quarterback Trent Edwards finding Evans on an 8-yard touchdown pass. The Bills would tie the game in the third quarter with kicker Rian Lindell booting a 25-yard field goal, yet the Titans would explode with points in the fourth quarter.  It began with Johnson's 1-yard touchdown run, followed by Bironas' 51-yard field goal.  It would follow up with safety Vincent Fuller returning an interception 26 yards for a touchdown and cornerback Rod Hood returning an interception 31 yards for a touchdown. Chris Johnson (26 carries, 132 yards, 2 TDs and 9 catches, 100 yards) would join Billy Cannon as the only players in franchise history to rush and receive for 100 yards in one game.  He would also join Earl Campbell in 1980 as the only players in franchise history to have two rushing touchdowns in three-straight games. At the end of the game, owner Bud Adams raised his middle fingers to Buffalo players from his box and was subsequently fined $250,000 by Roger Goodell, who was in attendance at the game.\n\nQUESTION: How many receiving yards did Chris Johnson have?",
    "answer": "100"
  },
  {
    "id": 2,
    "question": "PASSAGE: New Caledonia is a major source for nickel and contains roughly 10% of the worlds known nickel supply. The islands contain about 7,100,000 tonnes of nickel. With the annual production of about 107,000 tonnes in 2009, New Caledonia was the Nickel mining in New Caledonia after Russia (266,000), Indonesia (189,000), Canada (181,000) and Australia (167,000). In recent years, the economy has suffered because of depressed international demand for nickel, due to the ongoing financial crisis of 2007\u20132008. Only a negligible amount of the land is suitable for cultivation, and food accounts for about 20% of imports. In addition to nickel, the substantial financial support from France and tourism are keys to the health of the economy. In the 2000s, large additions were made to nickel mining capacity. The Goro, New Caledonia is expected to be one of the largest nickel producing plants on Earth. When full-scale production begins in 2013 this plant will produce an estimated 20% of the global nickel supply. However, the need to respond to environmental concerns over the countrys globally recognized ecological heritage, may increasingly need to be factored into capitalization of mining operations.\n\nQUESTION: Which country produced more tonnes of nickel in 2009, Canada or Australia?",
    "answer": "Canada"
  }
]

            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 3
        - Current explore/exploit balance: 40/60
        - Best accuracy achieved: 1.00 (iteration 1)

        APPROACH HISTORY (last 3 iterations):
        [
  {
    "iteration": 0,
    "strategy": "Exploration",
    "accuracy": 0.6666666666666666,
    "approach": "The script employs a multi-stage, LLM-driven approach to answer questions, using chain-of-thought reasoning and verification. It decomposes the problem into question analysis, relevant passage extraction, answer generation, and answer verification. The agent roles include question analyzer, passage extractor, answer generator, and answer verifier, each defined by system instructions in their respective functions. The functions used are `main`, `analyze_question`, `extract_relevant_passage`, `generate_answer`, `verify_answer`, and `call_llm`. The `main` function orchestrates the process, calling `analyze_question` to understand the question, `extract_relevant_passage` to find relevant context, `generate_answer` to formulate an initial answer, and `verify_answer` to refine the response, using `call_llm` to interface with the Gemini model for each step."
  },
  {
    "iteration": 1,
    "strategy": "Exploitation",
    "accuracy": 1.0,
    "approach": "The script uses a multi-stage LLM approach with chain-of-thought prompting to answer questions by identifying the question type, extracting relevant information, generating an answer, and then verifying the answer. The script uses the following functions: `main` orchestrates the entire process, `analyze_question` identifies the question type and keywords, `extract_relevant_passage` retrieves relevant text, `generate_answer` formulates an initial answer, `verify_answer` validates the answer, and `call_llm` interacts with the Gemini LLM, using system instructions and prompts to guide the LLM's reasoning at each step. The workflow starts with `main` calling `analyze_question`, followed by `extract_relevant_passage`, `generate_answer`, and `verify_answer`, each leveraging `call_llm` to interact with the LLM."
  },
  {
    "iteration": 2,
    "strategy": "Exploitation",
    "accuracy": 1.0,
    "approach": "The script uses a multi-stage LLM approach with example-driven reasoning to answer questions, incorporating question analysis, information extraction, answer generation, and verification. The problem is decomposed into four main steps: analyzing the question, extracting a relevant passage, generating an answer, and verifying the answer, each handled by a separate function. There are no agent roles indicated in the script. The script uses `analyze_question` to identify the question type and keywords, `extract_relevant_passage` to find the relevant information, `generate_answer` to form an answer, `verify_answer` to confirm its correctness, and `call_llm` to interface with the Gemini LLM. The overall workflow involves analyzing the question, extracting relevant information, generating an initial answer, verifying it for accuracy, and then returning the verified answer."
  }
]

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 3 iterations):
        [
  {
    "iteration": 0,
    "issue": "The system fails when it needs to identify the \"second longest\" touchdown pass. This is due to it not correctly identifying all touchdown passes first and then ordering them by yardage before selecting the second item. The core problem is the lack of a process for systematically identifying all instances of an event and then applying ordering logic."
  },
  {
    "iteration": 1,
    "issue": "Because no errors were provided, the primary issue cannot be determined."
  },
  {
    "iteration": 2,
    "issue": "Without error cases, it is impossible to determine the single most critical problem."
  }
]

        TARGETED IMPROVEMENTS:
        [
  "Test Suite Expansion:** Develop a more comprehensive and diverse test suite, including challenging edge cases and adversarial examples.",
  "External Knowledge Integration:** Explore integrating external knowledge sources (e.g., knowledge graphs, databases) to supplement the information provided in the passage.",
  "Ambiguity Resolution:** Improve the system's ability to handle ambiguous questions or passages, perhaps by generating multiple interpretations and evaluating them against the context.",
  "Constraint Satisfaction:** Explicitly model constraints within the problem and ensure that the generated solutions adhere to them.",
  "Error Handling & Logging:** Implement more detailed error logging and tracing to diagnose and debug failures more effectively. Use of print statements during the operation to determine values of variables at particular steps.",
  "Numerical Reasoning:** Add more robust numerical reasoning capabilities, including unit conversion, proportional reasoning, and handling ranges of values.",
  "Complex Reasoning Chains:** Implement strategies to handle more complex multi-step reasoning. This could involve techniques like chain-of-thought prompting or explicitly breaking down questions into smaller sub-problems."
]
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity ✓
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity ✓
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity ✓
    4. All operations have O(1) average time complexity ✓
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        

            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        ```
# DATASET-SPECIFIC LEARNINGS & EXPERIMENT LOG

This document serves as a running log of our learnings, experiments, and findings specific to the question-answering task on this particular dataset of sports narratives and demographic data.

## 1. DATASET PATTERNS & CHARACTERISTICS

*   **Question Structure:** Questions are consistently formatted as `PASSAGE:\n... \n\nQUESTION: ...`. This structure clearly separates the context from the query.

*   **Passage Content:** Passages consist of narratives describing sports games (primarily American football) or demographic data. They provide the context needed to answer the questions. Passages exhibit variability in content (sports, demographics, geography, nickel production) and length.

*   **Answer Types:** Answers are typically concise and factual, directly extracted or derived from the passage. Common answer types include proper nouns (player names), numerical values (counts, yardage, rates), and dates.

*   **Question Types:**
    *   **Fact Extraction:** Directly retrieving information from the passage (e.g., "Who caught the final touchdown of the game?", "How many receiving yards...").
    *   **Counting:** Determining the number of occurrences of an event or entity (e.g., "How many running backs ran for a touchdown?").
    *   **Calculation:** Performing arithmetic operations on values extracted from the passage (e.g., "How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?"). Requires unit awareness.
    *   **Comparison/Difference:** Comparing values or entities described in the passage (e.g., "How many more TFR did Malays have in 2004 compared to Ethnic Chinese TFR in 2004?", "How many yards longer..."). Requires numerical reasoning or extraction of specific numbers from the passage (e.g., "How many field goals?", "How many in percent...").
    *   **Ordinality:** Questions may include ordinal terms like "second longest", requiring ranking and comparison.
    *   **Multiple Instances:** Question has multiple valid answers in the passage (e.g., "List all the countries...").

*   **Temporal Reasoning:** Questions often involve temporal aspects, requiring understanding the sequence of events (e.g., "final touchdown," "first field goal"). The passages often present information in chronological order.

*   **Implicit Information:** The answer might need to be inferred based on multiple pieces of information, requiring the model to synthesize information from different parts of the passage.

*   **Extraneous Information:** Passages often contain rich contextual details that are irrelevant to the question, requiring the model to filter out noise.

*   **Synonyms and Paraphrasing:** The question might use different wording than the passage to refer to the same entity or event.

*   **Multiple Occurrences:** An event (e.g., a touchdown) might occur multiple times; the question could be specific about which occurrence to consider (e.g., "first," "last," "second longest").

*   **Negation:** Questions like "Who did *not* score a touchdown?" require careful handling of negated conditions.

*   **Units:** Need to be aware of units when performing calculations (yards, points, etc.)

## 2. EFFECTIVE TASK-SPECIFIC STRATEGIES

*   **Multi-Stage Approach:** A multi-stage approach, involving question analysis, passage retrieval/extraction, answer generation, and verification, shows promise. This allows for breaking down the complex task into smaller, manageable components.
    *   Breaking down the problem into question type identification, focused passage extraction, answer generation, and verification helps. This is particularly helpful where numerical or multi-step reasoning is required. Accuracy is 1.00 using this approach.
    *   Using separate LLM calls for each subtask (analyze, extract, generate, verify) isolates errors and allows for targeted improvements.

*   **Chain-of-Thought Prompting (LLM-focused):** Encouraging the LLM to explicitly show its reasoning steps (e.g., using "Let's think step by step:") can improve accuracy and allow for debugging. Example:

    ```
    PASSAGE: ...

    QUESTION: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?

    Let's think step by step:
    First, I need to identify Chris Johnson's first touchdown yardage. The passage says "Chris Johnson got a 6-yard TD run".
    Second, I need to identify Jason Hanson's first field goal yardage. The passage says "Jason Hanson getting a 53-yard field goal."
    Finally, I need to add those two numbers. 6 + 53 = 59.
    ANSWER: 59
    ```
    *   Hypothesis: Chain-of-thought prompting enhances the LLM's ability to reason and extract information from the passages. (Confirmed for at least a subset of questions).

*   **Example-Driven Reasoning (LLM-focused):** Providing the LLM with a few examples of questions and answers to guide its reasoning is effective. Suggests that the LLM benefits from having examples of how to handle different question types.

*   **Few-Shot Learning (LLM-focused):** Providing the LLM with a few examples of questions and answers to guide its reasoning.

*   **Answer Validation Prompt (LLM-focused):** Using a separate prompt to validate the generated answer. This prompt could ask the LLM to justify its answer based on the passage.

*   **Constrained Decoding (LLM-focused):** If the answer is known to be a number, use constrained decoding to ensure that the LLM only generates numerical answers.

## 3. COMMON FAILURE MODES ON THIS DATASET

*   **Inaccurate Handling of Ordinality:** Inability to correctly answer questions involving ordinality (e.g., "second longest"). The system often fails to systematically identify all instances of the relevant event, extract the relevant attribute, order them, and then select the item at the specified ordinal position. Instead, it appears to make direct, often incorrect, guesses based on incomplete context.

*   **Failure to Extract All Relevant Instances:** The system sometimes fails to identify all instances of events related to the question, leading to incomplete information for comparisons and ordinal ranking.

*   **Inability to Filter Extraneous Information:** Difficulty in filtering out irrelevant details from the passage, leading to confusion and incorrect answers.

*   **Arithmetic Errors:** Mistakes in performing calculations.

*   **Ambiguity Resolution:** Difficulty in disambiguating similar information or names within the passage.

*   **Incomplete Information Inference:** Failing to correctly infer the answer when it requires synthesizing information from multiple parts of the passage.

*   **Potential Failure Modes (Anticipated based on dataset characteristics):**
    *   *Numerical Reasoning Errors:* Questions involving more complex arithmetic (e.g., multiple steps, percentages) could lead to incorrect answers.
    *   *Ambiguous or Vague Questions:* Questions that are not clearly worded or require significant inference could be misinterpreted by the question analysis module.
    *   *Passage Extraction Errors:* If the relevant passage is not correctly identified, the subsequent answer generation will be flawed.  This could happen if keywords are ambiguous or if the passage requires deeper understanding.
    *   *Verification Errors:* The verification step could fail to detect subtle inaccuracies or inconsistencies in the generated answer.

## 4. EXPERIMENT LOG & FINDINGS

*   **2025-05-19 14:27:04 - INITIAL DATASET ANALYSIS:** Comprehensive analysis of dataset characteristics, challenges, potential approaches, and implementation recommendations. (See full analysis in original entry). Key findings included:
    *   The importance of understanding the question type (extraction, counting, calculation, comparison).
    *   The need for temporal reasoning and arithmetic reasoning.
    *   Potential solution strategies: information retrieval & extraction, direct answer generation.
    *   Decomposition steps: question understanding, relevant passage identification, information extraction/calculation, answer formatting.
    *   Validation techniques: self-consistency checks, units checks, reasonableness checks.

*   **ITERATION 0:** Initial multi-stage approach achieved 67% accuracy. Highlighted the need for improved handling of questions involving comparisons and ordinality. The system needs mechanisms for systematically extracting and processing multiple instances of events from the passage.

*   **ITERATION 1:** Achieved 1.0 accuracy using a multi-stage LLM approach with chain-of-thought prompting.
    *   The approach proved highly effective for the tested subset of the dataset.

*   **ITERATION 2:** Achieved 1.0 accuracy using a multi-stage LLM approach with example-driven reasoning.
    *   The modular approach (analyzing question, extracting passage, generating answer, verifying) seems to be effective.
    *   Using separate LLM calls for each subtask (analyze, extract, generate, verify) likely isolates errors and allows for targeted improvements.

## 5. NEXT RESEARCH DIRECTIONS

*   **Improve Ordinality Handling:**
    *   Modify the `extract_relevant_passage` function to identify *all* instances of events related to the question.
    *   Introduce a new function, `rank_answers`, that orders extracted information based on a specific criteria (e.g., yardage).
    *   Update the `analyze_question` function to specifically identify questions requiring ranking or comparison. If identified, flag the question for processing by the `rank_answers` function.
    *   In `generate_answer`, incorporate a step to select the correct ranked answer based on the ordinality requested in the question (e.g., "second longest").
    *   Add unit tests specifically targeting questions involving ordinality and comparisons to ensure the system can accurately rank and select answers.

*   **Enhance Information Extraction:** Investigate techniques for improving the accuracy and completeness of information extraction, including named entity recognition, dependency parsing, and semantic role labeling.

*   **Implement a Robust Validation Step:** Develop a more sophisticated validation step to catch errors before the final answer is generated. This could involve verifying units, checking for consistency with the passage, and assessing the reasonableness of the answer. Add adversarial examples designed to trick the verification step.

*   **Explore Different LLM Prompting Strategies:** Experiment with different prompting strategies to improve the LLM's reasoning abilities, including chain-of-thought prompting, few-shot learning, and answer validation prompts.

*   **Develop a More Sophisticated Question Analyzer:** Improve the `analyze_question` function to better identify the type of reasoning required to answer the question, including temporal reasoning, arithmetic reasoning, and logical reasoning.

*   **Expand Dataset for Stress-Testing:** Expand the dataset with new question types, more complex reasoning problems, passages with ambiguous or contradictory information, and passages containing data requiring external knowledge (dates, names, etc.) to further stress-test the system and identify failure points. Specifically:
    *   Introduce more complex or ambiguous questions that require more advanced reasoning or inference.
    *   Stress-test numerical reasoning with questions requiring multi-step calculations, unit conversions, or proportional reasoning.
    *   Test how well the passage retrieval works when the question has multiple valid answers in the passage (e.g., "List all the countries...").

*   **Evaluate the impact of different LLMs:** Experiment with different LLMs (e.g., GPT-4, Claude) to see if performance can be further improved or if certain LLMs are better suited for specific subtasks.

*   **Analyze the verification module:** Examine how the verification step is implemented and how robust it is to different types of errors.
```
        

            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        

            TOP PERFORMING APPROACHES TO BUILD UPON:
            
TOP PERFORMING APPROACH #1:
Iteration: 1
Accuracy: 1.00
Approach Summary: The script uses a multi-stage LLM approach with chain-of-thought prompting to answer questions by identifying the question type, extracting relevant information, generating an answer, and then verifying the answer. The script uses the following functions: `main` orchestrates the entire process, `analyze_question` identifies the question type and keywords, `extract_relevant_passage` retrieves relevant text, `generate_answer` formulates an initial answer, `verify_answer` validates the answer, and `call_llm` interacts with the Gemini LLM, using system instructions and prompts to guide the LLM's reasoning at each step. The workflow starts with `main` calling `analyze_question`, followed by `extract_relevant_passage`, `generate_answer`, and `verify_answer`, each leveraging `call_llm` to interact with the LLM.

FULL SCRIPT TO REFINE:
```python
import os
import re
import math

def main(question):
    """
    Solve the question using a multi-stage LLM approach.
    This approach focuses on breaking down the problem into question type identification, 
    focused passage extraction, and direct answer generation with verification.
    """
    try:
        # Step 1: Identify question type and keywords
        question_analysis = analyze_question(question)
        if "Error" in question_analysis:
            return "Error analyzing question"

        # Step 2: Extract relevant passage using identified keywords
        relevant_passage = extract_relevant_passage(question, question_analysis)
        if "Error" in relevant_passage:
            return "Error extracting passage"

        # Step 3: Generate answer using extracted passage and question type
        answer = generate_answer(question, relevant_passage, question_analysis)
        if "Error" in answer:
            return "Error generating answer"

        # Step 4: Verify answer
        verified_answer = verify_answer(question, answer, relevant_passage)
        if "Error" in verified_answer:
            return "Error verifying answer"
        
        return verified_answer

    except Exception as e:
        return f"General Error: {str(e)}"

def analyze_question(question):
    """Analyzes the question to identify its type and keywords. Includes multiple examples."""
    system_instruction = "You are an expert at analyzing questions to determine their type and keywords."
    prompt = f"""
    Analyze the following question and identify its type (e.g., fact extraction, calculation, comparison) and keywords.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Analysis: {{"type": "fact extraction", "keywords": ["final touchdown", "caught"]}}

    Example 2:
    Question: How many running backs ran for a touchdown?
    Analysis: {{"type": "counting", "keywords": ["running backs", "touchdown"]}}
    
    Example 3:
    Question: Which player kicked the only field goal of the game?
    Analysis: {{"type": "fact extraction", "keywords": ["player", "field goal"]}}

    Question: {question}
    Analysis:
    """
    return call_llm(prompt, system_instruction)

def extract_relevant_passage(question, question_analysis):
    """Extracts the relevant passage from the question based on keywords. Includes multiple examples."""
    system_instruction = "You are an expert at extracting relevant passages from text."
    prompt = f"""
    Extract the relevant passage from the following text based on the question and keywords.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Keywords: {{"type": "fact extraction", "keywords": ["final touchdown", "caught"]}}
    Text: PASSAGE: After a tough loss at home, the Browns traveled to take on the Packers. ... The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    Passage: The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    
    Example 2:
    Question: How many running backs ran for a touchdown?
    Keywords: {{"type": "counting", "keywords": ["running backs", "touchdown"]}}
    Text: PASSAGE: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. The Lions would respond with kicker Jason Hanson getting a 53-yard field goal. The Titans would answer with Johnson getting a 58-yard TD run, along with DE Dave Ball returning an interception 15 yards for a touchdown. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.
    Passage: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.

    Example 3:
    Question: Which player kicked the only field goal of the game?
    Keywords: {{"type": "fact extraction", "keywords": ["player", "field goal"]}}
    Text: PASSAGE: Game SummaryComing off their Thanksgiving road win over the Falcons, the Colts went home for a Week 13 AFC South rematch with the Jacksonville Jaguars.  ... In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.
    Passage: In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.

    Question: {question}
    Keywords: {question_analysis}
    Text: {question}
    Passage:
    """
    return call_llm(prompt, system_instruction)

def generate_answer(question, relevant_passage, question_analysis):
    """Generates the answer based on the question, relevant passage, and question type. Includes multiple examples."""
    system_instruction = "You are an expert at generating answers to questions based on provided text."
    prompt = f"""
    Generate the answer to the question based on the relevant passage and question type.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Passage: The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    Answer: Jarrett Boykin

    Example 2:
    Question: How many running backs ran for a touchdown?
    Passage: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. The Lions would respond with kicker Jason Hanson getting a 53-yard field goal. The Titans would answer with Johnson getting a 58-yard TD run, along with DE Dave Ball returning an interception 15 yards for a touchdown. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.
    Answer: 2
    
    Example 3:
    Question: Which player kicked the only field goal of the game?
    Passage: In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.
    Answer: Josh Scobee

    Question: {question}
    Passage: {relevant_passage}
    Answer:
    """
    return call_llm(prompt, system_instruction)

def verify_answer(question, answer, relevant_passage):
    """Verifies the generated answer. Includes multiple examples."""
    system_instruction = "You are an expert at verifying answers to questions."
    prompt = f"""
    Verify the following answer to the question based on the relevant passage.  Return the answer if it is correct.  Return the correct answer if it is incorrect.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Answer: Jarrett Boykin
    Passage: The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    Verification: Jarrett Boykin
    
    Example 2:
    Question: How many running backs ran for a touchdown?
    Answer: 2
    Passage: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.
    Verification: 2

    Example 3:
    Question: Which player kicked the only field goal of the game?
    Answer: Josh Scobee
    Passage: In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.
    Verification: Josh Scobee

    Question: {question}
    Answer: {answer}
    Passage: {relevant_passage}
    Verification:
    """
    return call_llm(prompt, system_instruction)

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
```

TOP PERFORMING APPROACH #2:
Iteration: 2
Accuracy: 1.00
Approach Summary: The script uses a multi-stage LLM approach with example-driven reasoning to answer questions, incorporating question analysis, information extraction, answer generation, and verification. The problem is decomposed into four main steps: analyzing the question, extracting a relevant passage, generating an answer, and verifying the answer, each handled by a separate function. There are no agent roles indicated in the script. The script uses `analyze_question` to identify the question type and keywords, `extract_relevant_passage` to find the relevant information, `generate_answer` to form an answer, `verify_answer` to confirm its correctness, and `call_llm` to interface with the Gemini LLM. The overall workflow involves analyzing the question, extracting relevant information, generating an initial answer, verifying it for accuracy, and then returning the verified answer.

Key approach aspects (full code available for reference)

TOP PERFORMING APPROACH #3:
Iteration: 0
Accuracy: 0.67
Approach Summary: The script employs a multi-stage, LLM-driven approach to answer questions, using chain-of-thought reasoning and verification. It decomposes the problem into question analysis, relevant passage extraction, answer generation, and answer verification. The agent roles include question analyzer, passage extractor, answer generator, and answer verifier, each defined by system instructions in their respective functions. The functions used are `main`, `analyze_question`, `extract_relevant_passage`, `generate_answer`, `verify_answer`, and `call_llm`. The `main` function orchestrates the process, calling `analyze_question` to understand the question, `extract_relevant_passage` to find relevant context, `generate_answer` to formulate an initial answer, and `verify_answer` to refine the response, using `call_llm` to interface with the Gemini model for each step.

Key approach aspects (full code available for reference)

            
FULL SCRIPT TO REFINE:
```python
import os
import re
import math

def main(question):
    """
    Solve the question using a multi-stage LLM approach.
    This approach focuses on breaking down the problem into question type identification, 
    focused passage extraction, and direct answer generation with verification.
    """
    try:
        # Step 1: Identify question type and keywords
        question_analysis = analyze_question(question)
        if "Error" in question_analysis:
            return "Error analyzing question"

        # Step 2: Extract relevant passage using identified keywords
        relevant_passage = extract_relevant_passage(question, question_analysis)
        if "Error" in relevant_passage:
            return "Error extracting passage"

        # Step 3: Generate answer using extracted passage and question type
        answer = generate_answer(question, relevant_passage, question_analysis)
        if "Error" in answer:
            return "Error generating answer"

        # Step 4: Verify answer
        verified_answer = verify_answer(question, answer, relevant_passage)
        if "Error" in verified_answer:
            return "Error verifying answer"
        
        return verified_answer

    except Exception as e:
        return f"General Error: {str(e)}"

def analyze_question(question):
    """Analyzes the question to identify its type and keywords. Includes multiple examples."""
    system_instruction = "You are an expert at analyzing questions to determine their type and keywords."
    prompt = f"""
    Analyze the following question and identify its type (e.g., fact extraction, calculation, comparison) and keywords.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Analysis: {{"type": "fact extraction", "keywords": ["final touchdown", "caught"]}}

    Example 2:
    Question: How many running backs ran for a touchdown?
    Analysis: {{"type": "counting", "keywords": ["running backs", "touchdown"]}}
    
    Example 3:
    Question: Which player kicked the only field goal of the game?
    Analysis: {{"type": "fact extraction", "keywords": ["player", "field goal"]}}

    Question: {question}
    Analysis:
    """
    return call_llm(prompt, system_instruction)

def extract_relevant_passage(question, question_analysis):
    """Extracts the relevant passage from the question based on keywords. Includes multiple examples."""
    system_instruction = "You are an expert at extracting relevant passages from text."
    prompt = f"""
    Extract the relevant passage from the following text based on the question and keywords.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Keywords: {{"type": "fact extraction", "keywords": ["final touchdown", "caught"]}}
    Text: PASSAGE: After a tough loss at home, the Browns traveled to take on the Packers. ... The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    Passage: The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    
    Example 2:
    Question: How many running backs ran for a touchdown?
    Keywords: {{"type": "counting", "keywords": ["running backs", "touchdown"]}}
    Text: PASSAGE: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. The Lions would respond with kicker Jason Hanson getting a 53-yard field goal. The Titans would answer with Johnson getting a 58-yard TD run, along with DE Dave Ball returning an interception 15 yards for a touchdown. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.
    Passage: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.

    Example 3:
    Question: Which player kicked the only field goal of the game?
    Keywords: {{"type": "fact extraction", "keywords": ["player", "field goal"]}}
    Text: PASSAGE: Game SummaryComing off their Thanksgiving road win over the Falcons, the Colts went home for a Week 13 AFC South rematch with the Jacksonville Jaguars.  ... In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.
    Passage: In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.

    Question: {question}
    Keywords: {question_analysis}
    Text: {question}
    Passage:
    """
    return call_llm(prompt, system_instruction)

def generate_answer(question, relevant_passage, question_analysis):
    """Generates the answer based on the question, relevant passage, and question type. Includes multiple examples."""
    system_instruction = "You are an expert at generating answers to questions based on provided text."
    prompt = f"""
    Generate the answer to the question based on the relevant passage and question type.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Passage: The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    Answer: Jarrett Boykin

    Example 2:
    Question: How many running backs ran for a touchdown?
    Passage: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. The Lions would respond with kicker Jason Hanson getting a 53-yard field goal. The Titans would answer with Johnson getting a 58-yard TD run, along with DE Dave Ball returning an interception 15 yards for a touchdown. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.
    Answer: 2
    
    Example 3:
    Question: Which player kicked the only field goal of the game?
    Passage: In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.
    Answer: Josh Scobee

    Question: {question}
    Passage: {relevant_passage}
    Answer:
    """
    return call_llm(prompt, system_instruction)

def verify_answer(question, answer, relevant_passage):
    """Verifies the generated answer. Includes multiple examples."""
    system_instruction = "You are an expert at verifying answers to questions."
    prompt = f"""
    Verify the following answer to the question based on the relevant passage.  Return the answer if it is correct.  Return the correct answer if it is incorrect.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Answer: Jarrett Boykin
    Passage: The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    Verification: Jarrett Boykin
    
    Example 2:
    Question: How many running backs ran for a touchdown?
    Answer: 2
    Passage: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.
    Verification: 2

    Example 3:
    Question: Which player kicked the only field goal of the game?
    Answer: Josh Scobee
    Passage: In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.
    Verification: Josh Scobee

    Question: {question}
    Answer: {answer}
    Passage: {relevant_passage}
    Verification:
    """
    return call_llm(prompt, system_instruction)

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
```

            PREVIOUSLY ATTEMPTED VARIATIONS:
            

            EXPLOITATION GUIDANCE:
            1. Review the error patterns, targeted improvements, and accumulated learnings carefully
            2. CRITICAL: Break down the problem into distinct reasoning steps before modifying code
            3. CRITICAL: Analyze the best scripts to identify which components are working well and which are failing. Focus your improvements on the weak points while preserving successful components.
            4. Maintain the core successful elements of the best approaches
            5. Consider how you can combine strengths from multiple top-performing approaches
            6. CRITICAL: Add EMBEDDED EXAMPLES to EVERY LLM prompt that illustrate:
               - Sample input that resembles the dataset
               - Step-by-step reasoning through the example
               - Properly formatted output
            7. Focus on fixing specific issues identified in previous error analyses. Create an explicit HYPOTHESIS for each targeted improvement, as well as a way to verify if it's successful.
            8. Enhance chain-of-thought reasoning and verification steps. Verification steps should be added to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
            9. Apply the key insights from ACCUMULATED LEARNINGS to enhance the approach
            10. Pay SPECIAL ATTENTION to the weaknesses and improvement suggestions from the capability assessment

            IMPROVEMENT STRATEGY:
            Analyze why the top approaches succeeded where others failed. Identify the key differentiators and strengthen them further.

            SYSTEMATIC ENHANCEMENT APPROACH:
            1. First, identify which specific function or component is underperforming based on error analysis
            2. Examine how error cases differ from successful cases
            3. For each identified weakness, implement a targeted enhancement
            4. Add additional verification steps around modified components
            5. Consider how components interact - ensure improvements don't break successful parts

            Consider enhancing the script with one or more of these patterns:
            - Repeated validation with feedback loops
            - Multi-perspective analysis with synthesis
            - Dynamic input-dependent routing
            - Hybrid approaches combining LLM with deterministic functions
            - Best-of-n solution generation and selection
            - ReAct pattern for interactive reasoning and action
            - If it is unknown how successful a processing state or part of the pipeline is, include verification steps to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
            - Answer checkers to validate the final answer against the problem statement. If the answer is incorrect, the checker can send the answer back to an earlier part of the system for refinement with feedback

            Here's how to call the Gemini API. Use this example without modification and don't invent configuration options:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

            Since this is an EXPLOITATION phase:
            - Build upon what's working well in the best approaches
            - Consider creative combinations of successful techniques from different scripts
            - Make TARGETED improvements to address specific error patterns
            - For EACH key LLM prompt, include a relevant example with:
              * Sample input similar to the dataset
              * Expected reasoning steps
              * Desired output format
            - Apply the knowledge from our accumulated learnings
            - Significantly enhance the script to address weaknesses identified in the capability assessment

            CRITICAL REQUIREMENTS:
            1. The script MUST properly handle all string literals - be extremely careful with quotes and triple quotes
            2. The script MUST NOT exceed 150 lines of code to prevent truncation
            3. Include detailed comments explaining your improvements
            4. EVERY SINGLE LLM PROMPT must include at least one embedded example showing:
               - Sample input with reasoning
               - Desired output format
            5. Make proper use of error handling
            6. Implement robust capabilities to address the specific weaknesses identified in the capability assessment
            7. Do NOT use json.loads() in the LLM calls to process input data. JSON formatting is good to use to structure information as inputs and outputs, but attempting to have functions process JSON data explicitly with strict built-in functionality is error prone due to formatting issues and additional text that appears as documentation, reasoning, or comments. When passing data into another LLM call, you can read it as plain text rather than trying to load it in strict json format, is the better approach.

            Return a COMPLETE, RUNNABLE Python script that:
            1. Has a main function that takes a question string as input and returns the answer string
            2. Makes multiple LLM calls for different reasoning steps
            3. Has proper error handling for API calls
            4. Includes embedded examples in EVERY LLM prompt
            5. Is COMPLETE - no missing code, no "..." placeholders
            6. Closes all string literals properly

            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            