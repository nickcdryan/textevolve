
            You are improving a Python script that solves problems from a dataset.
            Your goal is to REFINE and ENHANCE the best performing approaches by combining their strengths and addressing specific weaknesses identified in error analysis.

            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "PASSAGE: Nu Phoenicis is a yellow-white main sequence star of spectral type F9V and magnitude 4.96. Lying some 49 light years distant, it is around 1.2 times as massive as our sun, and likely to be surrounded by a disk of dust. It is the closest star in the constellation that is visible with the unaided eye. Gliese 915 is a white dwarf only 26 light years away. It is of magnitude 13.05, too faint to be seen with the naked eye. White dwarfs are extremely dense stars compacted into a volume the size of the Earth. With around 85% of the mass of the Sun, Gliese 915 has a surface gravity of 108.39 \u00b1 0.01 (2.45 \u00b7 108) centimetre\u00b7second\u22122, or approximately 250,000 of Earths gravity.\n\nQUESTION: Which star has a smaller mass, Nu Phoenicis or Gliese 915?",
    "answer": "Gliese 915"
  },
  {
    "id": 1,
    "question": "PASSAGE: Game SummaryComing off their Thanksgiving road win over the Falcons, the Colts went home for a Week 13 AFC South rematch with the Jacksonville Jaguars.  In the first quarter, Indianapolis scored first with QB Peyton Manning completing a 5-yard TD pass to TE Dallas Clark, along with a 48-yard TD pass to WR Reggie Wayne.  In the second quarter, the Jaguars got on the board with RB Maurice Jones-Drew getting a 2-yard TD run. Afterwards, the Colts replied with Manning and Clark hooking up with each other again on a 14-yard TD pass. In the third quarter, Jacksonville tried to come back as QB David Garrard completed a 2-yard TD pass to TE Mercedes Lewis for the only score of the period. In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal. However, the Colts responded with Manning completing a 1-yard TD pass to RB Luke Lawton. Afterwards, Jacksonville tried to come back as Garrard completed a 17-yard TD pass to WR Dennis Northcutt (along with getting the 2-point conversion run). Indianapolis' defense managed to seal the deal. With their season-sweep over the Jaguars, the Colts improved to 10-2. During the game, the Colts gave Garrard his first interception of the year, courtesy of Safety Antoine Bethea.\n\nQUESTION: Which player kicked the only field goal of the game?",
    "answer": "Josh Scobee"
  },
  {
    "id": 2,
    "question": "PASSAGE: Coming off their easy road win over the Rams, the Vikings went home for a Week 6 inter-conference duel with the Baltimore Ravens. Minnesota got off to a fast start in the first quarter with quarterback Brett Favre completing a 19-yard touchdown pass to tight end Visanthe Shiancoe and a 4-yard touchdown pass to wide receiver Bernard Berrian. Afterwards, the Ravens got the only points of the second quarter as kicker Steven Hauschka getting a 29-yard field goal. In the third quarter, the Vikings picked up where they left off with a 40-yard field goal from kicker Ryan Longwell. Baltimore responded with a 22-yard touchdown run from running back Ray Rice, yet Longwell helped out Minnesota by nailing a 22-yard field goal. Afterwards, an action-packed fourth quarter ensued. Minnesota increased its lead with Favre hooking up with Shiancoe again on a 1-yard touchdown pass, but the Ravens continued to hang around as quarterback Joe Flacco found wide receiver Mark Clayton on a 32-yard touchdown pass. The Vikings replied with Longwell's 29-yard field goal, but Baltimore took lead for the first time in the game as Flacco hooked up with wide receiver Derrick Mason on a 12-yard touchdown pass and Rice running 33 yards for a touchdown. Minnesota then regained the lead as Longwell booted a 31-yard field goal after a 58-yard pass from quarterback Brett Favre to wide receiver Sidney Rice. The Ravens got a last-minute drive into scoring range, but Hauschka's 44-yard field goal attempt went wide left, preserving the Vikings' perfect season. With the win, the Vikings acquired their first 6-0 start since 2003 (unfortunately that team did not make the playoffs). Also, dating back to Week 17 of the 2008 season, Minnesota has won seven-straight regular season games for the first time since 2000.\n\nQUESTION: Who threw the second longest touchdown pass?",
    "answer": "Brett Favre"
  }
]

            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 1
        - Current explore/exploit balance: 60/40
        - Best accuracy achieved: 0.67 (iteration 0)

        APPROACH HISTORY (last 1 iterations):
        [
  {
    "iteration": 0,
    "strategy": "Exploration",
    "accuracy": 0.6666666666666666,
    "approach": "The script uses chain-of-thought reasoning by decomposing a question into sub-questions, extracting relevant information, and then synthesizing an answer, with verification steps at each stage. The agents involved are question decomposer, information extraction expert, and answer synthesis expert, each responsible for their respective tasks. The `main` function orchestrates the process by calling `decompose_question`, `extract_information`, and `synthesize_answer` sequentially, using `call_llm` to interact with the LLM for each step, and validation checks are performed after each step to ensure the validity of the generated content. The overall workflow is question decomposition, information extraction, and answer synthesis, each validated by the LLM before proceeding to the next step."
  }
]

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 1 iterations):
        [
  {
    "iteration": 0,
    "issue": "The most critical problem is the **answer synthesis and validation logic's failure to consistently produce valid answers, even when the underlying reasoning and information extraction might be correct.** This is evidenced by the \"Error in answer synthesis\" message and the repeated validation failures. This is likely due to a mismatch between the expected answer format in the validation logic and the format being produced by the synthesis module."
  }
]

        TARGETED IMPROVEMENTS:
        []
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity âœ“
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity âœ“
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity âœ“
    4. All operations have O(1) average time complexity âœ“
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        

            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        ```
# DATASET-SPECIFIC LEARNINGS & EXPERIMENT LOG

This document tracks our evolving understanding and experimental findings related to the question answering task on the current dataset. It serves as a long-term memory to guide future research and development efforts.

## 1. DATASET PATTERNS & CHARACTERISTICS

*   **Question Structure:**
    *   All questions follow the format: `"PASSAGE:\n[passage text]\n\nQUESTION: [question text]"`.
    *   Questions are fact-based, requiring information retrieval and processing directly from the passage.
    *   Many questions involve numerical reasoning and comparison (e.g., "How many yards longer...", "Which star has a smaller mass...").
    *   Some questions ask for specific entities (e.g., "Who caught the final touchdown...", "Who threw the second longest touchdown pass?").
    *   Questions assume user access to and understanding of the passage.
    *   Focus areas include sports statistics/events and scientific descriptions.
*   **Answer Structure:**
    *   Answers are typically short phrases, numbers, or names.
    *   Answers are explicitly stated or directly derivable from the passage.
    *   Case sensitivity is observed.
    *   Answers don't require external knowledge.
*   **Passage Structure:**
    *   Passages vary in content (sports summaries, scientific descriptions like astronomy).
    *   Sports passages often involve more complex event tracking (who did what, in what order), which pose extraction challenges.
    *   Passages can be lengthy, requiring careful reading.
*   **Domain Knowledge:**
    *   Basic sports terminology (football positions, scoring) is needed for sports-related examples.
    *   General reading comprehension is crucial.
    *   For demographic examples, basic understanding of fertility rates is needed.
*   **Question Types:**
    *   Entity Extraction: Identifying specific players or entities.
    *   Numerical Comparison: Comparing numerical values (yards, scores, TFR, mass) to determine differences.
    *   Counting: Counting the number of occurrences of an event or entity.
*   **Reasoning Types:**
    *   Direct Extraction: Finding the answer directly stated in the passage.
    *   Simple Arithmetic: Performing basic calculations (addition, subtraction) using numbers from the passage.
    *   Logical Deduction: Combining information from different parts of the passage to arrive at the answer.
*   **Non-Obvious Patterns:**
    *   Chronological order of events is important. Time-related keywords (e.g., "final," "first," "later," "second") help narrow the search.
*   **Edge Cases/Complexities:**
    *   Passages with ambiguous or contradictory information.
    *   Questions requiring more complex arithmetic operations (e.g., multiplication, division).
    *   Questions with implicit rather than explicit answers, requiring deeper inference.
    *   Handling of units (yards, points, etc.) consistently.
    *   Sports narratives can induce errors because they require an understanding of event order and length, and they need accurate extraction of details such as players and actions.

## 2. EFFECTIVE TASK-SPECIFIC STRATEGIES

*   **Keyword-Based Retrieval:** Identify keywords from the question and search for those keywords in the passage to narrow down relevant sections.
*   **LLM-Based Extraction & Reasoning:** Use an LLM to directly answer the question based on the passage. Prompting is key here.
*   **Hybrid Approach:** Combine keyword retrieval with LLM reasoning. Use keywords to identify relevant sections, then use the LLM to extract the answer from those sections.
*   **Question Decomposition:** Decompose the question into smaller sub-questions that are easier to answer.
*   **Chain-of-Thought Reasoning:** While showing mixed results, using chain-of-thought prompting to encourage step-by-step reasoning has potential and should be refined.
*   **Zero-Shot Reasoning:** Directly ask the LLM to answer the question based on the passage, using a well-crafted prompt.
*   **Text Splitting:** If passages are too long, split them into smaller chunks and process each chunk separately. Be mindful of context loss.
*   **Focus on Answer Synthesis:** Prioritize improving the `synthesize_answer` function.

## 3. COMMON FAILURE MODES ON THIS DATASET

*   **Answer Synthesis Failure:** Inability to synthesize a valid answer. This is the most prominent failure mode. This suggests a problem with the agent responsible for generating the final answer from the extracted information or a mismatch between the expected answer format and the format being generated.
*   **Sports narrative complexity:** The questions about sports narratives such as "Who threw the second longest touchdown pass?" can induce errors because they require an understanding of event order and length and they need accurate extraction of details such as players and actions. If the model fails to correctly extract who completed each pass and their distance, it will fail to determine the second-longest pass.
*   **Validation sensitivity:** Validation checks can be overly strict. For example, if the correct answer is `Brett Favre` but the agent responds `Brett Favre.`, the answer would be marked as incorrect.
*   **Information Extraction Failure:** Errors in information extraction contribute to failures in answer synthesis. Inaccurate or incomplete extracted information leads to incorrect final answers.
*   **Ambiguous Passages:** Passages with ambiguous or contradictory information can lead to incorrect answers.
*   **Missing Information:** If the answer cannot be found in the passage, this leads to a failure.

## 4. EXPERIMENT LOG & FINDINGS

*   **2025-05-17 13:35:33: INITIAL DATASET ANALYSIS**
    *   Comprehensive analysis of dataset characteristics, challenges, potential approaches, and implementation recommendations.
    *   Identified key aspects of question and answer structure, domain knowledge required, and reasoning types.
    *   Highlighted potential solution strategies: Keyword-Based Retrieval, LLM-Based Extraction & Reasoning, Hybrid Approach.
    *   Outlined decomposition steps: Question Analysis, Passage Filtering, Information Extraction, Answer Generation.
    *   Proposed validation techniques: Consistency Checks, Unit Analysis, Numerical Validation, Fact Verification.
    *   Recommended implementation steps: Keyword Matching, Numerical Range Checks, Extracted Facts Representation, Relevant Sentences Storage.
    *   Prompt engineering techniques for effective text processing were proposed, including chain-of-thought prompting.
*   **ITERATION 0:**
    *   **Accuracy:** 0.67
    *   **Goal:** Improve performance using question decomposition and reasoning.
    *   **Finding:** The implementation of question decomposition has challenges with reliable answer synthesis.
    *   **Finding:** Frequent validation failures indicate problems with the downstream steps, even if decomposition is successful.
    *   **Finding:** Sports-related questions involving temporal reasoning (e.g., "Who threw the second longest touchdown pass?") pose significant challenges.
    *   **Insight:** Validation is helpful for diagnosing system performance, specifically highlighting failures in answer synthesis.

## 5. NEXT RESEARCH DIRECTIONS

*   **Answer Synthesis Improvement:**
    *   Debug and improve the `synthesize_answer` function.
    *   Examine the input it receives (extracted information) and the logic used to generate the final answer.
    *   Pay close attention to the expected answer format and ensure it is being correctly generated.
    *   Include thorough error logging to pinpoint where in the answer synthesis process the errors are occurring.
*   **Refine Validation Logic:**
    *   Review the validation checks to ensure they are not overly strict.
    *   Accurately reflect the expected answer format for this dataset, allowing for minor variations in phrasing or punctuation.
*   **Evaluate Information Extraction Success:**
    *   Assess how frequently information extraction fails.
    *   Use the validation component to diagnose problems with extraction.
*   **Dataset Split & Analysis:**
    *   Split the dataset by passage type (e.g., sports, science).
    *   Analyze performance on each subset to identify specific challenges.
    *   Focus on improving performance on the challenging sports scenarios that require temporal information and comparisons.
*   **Prompt Engineering:**
    *   Experiment with different prompt formulations for both information extraction and answer synthesis.
    *   Explore techniques like few-shot learning and chain-of-thought prompting to improve LLM reasoning.
*   **Context Window Management:**
    *   Implement strategies for handling long passages, such as text splitting and summarization.
    *   Ensure that relevant context is preserved when splitting passages.
*   **Error Analysis:**
    *   Manually analyze failed examples to identify the root cause of the errors.
    *   Categorize errors based on the type of question, passage, and reasoning required.
*   **External Knowledge Integration:**
    *   Investigate the use of external knowledge sources to augment the information provided in the passage.
    *   This may be helpful for resolving ambiguities or providing additional context.
```
        

            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        

            TOP PERFORMING APPROACHES TO BUILD UPON:
            
TOP PERFORMING APPROACH #1:
Iteration: 0
Accuracy: 0.67
Approach Summary: The script uses chain-of-thought reasoning by decomposing a question into sub-questions, extracting relevant information, and then synthesizing an answer, with verification steps at each stage. The agents involved are question decomposer, information extraction expert, and answer synthesis expert, each responsible for their respective tasks. The `main` function orchestrates the process by calling `decompose_question`, `extract_information`, and `synthesize_answer` sequentially, using `call_llm` to interact with the LLM for each step, and validation checks are performed after each step to ensure the validity of the generated content. The overall workflow is question decomposition, information extraction, and answer synthesis, each validated by the LLM before proceeding to the next step.

FULL SCRIPT TO REFINE:
```python
import os
import re

def main(question):
    """
    Solve the question by extracting relevant information from the passage and using chain-of-thought reasoning.
    This approach tests the hypothesis that focusing on question decomposition will increase performance,
    and validation calls are included throughout to help diagnose system performance
    """
    try:
        # Step 1: Decompose the question into sub-questions.
        decomposition_result = decompose_question(question)
        if not decomposition_result.get("is_valid"):
            return f"Error in question decomposition: {decomposition_result.get('validation_feedback')}"
        
        # Step 2: Extract relevant information based on sub-questions.
        information_extraction_result = extract_information(question, decomposition_result["sub_questions"])
        if not information_extraction_result.get("is_valid"):
            return f"Error in information extraction: {information_extraction_result.get('validation_feedback')}"

        # Step 3: Synthesize the answer from extracted information.
        answer_synthesis_result = synthesize_answer(question, information_extraction_result["extracted_info"])
        if not answer_synthesis_result.get("is_valid"):
            return f"Error in answer synthesis: {answer_synthesis_result.get('validation_feedback')}"
        
        return answer_synthesis_result["answer"]

    except Exception as e:
        return f"An unexpected error occurred: {str(e)}"

def decompose_question(question, max_attempts=3):
    """Decompose the main question into smaller, answerable sub-questions."""
    system_instruction = "You are an expert question decomposer."
    
    for attempt in range(max_attempts):
        decomposition_prompt = f"""
        Decompose the given question into smaller, self-contained sub-questions that, when answered, will fully answer the original question.
        
        Example 1:
        Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions:
        1. How many yards was Chris Johnson's first touchdown?
        2. How many yards was Jason Hanson's first field goal?
        3. What is the sum of those two values?
        
        Example 2:
        Question: Who caught the final touchdown of the game?
        Sub-questions:
        1. Who scored the final touchdown of the game?
        
        Question: {question}
        Sub-questions:
        """
        
        decomposition_result = call_llm(decomposition_prompt, system_instruction)
        
        # Verify if the decomposition is valid
        verification_prompt = f"""
        Verify if these sub-questions are valid and sufficient to answer the original question.
        
        Original Question: {question}
        Sub-questions: {decomposition_result}
        
        Is the decomposition valid and sufficient? (yes/no)
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)
        
        if "yes" in verification_result.lower():
            return {"is_valid": True, "sub_questions": decomposition_result}
        else:
            print(f"Decomposition validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to decompose the question successfully."}

def extract_information(question, sub_questions, max_attempts=3):
    """Extract relevant information from the passage based on the sub-questions."""
    system_instruction = "You are an information extraction expert."
    
    for attempt in range(max_attempts):
        extraction_prompt = f"""
        Given the original question and its sub-questions, extract the relevant information from the passage required to answer the sub-questions.
        
        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions:
        1. How many yards was Chris Johnson's first touchdown?
        2. How many yards was Jason Hanson's first field goal?
        Extracted Information:
        Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.
        
        Original Question: {question}
        Sub-questions: {sub_questions}
        Extracted Information:
        """
        
        extracted_info = call_llm(extraction_prompt, system_instruction)
        
        # Validate information extraction
        verification_prompt = f"""
        Verify if the extracted information is relevant and sufficient to answer the sub-questions.
        
        Original Question: {question}
        Sub-questions: {sub_questions}
        Extracted Information: {extracted_info}
        
        Is the extraction relevant and sufficient? (yes/no)
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)
        
        if "yes" in verification_result.lower():
            return {"is_valid": True, "extracted_info": extracted_info}
        else:
            print(f"Information extraction validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to extract relevant information successfully."}

def synthesize_answer(question, extracted_info, max_attempts=3):
    """Synthesize the answer from the extracted information to answer the main question."""
    system_instruction = "You are an answer synthesis expert."

    for attempt in range(max_attempts):
        synthesis_prompt = f"""
        Given the original question and the extracted information, synthesize the final answer.
        
        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Extracted Information: Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.
        Final Answer: 59
        
        Original Question: {question}
        Extracted Information: {extracted_info}
        Final Answer:
        """
        
        answer = call_llm(synthesis_prompt, system_instruction)

        # Answer checker
        verification_prompt = f"""
        Check if the answer is correct and answers the original question fully.
        
        Original Question: {question}
        Synthesized Answer: {answer}
        
        Is the answer correct and complete? (yes/no)
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)

        if "yes" in verification_result.lower():
            return {"is_valid": True, "answer": answer}
        else:
            print(f"Answer synthesis validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to synthesize a valid answer."}

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types
        import os  # Import the os module

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
```

            
FULL SCRIPT TO REFINE:
```python
import os
import re

def main(question):
    """
    Solve the question by extracting relevant information from the passage and using chain-of-thought reasoning.
    This approach tests the hypothesis that focusing on question decomposition will increase performance,
    and validation calls are included throughout to help diagnose system performance
    """
    try:
        # Step 1: Decompose the question into sub-questions.
        decomposition_result = decompose_question(question)
        if not decomposition_result.get("is_valid"):
            return f"Error in question decomposition: {decomposition_result.get('validation_feedback')}"
        
        # Step 2: Extract relevant information based on sub-questions.
        information_extraction_result = extract_information(question, decomposition_result["sub_questions"])
        if not information_extraction_result.get("is_valid"):
            return f"Error in information extraction: {information_extraction_result.get('validation_feedback')}"

        # Step 3: Synthesize the answer from extracted information.
        answer_synthesis_result = synthesize_answer(question, information_extraction_result["extracted_info"])
        if not answer_synthesis_result.get("is_valid"):
            return f"Error in answer synthesis: {answer_synthesis_result.get('validation_feedback')}"
        
        return answer_synthesis_result["answer"]

    except Exception as e:
        return f"An unexpected error occurred: {str(e)}"

def decompose_question(question, max_attempts=3):
    """Decompose the main question into smaller, answerable sub-questions."""
    system_instruction = "You are an expert question decomposer."
    
    for attempt in range(max_attempts):
        decomposition_prompt = f"""
        Decompose the given question into smaller, self-contained sub-questions that, when answered, will fully answer the original question.
        
        Example 1:
        Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions:
        1. How many yards was Chris Johnson's first touchdown?
        2. How many yards was Jason Hanson's first field goal?
        3. What is the sum of those two values?
        
        Example 2:
        Question: Who caught the final touchdown of the game?
        Sub-questions:
        1. Who scored the final touchdown of the game?
        
        Question: {question}
        Sub-questions:
        """
        
        decomposition_result = call_llm(decomposition_prompt, system_instruction)
        
        # Verify if the decomposition is valid
        verification_prompt = f"""
        Verify if these sub-questions are valid and sufficient to answer the original question.
        
        Original Question: {question}
        Sub-questions: {decomposition_result}
        
        Is the decomposition valid and sufficient? (yes/no)
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)
        
        if "yes" in verification_result.lower():
            return {"is_valid": True, "sub_questions": decomposition_result}
        else:
            print(f"Decomposition validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to decompose the question successfully."}

def extract_information(question, sub_questions, max_attempts=3):
    """Extract relevant information from the passage based on the sub-questions."""
    system_instruction = "You are an information extraction expert."
    
    for attempt in range(max_attempts):
        extraction_prompt = f"""
        Given the original question and its sub-questions, extract the relevant information from the passage required to answer the sub-questions.
        
        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions:
        1. How many yards was Chris Johnson's first touchdown?
        2. How many yards was Jason Hanson's first field goal?
        Extracted Information:
        Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.
        
        Original Question: {question}
        Sub-questions: {sub_questions}
        Extracted Information:
        """
        
        extracted_info = call_llm(extraction_prompt, system_instruction)
        
        # Validate information extraction
        verification_prompt = f"""
        Verify if the extracted information is relevant and sufficient to answer the sub-questions.
        
        Original Question: {question}
        Sub-questions: {sub_questions}
        Extracted Information: {extracted_info}
        
        Is the extraction relevant and sufficient? (yes/no)
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)
        
        if "yes" in verification_result.lower():
            return {"is_valid": True, "extracted_info": extracted_info}
        else:
            print(f"Information extraction validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to extract relevant information successfully."}

def synthesize_answer(question, extracted_info, max_attempts=3):
    """Synthesize the answer from the extracted information to answer the main question."""
    system_instruction = "You are an answer synthesis expert."

    for attempt in range(max_attempts):
        synthesis_prompt = f"""
        Given the original question and the extracted information, synthesize the final answer.
        
        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Extracted Information: Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.
        Final Answer: 59
        
        Original Question: {question}
        Extracted Information: {extracted_info}
        Final Answer:
        """
        
        answer = call_llm(synthesis_prompt, system_instruction)

        # Answer checker
        verification_prompt = f"""
        Check if the answer is correct and answers the original question fully.
        
        Original Question: {question}
        Synthesized Answer: {answer}
        
        Is the answer correct and complete? (yes/no)
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)

        if "yes" in verification_result.lower():
            return {"is_valid": True, "answer": answer}
        else:
            print(f"Answer synthesis validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to synthesize a valid answer."}

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types
        import os  # Import the os module

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
```

            PREVIOUSLY ATTEMPTED VARIATIONS:
            

            EXPLOITATION GUIDANCE:
            1. Review the error patterns, targeted improvements, and accumulated learnings carefully
            2. CRITICAL: Break down the problem into distinct reasoning steps before modifying code
            3. CRITICAL: Analyze the best scripts to identify which components are working well and which are failing. Focus your improvements on the weak points while preserving successful components.
            4. Maintain the core successful elements of the best approaches
            5. Consider how you can combine strengths from multiple top-performing approaches
            6. CRITICAL: Add EMBEDDED EXAMPLES to EVERY LLM prompt that illustrate:
               - Sample input that resembles the dataset
               - Step-by-step reasoning through the example
               - Properly formatted output
            7. Focus on fixing specific issues identified in previous error analyses. Create an explicit HYPOTHESIS for each targeted improvement, as well as a way to verify if it's successful.
            8. Enhance chain-of-thought reasoning and verification steps. Verification steps should be added to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
            9. Apply the key insights from ACCUMULATED LEARNINGS to enhance the approach
            10. Pay SPECIAL ATTENTION to the weaknesses and improvement suggestions from the capability assessment

            IMPROVEMENT STRATEGY:
            Analyze why the top approaches succeeded where others failed. Identify the key differentiators and strengthen them further.

            SYSTEMATIC ENHANCEMENT APPROACH:
            1. First, identify which specific function or component is underperforming based on error analysis
            2. Examine how error cases differ from successful cases
            3. For each identified weakness, implement a targeted enhancement
            4. Add additional verification steps around modified components
            5. Consider how components interact - ensure improvements don't break successful parts

            Consider enhancing the script with one or more of these patterns:
            - Repeated validation with feedback loops
            - Multi-perspective analysis with synthesis
            - Dynamic input-dependent routing
            - Hybrid approaches combining LLM with deterministic functions
            - Best-of-n solution generation and selection
            - ReAct pattern for interactive reasoning and action
            - If it is unknown how successful a processing state or part of the pipeline is, include verification steps to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
            - Answer checkers to validate the final answer against the problem statement. If the answer is incorrect, the checker can send the answer back to an earlier part of the system for refinement with feedback

            Here's how to call the Gemini API. Use this example without modification and don't invent configuration options:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

            Since this is an EXPLOITATION phase:
            - Build upon what's working well in the best approaches
            - Consider creative combinations of successful techniques from different scripts
            - Make TARGETED improvements to address specific error patterns
            - For EACH key LLM prompt, include a relevant example with:
              * Sample input similar to the dataset
              * Expected reasoning steps
              * Desired output format
            - Apply the knowledge from our accumulated learnings
            - Significantly enhance the script to address weaknesses identified in the capability assessment

            CRITICAL REQUIREMENTS:
            1. The script MUST properly handle all string literals - be extremely careful with quotes and triple quotes
            2. The script MUST NOT exceed 150 lines of code to prevent truncation
            3. Include detailed comments explaining your improvements
            4. EVERY SINGLE LLM PROMPT must include at least one embedded example showing:
               - Sample input with reasoning
               - Desired output format
            5. Make proper use of error handling
            6. Implement robust capabilities to address the specific weaknesses identified in the capability assessment
            7. Do NOT use json.loads() in the LLM calls to process input data. JSON formatting is good to use to structure information as inputs and outputs, but attempting to have functions process JSON data explicitly with strict built-in functionality is error prone due to formatting issues and additional text that appears as documentation, reasoning, or comments. When passing data into another LLM call, you can read it as plain text rather than trying to load it in strict json format, is the better approach.

            Return a COMPLETE, RUNNABLE Python script that:
            1. Has a main function that takes a question string as input and returns the answer string
            2. Makes multiple LLM calls for different reasoning steps
            3. Has proper error handling for API calls
            4. Includes embedded examples in EVERY LLM prompt
            5. Is COMPLETE - no missing code, no "..." placeholders
            6. Closes all string literals properly

            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            