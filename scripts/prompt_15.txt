
            You are improving a Python script that solves problems from a dataset.
            Your goal is to REFINE and ENHANCE the best performing approaches by combining their strengths and addressing specific weaknesses identified in error analysis.

            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "PASSAGE: Still looking for their first win, the Bengals flew to The Meadowlands for a Week 6 duel with the throwback-clad New York Jets.  With QB Carson Palmer out again nursing an injured elbow, QB Ryan Fitzpatrick was again named the starter. In the first quarter, Cincinnati pounced first as strong safety Chinedum Ndukwe returned a fumble 15 yards for a touchdown.  The Jets responded with QB Brett Favre completing a 2-yard TD pass to RB Thomas Jones.  In the second quarter, New York took the lead as kicker Jay Feely got a 38-yard field goal, while Jones got a 7-yard TD run.  The Bengals closed out the half with Fitzpatrick getting a 1-yard TD run. In the third quarter, the Jets began to pull away as Feely got a 43-yard field goal.  In the fourth quarter, New York sealed the win as Jones got a 1-yard TD run (with a failed 2-point conversion). With the loss, Cincinnati fell to 0-6 and it became their first 0-6 start as well as 6-game losing streak since 2002.\n\nQUESTION: Who scored the first touchdown of the game?",
    "answer": "Chinedum Ndukwe"
  },
  {
    "id": 1,
    "question": "PASSAGE: Hoping to rebound from their road loss to the Vikings, the Panthers went home for a Week 4 NFC South duel with the Atlanta Falcons.  In the first quarter, Carolina pounced first with rookie RB Jonathan Stewart getting an 8-yard TD run.  The Falcons responded with kicker Jason Elam getting a 33-yard field goal.  In the second quarter, Atlanta crept closer as Elam kicked a 33-yard field goal.  Carolina would reply with QB Jake Delhomme completing a 56-yard TD pass to WR Steve Smith.  The Falcons closed out the half with Elam getting a 44-yard field goal. In the third quarter, the Panthers answered with kicker John Kasay nailing a 44-yard field goal.  In the fourth quarter, Carolina closed the game out with Delhomme completing a 36-yard TD pass to WR Muhsin Muhammad. During the game, Muhsin Muhammad (8 receptions for 147 yards and a touchdown) became the Panthers All-Time WR Leader in Receptions (600) and TD Receptions (45).\n\nQUESTION: How many yards shorter was Jake Delhomme's second touchdown pass compared to his first?",
    "answer": "20"
  },
  {
    "id": 2,
    "question": "PASSAGE: In 2000 there were 79,667 households out of which 38.70% had children under the age of 18 living with them, 61.90% were married couples living together, 10.20% had a female householder with no husband present, and 24.20% were non-families. 19.70% of all households were made up of individuals and 6.80% had someone living alone who was 65 years of age or older.  The average household size was 2.72 and the average family size was 3.14.\n\nQUESTION: How many percent are not non-families?",
    "answer": "24.2"
  }
]

            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 15
        - Current explore/exploit balance: 40/60
        - Best accuracy achieved: 1.00 (iteration 1)

        APPROACH HISTORY (last 10 iterations):
        [
  {
    "iteration": 5,
    "strategy": "Exploitation",
    "accuracy": 0.8,
    "approach": "This script uses a multi-stage LLM approach with techniques like question analysis, relevant passage extraction, answer generation, and verification. The problem is decomposed into analyzing the question, extracting the relevant passage, generating an initial answer, and then verifying that answer for correctness. The agent roles are implied by the function names, such as a question analyzer, passage extractor, answer generator, and answer verifier. Other functions used are `call_llm` which is used by the agent roles to call the LLM with specific prompts and system instructions.\n\nThe overall workflow begins with `main` calling `analyze_question` to determine the question type and keywords, then `extract_relevant_passage` uses this analysis to extract the most relevant information, then `generate_answer` uses the extracted passage and question analysis to generate an answer, and finally, `verify_answer` verifies the generated answer against the passage."
  },
  {
    "iteration": 6,
    "strategy": "Exploitation",
    "accuracy": 0.8,
    "approach": "The script uses a multi-stage LLM approach to answer questions by analyzing the question, extracting relevant information, generating an initial answer, and then verifying it. The problem is decomposed into four main steps: question analysis, passage extraction, answer generation, and answer verification, each leveraging a specific LLM agent role. The functions used are `main`, `analyze_question`, `extract_relevant_passage`, `generate_answer`, `verify_answer`, and `call_llm`. The workflow begins with `main` calling `analyze_question`, then `extract_relevant_passage` using the output of `analyze_question`, followed by `generate_answer` utilizing the outputs of the previous two functions, and finally `verify_answer` leveraging the outputs of `generate_answer` and `extract_relevant_passage` before returning the verified answer."
  },
  {
    "iteration": 7,
    "strategy": "Exploitation",
    "accuracy": 0.4,
    "approach": "The script uses a multi-stage LLM approach to answer questions, incorporating question analysis and answer verification. It decomposes the problem into question analysis, passage extraction, answer generation, and answer verification, leveraging the LLM at each stage. No explicit agent roles are defined, but each function acts as a step in a chain. The functions `analyze_question`, `extract_relevant_passage`, `generate_answer`, and `verify_answer` sequentially process the question, each using `call_llm` to interact with the Gemini model. The `main` function orchestrates the overall workflow by calling these functions in sequence and handling potential errors."
  },
  {
    "iteration": 8,
    "strategy": "Exploitation",
    "accuracy": 0.7,
    "approach": "The script implements a multi-stage question answering system using the Gemini LLM. It decomposes the problem into question analysis, relevant passage extraction, answer generation, and answer verification. The `analyze_question` function identifies the question type and keywords, then `extract_relevant_passage` retrieves relevant information. An initial answer is generated using `generate_answer` which is then validated by the `verify_answer` function. The overall workflow starts with `main`, which orchestrates calls to `analyze_question`, `extract_relevant_passage`, `generate_answer`, and `verify_answer`, with `call_llm` serving as a centralized method for LLM interactions."
  },
  {
    "iteration": 9,
    "strategy": "Exploitation",
    "accuracy": 1.0,
    "approach": "The script uses a multi-stage LLM approach, including chain-of-thought reasoning by decomposing the problem into question analysis, relevant passage extraction, answer generation, and answer verification. It uses the `call_llm` function to interact with the Gemini model with specific system instructions and prompts at each stage, effectively assigning different \"expert\" roles to the LLM. The workflow involves `analyze_question` to determine the question type and keywords, `extract_relevant_passage` to find relevant text, `generate_answer` to create an initial answer, and `verify_answer` to confirm the answer and perform calculations if needed."
  },
  {
    "iteration": 10,
    "strategy": "Exploration",
    "accuracy": 0.7,
    "approach": "The script uses an iterative refinement approach driven by multiple LLM-based verifiers to answer a question. The problem is decomposed into initial answer generation, factual verification, arithmetic verification (if needed), and answer refinement. It leverages different agent roles: an initial answer generator, a factual verifier, an arithmetic verifier, and a refiner.\n\nThe functions used are `main` orchestrates the entire process, `generate_initial_answer` generates the first answer, `refine_answer_with_multiple_verifiers` manages the iterative refinement, `get_factual_feedback` and `get_arithmetic_feedback` provide verification feedback, `refine_answer` refines the answer based on feedback, and `call_llm` is the function that calls the LLM with a given prompt and system instruction. The workflow begins with an initial answer that is then iteratively refined using feedback from factual and arithmetic verifiers."
  },
  {
    "iteration": 11,
    "strategy": "Exploitation",
    "accuracy": 0.7,
    "approach": "The script implements a multi-stage question answering system leveraging LLMs with a focus on enhanced question analysis and answer verification. It decomposes the problem into four main steps: question analysis, relevant passage extraction, answer generation, and answer verification. Each step utilizes the `call_llm` function with a specific prompt and system instruction to engage the LLM as an expert in different roles. The workflow starts with `main()` which calls `analyze_question()` to determine the question type and keywords, then `extract_relevant_passage()` to find relevant information. Subsequently, `generate_answer()` creates an answer based on the extracted passage and question analysis, and finally, `verify_answer()` checks the answer for correctness."
  },
  {
    "iteration": 12,
    "strategy": "Exploitation",
    "accuracy": 0.9,
    "approach": "The script implements a multi-stage LLM approach to answer questions, using chain-of-thought reasoning and verification. It decomposes the problem into question analysis, relevant passage extraction, answer generation, and answer verification. The agent roles are question analyzer, passage extractor, answer generator, and answer verifier, each implemented within their corresponding functions. The `main` function orchestrates the process, calling `analyze_question`, `extract_relevant_passage`, `generate_answer`, and `verify_answer` in sequence, with `call_llm` used to interact with the Gemini LLM in each of these functions."
  },
  {
    "iteration": 13,
    "strategy": "Exploitation",
    "accuracy": 0.5,
    "approach": "The script employs a multi-stage LLM approach, using chain-of-thought and verification to answer questions. The problem is decomposed into question analysis, relevant passage extraction, answer generation, and answer verification.  The agent roles are implicit within each function which includes question analyzer, passage extractor, answer generator, and answer verifier. The script leverages the `call_llm` function to interact with the Gemini LLM.\n\nThe workflow is as follows: `main` calls `analyze_question` to determine question type and keywords, then `extract_relevant_passage` to get relevant context, `generate_answer` to create an initial answer, and finally `verify_answer` to validate the response."
  },
  {
    "iteration": 14,
    "strategy": "Exploitation",
    "accuracy": 0.8,
    "approach": "The script uses a multi-stage LLM approach to answer questions by analyzing the question, extracting relevant information, generating an initial answer, and then verifying it. The problem is decomposed into four main steps: question analysis, passage extraction, answer generation, and answer verification. Each step is handled by a specialized LLM agent with an \"expert\" role. The `call_llm` function is used to interact with the Gemini LLM, and other functions are `analyze_question`, `extract_relevant_passage`, `generate_answer`, and `verify_answer`, which are called in sequence within the `main` function to process the question and refine the answer. The overall workflow involves analyzing the question, extracting information, generating an answer and verifying the answer for accuracy."
  }
]

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 3 iterations):
        [
  {
    "iteration": 5,
    "issue": "The primary issue is the system's inability to correctly perform temporal reasoning, specifically calculating the time difference between events mentioned in the passage. This often involves correctly identifying the start and end points of the interval and then performing the subtraction."
  },
  {
    "iteration": 6,
    "issue": "The most critical problem is the **absence of a functional arithmetic reasoning component** that can identify and execute basic calculations based on extracted numerical information. This prevents the system from answering questions that go beyond simple information retrieval."
  },
  {
    "iteration": 7,
    "issue": "The primary issue is the system's **inadequate question understanding and answer type classification**. The system fails to determine the expected answer format (numerical, textual, boolean) and falls back to providing generic confirmations (\"Correct\" or \"True\") or verifications."
  },
  {
    "iteration": 8,
    "issue": "The primary issue is the system's unreliable ability to extract *all* relevant information from the passage to perform correct calculations and to discern the required level of precision for numerical answers. This manifests as incorrect calculations, failure to identify key elements, and overly broad range-based answers when a specific number is needed."
  },
  {
    "iteration": 9,
    "issue": "The primary issue is not an error, but an inefficiency: **Unnecessary verbosity and potentially redundant reasoning steps**, leading to a less-than-optimal response."
  },
  {
    "iteration": 10,
    "issue": "The most critical problem is the **inaccurate and imprecise temporal reasoning**, specifically when calculating durations and dealing with nuances in time expressions (e.g., \"roughly\"). This leads to incorrect answers even when the necessary information is present in the passage."
  },
  {
    "iteration": 11,
    "issue": "The most critical problem to fix is the system's **inability to accurately perform arithmetic operations, specifically addition,** which prevents it from arriving at the correct answer even when it identifies the relevant information."
  },
  {
    "iteration": 12,
    "issue": "The primary issue is the system's inconsistent answer formatting, specifically the omission of units of measurement and incorrect ordering of multiple answer components, leading to mismatches with the golden answers."
  },
  {
    "iteration": 13,
    "issue": "The primary issue is the system's inability to perform multi-step reasoning, arithmetic calculations, and handle contextual misdirection when required to answer questions."
  },
  {
    "iteration": 14,
    "issue": "The most critical problem is **the system's inability to robustly interpret questions involving negation and to connect the numbers in the passage to the correct context.** This hinders its ability to extract the correct information and apply the correct reasoning to solve the problem."
  }
]

        TARGETED IMPROVEMENTS:
        [
  "Implement a constraint satisfaction mechanism to ensure that the generated answers satisfy all constraints mentioned in the passage.",
  "Add a module that analyzes the question to determine the expected output format (e.g., a specific number, a range, a named entity).",
  "Develop a more robust reasoning engine that can chain together multiple inferences and calculations based on extracted information.",
  "Implement Date Parsing and Calculation:** Integrate a date parsing library to accurately extract date information from the passage and calculate time differences with precision.",
  "Fine-tune Temporal Reasoning Rules:** Refine the rules for temporal reasoning to account for modifiers like \"roughly\" and to distinguish between different units of time.",
  "Implement a system that breaks down complex extraction tasks into smaller, more manageable steps, ensuring that *all* relevant information is considered.",
  "Add logic to specifically handle numerical ranges, identify the individual numbers and relationships between them.",
  "Enhanced Relationship Extraction:** Integrate an advanced NER (Named Entity Recognition) with relationship extraction.",
  "Implement more sophisticated techniques for identifying entities, relationships, and attributes within the passage. This may involve using semantic parsing or relation extraction models.",
  "Fine-tune the system to provide precise answers whenever possible and to only provide ranges when the question explicitly asks for them or when uncertainty exists. Add a rule to consider the end-points of ranges as options."
]
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity ✓
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity ✓
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity ✓
    4. All operations have O(1) average time complexity ✓
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        

            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        ```
# DATASET-SPECIFIC LEARNINGS & EXPERIMENT LOG

This document serves as a running log of our learnings, experiments, and findings specific to the question-answering task on this particular dataset of sports narratives and demographic data.

## 1. DATASET PATTERNS & CHARACTERISTICS

*   **Passage Dependency:** Questions are heavily reliant on the provided passage for accurate answers.
*   **Question Structure:** Questions are consistently formatted as `PASSAGE:\n... \n\nQUESTION: ...`.
*   **Passage Content:** Passages consist of narratives describing sports games (primarily American football) or demographic data. They provide the context needed to answer the questions. Passages exhibit variability in content (sports, demographics, geography, nickel production, history) and length. Passages often take the form of narratives describing events, which can be challenging for precise information extraction. Passages contain extraneous information.
*   **Mixed Answer Types:** The dataset contains questions that require different answer formats, including numerical values, proper nouns, and locations.
*   **Sports and Historical Context:** Significant portion of questions related to sports (American Football specifically) and historical events, requiring knowledge of team names, player names, and historical locations/events.
*   **Answer Types:** Answers are typically concise and factual, directly extracted or derived from the passage. Common answer types include proper nouns (player names), numerical values (counts, yardage, rates), and dates.
*   **Question Types:**
    *   **Fact Extraction:** Directly retrieving information from the passage (e.g., "Who caught the final touchdown of the game?", "How many receiving yards...").
    *   **Counting:** Determining the number of occurrences of an event or entity (e.g., "How many running backs ran for a touchdown?"). Requires correlation and aggregation across the passage.
    *   **Calculation:** Performing arithmetic operations on values extracted from the passage (e.g., "How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?"). Requires unit awareness.
    *   **Comparison/Difference:** Comparing values or entities described in the passage (e.g., "How many more TFR did Malays have in 2004 compared to Ethnic Chinese TFR in 2004?", "How many yards longer..."). Requires numerical reasoning or extraction of specific numbers from the passage.
    *   **Ordinality:** Questions may include ordinal terms like "second longest", requiring ranking and comparison.
    *   **Multiple Instances:** Question has multiple valid answers in the passage (e.g., "List all the countries...").
    *   **Score Tracking:** Questions that require tracking and aggregating scores based on individual plays or events described in the passage. Requires addition/subtraction of point values.
    *   **Implicit Numerical Reasoning:** Questions require *implicit* numerical reasoning or inference. For example, extracting individual point values, and combining them to determine a final score.
    *   **Temporal Reasoning:** Questions often involve temporal aspects, requiring understanding the sequence of events (e.g., "final touchdown," "first field goal"). Passages are often chronological. Involves calculating the duration between two events.
    *   **Complex Date/Event Identification:** Correctly pinpointing the exact start and end dates of events can be challenging due to varied phrasing and implied timelines within the passages. The passages contain many dates and events, requiring careful extraction.
    *   **Holistic Understanding for Final Score Calculation:** Some questions require a holistic understanding of the passage to determine the final score.
    *   **Numerical Reasoning and Aggregation:** Many questions require aggregating numerical information presented in the passage. This includes addition, subtraction, and potentially weighted averages or other calculations (e.g., "How many total casualties were there?").
    *   **Percentage Interpretation:** A significant subset of questions involves understanding and manipulating percentages.
    *   **Implicit Information Retrieval:** Some questions require synthesizing information from multiple parts of the passage or performing logical inference based on the extracted information.
    *   **Negation:** Questions involving negation (e.g., "How many percent are *not* non-families?") require the system to understand and reverse the direct interpretation of information.
*   **Quantity-based Questions Dominate:** A significant portion of the questions revolve around extracting numerical quantities and performing simple arithmetic. Examples include finding population percentages, calculating differences in counts (water boards), and identifying distances (Stafford's touchdowns).
*   **Unit Sensitivity:** The presence of units of measurement (e.g., "yards," "percent") is crucial for answer correctness. The golden answers frequently include units, and the system's failure to include them results in incorrect answers.
*   **Implicit Information:** The answer might need to be inferred based on multiple pieces of information.
*   **Precision Requirements:** Questions often require precise numerical answers.
*   **Extraneous Information:** Passages often contain rich contextual details that are irrelevant to the question.
*   **Synonyms and Paraphrasing:** The question might use different wording than the passage to refer to the same entity or event.
*   **Multiple Occurrences:** An event (e.g., a touchdown) might occur multiple times; the question could be specific about which occurrence to consider (e.g., "first," "last," "second longest").
*   **Negation:** Questions like "Who did *not* score a touchdown?" require careful handling of negated conditions.
*   **Units:** Need to be aware of units when performing calculations (yards, points, etc.)
*   **Numerical Reasoning Requirement:** A significant portion of questions requires numerical reasoning.
*   **Ambiguity in Expected Answer Granularity:** The "correctness" of an answer can be subjective. The golden answer might expect a single aggregated number, while the system provides the disaggregated components. Example: providing "12 Dutch and 10 English" when the answer is expected to be "22".
*   **Census and Demographics Questions:** The presence of census-related questions which frequently require percentage or difference calculations based on population data.
*   **Explicit Information Assumption:** The questions assume the information needed to answer them is explicitly stated in the passage and do not require external knowledge.
*   **Numerical Reasoning & Misdirection:** Questions frequently require numerical reasoning, often involving addition, subtraction, or comparison of numbers extracted from the provided text. The dataset also includes "trick" questions that introduce irrelevant entities or require attention to detail to avoid misinterpreting the question's intent (e.g., asking about "the Suns" when the passage discusses the "Bills").
*   **Complex Sentence Structure:** The passages often feature complex sentence structures, including parenthetical clauses, multiple modifiers, and coordinating conjunctions. This complexity increases the difficulty of identifying key information and performing accurate calculations.
*   **Entity Recognition and Coreference:** Questions frequently hinge on identifying specific entities (people, teams, locations) and resolving coreferences (pronouns referring back to those entities). The system needs to accurately track these entities across the passage.

## 2. EFFECTIVE TASK-SPECIFIC STRATEGIES

*   **Multi-Stage Approach:** A multi-stage approach (question analysis, passage retrieval/extraction, answer generation, and verification) shows promise. It allows for breaking down the complex task into smaller, manageable components. Given the relatively high accuracy of 80% in Iteration 14, the multi-stage LLM approach shows general promise.
    *   Using separate LLM calls for each subtask (analyze, extract, generate, verify) isolates errors and allows for targeted improvements.
    *   Decomposing the problem into smaller steps, each handled by a dedicated function (agent), likely contributes to success by isolating concerns and simplifying individual tasks.
    *   Iteration 1,2 and 9 accuracy of 1.00 using this approach
*   **Chain-of-Thought Prompting (LLM-focused):** Encouraging the LLM to explicitly show its reasoning steps (e.g., using "Let's think step by step:") can improve accuracy and allow for debugging. (Confirmed for at least a subset of questions in Iteration 1).
*   **Example-Driven Reasoning (LLM-focused):** Providing the LLM with a few examples of questions and answers to guide its reasoning is effective. (Effective in Iteration 2).
*   **Few-Shot Learning (LLM-focused):** Providing the LLM with a few examples of questions and answers to guide its reasoning.
*   **Answer Validation Prompt (LLM-focused):** Using a separate prompt to validate the generated answer. This prompt could ask the LLM to justify its answer based on the passage. (Important for questions that require calculations (Iteration 9)).
*   **Constrained Decoding (LLM-focused):** If the answer is known to be a number, use constrained decoding to ensure that the LLM only generates numerical answers.
*   **Explicit Scoring Rule Definition:** Explicitly define the scoring rules for each type of play in prompt engineering to provide context for the LLM to calculate final scores.
*   **Leveraging Expert Roles:** When guided with specific system instructions ("expert roles"), the LLM can effectively perform question analysis, passage extraction, answer generation, and verification.
*   **Iterative Refinement:** The iterative refinement approach shows promise, given the relatively high accuracy of 0.70 in Iteration 10, suggesting that the verifiers are providing useful feedback. However, the benefits of iterative refinement might be masked by other issues such as the identified imprecise temporal reasoning.
*   **Question Analysis for Type Classification:** The initial question analysis step seems beneficial for identifying question types and extracting keywords, guiding the subsequent passage extraction. *Reasoning:* Understanding the question's intent allows for more targeted information retrieval.
*   **Focused Passage Extraction:** Extracting only the most relevant portions of the passage, rather than processing the entire text, helps to reduce noise and computational load. *Reasoning:* Filtering out irrelevant information focuses the LLM's attention on the most important details.
*   **(INSUFFICIENT DATA FROM ITERATION 12):** Due to only a single run and no specific examples of success in Iteration 12, there is not sufficient data to discern any working strategies beyond the general multi-stage LLM approach itself.
*   **Ineffective Strategies:** The "exploitation" strategy (Iteration 5), while generally aiming to leverage previously successful patterns, does not adequately address the specific challenges of temporal reasoning and arithmetic calculation present in this dataset (Iteration 13). The "Exploitation" strategy in Iteration 14 revealed critical weaknesses in handling numerical reasoning and negated queries within this specific dataset.

## 3. COMMON FAILURE MODES ON THIS DATASET

*   **Arithmetic Errors:** The system consistently fails when arithmetic operations are required to derive the correct answer (Iteration 11, Iteration 13). Example: "How many points were scored in the first half?". Even when the correct numbers are identified, the addition is often performed incorrectly. The question "How many total passing touchdown yards did Dalton have?" expects a calculation (sum of touchdown yardages), but the system returns a single value (total passing yards) instead.
*   **Incorrect Answer Type Handling:** The system frequently fails to identify the correct answer type. For example, instead of providing a numerical answer to a "How many..." question, it returns a verification status ("Correct"). Observed in ITERATION 7 and Iteration 3. If the `analyze_question` step misidentifies the question type, the subsequent steps are likely to fail (Iteration 9).
*   **Over-reliance on Verification:** The system seems to prematurely stop at verification, without extracting the specific answer from the extracted passage. This leads to responses like "True" or "Correct" when a more detailed answer is required. Observed in ITERATION 7 and Iteration 3.
*   **Passage Understanding Deficiencies:** While the passages contain the answer, the system struggles to extract the correct information, even when it correctly identifies the relevant passage. Observed in ITERATION 7.
*   **Incomplete Numerical Extraction:** The system struggles to extract *all* relevant numbers required for calculations.
*   **Difficulty in Combining Numerical Data:** The system fails to correlate and aggregate information across the passage. (e.g., "How many touchdown runs were made for the same yardage?").
*   **Missing Implicit Information:** Errors occur when the model fails to recognize and use implicit information.
*   **Missing Arithmetic Reasoning:** The system fails when questions require numerical computation.
*   **Incorrect Date Identification:** The system struggles to consistently identify the correct start and end dates for calculating time differences.
*   **Inaccurate Handling of Ordinality:** Inability to correctly answer questions involving ordinality (e.g., "second longest").
*   **Precision Mismatch:** Providing a range when a precise answer is expected is a frequent failure mode.
*   **Failure to Extract All Relevant Instances:** The system sometimes fails to identify all instances of events related to the question.
*   **Inability to Filter Extraneous Information:** Difficulty in filtering out irrelevant details from the passage.
*   **Missing Numerical Aggregation:** The system fails when questions require combining multiple numbers from the passage into a single answer (e.g., summing counts of different items). Example: "How many Dutch and English warships were blocking Spanish support?" expects the sum of 12 Dutch and 10 English warships (22), but the system returns "12 Dutch and 10 English". Observed in Iteration 3.
*   **Lack of Calculation Awareness and Execution:** The system is not reliably detecting the need for any calculation. Example: "How many in percent from the census weren't African American?" requires subtracting the percentage of African Americans from 100%, but the system fails to perform this operation and gives 2.8% instead of 97.2%. Observed in Iteration 3.
*   **Ambiguity Resolution:** Difficulty in disambiguating similar information or names within the passage.
*   **Incomplete Information Inference:** Failing to correctly infer the answer when it requires synthesizing information from multiple parts of the passage.
*   **Misinterpreting Data:** The system misinterpreted the question "How many total points were scored in the game?" and therefore did not successfully parse the result in the passage to extract the correct result.
*   **Temporal Reasoning Errors:** The system struggles with precise temporal calculations, particularly when dealing with months and approximate time phrases (e.g., "roughly"). In the first error example from Iteration 10, the system answered "Roughly 14 months" when the expected answer was "15".
*   **Semantic Equivalence Failure:** The system fails to understand that the golden answer, a number, is semantically different from a quantity like "one month".
*   **Incorrect Information Extraction:** The system sometimes extracts and provides incorrect or unrelated pieces of information from the passage, as seen in the third error example from Iteration 10 where the system provided the wrong person who King Tancred seized power from.
*   **Inaccurate Question Analysis:** Incorrectly identifying the question type leads to extracting irrelevant information and generating wrong answers (Iteration 11).
*   **Inability to Handle Temporal Reasoning:** The system struggles with questions requiring temporal reasoning or comparison, such as "Which happened later..." questions (Iteration 11).
*   **Unit Omission:** The most prominent failure mode is the omission of units of measurement in the answer. Even if the numerical value is correct, the absence of the correct unit leads to a mismatch with the expected answer (e.g., "77 and 1" instead of "1-yard, 77-yard"). Observed in ITERATION 12.
*   **Incorrect Ordering of Answer Components:** When multiple pieces of information are required in the answer, the system sometimes provides them in the wrong order. This suggests issues in understanding the question's implicit requirements regarding answer structure. Observed in ITERATION 12.
*   **Lack of Contextual Understanding:** The single example demonstrates a lack of robust contextual understanding leading to a formatting error. Observed in ITERATION 12.
*   **Misdirection and Contextual Errors:** The system fails to correctly interpret the context and is easily misled by irrelevant information. *Example:* The question asking about "the Suns" leads the system to incorrectly state that the Suns are not mentioned in the text, rather than calculating based on relevant information and ignoring that specification (Iteration 13).
*   **Entity Confusion:** The system struggles to correctly link information to the correct entity when multiple entities are mentioned in the passage. *Example:* Questions about which group in the census is larger can result in incorrect answers as the system may not consistently track and compare the percentages associated with each group (Iteration 13).
*   **Negation Interpretation:** The system struggles with questions involving negation. In the example "How many percent are not non-families?", it failed to correctly subtract the percentage of "non-families" from 100%. This indicates a failure in the "question analysis" stage to correctly identify the required operation (Iteration 14). The LLM likely extracted the percentage of non-families correctly but failed to apply the necessary subtraction.
*   **Contextual Numerical Extraction:** The system incorrectly identified the shortest run for a touchdown (1 yard) instead of the longest (8 yards) because it was unable to correctly determine which number in the passage was responsive to the prompt (Iteration 14). The `extract_relevant_passage` component of the model is likely failing, by not identifying the longest TD run as the critical context for answering the question.

## 4. EXPERIMENT LOG & FINDINGS

*   **2025-05-19 14:27:04 - INITIAL DATASET ANALYSIS:** Comprehensive analysis of dataset characteristics, challenges, potential approaches, and implementation recommendations.
*   **ITERATION 0:** Initial multi-stage approach achieved 67% accuracy. Highlighted the need for improved handling of questions involving comparisons and ordinality. The system needs mechanisms for systematically extracting and processing multiple instances of events from the passage.
*   **ITERATION 1:** Achieved 1.0 accuracy using a multi-stage LLM approach with chain-of-thought prompting. Proved highly effective for the tested subset of the dataset.
*   **ITERATION 2:** Achieved 1.0 accuracy using a multi-stage LLM approach with example-driven reasoning. The modular approach is effective.
*   **ITERATION 3:** The multi-stage LLM approach struggles with questions that necessitate any form of numerical calculation or aggregation, even when the component numbers are correctly extracted from the passage. Rejected the hypothesis that LLMs can inherently perform complex reasoning from raw text, at least without explicit instruction and verification steps targeted at numerical operations. The system also exhibited a tendency to answer "The answer is correct" instead of providing the actual answer.
*   **ITERATION 4:** The initial hypothesis that a multi-stage LLM approach can effectively answer questions based on text passages is partially confirmed. The system achieves high accuracy on simpler questions but fails when arithmetic reasoning is involved (90% accuracy on simpler questions). Rejected the hypothesis that the current multi-stage approach is sufficient for all types of questions, as demonstrated by the failure to handle questions requiring score tracking and comparison.
*   **ITERATION 5:** The "exploitation" strategy does not adequately address the specific challenges of temporal reasoning and arithmetic calculation present in this dataset. The 80% accuracy indicates a ceiling is being reached with the current approach.
*   **ITERATION 6:** The multi-stage LLM approach shows promise in decomposing the problem but needs to be augmented with a dedicated arithmetic reasoning component to solve quantitative questions. Errors occur primarily in the "generate" and "verify" stages due to the lack of arithmetic capability.
*   **ITERATION 7:** Rejected the hypothesis that a multi-stage LLM approach with question analysis and answer verification would be sufficient for this dataset. The 40% accuracy indicates that the current implementation needs significant improvements. The system struggles with correct answer type identification, and prematurely stops at verification, without extracting the specific answer from the extracted passage. Passage understanding also remains deficient. The question analysis stage is likely a bottleneck.
*   **ITERATION 8:** The multi-stage approach shows promise (70% accuracy), but the LLM's inherent limitations in precise numerical reasoning and information extraction are the bottleneck. The successful cases likely involve questions where the relevant passage is easily identifiable and the required calculations are straightforward and explicitly stated in the passage.
*   **ITERATION 9:** Achieved 1.00 accuracy using a multi-stage approach. The system's ability to handle quantitative reasoning, as demonstrated by the sports question (calculating the difference in yards), is crucial for this dataset. `verify_answer` is an important step. The success indicates that the LLM, when guided with specific system instructions ("expert roles"), can effectively perform question analysis, passage extraction, answer generation, and verification.
*   **ITERATION 10:** Accuracy of 0.70. The use of multiple LLM-based verifiers is not sufficient to overcome the temporal reasoning weaknesses. The iterative refinement approach doesn't appear to adequately address the challenges posed by time-related questions in this dataset. The fact verification is either insufficient or not properly utilized to solve the temporal reasoning failures. The experiment highlights that while LLMs can extract facts, accurately calculating durations and dealing with nuances of time requires more specialized processing or fine-tuning.
*   **ITERATION 11:** Accuracy of 0.70. The results highlight that while the overall architecture of the system is reasonable, the individual components, particularly the answer generation and verification steps, need improvement. The reliance on LLMs for arithmetic and logical reasoning is a weak point. LLMs may not reliably perform these operations without specific training or tools.
*   **ITERATION 12:** Results highlight the need for explicit unit handling in both answer generation and verification. The single run underscores the LLM's struggle with output formatting (unit omission and incorrect ordering) and the limitations of the current verification stage in correcting these issues.
*   **ITERATION 13:** The Exploitation strategy, while leveraging a multi-stage approach, highlights the limitations of the LLM in performing arithmetic and complex reasoning tasks directly. The initial question analysis and focused passage extraction are promising steps, but the LLM needs additional support for accurate calculation and contextual awareness.
*   **ITERATION 14:** Accuracy of 80%. The "Exploitation" strategy, which aims to refine an existing successful approach, has revealed critical weaknesses in handling numerical reasoning and negated queries within this specific dataset. The results confirm the hypothesis that a multi-stage approach *can* work, but it *rejects* the hypothesis that the current implementation is sufficiently robust for all types of questions within this dataset.

## 5. NEXT RESEARCH DIRECTIONS

*   **Enhanced Negation Handling:** Implement a specific module within the "question analysis" stage to explicitly identify and handle negation. This could involve techniques like identifying negative keywords (e.g., "not," "except," "without") and transforming the question into its affirmative equivalent before information extraction (Iteration 14).
*   **Context-Aware Numerical Extraction:** Improve the "extract relevant passage" stage by incorporating techniques that understand the *context* of numerical values (Iteration 14). This could involve:
    *   Using LLMs to identify the relationships between numbers and entities in the passage.
    *   Training the LLM to specifically recognize when a question is asking for the maximum or minimum value within a given context.
    *   Adding additional steps to verify the identified numbers with the context.
*   **Focus on Error Examples:** Prioritize training and fine-tuning the LLMs on the specific error cases (negation, contextual number extraction) to improve their robustness.  Create a dataset of similar challenging examples (Iteration 14).
*   **Implement a Calculation Module:** Integrate a dedicated calculation module or function that can perform arithmetic operations based on extracted numerical values. This module should be invoked specifically when the question analysis identifies a need for calculation (Iteration 13, Iteration 11).
*   **Improve Entity Tracking:** Enhance the question analysis and passage extraction stages to explicitly identify and track key entities mentioned in the passage. Use this entity tracking to resolve coreferences and ensure that extracted information is associated with the correct entity (Iteration 13).
*   **Add Misdirection Filtering:** Specifically train the system to identify and ignore misdirection or irrelevant information in the question. This might involve adding a "relevance filter" that scores the relevance of different parts of the passage to the core question being asked (Iteration 13).
*   **Detailed Error Analysis:** Implement a more detailed error logging system to track the specific steps where the system fails (e.g., failure in question analysis, incorrect passage extraction, calculation error). This will allow for more targeted improvements (Iteration 13).
*   **Refine Prompting for Unit Inclusion:** Modify the prompts for both the answer generation and answer verification stages to explicitly require the inclusion of units when the question refers to measurable quantities. Add examples demonstrating the desired output format with units.
*   **Implement Unit Verification:** Enhance the `verify_answer` function to specifically check for the presence and correctness of units. This could involve using regular expressions or a lookup table of common units.
*   **Prompt Engineering for Ordering:** Add explicit instructions in the prompt to the answer generator specifying the desired order of answer components based on the question's structure. For example, if the question asks "How many yards were each of Stafford's touchdowns?", the prompt should encourage a response structured as "touchdown1-yards, touchdown2-yards".
*   **Evaluate Different LLMs:** Given the importance of precise and consistent answer formatting, it might be beneficial to evaluate other LLMs that might be better at adhering to specific output formats.
*   **Error Analysis and Data Augmentation:** Conduct a more thorough error analysis of a larger set of failures to identify additional error patterns. Use these patterns to augment the training data with examples that specifically address these failure modes.
*   **Implement a reliable calculator module:** Integrate a calculator module to perform arithmetic operations instead of relying on the LLM. Ensure this module can handle basic operations like addition, subtraction, and potentially more complex calculations (Iteration 11).
*   **Enhance Question Analysis:** Improve the `analyze_question` function to better classify questions that require calculations, temporal reasoning, or comparisons. This could involve training a separate classifier or using more sophisticated prompt engineering (Iteration 11).
*   **Refine Prompting for Verification:** Modify the `verify_answer` function to include specific checks for numerical accuracy and logical consistency. For numerical answers, compare the calculated result with the LLM's result (Iteration 11).
*   **Dataset Augmentation:** Add more examples to the dataset that specifically require arithmetic, temporal reasoning, and comparison to better train and evaluate the system's performance on these challenging question types (Iteration 11).
*   **Enhance Temporal Reasoning**: Incorporate specialized modules or fine-tune the LLMs specifically for temporal reasoning. This could involve pre-training on time-related datasets or using a dedicated time-aware reasoning module.
*   **Improve Semantic Understanding**: Train the LLM to better understand and compare numerical and temporal expressions. Explicitly define what constitutes a semantically equivalent answer, especially for numerical values. This can be done using data augmentation or specific examples during training.
*   **Refine Verification Process**: Adjust the prompts for factual and arithmetic verifiers to specifically target temporal inconsistencies. Make sure the verifiers are focused on verifying the dates and durations extracted from the text.
*   **Add a Time Normalization Layer**: Implement a module that normalizes time expressions (e.g., converts "roughly 14 months" to "14 months") before feeding them into the LLM for reasoning.
*   **Investigate Stage Necessity:** Investigate the necessity of all stages in the current implementation. Can any stages be streamlined or eliminated without sacrificing accuracy?
*   **Challenge Verification:** Introduce more challenging questions that require more complex reasoning or calculations to test the limits of the current approach.
*   **Error Detection in Question Analysis:** Implement a mechanism to detect and correct errors in the `analyze_question` stage.
*   **LLM Exploration:** Explore the use of different LLMs or fine-tuning the existing LLM to improve the accuracy and efficiency of each stage.
*   **Prompt Optimization:** Evaluate the impact of different prompts and system instructions on the performance of each stage.
*   **Strengthen Numerical Reasoning:** Implement a dedicated numerical reasoning module, potentially using a symbolic calculator or a more robust method for numerical extraction.
*   **Improve Information Aggregation:** Enhance the passage extraction and analysis phases to explicitly identify and extract sets of related numerical data points.
*   **Refine Verification:** The `verify_answer` stage needs a more nuanced approach for numerical answers. Consider a calibration phase to adjust how aggressively this module corrects answers.
*   **Implement a mechanism to resolve implicit relationships:** Before generating an answer, add a step where the model is explicitly prompted to identify and state implicit relationships.
*   **Enhance Question Analysis:** Fine-tune the `analyze_question` function to more accurately classify the expected answer type. Implement more robust pattern recognition in the `analyze_question` function to identify question keywords.
*   **Refine Passage Extraction:** Experiment with different prompting strategies for the `extract_relevant_passage` function. Implement a scoring mechanism to rank the relevance of different passage snippets.
*   **Improve Answer Generation:** Ensure that the `generate_answer` function explicitly extracts the identified answer from the relevant passage snippet. Add specific instructions to the prompt to force the LLM to extract the answer and include it in its response.
*   **Evaluation and Error Analysis:** Implement a more detailed error logging and tracking system to better understand the root causes of failures. Manually analyze a larger sample of failed examples to identify common patterns and areas for improvement.
*   **Explore Few-Shot Learning/Fine-tuning:** Consider fine-tuning a smaller language model specifically for this question-answering task, using a dataset of similar questions and answers.
*   **Implement an Arithmetic Reasoning Module:** Introduce a module specifically designed for identifying numerical values, understanding the required calculations, and executing those calculations.
*   **Improve Score Interpretation:** Improve the ability of the models to extract the relevant scoring information and distinguish what each event implies about the numerical score.
*   **Enhance Verification:** Augment the "verify\_answer" function to check not only the factual correctness but also the numerical validity of the answer given the extracted information.
*   **Implement a Date Normalization Module:** Develop a module that can reliably extract and normalize dates from the passage text into a standard format (YYYY-MM-DD) to ensure consistent date representation.
*   **Arithmetic Solver Module:** Integrate a simple arithmetic solver module that takes two dates as input and calculates the difference in years, months, or days.
*   **Hybrid Approach:** Experiment with a hybrid approach that combines LLM-based reasoning with specialized modules for date extraction and arithmetic calculation.
*   **Implement Score Tracking Agent:** Introduce a "Score Tracker" agent specifically designed to track and update scores based on the narrative descriptions in the passage.
*   **Incorporate Numerical Reasoning Module:** Integrate a numerical reasoning module within the "Generate Answer" agent.
*   **Refine Passage Extraction:** Enhance the "Extract Relevant Passage" agent to specifically identify and extract sections of the passage that describe scoring events or relevant numerical data.
*   **Improve Ordinality Handling:** Modify the `extract_relevant_passage` function to identify *all* instances of events related to the question. Introduce a new function, `rank_answers`, that orders extracted information based on a specific criteria. Update the `analyze_question` function to specifically identify questions requiring ranking or comparison.
*   **Enhance Information Extraction:** Investigate techniques for improving the accuracy and completeness of information extraction, including named entity recognition, dependency parsing, and semantic role labeling.
*   **Implement a Robust Validation Step:** Develop a more sophisticated validation step to catch errors before the final answer is generated. Add adversarial examples designed to trick the verification step.
*   **Explore Different LLM Prompting Strategies:** Experiment with different prompting strategies to improve the LLM's reasoning abilities.
*   **Develop a More Sophisticated Question Analyzer:** Improve the `analyze_question` function to better identify the type of reasoning required to answer the question.
*   **Implement Calculation Detection Module:** Introduce a module within the `analyze_question` function to explicitly identify questions requiring calculation.
*   **Calculation Execution Module:** If the calculation detection module flags a question, activate a "calculation execution" module.
*   **Enhance Answer Generation:** Modify the `generate_answer` function to prioritize providing explicit numerical answers when a calculation is detected.
*   **Post-Processing Numeric Answers:** Implement a post-processing step to ensure that numeric answers are formatted correctly and include appropriate units.
*   **Explicit Answer Reporting:** Make sure the `generate_answer` provides the answer and not just a verification of the correctness.
*   **Expand Dataset for Stress-Testing:** Expand the dataset with new question types, more complex reasoning problems, passages with ambiguous or contradictory information, and passages containing data requiring external knowledge to further stress-test the system and identify failure points.
*   **Evaluate the impact of different LLMs:** Experiment with different LLMs to see if performance can be further improved or if certain LLMs are better suited for specific subtasks.
*   **Analyze the verification module:** Examine how the verification step is implemented and how robust it is to different types of errors.
```
        

            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        

            TOP PERFORMING APPROACHES TO BUILD UPON:
            
TOP PERFORMING APPROACH #1:
Iteration: 1
Accuracy: 1.00
Approach Summary: The script uses a multi-stage LLM approach with chain-of-thought prompting to answer questions by identifying the question type, extracting relevant information, generating an answer, and then verifying the answer. The script uses the following functions: `main` orchestrates the entire process, `analyze_question` identifies the question type and keywords, `extract_relevant_passage` retrieves relevant text, `generate_answer` formulates an initial answer, `verify_answer` validates the answer, and `call_llm` interacts with the Gemini LLM, using system instructions and prompts to guide the LLM's reasoning at each step. The workflow starts with `main` calling `analyze_question`, followed by `extract_relevant_passage`, `generate_answer`, and `verify_answer`, each leveraging `call_llm` to interact with the LLM.

FULL SCRIPT TO REFINE:
```python
import os
import re
import math

def main(question):
    """
    Solve the question using a multi-stage LLM approach.
    This approach focuses on breaking down the problem into question type identification, 
    focused passage extraction, and direct answer generation with verification.
    """
    try:
        # Step 1: Identify question type and keywords
        question_analysis = analyze_question(question)
        if "Error" in question_analysis:
            return "Error analyzing question"

        # Step 2: Extract relevant passage using identified keywords
        relevant_passage = extract_relevant_passage(question, question_analysis)
        if "Error" in relevant_passage:
            return "Error extracting passage"

        # Step 3: Generate answer using extracted passage and question type
        answer = generate_answer(question, relevant_passage, question_analysis)
        if "Error" in answer:
            return "Error generating answer"

        # Step 4: Verify answer
        verified_answer = verify_answer(question, answer, relevant_passage)
        if "Error" in verified_answer:
            return "Error verifying answer"
        
        return verified_answer

    except Exception as e:
        return f"General Error: {str(e)}"

def analyze_question(question):
    """Analyzes the question to identify its type and keywords. Includes multiple examples."""
    system_instruction = "You are an expert at analyzing questions to determine their type and keywords."
    prompt = f"""
    Analyze the following question and identify its type (e.g., fact extraction, calculation, comparison) and keywords.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Analysis: {{"type": "fact extraction", "keywords": ["final touchdown", "caught"]}}

    Example 2:
    Question: How many running backs ran for a touchdown?
    Analysis: {{"type": "counting", "keywords": ["running backs", "touchdown"]}}
    
    Example 3:
    Question: Which player kicked the only field goal of the game?
    Analysis: {{"type": "fact extraction", "keywords": ["player", "field goal"]}}

    Question: {question}
    Analysis:
    """
    return call_llm(prompt, system_instruction)

def extract_relevant_passage(question, question_analysis):
    """Extracts the relevant passage from the question based on keywords. Includes multiple examples."""
    system_instruction = "You are an expert at extracting relevant passages from text."
    prompt = f"""
    Extract the relevant passage from the following text based on the question and keywords.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Keywords: {{"type": "fact extraction", "keywords": ["final touchdown", "caught"]}}
    Text: PASSAGE: After a tough loss at home, the Browns traveled to take on the Packers. ... The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    Passage: The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    
    Example 2:
    Question: How many running backs ran for a touchdown?
    Keywords: {{"type": "counting", "keywords": ["running backs", "touchdown"]}}
    Text: PASSAGE: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. The Lions would respond with kicker Jason Hanson getting a 53-yard field goal. The Titans would answer with Johnson getting a 58-yard TD run, along with DE Dave Ball returning an interception 15 yards for a touchdown. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.
    Passage: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.

    Example 3:
    Question: Which player kicked the only field goal of the game?
    Keywords: {{"type": "fact extraction", "keywords": ["player", "field goal"]}}
    Text: PASSAGE: Game SummaryComing off their Thanksgiving road win over the Falcons, the Colts went home for a Week 13 AFC South rematch with the Jacksonville Jaguars.  ... In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.
    Passage: In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.

    Question: {question}
    Keywords: {question_analysis}
    Text: {question}
    Passage:
    """
    return call_llm(prompt, system_instruction)

def generate_answer(question, relevant_passage, question_analysis):
    """Generates the answer based on the question, relevant passage, and question type. Includes multiple examples."""
    system_instruction = "You are an expert at generating answers to questions based on provided text."
    prompt = f"""
    Generate the answer to the question based on the relevant passage and question type.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Passage: The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    Answer: Jarrett Boykin

    Example 2:
    Question: How many running backs ran for a touchdown?
    Passage: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. The Lions would respond with kicker Jason Hanson getting a 53-yard field goal. The Titans would answer with Johnson getting a 58-yard TD run, along with DE Dave Ball returning an interception 15 yards for a touchdown. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.
    Answer: 2
    
    Example 3:
    Question: Which player kicked the only field goal of the game?
    Passage: In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.
    Answer: Josh Scobee

    Question: {question}
    Passage: {relevant_passage}
    Answer:
    """
    return call_llm(prompt, system_instruction)

def verify_answer(question, answer, relevant_passage):
    """Verifies the generated answer. Includes multiple examples."""
    system_instruction = "You are an expert at verifying answers to questions."
    prompt = f"""
    Verify the following answer to the question based on the relevant passage.  Return the answer if it is correct.  Return the correct answer if it is incorrect.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Answer: Jarrett Boykin
    Passage: The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    Verification: Jarrett Boykin
    
    Example 2:
    Question: How many running backs ran for a touchdown?
    Answer: 2
    Passage: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.
    Verification: 2

    Example 3:
    Question: Which player kicked the only field goal of the game?
    Answer: Josh Scobee
    Passage: In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.
    Verification: Josh Scobee

    Question: {question}
    Answer: {answer}
    Passage: {relevant_passage}
    Verification:
    """
    return call_llm(prompt, system_instruction)

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
```

TOP PERFORMING APPROACH #2:
Iteration: 2
Accuracy: 1.00
Approach Summary: The script uses a multi-stage LLM approach with example-driven reasoning to answer questions, incorporating question analysis, information extraction, answer generation, and verification. The problem is decomposed into four main steps: analyzing the question, extracting a relevant passage, generating an answer, and verifying the answer, each handled by a separate function. There are no agent roles indicated in the script. The script uses `analyze_question` to identify the question type and keywords, `extract_relevant_passage` to find the relevant information, `generate_answer` to form an answer, `verify_answer` to confirm its correctness, and `call_llm` to interface with the Gemini LLM. The overall workflow involves analyzing the question, extracting relevant information, generating an initial answer, verifying it for accuracy, and then returning the verified answer.

Key approach aspects (full code available for reference)

TOP PERFORMING APPROACH #3:
Iteration: 9
Accuracy: 1.00
Approach Summary: The script uses a multi-stage LLM approach, including chain-of-thought reasoning by decomposing the problem into question analysis, relevant passage extraction, answer generation, and answer verification. It uses the `call_llm` function to interact with the Gemini model with specific system instructions and prompts at each stage, effectively assigning different "expert" roles to the LLM. The workflow involves `analyze_question` to determine the question type and keywords, `extract_relevant_passage` to find relevant text, `generate_answer` to create an initial answer, and `verify_answer` to confirm the answer and perform calculations if needed.

Key approach aspects (full code available for reference)

            
FULL SCRIPT TO REFINE:
```python
import os
import re
import math

def main(question):
    """
    Solve the question using a multi-stage LLM approach.
    This approach focuses on breaking down the problem into question type identification, 
    focused passage extraction, and direct answer generation with verification.
    """
    try:
        # Step 1: Identify question type and keywords
        question_analysis = analyze_question(question)
        if "Error" in question_analysis:
            return "Error analyzing question"

        # Step 2: Extract relevant passage using identified keywords
        relevant_passage = extract_relevant_passage(question, question_analysis)
        if "Error" in relevant_passage:
            return "Error extracting passage"

        # Step 3: Generate answer using extracted passage and question type
        answer = generate_answer(question, relevant_passage, question_analysis)
        if "Error" in answer:
            return "Error generating answer"

        # Step 4: Verify answer
        verified_answer = verify_answer(question, answer, relevant_passage)
        if "Error" in verified_answer:
            return "Error verifying answer"
        
        return verified_answer

    except Exception as e:
        return f"General Error: {str(e)}"

def analyze_question(question):
    """Analyzes the question to identify its type and keywords. Includes multiple examples."""
    system_instruction = "You are an expert at analyzing questions to determine their type and keywords."
    prompt = f"""
    Analyze the following question and identify its type (e.g., fact extraction, calculation, comparison) and keywords.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Analysis: {{"type": "fact extraction", "keywords": ["final touchdown", "caught"]}}

    Example 2:
    Question: How many running backs ran for a touchdown?
    Analysis: {{"type": "counting", "keywords": ["running backs", "touchdown"]}}
    
    Example 3:
    Question: Which player kicked the only field goal of the game?
    Analysis: {{"type": "fact extraction", "keywords": ["player", "field goal"]}}

    Question: {question}
    Analysis:
    """
    return call_llm(prompt, system_instruction)

def extract_relevant_passage(question, question_analysis):
    """Extracts the relevant passage from the question based on keywords. Includes multiple examples."""
    system_instruction = "You are an expert at extracting relevant passages from text."
    prompt = f"""
    Extract the relevant passage from the following text based on the question and keywords.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Keywords: {{"type": "fact extraction", "keywords": ["final touchdown", "caught"]}}
    Text: PASSAGE: After a tough loss at home, the Browns traveled to take on the Packers. ... The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    Passage: The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    
    Example 2:
    Question: How many running backs ran for a touchdown?
    Keywords: {{"type": "counting", "keywords": ["running backs", "touchdown"]}}
    Text: PASSAGE: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. The Lions would respond with kicker Jason Hanson getting a 53-yard field goal. The Titans would answer with Johnson getting a 58-yard TD run, along with DE Dave Ball returning an interception 15 yards for a touchdown. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.
    Passage: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.

    Example 3:
    Question: Which player kicked the only field goal of the game?
    Keywords: {{"type": "fact extraction", "keywords": ["player", "field goal"]}}
    Text: PASSAGE: Game SummaryComing off their Thanksgiving road win over the Falcons, the Colts went home for a Week 13 AFC South rematch with the Jacksonville Jaguars.  ... In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.
    Passage: In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.

    Question: {question}
    Keywords: {question_analysis}
    Text: {question}
    Passage:
    """
    return call_llm(prompt, system_instruction)

def generate_answer(question, relevant_passage, question_analysis):
    """Generates the answer based on the question, relevant passage, and question type. Includes multiple examples."""
    system_instruction = "You are an expert at generating answers to questions based on provided text."
    prompt = f"""
    Generate the answer to the question based on the relevant passage and question type.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Passage: The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    Answer: Jarrett Boykin

    Example 2:
    Question: How many running backs ran for a touchdown?
    Passage: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. The Lions would respond with kicker Jason Hanson getting a 53-yard field goal. The Titans would answer with Johnson getting a 58-yard TD run, along with DE Dave Ball returning an interception 15 yards for a touchdown. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.
    Answer: 2
    
    Example 3:
    Question: Which player kicked the only field goal of the game?
    Passage: In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.
    Answer: Josh Scobee

    Question: {question}
    Passage: {relevant_passage}
    Answer:
    """
    return call_llm(prompt, system_instruction)

def verify_answer(question, answer, relevant_passage):
    """Verifies the generated answer. Includes multiple examples."""
    system_instruction = "You are an expert at verifying answers to questions."
    prompt = f"""
    Verify the following answer to the question based on the relevant passage.  Return the answer if it is correct.  Return the correct answer if it is incorrect.

    Example 1:
    Question: Who caught the final touchdown of the game?
    Answer: Jarrett Boykin
    Passage: The Packers would later on seal the game when Rodgers found Jarrett Boykin on a 20-yard pass for the eventual final score 31-13.
    Verification: Jarrett Boykin
    
    Example 2:
    Question: How many running backs ran for a touchdown?
    Answer: 2
    Passage: In the first quarter, Tennessee drew first blood as rookie RB Chris Johnson got a 6-yard TD run. In the second quarter, Tennessee increased their lead with RB LenDale White getting a 6-yard and a 2-yard TD run.
    Verification: 2

    Example 3:
    Question: Which player kicked the only field goal of the game?
    Answer: Josh Scobee
    Passage: In the fourth quarter, the Jaguars drew closer as kicker Josh Scobee nailed a 47-yard field goal.
    Verification: Josh Scobee

    Question: {question}
    Answer: {answer}
    Passage: {relevant_passage}
    Verification:
    """
    return call_llm(prompt, system_instruction)

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
```

            PREVIOUSLY ATTEMPTED VARIATIONS:
            

            EXPLOITATION GUIDANCE:
            1. Review the error patterns, targeted improvements, and accumulated learnings carefully
            2. CRITICAL: Break down the problem into distinct reasoning steps before modifying code
            3. CRITICAL: Analyze the best scripts to identify which components are working well and which are failing. Focus your improvements on the weak points while preserving successful components.
            4. Maintain the core successful elements of the best approaches
            5. Consider how you can combine strengths from multiple top-performing approaches
            6. CRITICAL: Add EMBEDDED EXAMPLES to EVERY LLM prompt that illustrate:
               - Sample input that resembles the dataset
               - Step-by-step reasoning through the example
               - Properly formatted output
            7. Focus on fixing specific issues identified in previous error analyses. Create an explicit HYPOTHESIS for each targeted improvement, as well as a way to verify if it's successful.
            8. Enhance chain-of-thought reasoning and verification steps. Verification steps should be added to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
            9. Apply the key insights from ACCUMULATED LEARNINGS to enhance the approach
            10. Pay SPECIAL ATTENTION to the weaknesses and improvement suggestions from the capability assessment

            IMPROVEMENT STRATEGY:
            Analyze why the top approaches succeeded where others failed. Identify the key differentiators and strengthen them further.

            SYSTEMATIC ENHANCEMENT APPROACH:
            1. First, identify which specific function or component is underperforming based on error analysis
            2. Examine how error cases differ from successful cases
            3. For each identified weakness, implement a targeted enhancement
            4. Add additional verification steps around modified components
            5. Consider how components interact - ensure improvements don't break successful parts

            Consider enhancing the script with one or more of these patterns:
            - Repeated validation with feedback loops
            - Multi-perspective analysis with synthesis
            - Dynamic input-dependent routing
            - Hybrid approaches combining LLM with deterministic functions
            - Best-of-n solution generation and selection
            - ReAct pattern for interactive reasoning and action
            - If it is unknown how successful a processing state or part of the pipeline is, include verification steps to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
            - Answer checkers to validate the final answer against the problem statement. If the answer is incorrect, the checker can send the answer back to an earlier part of the system for refinement with feedback

            Here's how to call the Gemini API. Use this example without modification and don't invent configuration options:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

            Since this is an EXPLOITATION phase:
            - Build upon what's working well in the best approaches
            - Consider creative combinations of successful techniques from different scripts
            - Make TARGETED improvements to address specific error patterns
            - For EACH key LLM prompt, include a relevant example with:
              * Sample input similar to the dataset
              * Expected reasoning steps
              * Desired output format
            - Apply the knowledge from our accumulated learnings
            - Significantly enhance the script to address weaknesses identified in the capability assessment

            CRITICAL REQUIREMENTS:
            1. The script MUST properly handle all string literals - be extremely careful with quotes and triple quotes
            2. The script MUST NOT exceed 150 lines of code to prevent truncation
            3. Include detailed comments explaining your improvements
            4. EVERY SINGLE LLM PROMPT must include at least one embedded example showing:
               - Sample input with reasoning
               - Desired output format
            5. Make proper use of error handling
            6. Implement robust capabilities to address the specific weaknesses identified in the capability assessment
            7. Do NOT use json.loads() in the LLM calls to process input data. JSON formatting is good to use to structure information as inputs and outputs, but attempting to have functions process JSON data explicitly with strict built-in functionality is error prone due to formatting issues and additional text that appears as documentation, reasoning, or comments. When passing data into another LLM call, you can read it as plain text rather than trying to load it in strict json format, is the better approach.

            Return a COMPLETE, RUNNABLE Python script that:
            1. Has a main function that takes a question string as input and returns the answer string
            2. Makes multiple LLM calls for different reasoning steps
            3. Has proper error handling for API calls
            4. Includes embedded examples in EVERY LLM prompt
            5. Is COMPLETE - no missing code, no "..." placeholders
            6. Closes all string literals properly

            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            