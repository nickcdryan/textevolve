
            You are developing a Python script to solve problems using LLM reasoning capabilities.
            You are in the EXPLORATION PHASE. You must generate a NEW approach that's different from previous approaches but informed by their successes and failures. With this approach, you will have a specific NEW HYPOTHESIS or variable you are trying to test. Your goal is to see if this new approach works, and you must add verification and validation steps to deduce if this new change is helpful. You may also test RADICAL NEW APPROACHES that are substantially different from previous approaches. 
            
            You should try NEW THINGS:
            
            Break down the problem into smaller pieces
            Think CREATIVELY about how to solve your problem if other approaches aren't working
            Transform data into different formats to see if it helps

            # YOUR TASK
            You are deeply familiar with prompting techniques and the agent works from the literature. 
            Your goal is to maximize the specified performance metrics by proposing interestingly new agents.
            Observe the past discovered agents and scripts carefully and think about what insights, lessons, or stepping stones can be learned from them.
            Be creative when thinking about the next interesting agent to try. You are encouraged to draw inspiration from related agent papers or academic papers from other research areas.
            Use the knowledge from the archive and inspiration from academic literature to propose the next interesting agentic system design.
            THINK OUTSIDE THE BOX.
            

            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "PASSAGE: Between 2001 and 2011 shares in Tottenham Hotspur F.C. were listed on the Alternative Investment Market (AIM index). The majority shareholder, ENIC International Ltd, is an investment company established by the British billionaire Joe Lewis (British businessman). Daniel Levy (businessman), Lewiss partner at ENIC, is Executive Chairman of the club. They had acquired 29.9% after buying 27% share of the club for \u00a322 million from Sugar in 1991. Shareholding by ENIC increased over the decade through the purchase of the remaining 12% holding of Alan Sugar in 2007 for \u00a325m, and the 9.9% stake belonging to Stelios Haji-Ioannou through Hodram Inc in 2009. On 21 August 2009 the club reported that they had issued a further 30 million shares to fund the initial development costs of the new stadium project, and that 27.8 million of these new shares had been purchased by ENIC. The Annual Report for 2010 indicated that ENIC had acquired 76% of all Ordinary Shares and also held 97% of all convertible redeemable preference shares, equivalent to a holding of 85% of share capital. Following an announcement at the 2011 AGM, in January 2012 the club confirmed that they had been transferred into the private ownership of ENIC.\n\nQUESTION: How many years were shares of Tottenham Hotspur F.C. listed on the Alternative Investment Market?",
    "answer": "10"
  },
  {
    "id": 1,
    "question": "PASSAGE: As of the census of 2010, there were 31,894 people, 13,324 households, and 8,201 families residing in the city. The population density was . There were 14,057 housing units at an average density of . The racial makeup of the city was 93.9% White (U.S. Census), 0.3% African American (U.S. Census), 1.7% Native American (U.S. Census), 0.8% Asian (U.S. Census), 0.1% Race (U.S. Census), 0.7% from Race (U.S. Census), and 2.4% from two or more races. Hispanic (U.S. Census) or Latino (U.S. Census) of any race were 2.8% of the population.\n\nQUESTION: Which group from the census is smaller: Asian or African American?",
    "answer": "African American"
  },
  {
    "id": 2,
    "question": "PASSAGE: Trying to snap a two-game skid, the Chargers went home for an AFC West duel with the Kansas City Chiefs.  In the first quarter, San Diego's struggling offense found some life with kicker Nate Kaeding getting a 24-yard field goal, while RB LaDainian Tomlinson got a 5-yard TD run.  In the second quarter, the Chargers increased its lead with Kaeding kicking a 51-yard field goal.  The Chiefs would answer with kicker Dave Rayner getting a 25-yard field goal.  Afterwards, San Diego ended the half with Kaeding getting a 38-yard field goal.  Unfortunately, in the third quarter, the Chargers lost their lead Kansas City getting a 41-yard field goal from Rayner and QB Damon Huard completing a 22-yard TD pass to TE Tony Gonzalez.  Even worse, San Diego ended up losing in the fourth quarter with Huard completing a 51-yard TD pass to WR Dwayne Bowe, along with CB Tyron Brackenridge returning a fumble 50 yards for a touchdown. One of the few positives from the game, Tomlinson would finally get his first 100-yard game of the year.  He would get 132 rushing yards on 20 carries.\n\nQUESTION: How many more yards was Nate Kaedings second field goal over his first?",
    "answer": "1"
  }
]

            HISTORICAL CONTEXT:
            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 16
        - Current explore/exploit balance: 50/50
        - Best accuracy achieved: 1.00 (iteration 1)

        APPROACH HISTORY (last 10 iterations):
        [
  {
    "iteration": 6,
    "strategy": "Exploitation",
    "accuracy": 0.8,
    "approach": "The script uses chain-of-thought reasoning with a question decomposition, information extraction, and answer synthesis approach, where each step is validated by the LLM. It leverages three agent roles: question decomposer, information extraction expert, and answer synthesis expert, each guided by specific system instructions. The overall workflow involves `main` calling `decompose_question` to break down the question, then `extract_information` to find relevant information, and finally `synthesize_answer` to generate the final answer. The `call_llm` function is used to interact with the Gemini model, while the other functions decompose the problem and provide prompts to the LLM."
  },
  {
    "iteration": 7,
    "strategy": "Exploitation",
    "accuracy": 0.0,
    "approach": "The script uses a chain-of-thought approach with verification to answer questions, first determining the question type (numerical or general) and then processing it accordingly. Numerical questions are processed by extracting numerical information and then calculating the answer, while general questions are processed by decomposing the question into sub-questions, extracting information based on those sub-questions, and synthesizing the answer. The agent roles include a question type identifier, numerical information extractor, calculator, and (implicitly) a question decomposer and answer synthesizer.\n\nThe functions used are: `main` which orchestrates the process, `determine_question_type` to identify the type of question, `process_numerical_question` and `process_general_question` to handle the different question types, `extract_numerical_info` to extract numerical values, `calculate_answer` to perform calculations, `decompose_question` to break down general questions, `extract_information` to gather relevant details, and `synthesize_answer` to formulate the final response. The `call_llm` function is called within each of these functions. The overall workflow involves determining the question type, processing it based on its type (numerical or general), and returning the answer or an error message if any step fails."
  },
  {
    "iteration": 8,
    "strategy": "Exploitation",
    "accuracy": 0.0,
    "approach": "This script uses LLM-driven chain-of-thought reasoning and verification to answer questions, first determining if the question is numerical or general. Numerical questions are processed by extracting numbers and performing calculations, while general questions are decomposed into sub-questions, have information extracted for each, and then synthesize an answer. The script defines specialized agents for question type determination, numerical information extraction, calculation, question decomposition, information extraction, and answer synthesis; each leverages examples and a validation step.\n\nThe functions used are:\n- `main()`: Orchestrates the entire process.\n- `determine_question_type()`: Determines question type.\n- `process_numerical_question()`: Processes numerical questions.\n- `extract_numerical_info()`: Extracts numerical data.\n- `calculate_answer()`: Calculates numerical answers.\n- `process_general_question()`: Processes general questions.\n- `decompose_question()`: Decomposes complex questions.\n- `extract_information()`: Extracts information from decomposed questions.\n- `synthesize_answer()`: Synthesizes the final answer.\n- `call_llm()`: Calls the Gemini LLM with a prompt.\n\nThe overall workflow is as follows: `main()` calls `determine_question_type()`, then either `process_numerical_question()` which uses `extract_numerical_info()` and `calculate_answer()`, or `process_general_question()` which uses `decompose_question()`, `extract_information()`, and `synthesize_answer()`, with all *process, extract, calculate, decompose, and synthesize* functions using `call_llm()` to interact with the LLM."
  },
  {
    "iteration": 9,
    "strategy": "Exploration",
    "accuracy": 0.6,
    "approach": "This script uses a \"Reading Comprehension Expert\" agent employing self-debate and verification to answer questions. The problem is decomposed into analyzing the question, conducting a self-debate, and verifying the result. The agent embodies the role of a reading comprehension expert and an expert debater.\n\nThe functions used are: `main` to orchestrate the process, `ReadingComprehensionExpert` to contain the agent, `answer_question` to coordinate the self-debate and verification, `_analyze_question` to extract key information, `_conduct_self_debate` to engage in a self-debate, and `call_llm` to interact with the Gemini LLM. The overall workflow involves analyzing the question, conducting a self-debate to arrive at an answer, and verifying the answer for validity using the LLM."
  },
  {
    "iteration": 10,
    "strategy": "Exploration",
    "accuracy": 0.8,
    "approach": "The script uses a \"Holistic Reading & Arithmetic Reasoner\" agent to answer questions by combining reading comprehension and arithmetic problem-solving in a single step, using the Gemini LLM. The problem is approached holistically, with the agent reasoning about the question and passage to formulate an answer and extracting relevant numerical quantities, and then uses a verification step to determine the validity of the answer. The `HolisticReadingArithmeticReasoner` class contains the `answer_question` and `_reason_about_question` functions to provide the answer and reasoning, and `call_llm` sends prompts to the Gemini LLM. The `main` function initializes the `HolisticReadingArithmeticReasoner` and returns the answer to the question."
  },
  {
    "iteration": 11,
    "strategy": "Exploration",
    "accuracy": 0.8,
    "approach": "The script employs a \"Question Clarification & Focused Extraction\" approach, using LLMs to clarify a given question, extract relevant information, and synthesize an answer. The problem is decomposed into three main steps: question clarification, information extraction, and answer synthesis, each using the `call_llm` function with specific system instructions to act as an expert in that step. The `clarify_question` and `extract_information` functions include a verification step to ensure the validity of the LLM's output. The overall workflow is: `main` calls `clarify_question`, then `extract_information`, and finally `synthesize_answer`, using `call_llm` in each to interact with the LLM."
  },
  {
    "iteration": 12,
    "strategy": "Exploitation",
    "accuracy": 0.6,
    "approach": "The script solves a question by decomposing it into sub-questions, extracting relevant information, and synthesizing an answer, using the Gemini LLM for each step with a chain-of-thought approach and validation at each stage. It uses the `decompose_question`, `extract_information`, and `synthesize_answer` functions sequentially, each acting as a distinct agent role (question decomposer, information extractor, and answer synthesizer, respectively) and validating their results with an LLM before proceeding. The `call_llm` function is used to interact with the Gemini model, and the main function orchestrates the overall workflow."
  },
  {
    "iteration": 13,
    "strategy": "Exploitation",
    "accuracy": 0.7,
    "approach": "The script uses chain-of-thought reasoning with LLMs to answer questions by decomposing them into sub-questions, extracting relevant information, and synthesizing a final answer. Each step involves a dedicated LLM agent (question decomposer, information extraction expert, and answer synthesis expert) that uses examples to guide the reasoning process. Each step also uses verification to ensure that the produced output is valid. The functions `decompose_question`, `extract_information`, and `synthesize_answer` orchestrate these steps, calling `call_llm` to interact with the Gemini model with specific prompts and system instructions. The overall workflow involves decomposing the initial question, extracting information related to sub-questions, and synthesizing these extractions into a complete final answer."
  },
  {
    "iteration": 14,
    "strategy": "Exploitation",
    "accuracy": 0.6,
    "approach": "The script implements a question-answering system using chain-of-thought reasoning, where the problem is decomposed into sub-questions, information extraction, and answer synthesis. It employs three distinct agent roles: a question decomposer, an information extraction expert, and an answer synthesis expert, each responsible for a specific stage of the process. The functions `decompose_question`, `extract_information`, and `synthesize_answer` use the `call_llm` function to interact with the LLM, and each function also validates its results with the LLM before proceeding to the next stage. The overall workflow involves decomposing the initial question, extracting relevant information to answer sub-questions, synthesizing these answers into a final response, and then returning the final answer."
  },
  {
    "iteration": 15,
    "strategy": "Exploitation",
    "accuracy": 0.7,
    "approach": "The script solves a question by decomposing it into sub-questions, extracting relevant information to answer them, and synthesizing a final answer, using chain-of-thought prompting with examples. Each stage (decomposition, extraction, synthesis) uses a specific LLM agent role and validates the output using another LLM call. The workflow involves `main` calling `decompose_question`, then `extract_information`, then `synthesize_answer`; each of these functions calls `call_llm` to interact with the LLM and also validate the response from the LLM."
  }
]

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 3 iterations):
        [
  {
    "iteration": 6,
    "issue": "The primary issue is **inconsistent and unreliable arithmetic calculation, compounded by a lack of rigorous answer verification before output**. Even simple addition or subtraction sometimes fails, and the system doesn't catch these errors before finalizing the answer. This, coupled with inconsistent unit handling, reduces the trustworthiness of the entire process."
  },
  {
    "iteration": 7,
    "issue": "The single most critical problem is the undefined `call_llm` function. This function is presumably the core component responsible for interacting with the LLM, and its absence effectively disables the entire system. The definition or import of this function needs to be addressed immediately."
  },
  {
    "iteration": 8,
    "issue": "The most critical problem is the failure of the question type determination module. This module needs to be redesigned or debugged to correctly classify the types of questions being asked (e.g., comparison, extraction, calculation). The validation logic needs to be checked for correctness or adjusted."
  },
  {
    "iteration": 9,
    "issue": "The primary issue is the **consistent off-by-one arithmetic errors** specifically in calculating the difference between two dates or years. This indicates a need for more robust and tested arithmetic functions."
  },
  {
    "iteration": 10,
    "issue": "The primary issue is the system's failure to accurately calculate the number of months between two specific dates mentioned in the passage, even with the information explicitly provided. This indicates a weakness in the system's temporal reasoning capabilities."
  },
  {
    "iteration": 11,
    "issue": "The most critical problem is the system's inability to accurately identify the subject of the action \"seizing power\" from the text. The system can identify that someone seized power, but grabs the wrong person as being the target."
  },
  {
    "iteration": 12,
    "issue": "The primary issue is the system's lack of robust temporal reasoning capabilities, leading to incorrect answers when questions require understanding the order of events or specific dates within the passage."
  },
  {
    "iteration": 13,
    "issue": "The primary issue is the system's **inability to perform precise contextual reasoning and constraint handling when numerical answers are needed**. It struggles to connect related pieces of information, apply constraints to filter out irrelevant information, and give accurate numerical answers without rounding or formatting errors."
  },
  {
    "iteration": 14,
    "issue": "The most critical problem is the system's inability to accurately extract and filter information based on multiple conditions and perform accurate arithmetic calculation based on this information. Specifically, when a question requires combining information extraction with even simple arithmetic operations, the system tends to fail."
  },
  {
    "iteration": 15,
    "issue": "The single most critical problem is the inaccurate extraction of numerical values relevant to answering questions that require mathematical operations such as finding the difference between values, or calculating percentages. The accurate identification of what values to compare/operate on is key."
  }
]

        TARGETED IMPROVEMENTS:
        [
  "Prompt Engineering:** Use prompt engineering to explicitly ask the LLM to extract all relevant numbers and their corresponding units and then perform the calculation. For example: \"Extract all numbers corresponding to the entities and their units. What arithmetic operations should be performed to arrive at the answer? Perform the calculation and give the final answer.\"",
  "Enhance Reasoning Chain for Temporal Questions:** When the question involves time-related calculations, force the system to explicitly state the start date, end date, and the steps used to calculate the time difference. This will allow for easier debugging.",
  "Implement Date Parsing and Calculation:** Integrate a robust date parsing library to accurately extract date information from the passage. Then, use built-in date difference functions (e.g., from the `datetime` module in Python) to calculate the exact number of months between the dates.",
  "Introduce Contrastive Learning:** Train the system using contrastive learning techniques, where it's presented with pairs of similar questions that require different answers. This will help the system learn to differentiate between subtle nuances in question wording.",
  "Implement more print statements:** Add more print statements, especially when numerical computations are performed, so that one can track how numerical data and question data is extracted and utilized by the AI system.",
  "Refine Question Parsing:** Implement more sophisticated techniques for question parsing, including dependency parsing, semantic role labeling, and question type classification. This will enable the system to better understand the relationships between words and phrases in the question.",
  "Add Unit Tests Focused on Time-Based Reasoning:** Create a suite of unit tests specifically designed to test the system's temporal reasoning capabilities, including questions that require calculating time differences in days, weeks, months, and years.",
  "Add Temporal Reasoning Module:** Implement a dedicated module for temporal reasoning that can handle date calculations, time intervals, and event ordering.",
  "Implement Semantic Similarity Measures:** Utilize semantic similarity metrics (e.g., word embeddings, sentence embeddings) to compare the question with different parts of the passage. This will help the system identify the most relevant information and avoid focusing on irrelevant details.",
  "Answer type classification:** Before generating the final answer, classify the question based on the expected answer type (e.g., numerical, descriptive, boolean). Use this classification to guide the answer generation process. For example, if the expected answer type is numerical, prioritize extracting numbers and performing calculations."
]
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity ✓
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity ✓
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity ✓
    4. All operations have O(1) average time complexity ✓
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        

            PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS). YOUR APPROACH MUST BE SUBSTANTIVELY DIFFERENT THAN THESE:
            
PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS):

=== SCRIPT FROM ITERATION 15 (Exploitation, ACCURACY: 0.70) ===
Approach: The script solves a question by decomposing it into sub-questions, extracting relevant information to answer them, and synthesizing a final answer, using chain-of-thought prompting with examples. Each stage (decomposition, extraction, synthesis) uses a specific LLM agent role and validates the output using another LLM call. The workflow involves `main` calling `decompose_question`, then `extract_information`, then `synthesize_answer`; each of these functions calls `call_llm` to interact with the LLM and also validate the response from the LLM.

```python
import os
import re

def main(question):
    """
    Solve the question by extracting relevant information from the passage and using chain-of-thought reasoning.
    This approach builds upon a prior attempt to use question decomposition, strengthens answer synthesis, and includes examples in all LLM prompts.
    """
    try:
        # Step 1: Decompose the question into sub-questions.
        decomposition_result = decompose_question(question)
        if not decomposition_result.get("is_valid"):
            return f"Error in question decomposition: {decomposition_result.get('validation_feedback')}"
        
        # Step 2: Extract relevant information based on sub-questions.
        information_extraction_result = extract_information(question, decomposition_result["sub_questions"])
        if not information_extraction_result.get("is_valid"):
            return f"Error in information extraction: {information_extraction_result.get('validation_feedback')}"

        # Step 3: Synthesize the answer from extracted information.
        answer_synthesis_result = synthesize_answer(question, information_extraction_result["extracted_info"])
        if not answer_synthesis_result.get("is_valid"):
            return f"Error in answer synthesis: {answer_synthesis_result.get('validation_feedback')}"
        
        return answer_synthesis_result["answer"]

    except Exception as e:
        return f"An unexpected error occurred: {str(e)}"

def decompose_question(question, max_attempts=3):
    """Decompose the main question into smaller, answerable sub-questions."""
    system_instruction = "You are an expert question decomposer."
    
    for attempt in range(max_attempts):
        decomposition_prompt = f"""
        Decompose the given question into smaller, self-contained sub-questions that, when answered, will fully answer the original question.

        Example 1:
        Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions:
        1. How many yards was Chris Johnson's first touchdown?
        2. How many yards was Jason Hanson's first field goal?
        3. What is the sum of those two values?

		Reasoning: The main question asks for a combination of two values. Decomposing it into finding each value separately and then summing them solves this.

        Example 2:
        Question: Who caught the final touchdown of the game?
        Sub-questions:
        1. Who scored the final touchdown of the game?

		Reasoning: The question seeks the identity of a player based on an event. A single sub-question can directly retrieve this information.

        Question: {question}
        Sub-questions:
        """
        
        decomposition_result = call_llm(decomposition_prompt, system_instruction)
        
        # Verify if the decomposition is valid
        verification_prompt = f"""
        Verify if these sub-questions are valid and sufficient to answer the original question.

        Original Question: {question}
        Sub-questions: {decomposition_result}

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions: 1. How many yards was Chris Johnson's first touchdown? 2. How many yards was Jason Hanson's first field goal? 3. What is the sum of those two values?
        Validation: Valid
		Reasoning: All sub-questions directly address necessary parts of the main question, and when answered fully answer the main question

        Is the decomposition valid and sufficient? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)
        
        if "valid" in verification_result.lower():
            return {"is_valid": True, "sub_questions": decomposition_result}
        else:
            print(f"Decomposition validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to decompose the question successfully."}

def extract_information(question, sub_questions, max_attempts=3):
    """Extract relevant information from the passage based on the sub-questions."""
    system_instruction = "You are an information extraction expert."
    
    for attempt in range(max_attempts):
        extraction_prompt = f"""
        Given the original question and its sub-questions, extract the relevant information from the passage required to answer the sub-questions.

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions:
        1. How many yards was Chris Johnson's first touchdown?
        2. How many yards was Jason Hanson's first field goal?
        Extracted Information:
        Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.
		Reasoning: This example correctly identifies the yards for both Chris Johnson's touchdown and Jason Hanson's field goal, directly answering both sub-questions.

        Original Question: {question}
        Sub-questions: {sub_questions}
        Extracted Information:
        """
        
        extracted_info = call_llm(extraction_prompt, system_instruction)
        
        # Validate information extraction
        verification_prompt = f"""
        Verify if the extracted information is relevant and sufficient to answer the sub-questions.

        Original Question: {question}
        Sub-questions: {sub_questions}
        Extracted Information: {extracted_info}

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions: 1. How many yards was Chris Johnson's first touchdown? 2. How many yards was Jason Hanson's first field goal?
        Extracted Information: Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.
        Validation: Valid
		Reasoning: The extracted information directly corresponds to the information needed to answer the sub-questions

        Is the extraction relevant and sufficient? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)
        
        if "valid" in verification_result.lower():
            return {"is_valid": True, "extracted_info": extracted_info}
        else:
            print(f"Information extraction validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to extract relevant information successfully."}

def synthesize_answer(question, extracted_info, max_attempts=3):
    """Synthesize the answer from the extracted information to answer the main question."""
    system_instruction = "You are an answer synthesis expert."

    for attempt in range(max_attempts):
        synthesis_prompt = f"""
        Given the original question and the extracted information, synthesize the final answer.

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Extracted Information: Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.
        Final Answer: 59
		Reasoning: Adding the two yardages from the extracted information results in the final answer.

        Original Question: {question}
        Extracted Information: {extracted_info}
        Final Answer:
        """
        
        answer = call_llm(synthesis_prompt, system_instruction)

        # Answer checker
        verification_prompt = f"""
        Check if the answer is correct and answers the original question fully.

        Original Question: {question}
        Synthesized Answer: {answer}

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Synthesized Answer: 59
        Validation: Valid
		Reasoning: This is the correct yardage according to addition and matches what is expected from the original question.

        Is the answer correct and complete? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)

        if "valid" in verification_result.lower():
            return {"is_valid": True, "answer": answer}
        else:
            print(f"Answer synthesis validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to synthesize a valid answer."}

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types
        import os  # Import the os module

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
```

=== SCRIPT FROM ITERATION 14 (Exploitation, ACCURACY: 0.60) ===
Approach: The script implements a question-answering system using chain-of-thought reasoning, where the problem is decomposed into sub-questions, information extraction, and answer synthesis. It employs three distinct agent roles: a question decomposer, an information extraction expert, and an answer synthesis expert, each responsible for a specific stage of the process. The functions `decompose_question`, `extract_information`, and `synthesize_answer` use the `call_llm` function to interact with the LLM, and each function also validates its results with the LLM before proceeding to the next stage. The overall workflow involves decomposing the initial question, extracting relevant information to answer sub-questions, synthesizing these answers into a final response, and then returning the final answer.

```python
import os
import re

def main(question):
    """
    Solve the question by extracting relevant information from the passage and using chain-of-thought reasoning.
    This approach builds upon a prior attempt to use question decomposition, strengthens answer synthesis, and includes examples in all LLM prompts.
    Addresses weaknesses in temporal reasoning and contextual understanding.
    """
    try:
        # Step 1: Decompose the question into sub-questions.
        decomposition_result = decompose_question(question)
        if not decomposition_result.get("is_valid"):
            return f"Error in question decomposition: {decomposition_result.get('validation_feedback')}"
        
        # Step 2: Extract relevant information based on sub-questions.
        information_extraction_result = extract_information(question, decomposition_result["sub_questions"])
        if not information_extraction_result.get("is_valid"):
            return f"Error in information extraction: {information_extraction_result.get('validation_feedback')}"

        # Step 3: Synthesize the answer from extracted information.
        answer_synthesis_result = synthesize_answer(question, information_extraction_result["extracted_info"])
        if not answer_synthesis_result.get("is_valid"):
            return f"Error in answer synthesis: {answer_synthesis_result.get('validation_feedback')}"
        
        return answer_synthesis_result["answer"]

    except Exception as e:
        return f"An unexpected error occurred: {str(e)}"

def decompose_question(question, max_attempts=3):
    """Decompose the main question into smaller, answerable sub-questions."""
    system_instruction = "You are an expert question decomposer."
    
    for attempt in range(max_attempts):
        decomposition_prompt = f"""
        Decompose the given question into smaller, self-contained sub-questions that, when answered, will fully answer the original question.

        Example 1:
        Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions:
        1. How many yards was Chris Johnson's first touchdown?
        2. How many yards was Jason Hanson's first field goal?
        3. What is the sum of those two values?

        Example 2:
        Question: Who caught the final touchdown of the game?
        Sub-questions:
        1. Who scored the final touchdown of the game?

        Example 3:
        Question: What percent of Hispaniola did the germans own?
        Sub-questions:
        1. What percentage of Hispaniola's commerce was controlled by Germans?

        Question: {question}
        Sub-questions:
        """
        
        decomposition_result = call_llm(decomposition_prompt, system_instruction)
        
        # Verify if the decomposition is valid
        verification_prompt = f"""
        Verify if these sub-questions are valid and sufficient to answer the original question.

        Original Question: {question}
        Sub-questions: {decomposition_result}

        Example 1:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions: 1. How many yards was Chris Johnson's first touchdown? 2. How many yards was Jason Hanson's first field goal? 3. What is the sum of those two values?
        Validation: Valid

        Example 2:
        Original Question: What percent of Hispaniola did the germans own?
        Sub-questions: 1. What percentage of Hispaniola's commerce was controlled by Germans?
        Validation: Valid

        Is the decomposition valid and sufficient? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)
        
        if "valid" in verification_result.lower():
            return {"is_valid": True, "sub_questions": decomposition_result}
        else:
            print(f"Decomposition validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to decompose the question successfully."}

def extract_information(question, sub_questions, max_attempts=3):
    """Extract relevant information from the passage based on the sub-questions."""
    system_instruction = "You are an information extraction expert."
    
    for attempt in range(max_attempts):
        extraction_prompt = f"""
        Given the original question and its sub-questions, extract the relevant information from the passage required to answer the sub-questions.

        Example 1:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions:
        1. How many yards was Chris Johnson's first touchdown?
        2. How many yards was Jason Hanson's first field goal?
        Extracted Information:
        Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.

        Example 2:
        Original Question: What percent of Hispaniola did the germans own?
        Sub-questions:
        1. What percentage of Hispaniola's commerce was controlled by Germans?
        Extracted Information:
        German nationals controlled about 80 percent of the country's international commerce.

        Original Question: {question}
        Sub-questions: {sub_questions}
        Extracted Information:
        """
        
        extracted_info = call_llm(extraction_prompt, system_instruction)
        
        # Validate information extraction
        verification_prompt = f"""
        Verify if the extracted information is relevant and sufficient to answer the sub-questions.

        Original Question: {question}
        Sub-questions: {sub_questions}
        Extracted Information: {extracted_info}

        Example 1:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions: 1. How many yards was Chris Johnson's first touchdown? 2. How many yards was Jason Hanson's first field goal?
        Extracted Information: Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.
        Validation: Valid

        Example 2:
        Original Question: What percent of Hispaniola did the germans own?
        Sub-questions: 1. What percentage of Hispaniola's commerce was controlled by Germans?
        Extracted Information: German nationals controlled about 80 percent of the country's international commerce.
        Validation: Valid

        Is the extraction relevant and sufficient? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)
        
        if "valid" in verification_result.lower():
            return {"is_valid": True, "extracted_info": extracted_info}
        else:
            print(f"Information extraction validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to extract relevant information successfully."}

def synthesize_answer(question, extracted_info, max_attempts=3):
    """Synthesize the answer from the extracted information to answer the main question."""
    system_instruction = "You are an answer synthesis expert."

    for attempt in range(max_attempts):
        synthesis_prompt = f"""
        Given the original question and the extracted information, synthesize the final answer.

        Example 1:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Extracted Information: Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.
        Final Answer: 59

        Example 2:
        Original Question: What percent of Hispaniola did the germans own?
        Extracted Information: German nationals controlled about 80 percent of the country's international commerce.
        Final Answer: 80

        Original Question: {question}
        Extracted Information: {extracted_info}
        Final Answer:
        """
        
        answer = call_llm(synthesis_prompt, system_instruction)

        # Answer checker
        verification_prompt = f"""
        Check if the answer is correct and answers the original question fully.

        Original Question: {question}
        Synthesized Answer: {answer}

        Example 1:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Synthesized Answer: 59
        Validation: Valid

        Example 2:
        Original Question: What percent of Hispaniola did the germans own?
        Synthesized Answer: 80
        Validation: Valid

        Is the answer correct and complete? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)

        if "valid" in verification_result.lower():
            return {"is_valid": True, "answer": answer}
        else:
            print(f"Answer synthesis validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to synthesize a valid answer."}

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types
        import os  # Import the os module

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
```

=== SCRIPT FROM ITERATION 13 (Exploitation, ACCURACY: 0.70) ===
Approach: The script uses chain-of-thought reasoning with LLMs to answer questions by decomposing them into sub-questions, extracting relevant information, and synthesizing a final answer. Each step involves a dedicated LLM agent (question decomposer, information extraction expert, and answer synthesis expert) that uses examples to guide the reasoning process. Each step also uses verification to ensure that the produced output is valid. The functions `decompose_question`, `extract_information`, and `synthesize_answer` orchestrate these steps, calling `call_llm` to interact with the Gemini model with specific prompts and system instructions. The overall workflow involves decomposing the initial question, extracting information related to sub-questions, and synthesizing these extractions into a complete final answer.

```python
import os
import re

def main(question):
    """
    Solve the question by extracting relevant information from the passage and using chain-of-thought reasoning.
    This approach builds upon a prior attempt to use question decomposition, strengthens answer synthesis, and includes examples in all LLM prompts.
    """
    try:
        # Step 1: Decompose the question into sub-questions.
        decomposition_result = decompose_question(question)
        if not decomposition_result.get("is_valid"):
            return f"Error in question decomposition: {decomposition_result.get('validation_feedback')}"
        
        # Step 2: Extract relevant information based on sub-questions.
        information_extraction_result = extract_information(question, decomposition_result["sub_questions"])
        if not information_extraction_result.get("is_valid"):
            return f"Error in information extraction: {information_extraction_result.get('validation_feedback')}"

        # Step 3: Synthesize the answer from extracted information.
        answer_synthesis_result = synthesize_answer(question, information_extraction_result["extracted_info"])
        if not answer_synthesis_result.get("is_valid"):
            return f"Error in answer synthesis: {answer_synthesis_result.get('validation_feedback')}"
        
        return answer_synthesis_result["answer"]

    except Exception as e:
        return f"An unexpected error occurred: {str(e)}"

def decompose_question(question, max_attempts=3):
    """Decompose the main question into smaller, answerable sub-questions."""
    system_instruction = "You are an expert question decomposer."
    
    for attempt in range(max_attempts):
        decomposition_prompt = f"""
        Decompose the given question into smaller, self-contained sub-questions that, when answered, will fully answer the original question.

        Example 1:
        Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions:
        1. How many yards was Chris Johnson's first touchdown?
        2. How many yards was Jason Hanson's first field goal?
        3. What is the sum of those two values?

		Example 2:
        Question: Which happened later, Chinese invasion of tibet or the outbreak of the Xinhai Revolution?
        Sub-questions:
        1. When was the Chinese invasion of Tibet?
        2. When did the outbreak of the Xinhai Revolution occur?
        3. Which of the two dates is later?

        Question: {question}
        Sub-questions:
        """
        
        decomposition_result = call_llm(decomposition_prompt, system_instruction)
        
        # Verify if the decomposition is valid
        verification_prompt = f"""
        Verify if these sub-questions are valid and sufficient to answer the original question.

        Original Question: {question}
        Sub-questions: {decomposition_result}

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions: 1. How many yards was Chris Johnson's first touchdown? 2. How many yards was Jason Hanson's first field goal? 3. What is the sum of those two values?
        Validation: Valid

        Is the decomposition valid and sufficient? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)
        
        if "valid" in verification_result.lower():
            return {"is_valid": True, "sub_questions": decomposition_result}
        else:
            print(f"Decomposition validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to decompose the question successfully."}

def extract_information(question, sub_questions, max_attempts=3):
    """Extract relevant information from the passage based on the sub-questions."""
    system_instruction = "You are an information extraction expert."
    
    for attempt in range(max_attempts):
        extraction_prompt = f"""
        Given the original question and its sub-questions, extract the relevant information from the passage required to answer the sub-questions.

        Example 1:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions:
        1. How many yards was Chris Johnson's first touchdown?
        2. How many yards was Jason Hanson's first field goal?
        Extracted Information:
        Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.

        Example 2:
        Original Question: Which happened later, Chinese invasion of tibet or the outbreak of the Xinhai Revolution?
        Sub-questions:
        1. When was the Chinese invasion of Tibet?
        2. When did the outbreak of the Xinhai Revolution occur?
        Extracted Information:
        The 1910 Chinese expedition to Tibet or the Chinese invasion of Tibet in 1910. after the outbreak of the Xinhai Revolution and the Xinhai Lhasa turmoil in 1911-1912

        Original Question: {question}
        Sub-questions: {sub_questions}
        Extracted Information:
        """
        
        extracted_info = call_llm(extraction_prompt, system_instruction)
        
        # Validate information extraction
        verification_prompt = f"""
        Verify if the extracted information is relevant and sufficient to answer the sub-questions.

        Original Question: {question}
        Sub-questions: {sub_questions}
        Extracted Information: {extracted_info}

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions: 1. How many yards was Chris Johnson's first touchdown? 2. How many yards was Jason Hanson's first field goal?
        Extracted Information: Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.
        Validation: Valid

        Is the extraction relevant and sufficient? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)
        
        if "valid" in verification_result.lower():
            return {"is_valid": True, "extracted_info": extracted_info}
        else:
            print(f"Information extraction validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to extract relevant information successfully."}

def synthesize_answer(question, extracted_info, max_attempts=3):
    """Synthesize the answer from the extracted information to answer the main question."""
    system_instruction = "You are an answer synthesis expert."

    for attempt in range(max_attempts):
        synthesis_prompt = f"""
        Given the original question and the extracted information, synthesize the final answer.

        Example 1:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Extracted Information: Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.
        Final Answer: 59

        Example 2:
        Original Question: Which happened later, Chinese invasion of tibet or the outbreak of the Xinhai Revolution?
        Extracted Information: The 1910 Chinese expedition to Tibet or the Chinese invasion of Tibet in 1910. after the outbreak of the Xinhai Revolution and the Xinhai Lhasa turmoil in 1911-1912
        Final Answer: n 1910 China invaded Tibet
        Original Question: {question}
        Extracted Information: {extracted_info}
        Final Answer:
        """
        
        answer = call_llm(synthesis_prompt, system_instruction)

        # Answer checker
        verification_prompt = f"""
        Check if the answer is correct and answers the original question fully.

        Original Question: {question}
        Synthesized Answer: {answer}

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Synthesized Answer: 59
        Validation: Valid

        Is the answer correct and complete? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)

        if "valid" in verification_result.lower():
            return {"is_valid": True, "answer": answer}
        else:
            print(f"Answer synthesis validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to synthesize a valid answer."}

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types
        import os  # Import the os module

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
```

=== SCRIPT FROM ITERATION 12 (Exploitation, ACCURACY: 0.60) ===
Approach: The script solves a question by decomposing it into sub-questions, extracting relevant information, and synthesizing an answer, using the Gemini LLM for each step with a chain-of-thought approach and validation at each stage. It uses the `decompose_question`, `extract_information`, and `synthesize_answer` functions sequentially, each acting as a distinct agent role (question decomposer, information extractor, and answer synthesizer, respectively) and validating their results with an LLM before proceeding. The `call_llm` function is used to interact with the Gemini model, and the main function orchestrates the overall workflow.

```python
import os
import re

def main(question):
    """
    Solve the question by extracting relevant information from the passage and using chain-of-thought reasoning.
    This approach builds upon a prior attempt to use question decomposition, strengthens answer synthesis, and includes examples in all LLM prompts.
    """
    try:
        # Step 1: Decompose the question into sub-questions.
        decomposition_result = decompose_question(question)
        if not decomposition_result.get("is_valid"):
            return f"Error in question decomposition: {decomposition_result.get('validation_feedback')}"
        
        # Step 2: Extract relevant information based on sub-questions.
        information_extraction_result = extract_information(question, decomposition_result["sub_questions"])
        if not information_extraction_result.get("is_valid"):
            return f"Error in information extraction: {information_extraction_result.get('validation_feedback')}"

        # Step 3: Synthesize the answer from extracted information.
        answer_synthesis_result = synthesize_answer(question, information_extraction_result["extracted_info"])
        if not answer_synthesis_result.get("is_valid"):
            return f"Error in answer synthesis: {answer_synthesis_result.get('validation_feedback')}"
        
        return answer_synthesis_result["answer"]

    except Exception as e:
        return f"An unexpected error occurred: {str(e)}"

def decompose_question(question, max_attempts=3):
    """Decompose the main question into smaller, answerable sub-questions."""
    system_instruction = "You are an expert question decomposer."
    
    for attempt in range(max_attempts):
        decomposition_prompt = f"""
        Decompose the given question into smaller, self-contained sub-questions that, when answered, will fully answer the original question.

        Example 1:
        Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions:
        1. How many yards was Chris Johnson's first touchdown?
        2. How many yards was Jason Hanson's first field goal?
        3. What is the sum of those two values?

        Example 2:
        Question: Who caught the final touchdown of the game?
        Sub-questions:
        1. Who scored the final touchdown of the game?
        2. What was the name of the person who scored it?

        Question: {question}
        Sub-questions:
        """
        
        decomposition_result = call_llm(decomposition_prompt, system_instruction)
        
        # Verify if the decomposition is valid
        verification_prompt = f"""
        Verify if these sub-questions are valid and sufficient to answer the original question.

        Original Question: {question}
        Sub-questions: {decomposition_result}

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions: 1. How many yards was Chris Johnson's first touchdown? 2. How many yards was Jason Hanson's first field goal? 3. What is the sum of those two values?
        Validation: Valid

        Is the decomposition valid and sufficient? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)
        
        if "valid" in verification_result.lower():
            return {"is_valid": True, "sub_questions": decomposition_result}
        else:
            print(f"Decomposition validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to decompose the question successfully."}

def extract_information(question, sub_questions, max_attempts=3):
    """Extract relevant information from the passage based on the sub-questions."""
    system_instruction = "You are an information extraction expert."
    
    for attempt in range(max_attempts):
        extraction_prompt = f"""
        Given the original question and its sub-questions, extract the relevant information from the passage required to answer the sub-questions.

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions:
        1. How many yards was Chris Johnson's first touchdown?
        2. How many yards was Jason Hanson's first field goal?
        Extracted Information:
        Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.

        Original Question: {question}
        Sub-questions: {sub_questions}
        Extracted Information:
        """
        
        extracted_info = call_llm(extraction_prompt, system_instruction)
        
        # Validate information extraction
        verification_prompt = f"""
        Verify if the extracted information is relevant and sufficient to answer the sub-questions.

        Original Question: {question}
        Sub-questions: {sub_questions}
        Extracted Information: {extracted_info}

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions: 1. How many yards was Chris Johnson's first touchdown? 2. How many yards was Jason Hanson's first field goal?
        Extracted Information: Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.
        Validation: Valid

        Is the extraction relevant and sufficient? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)
        
        if "valid" in verification_result.lower():
            return {"is_valid": True, "extracted_info": extracted_info}
        else:
            print(f"Information extraction validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to extract relevant information successfully."}

def synthesize_answer(question, extracted_info, max_attempts=3):
    """Synthesize the answer from the extracted information to answer the main question."""
    system_instruction = "You are an answer synthesis expert."

    for attempt in range(max_attempts):
        synthesis_prompt = f"""
        Given the original question and the extracted information, synthesize the final answer. Provide the answer without explanations.

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Extracted Information: Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.
        Final Answer: 59

        Original Question: {question}
        Extracted Information: {extracted_info}
        Final Answer:
        """
        
        answer = call_llm(synthesis_prompt, system_instruction)

        # Answer checker
        verification_prompt = f"""
        Check if the answer is correct and answers the original question fully. Consider if the answer is a valid value if that's the case.

        Original Question: {question}
        Synthesized Answer: {answer}

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Synthesized Answer: 59
        Validation: Valid

        Is the answer correct and complete? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)

        if "valid" in verification_result.lower():
            return {"is_valid": True, "answer": answer}
        else:
            print(f"Answer synthesis validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to synthesize a valid answer."}

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types
        import os  # Import the os module

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
```

=== SCRIPT FROM ITERATION 11 (Exploration, ACCURACY: 0.80) ===
Approach: The script employs a "Question Clarification & Focused Extraction" approach, using LLMs to clarify a given question, extract relevant information, and synthesize an answer. The problem is decomposed into three main steps: question clarification, information extraction, and answer synthesis, each using the `call_llm` function with specific system instructions to act as an expert in that step. The `clarify_question` and `extract_information` functions include a verification step to ensure the validity of the LLM's output. The overall workflow is: `main` calls `clarify_question`, then `extract_information`, and finally `synthesize_answer`, using `call_llm` in each to interact with the LLM.

```python
import os
import re
import math

def main(question):
    """
    This script implements a "Question Clarification & Focused Extraction" approach.
    The hypothesis is that by first clarifying the question's intent with the LLM,
    we can significantly improve the precision and relevance of information extraction,
    avoiding previous issues with misinterpretation and unnecessary decomposition.
    This will be followed by a verification step to ensure the extraction worked.
    """
    try:
        # Step 1: Clarify the question to better understand the user's intent
        clarification_result = clarify_question(question)
        if not clarification_result.get("is_valid"):
            return f"Error in question clarification: {clarification_result.get('validation_feedback')}"
        clarified_question = clarification_result["clarified_question"]
        print(f"Clarified question: {clarified_question}")

        # Step 2: Extract relevant information based on the clarified question
        extraction_result = extract_information(clarified_question, question)
        if not extraction_result.get("is_valid"):
            return f"Error in information extraction: {extraction_result.get('validation_feedback')}"
        extracted_info = extraction_result["extracted_info"]
        print(f"Extracted info: {extracted_info}")

        # Step 3: Synthesize the answer from extracted information
        answer = synthesize_answer(clarified_question, extracted_info)
        return answer

    except Exception as e:
        return f"An unexpected error occurred: {str(e)}"

def clarify_question(question, max_attempts=3):
    """Clarifies the question to better understand the user's intent."""
    system_instruction = "You are an expert at clarifying ambiguous questions."

    for attempt in range(max_attempts):
        clarification_prompt = f"""
        Given a question, rephrase it to be more specific and unambiguous, clarifying the user's intent.

        Example:
        Question: How many yards did the players combine for?
        Clarified Question: What was the total number of yards gained by all players mentioned in the passage?

        Question: {question}
        Clarified Question:
        """

        clarified_question = call_llm(clarification_prompt, system_instruction)

        verification_prompt = f"""
        Verify that the clarified question maintains the original intent while being more specific.

        Original Question: {question}
        Clarified Question: {clarified_question}

        Example:
        Original Question: How many yards did the players combine for?
        Clarified Question: What was the total number of yards gained by all players mentioned in the passage?
        Verification: Valid

        Is the clarified question valid? Respond with 'Valid' or 'Invalid'.
        """

        verification_result = call_llm(verification_prompt, system_instruction)

        if "valid" in verification_result.lower():
            return {"is_valid": True, "clarified_question": clarified_question}
        else:
            print(f"Clarification validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")

    return {"is_valid": False, "validation_feedback": "Failed to clarify the question successfully."}

def extract_information(clarified_question, original_question, max_attempts=3):
    """Extracts relevant information from the passage based on the clarified question."""
    system_instruction = "You are an information extraction expert. Extract only the relevant information and nothing else."

    for attempt in range(max_attempts):
        extraction_prompt = f"""
        Extract the information needed to answer the following question, using the original question for context.

        Original Question: {original_question}
        Question: {clarified_question}
        Extracted Information:
        """

        extracted_info = call_llm(extraction_prompt, system_instruction)

        verification_prompt = f"""
        Verify if the extracted information is relevant and complete for answering the question.

        Question: {clarified_question}
        Extracted Information: {extracted_info}

        Example:
        Question: What was the total number of yards gained by all players mentioned in the passage?
        Extracted Information: Player A gained 100 yards. Player B gained 50 yards.
        Verification: Valid

        Is the extracted information valid? Respond with 'Valid' or 'Invalid'.
        """

        verification_result = call_llm(verification_prompt, system_instruction)

        if "valid" in verification_result.lower():
            return {"is_valid": True, "extracted_info": extracted_info}
        else:
            print(f"Extraction validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")

    return {"is_valid": False, "validation_feedback": "Failed to extract the information successfully."}

def synthesize_answer(question, extracted_info):
    """Synthesizes the answer from the extracted information."""
    system_instruction = "You are an expert at synthesizing answers from extracted information. Provide a direct answer based on the extracted information."
    synthesis_prompt = f"""
        Given the question and the extracted information, create a final answer.

        Example:
        Question: What was the total number of yards gained by all players mentioned in the passage?
        Extracted Information: Player A gained 100 yards. Player B gained 50 yards.
        Answer: 150 yards

        Question: {question}
        Extracted Information: {extracted_info}
        Answer:
        """
    answer = call_llm(synthesis_prompt, system_instruction)
    return answer
def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types
        import os  # Import the os module

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
```


            LEARNINGS FROM PREVIOUS ITERATIONS:
            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        ```
# DATASET-SPECIFIC LEARNINGS & EXPERIMENT LOG

This document tracks our evolving understanding and experimental findings related to the question answering task on the current dataset. It serves as a long-term memory to guide future research and development efforts.

## 1. DATASET PATTERNS & CHARACTERISTICS

*   **General:**
    *   The dataset consists of passages describing events (primarily American football games or political events), followed by questions about the details.
    *   Questions are fact-based, requiring information retrieval and processing directly from the passage. No external knowledge is needed.
    *   Questions often involve numerical reasoning (arithmetic, comparisons) and general knowledge application.
    *   Passages can be lengthy, requiring careful reading.
    *   Case sensitivity is observed.
    *   Questions often involve comparing or relating two or more pieces of information in the passage.
    *   Questions frequently ask for numerical information (quantities, dates, names), requiring precise extraction and comparison.
    *   Passages often present multiple entities or events; questions require distinguishing between them and relating correct information.
    *   Questions may require synthesizing information from multiple sentences or inferring from disparate facts.
    *   Many questions involve interpreting percentages or proportions, requiring correct identification of the base number.
    *   Questions often test the understanding of specific terminology within the passage's context.
    *   Questions often require understanding of sports game summaries, particularly football.
    *   Questions often involve numerical reasoning based on events within the passage (e.g., counting touchdowns, calculating yardage differences).
    *   Questions can be complex, requiring differentiation between types of scoring events and temporal reasoning.
    *   Questions often require extracting relevant dates from a passage and then calculating the difference between them. The wording is precise, requiring careful attention to specific events and timelines.
    *   Passages contain extraneous information; therefore, the agent must be able to identify and extract specific dates relevant to the question.
    *   The dataset presents questions that require **numerical reasoning and contextual linking**. Many questions ask for percentage differences or values derived from information spread across sentences.
    *   Questions frequently involve **extracting and comparing numerical values**. The system needs to identify the correct numerical values from the passage and perform arithmetic operations.
    *   The dataset contains questions where the **answer is not explicitly stated** but requires inferential reasoning based on the context provided in the passage. Sometimes the answer can't be found in the text.
    *   **Complex Reasoning:** The questions frequently require integrating information from multiple parts of the passage, going beyond simple fact retrieval.

*   **Entity Relationship Extraction:**
    *   The dataset presents questions requiring identifying relationships between entities within a passage. The target relationship often involves actions (e.g., seizing power, causing casualties) and the entities involved (agent and target).
    *   Questions frequently require pinpointing the target/subject of an action, even when multiple related entities are mentioned in close proximity. This demands precise understanding of sentence structure and pronoun references. The model confuses the person who *preceded* the power seizure with the *victim* of the power seizure (e.g., confusing King William II of Sicily with Queen Joan).
    *   The passages are concise, but contain sufficient detail that incorrect attribution is easy.

*   **Date/Time Reasoning:**
    *   A recurring pattern is the need to calculate durations (years, months, days) based on dates or time spans mentioned in the passage. This requires precise arithmetic and attention to inclusivity/exclusivity.
    *   A significant portion of the questions requires understanding and comparing dates and events described in the passage. The questions often ask "Which happened later?" or require calculating time differences.
    *   **Temporal Reasoning:** Some questions require understanding and comparing events that occur at different times in the passage.

*   **Explicit Information Retrieval:**
    *   The questions often require directly extracting and processing information explicitly stated in the passage, rather than making complex inferences.

*   **Numeric Answers:**
    *   Many questions seek numeric answers, making them easily verifiable but also sensitive to arithmetic errors.
    *   Some questions require extracting specific numbers from the passage and then comparing them to determine the highest or lowest value.
    *   **Numerical Reasoning:** A significant number of questions involve numerical comparisons or calculations based on extracted data.

*   **Question Structure:**
    *   All questions follow the format: `"PASSAGE:\n[passage text]\n\nQUESTION: [question text]"`.
    *   Many questions involve numerical reasoning and comparison (e.g., "How many yards longer...", "Which star has a smaller mass...").
    *   Some questions ask for specific entities (e.g., "Who caught the final touchdown...", "Who threw the second longest touchdown pass?").
    *   Questions frequently ask for numerical information (e.g., "How many...?", "How many yards...?").
    *   The system does not appear to know how to resolve "who" questions.

*   **Answer Structure:**
    *   Answers are typically short phrases, numbers, or names.

*   **Passage Structure:**
    *   Passages vary in content (sports summaries, scientific descriptions like astronomy, political events).
    *   Sports passages often involve more complex event tracking, which poses extraction challenges.
    *   Passages are dense with numbers, requiring precise parsing, especially in sports narratives.

*   **Domain Knowledge:**
    *   Basic sports terminology (football positions, scoring) is needed for sports-related examples.
    *   General reading comprehension is crucial.
    *   For demographic examples, basic understanding of fertility rates is needed.

*   **Question Types:**
    *   Entity Extraction: Identifying specific players or entities involved in events (e.g., identifying the subject of a power seizure).
    *   Numerical Comparison: Comparing numerical values (yards, scores, TFR, mass) to determine differences.
    *   Counting: Counting the number of occurrences of an event or entity.

*   **Reasoning Types:**
    *   Direct Extraction: Finding the answer directly stated in the passage.
    *   Simple Arithmetic: Performing basic calculations (addition, subtraction) using numbers from the passage.
    *   Logical Deduction: Combining information from different parts of the passage to arrive at the answer.
    *   Multi-Sentence Reasoning: Correct answers often require synthesizing information from multiple sentences scattered throughout the passage, rather than being directly stated in a single sentence.

*   **Non-Obvious Patterns:**
    *   Chronological order of events is important. Time-related keywords (e.g., "final," "first," "later," "second") help narrow the search.

*   **Edge Cases/Complexities:**
    *   Passages with ambiguous or contradictory information.
    *   Questions requiring more complex arithmetic operations (e.g., multiplication, division).
    *   Questions with implicit rather than explicit answers, requiring deeper inference.
    *   Handling of units (yards, points, etc.) consistently.

## 2. EFFECTIVE TASK-SPECIFIC STRATEGIES

*   **Keyword-Based Retrieval:** Identify keywords from the question and search for those keywords in the passage.
*   **LLM-Based Extraction & Reasoning:** Use an LLM to directly answer the question based on the passage; prompting is key.
*   **Hybrid Approach:** Combine keyword retrieval with LLM reasoning.
*   **Question Decomposition:** Decompose the question into smaller sub-questions. Breaking down complex questions into smaller, more manageable extraction tasks improves accuracy. A reasonable starting point as many questions require multiple steps of reasoning.
*   **Chain-of-Thought Reasoning (CoT):** Breaking down complex questions into smaller steps (decomposition, information extraction, synthesis) improves interpretability. However, effectiveness is limited by the system's ability to accurately understand the question's intent and extract relevant numerical data.
*   **Zero-Shot Reasoning:** Directly ask the LLM to answer the question based on the passage, using a well-crafted prompt.
*   **Text Splitting:** If passages are too long, split them into smaller chunks. Be mindful of context loss.
*   **Focus on Answer Synthesis:** Prioritize improving the `synthesize_answer` function.
*   **Specialized Agents:** The use of specialized agents (question decomposer, information extraction expert, answer synthesis expert) allows for a modular approach. Individual role definitions for LLMs need definition.
*   **Sequential Workflow:** The sequential workflow (decompose, extract, synthesize) enforces a structured problem-solving approach.
*   **Question-Type Determination and Specialized Processing:** Determine the question type (numerical vs. general) and use specialized processing. Failure in question type determination renders the entire system useless.
*   **LLM-Driven Techniques with Verification:** Combining LLM-driven techniques with verification steps at each stage is a successful pattern. However, verification is inadequate for semantic errors that stem from misinterpreting the question. The verification step in `clarify_question` and `extract_information` functions, while present, is not consistently working as intended.
*   **Dynamic Approach Selection:** Adapting the strategy based on the type of question being asked proves important.
*   **Extraction of Relevant Information:** The system has a strength in extracting relevant information from the passage.
*   **Leveraging distinct agent roles:** Question decomposer, information extraction expert, answer synthesizer help specialize the tasks.
*   **Chain-of-thought for focusing information extraction:** Aids in focusing the information extraction process.
*   **Validating intermediate steps:** Decomposition and extraction likely contributes to the overall accuracy.
*   Providing examples in LLM prompts improves performance.
*   Explicitly prompting the LLM to show its work (intermediate calculations) helps in debugging.
*   **Self-Debate:** The self-debate strategy *attempts* to address potential errors. The agent tries to verify their answers. The agent recognizes the importance of calculation but isn't consistently accurate.
*   **Question Clarification & Focused Extraction:** The "Question Clarification & Focused Extraction" approach, while not perfect, demonstrates potential. Clarifying the question's intent before extraction *should* ideally improve precision.
*   **Decomposition:** Decomposing questions into smaller parts, especially for multi-step reasoning, has promise. This helps to isolate the reasoning steps required to reach the final answer.
*   **Information Extraction:** Using the LLM to pull relevant snippets of text appears to be generally functional, providing the data needed for the synthesis stage.
*   The high-level approach of chain-of-thought reasoning shows potential, but the implementation needs refinement, particularly in numerical reasoning and constraint handling. The approach might work better if combined with other techniques. Explicit examples in the LLM prompts likely helped to guide the model toward the desired format and reasoning steps.

## 3. COMMON FAILURE MODES ON THIS DATASET

*   **General Script Errors:**
    *   `NameError: name 'call_llm' is not defined`. This prevents any actual processing.
*   **Root Cause: Incorrect Question Type Determination:** The entire system fails because the `determine_question_type()` function consistently returns an error. This highlights that the question type determination module is a single point of failure.
*   **Temporal Reasoning Errors:** The system struggles with correctly ordering events and calculating time differences. For example, it incorrectly determined which event happened later in the Tibet passage during iteration 12. The primary failure mode involves incorrect calculation of the number of months between two dates mentioned in the passage. This indicates a deficiency in the system's temporal arithmetic capabilities.
*   **Answer Synthesis Failure:** Inability to synthesize a valid answer, even when the correct information is extracted. The system struggles to provide fully complete answers, especially when context, units, or modifiers from the original passage are needed. When the question requires identifying entities based on multiple criteria (e.g., "countries with more than one date"), the system struggles to filter out irrelevant entities. In the "countries on the 2014 Summer tour" question, the system incorrectly included "Germany" because it didn't properly filter based on the "more than one date" condition. This is a failure in the `synthesize_answer` stage, which should be applying the question's constraints to the extracted information.
*   **Sports narrative complexity:** Questions about sports narratives can induce errors because they require understanding of event order and accurate extraction of details. Passages are dense with numbers, requiring precise parsing, especially in sports narratives.
*   **Validation sensitivity:** Validation checks can be overly strict.
*   **Information Extraction Failure:** Errors in information extraction contribute to failures in answer synthesis. Inaccurate or incomplete extracted information leads to incorrect final answers. The agent struggles to correctly extract relevant information from the passages when multiple dates are present, leading to confusion in the arithmetic step.
    *   In some cases, the extraction mechanism pulls *additional* correct information when only *specific* information is requested, leading to incorrect answers.
    *   **Inaccurate Information Extraction & Arithmetic:** The system fails when questions require extracting specific data points from the passage (e.g., field goal lengths) and then performing arithmetic operations (e.g., subtraction) on them. For example, in the "Billy Cundiff vs. Sebastian Janikowski" question, the system incorrectly identified the field goal lengths, leading to a wrong calculation. The root cause is a failure in the `extract_information` stage to correctly identify the relevant numbers.
*   **Ambiguous Passages:** Passages with ambiguous or contradictory information can lead to incorrect answers.
*   **Missing Information:** If the answer cannot be found in the passage, this leads to a failure.
*   **Lack of Arithmetic Reasoning:** Inability to perform arithmetic operations, particularly simple addition.
*   **Potential for Masked Limitations:** High accuracy can mask the system's vulnerability to more complex reasoning.
*   **Numerical extraction errors:** Incorrectly identifying or extracting numerical values from the passage. e.g., in the question about Nate Kaeding's field goals, the system extracted "27" instead of calculating the difference between 51 and 24. This suggests a flaw in the extraction or calculation logic.
*   **Calculation errors:** Performing incorrect arithmetic operations on extracted numbers. Even when the correct numerical values are extracted, the system sometimes fails to perform the correct calculation or comparison.
*   **Synthesis errors:** Generating a response that is not a coherent or accurate answer to the question.
*   **Question type misclassification**: Incorrectly classifying a question as numerical or general.
*   **Unit Handling:** Lack of proper unit handling leads to incomplete or misinterpreted answers.
*   **Misinterpretation of Question Intent:** The system struggles to accurately interpret the nuances of the question.
*   **Incorrect Numerical Calculation:** The system makes errors in numerical calculations.
*   **Lack of Verification:** The system lacks a final answer verification step to check the reasonableness of the answer.
*   **Numerical Extraction in Dense Passages:** Difficulty in accurately extracting numerical values and associating them with the correct entities.
*   **Off-by-One Errors in Duration Calculation:** Consistently making off-by-one errors when calculating the duration between two dates or years.
    *   **Misinterpretation of Inclusion/Exclusion:** The agent sometimes struggles to correctly interpret whether the start and end dates/years should be included in the duration calculation.
*   **Incorrect Subject Identification:** The system fails to correctly identify the subject of an action. For example, in the question about King Tancred seizing power, the LLM incorrectly identifies "King William II of Sicily" instead of the correct answer, "Queen Joan." This indicates a weakness in parsing complex sentences and understanding the relationships between entities and actions. The model confuses the person who *preceded* the power seizure with the *victim* of the power seizure.
*   **Proximity Bias:** The LLM seems to exhibit proximity bias, selecting entities mentioned closer to the action verb, even if they are not the correct subject.
*   **Lack of Deep Understanding of Numerical Comparisons:** While the system can extract numbers, it appears to sometimes fail to associate the numbers with the correct context, leading to incorrect comparisons.
*   **Lack of "Who" Question Understanding:** The system does not appear to know how to resolve "who" questions.
*   **Validation may be too simplistic:** The validation step appears to be useful in preventing some errors, but doesn't necessarily solve the core reasoning issues and the final answer may still be wrong.
*   **Incorrect numerical extraction and calculation:** The system failed to correctly identify and extract the relevant numerical values for calculation and comparison (Example 2 in ITERATION 13). The system incorrectly formatted the answer.
*   **Misinterpretation of context:** The system misinterprets what the questions are asking (Example 1 in ITERATION 13), leading to an incorrect answer or a denial to answer the question at all.
*   **Incorrect extractions:** The system extracts the wrong entities from the passage leading to a wrong answer (Example 3 in ITERATION 13).
*   **Incorrect formatting:** The system incorrectly formatted the answer (added extra spaces in the answer in ITERATION 13). Make sure the extraction returns a number.
*   **Semantic Equivalence:** Errors occur when the system gives an answer that is not semantically equivalent to the reference answer, for example, "Novgorod" vs "Novgorodians".

## 4. EXPERIMENT LOG & FINDINGS

*   **2025-05-17 13:35:33: INITIAL DATASET ANALYSIS**
    *   Comprehensive analysis of dataset characteristics, challenges, potential approaches, and implementation recommendations.
    *   Highlighted potential solution strategies: Keyword-Based Retrieval, LLM-Based Extraction & Reasoning, Hybrid Approach.
    *   Outlined decomposition steps: Question Analysis, Passage Filtering, Information Extraction, Answer Generation.
    *   Proposed validation techniques: Consistency Checks, Unit Analysis, Numerical Validation, Fact Verification.
    *   Prompt engineering techniques for effective text processing were proposed, including chain-of-thought prompting.
*   **ITERATION 0:**
    *   **Accuracy:** 0.67
    *   **Goal:** Improve performance using question decomposition and reasoning.
    *   **Finding:** The implementation of question decomposition has challenges with reliable answer synthesis.
    *   **Finding:** Frequent validation failures indicate problems with the downstream steps, even if decomposition is successful.
    *   **Finding:** Sports-related questions involving temporal reasoning pose significant challenges.
    *   **Insight:** Validation is helpful for diagnosing system performance, specifically highlighting failures in answer synthesis.
*   **ITERATION 1:**
    *   **Accuracy:** 1.00
*   **ITERATION 2:**
    *   **Accuracy:** 1.00
    *   **Goal:** Improve performance using question-type determination and specialized processing.
    *   **Finding:** High accuracy suggests the strategy of question-type determination followed by specialized processing is highly effective.
    *   **Finding:** The successful implementation of the dynamic approach selection emphasizes the importance of adapting the strategy based on the type of question being asked.
    *   **Caveat:** The lack of error cases makes it impossible to determine the system's limitations definitively.
*   **ITERATION 3:**
    *   **Finding:** CoT is a beneficial approach for this type of question answering task.
    *   **Finding:** A significant limitation is the system's lack of arithmetic reasoning capabilities.
    *   **Finding:** The individual role definitions for the LLMs (question decomposer, information extractor, answer synthesizer) needs more definition.
*   **ITERATION 4:**
    *   **Accuracy:** 1.0
    *   **Goal:** Improve answer completeness, focusing on context, units, and modifiers.
    *   **Finding:** Achieving 1.0 accuracy suggests that the overall strategy of chain-of-thought, role-based agents, and validation is effective.
    *   **Finding:** The primary area for improvement is in the completeness and context of the final answer.
*   **ITERATION 5:**
    *   **Finding:** Chain-of-thought benefits are diminished by semantic errors in question interpretation.
    *   **Finding:** Verification steps are inadequate for addressing semantic errors.
*   **ITERATION 6:**
    *   **Accuracy:** 0.80
    *   **Goal:** Improve semantic understanding and numerical reasoning capabilities by leveraging chain-of-thought, prompt examples, and showing work.
    *   **Finding:** The chain-of-thought approach with question decomposition, information extraction, and answer synthesis showed promise.
    *   **Finding:** Providing examples in LLM prompts seems to improve performance.
    *   **Finding:** Explicitly prompting the LLM to show its work (intermediate calculations) helped in debugging.
    *   **Finding:** Inconsistent and unreliable arithmetic calculations are a major failure mode.
    *   **Finding:** The LLM sometimes fails to correctly interpret or apply units.
    *   **Finding:** The system lacks a final answer verification step to check the reasonableness of the answer.
    *   **Finding:** Arithmetic and unit errors indicate a need for more robust numerical reasoning capabilities.
*   **ITERATION 7:**
    *   **Accuracy:** 0.0
    *   **Goal:** Improve semantic understanding and numerical reasoning capabilities.
    *   **Finding:** This iteration provided no usable data due to the undefined `call_llm` function.
    *   **Finding:** The script is fundamentally broken without the `call_llm` function.
*   **ITERATION 8:**
    *   **Accuracy:** 0.0
    *   **Goal:** Improve semantic understanding and numerical reasoning capabilities.
    *   **Finding:** The `determine_question_type()` function consistently failed, leading to a complete system failure.
    *   **Finding:** The question type determination module is a single point of failure.
*   **ITERATION 9:**
    *   **Goal:** Improve handling of date/time reasoning, specifically duration calculations.
    *   **Finding:** The "Reading Comprehension Expert" agent with self-debate, in its current form, is not robust enough to prevent common arithmetic errors, especially off-by-one errors in duration calculation.
    *   **Finding:** The agent's general reasoning capabilities are insufficient for accurate duration calculations.
    *   **Finding:** Self-Debate Alone Is Insufficient: The agent identifies the potential for errors but fails to consistently avoid them.
*   **ITERATION 10:**
    *   **Accuracy:** 0.80
*   **2025-05-17 20:26:38: SCRIPT ERROR**
    *   Error detected during script repair (attempt 1): ERROR: Script requires external information to provide an answer.
*   **2025-05-17 20:26:49: SCRIPT ERROR**
    *   Error detected during script repair (attempt 2): ERROR: No passage provided.
*   **ITERATION 11:**
    *   **Accuracy:** 0.80
    *   **Goal:** Improve accuracy by clarifying the question's intent before information extraction.
    *   **Finding:** Clarifying the question isn't enough; the extraction phase needs to be more robust to identifying the correct entity related to a specific action.
    *   **Failure Example:** Incorrect subject identification. For example, in the question about King Tancred seizing power, the LLM incorrectly identifies "King William II of Sicily" instead of the correct answer, "Queen Joan." This indicates a weakness in parsing complex sentences and understanding the relationships between entities and actions. The model confuses the person who *preceded* the power seizure with the *victim* of the power seizure.
    *   **Finding:** The LLM seems to exhibit proximity bias, selecting entities mentioned closer to the action verb.
*   **ITERATION 12:**
    *   **Accuracy:** 0.60
    *   **Goal:** Improve performance with better handling of temporal and numerical reasoning by leveraging question decomposition and validation.
    *   **Finding:** Validation may be too simplistic and the final answer may still be wrong.
*   **ITERATION 13:**
    *   **Accuracy:** 0.70
    *   **Goal:** Improve performance with an exploitation strategy with chain-of-thought reasoning.
    *   **Finding:** The use of examples in LLM prompts alone is insufficient to overcome the challenges of numerical reasoning and constraint application.
*   **ITERATION 14:**
    *   **Accuracy:** 0.60
    *   **Goal:** Improve performance with an exploitation strategy with chain-of-thought reasoning.
    *   **Finding:** The chain-of-thought approach helps, but the individual components need to be more robust, especially in handling numerical data and conditional filtering.
    *   **Failure Examples:**
        *   "Billy Cundiff vs. Sebastian Janikowski" question: Incorrect identification of field goal lengths, leading to a wrong calculation due to failure in the `extract_information` stage.
        *   "Countries on the 2014 Summer tour" question: Incorrect inclusion of "Germany" due to failure to properly filter based on the "more than one date" condition in the `synthesize_answer` stage.
*   **ITERATION 15:**
    *   **Accuracy:** 0.70
    *   **Finding:** The reliance on chain-of-thought prompting is not sufficient to guarantee accurate numerical extraction and calculation.
    *   **Failure Examples:**
        *   "How many more yards was Nate Kaeding's second field goal over his first?": The system extracted "27" instead of calculating the difference between 51 and 24.
        *   Semantic equivalence: Errors occur when the system gives an answer that is not semantically equivalent to the reference answer, for example, "Novgorod" vs "Novgorodians".

## 5. NEXT RESEARCH DIRECTIONS

*   **Define or import `call_llm` immediately.**
*   **Critically, redesign/debug `determine_question_type()`.** Consider:
    *   **More examples:** Providing significantly more examples to the LLM prompt.
    *   **Simpler prompt:** Attempting a simpler prompt focused solely on question classification.
    *   **Fine-tuning:** Fine-tuning a smaller, specialized LLM on question type classification.
    *   **Direct few-shot classification:** Prompt the LLM to directly classify questions with a few examples without chain-of-thought.
*   **Implement Validation & Fallback:** Add validation logic to `determine_question_type()` with a fallback mechanism.
*   **Error Handling:** Implement better error handling throughout the system to provide more informative debugging messages.
*   **Analyze the results of the re-run. Focus on the accuracy of `determine_question_type`, `extract_numerical_info`, and `extract_information`.**
*   **If the above modules are problematic, refine the prompts used by `call_llm` within each of those functions, providing more examples specific to football game summaries and question types.**
*   **Implement a dedicated numerical reasoning module:** Integrate a calculator or numerical reasoning tool to perform arithmetic operations.
*   **Add Unit Verification:** Implement a module that checks the predicted unit against the expected unit based on the question.
*   **Implement Answer Sanity Checks:** Add a final step to evaluate the answer for sanity.
*   **Enhance training examples for numerical and unit understanding:** Fine-tune the LLM with more examples that focus on numerical reasoning and unit conversions.
*   **Test on edge cases:** Create specific test cases that target potential arithmetic errors, incorrect unit handling, and misunderstanding of specific terminology.
*   **Enhance Semantic Understanding:** Focus on improving the system's ability to understand the nuances of the questions.
*   **Improve Numerical Reasoning:** Implement more robust numerical reasoning techniques.
*   **Refine Verification Strategies:** Adapt the verification steps to specifically target potential semantic errors. Enhance Semantic Validation. Refine the validation call to the LLM to include a more robust assessment of the semantic equivalence between the system and golden answers.
*   **Enhance the answer synthesis stage:** Modify the prompt for the answer synthesis agent to explicitly request that it include all relevant context, units, and modifiers from the original passage.
*   **Improve unit handling:** Implement a more robust system for tracking and including units in numerical answers.
*   **Post-processing for completeness:** Explore post-processing steps to check for missing information (e.g., units, quantities).
*   **Arithmetic Reasoning Module:** Integrate a dedicated arithmetic reasoning module into the answer synthesis step.
*   **Refine Answer Synthesis Prompt:** Refine the prompt for the answer synthesis LLM to explicitly instruct it to perform calculations when necessary and provide the answer in the expected format.
*   **Include Examples of Arithmetic Reasoning in Prompts:** Add examples to the prompts for all LLM calls that explicitly show how to extract numbers and perform calculations.
*   **Evaluate Different LLMs for Numerical Reasoning:** Evaluate different LLMs that are known to have better numerical reasoning capabilities.
*   **Answer Synthesis Improvement:** Debug and improve the `synthesize_answer` function.
*   **Refine Validation Logic:** Review the validation checks to ensure they are not overly strict.
*   **Evaluate Information Extraction Success:** Assess how frequently information extraction fails.
*   **Dataset Split & Analysis:** Split the dataset by passage type.
*   **Prompt Engineering:** Experiment with different prompt formulations.
*   **Context Window Management:** Implement strategies for handling long passages.
*   **Error Analysis:** Manually analyze failed examples to identify the root cause of the errors.
*   **External Knowledge Integration:** Investigate the use of external knowledge sources to augment the information provided in the passage.
*   **Introduce Complexity:** Adapt the dataset to include questions that require more sophisticated reasoning.
*   **Implement Dedicated Date/Time Arithmetic Functions:** Replace the agent's general arithmetic with dedicated functions for calculating durations between dates.
*   **Formalize Inclusion/Exclusion Reasoning:** Explicitly represent the concept of inclusion/exclusion of start and end dates in the reasoning process.
*   **Introduce Unit Tests:** Create a suite of unit tests specifically focused on testing the date/time arithmetic functions.
*   **Verification specifically for temporal calculations:** Given the observed failure mode, implement a specific verification step focused on validating the temporal calculation.
    *   This could involve cross-checking the calculated difference with the passage or using an external calculator.
*   **Analyze successes:** Investigate the questions that were answered correctly to understand which aspects of the holistic approach are effective. Use this information to strengthen the successful components of the system.
*   **Refine Extraction Prompts:** Modify the `extract_information` prompt to explicitly instruct the LLM to focus on identifying the *subject* of the action described in the question, not just any related entity. Include examples of correct and incorrect subject identification.
*   **Add Subject-Verb-Object Triplet Identification:** Incorporate a step to identify subject-verb-object triplets in the passage, focusing on the verbs related to the question. This can help the model explicitly reason about the agent and target of the action.
*   **Improve Clarification of "Who" questions:** Explicitly instruct the LLM that "who" questions must be answered with the name of a person.
*   **Implement a "reasoning chain"**: Provide the model with space to include steps justifying its conclusion; this could improve the ability to verify the result.
*   **Add more error examples**: The error examples provided to the model are very scarce, so they are not helpful.
*   **Improve Temporal Reasoning:**
    *   Implement explicit logic for handling dates and time periods. This could involve converting dates to a standard format and using explicit comparison operators.
    *   Fine-tune the LLM on a dataset specifically designed to improve temporal reasoning skills, focusing on ordering events and calculating time differences.
*   **Enhance Numerical Understanding and Comparison:**
    *   When extracting numbers, ensure the context is also extracted to maintain the association between the number and its meaning.
    *   Implement specific comparison logic to verify that the numbers are compared in the correct context.
*   **Refine Validation:**
    *   Change the validation prompt so that it requests the LLM to perform the calculations itself, providing a way to verify the final result.
    *   Consider adding validation after the synthesis step to catch errors that occur during the final reasoning process.
*   **Address Quantifier Issues:** Modify the extraction agent to be more precise in adhering to the prompt.
*   **Enhance numerical reasoning capabilities:** Implement explicit numerical reasoning modules or prompts that guide the LLM to perform calculations accurately. Focus on extracting numbers with higher precision and avoiding rounding errors.
*   **Improve context understanding and constraint handling:** Refine the information extraction and answer synthesis steps to better understand the relationships between different pieces of information and apply constraints effectively. Try incorporating contextual clues and cues into prompts.
*   **Improve the answer formatting:** Avoid the addition of extra spaces in the answer. Make sure the extraction returns a number.
*   **Strengthen Information Extraction for Numerical Data:** Refine the `extract_information` function to be more precise in identifying and extracting relevant numerical values. This could involve using regular expressions or named entity recognition (NER) models specifically trained to identify numbers and units.
*   **Strengthen Calculation Logic:** Explicitly implement calculation logic within the `synthesize_answer` function to ensure that numerical values are processed correctly. This could involve creating separate functions for addition, subtraction, comparison, etc.
*   **Improve Conditional Filtering in Answer Synthesis:** Enhance the `synthesize_answer` function to better apply multiple conditions and constraints when filtering extracted information. Explore using a more structured representation of the conditions to ensure they are correctly applied.
*   **Explicitly train on examples involving Arithmetic Reasoning:** Fine-tune the LLM with examples that explicitly showcase how to extract information and perform arithmetic.
*   **Implement Unit Tests for Each Stage:** Develop unit tests for each stage of the process (decomposition, extraction, synthesis) to ensure individual components are functioning correctly. These tests should specifically target scenarios involving numerical data and conditional filtering.
```
        

            CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        

            EXPLORATION GUIDANCE:
            1. Review the historical approaches, error patterns, and accumulated learnings carefully
            2. Review the FULL CODE of previous scripts to understand what has already been tried
            3. Design a new approach that is DISTINCTLY DIFFERENT from previous attempts. This approach should have a specific NEW HYPOTHESIS or variable you are trying to test. 
            4. CRITICAL: Include EMBEDDED EXAMPLES directly within your LLM prompts
            5. For each key function, show a complete worked example, or include multiple examples, including:
               - Input example that resembles the dataset
               - Step-by-step reasoning through the example
               - Properly formatted output
            6. Apply the insights from the ACCUMULATED LEARNINGS section to avoid repeating past mistakes
            7. Pay SPECIAL ATTENTION to the weaknesses and improvement suggestions from the capability assessment
            8. Consider implementing one or more of these LLM usage patterns:
               - Repeated validation with feedback loops
               - Multi-perspective analysis with synthesis
               - Dynamic input-dependent routing with an orchestrator
               - Hybrid approaches combining LLM with deterministic functions
               - Best-of-n solution generation and selection
               - ReAct pattern for interactive reasoning and action
               - If it is unknown how successful a processing state or part of the pipeline is, include verification steps to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
               - Answer checkers to validate the final answer against the problem statement. If the answer is incorrect, the checker can send the answer back to an earlier part of the system for for refinement with feedback

            Here's how to call the Gemini API. Use this example without modification and don't invent configuration options:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

            Since this is an EXPLORATION phase:
            - Try a fundamentally different approach to reasoning about the problem. Test a NEW HYPOTHESIS or variable, and add verification steps to deduce if this new change is helpful.
            - THIS IS KEY: Break down the problem into new, distinct reasoning steps based on past performance before you start coding
            - For EACH key LLM prompt, include a relevant example with:
              * Sample input similar to the dataset
              * Expected reasoning steps
              * Desired output format
            - Apply a verifier call to different parts of the pipeline in order to understand what parts of the pipeline of calls is successful and where the system is breaking
            - Pay special attention to addressing the primary issues from previous iterations
            - Ensure your new approach addresses the weaknesses identified in the capability assessment

            CRITICAL REQUIREMENTS:
            1. The script MUST properly handle all string literals - be extremely careful with quotes and triple quotes
            2. The script MUST NOT exceed 150 lines of code to prevent truncation
            3. Include detailed comments explaining your reasoning approach
            4. EVERY SINGLE LLM PROMPT must include at least one embedded example showing:
               - Sample input with reasoning
               - Desired output format
            5. Make proper use of error handling
            6. Implement robust capabilities to address the specific weaknesses identified in the capability assessment
            7. Do NOT use json.loads() in the LLM calls to process input data. JSON formatting is good to use to structure information as inputs and outputs, but attempting to have functions process JSON data explicitly with strict built-in functionality is error prone due to formatting issues and additional text that appears as documentation, reasoning, or comments. When passing data into another LLM call, you can read it as plain text rather than trying to load it in strict json format, is the better approach.

            Return a COMPLETE, RUNNABLE Python script that:
            1. Has a main function that takes a question string as input and returns the answer string
            2. Makes multiple LLM calls for different reasoning steps
            3. Has proper error handling for API calls
            4. Includes embedded examples in EVERY LLM prompt
            5. Is COMPLETE - no missing code, no "..." placeholders
            6. Closes all string literals properly

            This should be FUNDAMENTALLY DIFFERENT from all previous approaches. Do not reuse the same overall structure.

            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            