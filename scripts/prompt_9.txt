
            You are creating a NEW Python script by SYNTHESIZING the best elements from multiple successful approaches.
            Your goal is to identify what makes each approach successful and combine these strengths into a superior hybrid solution.
    
            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "At what hospital did Communist politician Georgi Dimitrov die in 1949?",
    "answer": " Barvikha sanatorium "
  },
  {
    "id": 1,
    "question": "What was the name of the first elephant born in Valencia Bioparc?",
    "answer": "Makena"
  },
  {
    "id": 2,
    "question": "Who are Myrtle Eagan's parents in the show \"Severance\"?",
    "answer": "Kier and Imogene Eagan"
  }
]
    
            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 9
        - Current explore/exploit balance: 18/55
        - Best accuracy achieved: 0.67 (iteration 2)

        APPROACH HISTORY (last 9 iterations):
        [
  {
    "iteration": 0,
    "strategy": "baseline",
    "accuracy": 0.1,
    "approach": "Simple baseline script: Direct LLM call without sophisticated techniques"
  },
  {
    "iteration": 1,
    "strategy": "explore",
    "accuracy": 0.0,
    "approach": "This script answers questions using LLMs by decomposing the problem into entity/relationship extraction, search query generation, information retrieval, answer generation, and answer validation. Each step leverages the Gemini LLM with a specific system instruction to act as a specialized agent (e.g., information extractor, query generator). The script uses a few functions: `call_llm` for interacting with the Gemini model, `extract_entities_and_relationships` to identify key information, `generate_search_query` to create a search query, `retrieve_information` to simulate a search engine, `generate_answer` to formulate an answer, and `validate_answer` to check the answer's correctness. The overall workflow involves sequentially calling these functions, passing the output of one as input to the next, ultimately returning the validated answer or an error message."
  },
  {
    "iteration": 2,
    "strategy": "exploit",
    "accuracy": 0.6666666666666666,
    "approach": "The script employs a validation-driven problem-solving approach using the Gemini LLM, where a problem is first solved and then iteratively refined based on validation feedback. The problem is decomposed into solution generation and solution validation steps. There are two agent roles: a solver and a validator, each with specific system instructions. `call_llm` is used to interact with the LLM by passing prompts and instructions, while `solve_with_validation_loop` manages the iterative solving and validation process, and `main` is the entry point that calls `solve_with_validation_loop`. The overall workflow involves generating an initial solution with `call_llm` using the solver agent, validating it with `call_llm` using the validator agent, and refining the solution using `call_llm` with feedback until it's deemed valid or the maximum number of attempts is reached."
  },
  {
    "iteration": 3,
    "strategy": "explore",
    "accuracy": 0.3333333333333333,
    "approach": "The script uses LLM-based information retrieval and answer generation. It decomposes the problem into retrieving relevant context and then generating an answer using that context. The `retrieve_relevant_context` function acts as a search agent, generating search queries, simulating search, and validating the retrieved context for relevance. The `generate_answer_with_context` function acts as an expert at answering questions using the provided context.\n\nThe `call_llm` function is used to interact with the Gemini LLM, taking prompts and system instructions as input. The `retrieve_relevant_context` calls `call_llm` to generate search queries, simulate search, and validate context relevance; this function is then called by the `main` function, which then calls `generate_answer_with_context`, which then calls `call_llm` to create an answer using the retrieved context, finally returning it. The `main` function orchestrates the workflow by calling both functions sequentially."
  },
  {
    "iteration": 4,
    "strategy": "explore",
    "accuracy": 0.6666666666666666,
    "approach": "The script implements a retrieval-augmented generation (RAG) approach to answer questions using an LLM. It decomposes the problem into query generation, search snippet validation, and answer generation. The `generate_query_and_validate` function uses an LLM with the \"expert at generating effective search queries\" role to create a search query and then validates the query using another LLM call with the role \"expert at validating search snippets\" before proceeding; It uses a few-shot validation technique. The `generate_answer_with_snippets` function uses an LLM with the role \"expert at answering questions given relevant search snippets\" to formulate an answer based on the validated search snippets.\n\nThe overall workflow begins in `main` which calls `generate_query_and_validate` with a question, which in turn uses `call_llm` to generate a search query and validate the results. `main` then calls `generate_answer_with_snippets` with the original question and the validated search snippets, which in turn uses `call_llm` to generate the final answer. The `call_llm` function is used throughout the script to interface with the Gemini LLM for query generation, snippet validation, and answer generation."
  },
  {
    "iteration": 5,
    "strategy": "exploit",
    "accuracy": 0.0,
    "approach": "The script implements a RAG-based approach with a validation loop for answering questions. It decomposes the problem into query generation, search snippet retrieval (simulated), answer generation, and solution validation. The script uses two agent roles: a problem solver/answer generator and a validator, both driven by the `call_llm` function.\n\nKey functions:\n*   `call_llm`: Interacts with the Gemini model.\n*   `generate_query_and_validate`: Generates and validates a search query against search snippets to ensure relevance.\n*   `generate_answer_with_snippets`: Generates an answer based on the provided search snippets.\n*   `solve_with_validation_loop`: Orchestrates the RAG process, incorporating a validation loop to refine the answer.\n*   `main`: Calls `solve_with_validation_loop` to return an answer to the user's question\n\nThe workflow starts with `solve_with_validation_loop`, which calls `generate_query_and_validate` and `generate_answer_with_snippets` to get an initial answer. This answer is then iteratively validated, and if found invalid, the query and answer generation steps are rerun."
  },
  {
    "iteration": 6,
    "strategy": "exploit",
    "accuracy": 0.3333333333333333,
    "approach": "The script uses a combination of RAG and a validation loop to answer questions. It first generates a search query, retrieves and validates search snippets, and then generates an initial answer using these snippets. The answer is then iteratively validated and refined through a validation loop, with the LLM acting as a validator and a refiner, until a valid answer is found or the maximum attempts are reached. The functions used include `call_llm` for LLM interaction, `generate_query_and_validate` for RAG, `generate_answer_with_snippets` for answer generation, `solve_with_validation_loop` for the iterative process, and `main` to orchestrate everything."
  },
  {
    "iteration": 7,
    "strategy": "explore",
    "accuracy": 0.6666666666666666,
    "approach": "The script implements LLM-Guided Recursive Decomposition & Verification (LLM-RDRV) to answer complex questions. It decomposes the original question into sub-questions, answers each sub-question individually, verifies the answers, and synthesizes them into a final answer. This involves agent roles like question decomposer, answerer, and validator. The functions used are `call_llm`, `decompose_question`, `answer_sub_question`, `verify_answer`, `synthesize_answers`, and `main`. The `main` function orchestrates the process by calling `decompose_question` to break down the initial question, then iterates through the sub-questions, using `answer_sub_question` to find answers, and `verify_answer` to check the validity of each response before finally using `synthesize_answers` to give the final output."
  },
  {
    "iteration": 8,
    "strategy": "explore",
    "accuracy": 0.6666666666666666,
    "approach": "The script uses LLM-Guided Iterative Context Expansion & Focused Summarization (LLM-ICE-FS) to answer questions. It decomposes the problem into entity extraction, iterative context expansion, focused summarization, and answer verification. The agent roles include an entity extractor, information gatherer, summarizer, and validator, all implemented via prompting the LLM with specific system instructions.\n\nKey functions include: `extract_key_entities` (extracts entities from the question), `expand_context` (gathers information about entities), `summarize_context` (summarizes the context to answer the question), and `verify_answer` (verifies the answer). The overall workflow involves first extracting entities, then iteratively expanding the context around those entities, summarizing the context to generate an answer, and finally verifying the answer for accuracy."
  }
]

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 3 iterations):
        [
  {
    "iteration": 0,
    "issue": "The most critical problem is the system's reliance on an unreliable knowledge source which leads to the retrieval and provision of factually incorrect information. The lack of a verification mechanism exacerbates this issue, as the system blindly trusts the incorrect information."
  },
  {
    "iteration": 1,
    "issue": "The most critical problem is the **inaccurate and unreliable information extraction process, coupled with insufficient validation and error detection**. The system needs to be significantly improved in its ability to pinpoint the specific information required to answer the question correctly and verify that the retrieved information is accurate and relevant."
  },
  {
    "iteration": 2,
    "issue": "The primary issue is **inaccurate knowledge retrieval**. The system provides a definite answer that is factually incorrect, indicating a flaw in its information gathering or database. This highlights the need for improved source reliability and validation."
  },
  {
    "iteration": 3,
    "issue": "The primary issue is **inaccurate factual recall, particularly involving dates and specific names**. This leads to the system providing incorrect answers even when it understands the question's intent. The root cause could stem from flaws in the training data, the knowledge retrieval mechanism, or the final answer selection process."
  },
  {
    "iteration": 4,
    "issue": "The single most critical problem is the **inadequate retrieval of relevant information from search snippets**. The search queries and information extraction methods used by the system are not specific or robust enough to find the required details, leading to \"information not present\" answers even when the information is potentially available."
  },
  {
    "iteration": 5,
    "issue": "The primary issue is **inaccurate and/or incomplete information retrieval from the knowledge source.** This manifests as providing imprecise answers, failing to find existing answers, or providing incorrect specific details. The system's retrieval mechanism needs to be improved to ensure accuracy and completeness."
  },
  {
    "iteration": 6,
    "issue": "The system's inability to extract precise information, specifically dates and time periods, from the available data is the most critical problem. It relies too heavily on exact matches and cannot synthesize or infer answers when precise details are missing."
  },
  {
    "iteration": 7,
    "issue": "The primary issue is the system's inability to systematically and reliably process generated sub-questions and integrate the answers to derive the final response. The sub-question generation is effective, but the execution and synthesis steps are flawed."
  },
  {
    "iteration": 8,
    "issue": "The primary issue is the system's premature conclusion that information is unavailable coupled with a flawed validation process that confirms this incorrect conclusion. This leads to the system failing to find and provide correct answers that require more in-depth search or inference."
  }
]

        TARGETED IMPROVEMENTS:
        [
  "Confidence Scoring and Thresholding:** Develop a confidence scoring mechanism for potential answers and set a threshold for acceptance. Answers below the threshold should be rejected, and the system should indicate its uncertainty or seek clarification.",
  "Stage 3: Verification and Validation:** Verify the accuracy and consistency of the filtered information by cross-referencing with other sources, applying logical rules, or using external validation tools.",
  "Stage 1: Focused Retrieval:** Retrieve a narrow set of potentially relevant information based on keywords and entity recognition.",
  "Improved Ranking Algorithm:** Experiment with different ranking algorithms to prioritize answers based on factors such as source credibility, frequency of mention, and relevance to the question's context. For example, prioritize information extracted from reputable encyclopedias or biographical databases.",
  "Fact Verification Module:** Integrate a fact verification module that cross-references potential answers against multiple external sources to assess their accuracy and reliability.",
  "Stage 2: Semantic Filtering:** Filter the retrieved information based on semantic relevance to the question. Use techniques like sentence similarity or knowledge graph traversal to identify the most relevant pieces of information.",
  "Data Verification and Augmentation:** Implement automated and manual processes to verify and correct information in the knowledge base. Focus on fact-checking and updating numerical data (especially dates). Consider augmenting the knowledge base with more comprehensive biographical data, especially for prominent figures."
]
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity ✓
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity ✓
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity ✓
    4. All operations have O(1) average time complexity ✓
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        
    
            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        ```
# Dataset-Specific Experiment Log: Question Answering

This document serves as a continuously updated log of patterns, strategies, and findings related to the question-answering task for this specific dataset. It prioritizes concrete, task-specific insights over general principles.

## 1. DATASET PATTERNS & CHARACTERISTICS

*   **Question Type Dominance:** The dataset predominantly features *Who* questions, seeking individuals or groups associated with specific events, creations, or awards. Example: "Who created Groove Coaster?". There is also at least one example of a "What" question focusing on a specific part of a person's name. Example: "What was the first name of Ralph E. Oesper?".
*   **Answer Type:** Answers are typically short-form, factual names of people, groups, sometimes dates/numbers, or numerical facts. They represent precise details directly related to the question. Example: "How many losses...?" require a numerical answer.
*   **Knowledge Breadth:** Questions span a wide range of topics, requiring broad domain knowledge. Examples include music production (Groove Coaster), chemistry history (Ralph E. Oesper), oceanography (Jerlov Award), music band creation (Sho?), and TV series details ("El guardián invisible"). Questions also require knowledge of historical events or biographies.
*   **Question Specificity:** Varies from very precise to allowing some interpretation.
*   **Structure and Format:** Questions are natural language sentences. Answers are simple noun phrases or names. Each entry has an ID field (string).
*   **Reasoning Type:** Primarily fact retrieval. Answers are facts needing extraction from a knowledge source.
*   **Entity and Relationship Focus:** Questions often contain specific entities (person, place, organization) and relationships (purchased, announced). Examples include the need to identify relationships between entities across different time periods or classification schemes (e.g., genus changes, visit dates). This requires the system to track and correlate information from potentially disparate sources.
*   **Date Sensitivity:** Correctness is highly sensitive to dates; even slight variations are considered incorrect.
    *   *Example:* Needs to be able to distinguish between "October 20" vs. "21 of October".
    *   Questions often demand precise factual recall, including specific years ("In which year did..."), months ("In which month of 2005...").
*   **Precise Date & Identity Focus:** Many questions require pinpoint accuracy regarding dates (day, month, year) or specific individual identities (e.g., the name of a scientist who received a specific award). Partial or approximate answers are considered incorrect.
*   **Factual Recall, Not Reasoning:** The questions predominantly test factual recall of specific details rather than requiring complex reasoning or inference. The answers are likely directly stated in the (simulated) knowledge source, but finding the exact right snippet is crucial.
*   **Varied Temporal Scope:** The questions span a range of historical periods, meaning that the LLM and retrieval mechanisms need to handle diverse and potentially obscure historical information.
*   **Need for Completeness:** Answers must include all parts requested in the question (e.g., day, month, and year when asked for).
*   **Complex Relationships:** Questions frequently involve identifying relationships between entities across different time periods or classification schemes (e.g., genus changes, visit dates). This requires the system to track and correlate information from potentially disparate sources.
*   **Implicit Assumptions:** Some questions rely on implicit assumptions or background knowledge that the LLM might not possess. For example, understanding the taxonomic hierarchy to follow genus changes.
*   **Comparative Reasoning:** The dataset requires the system to compare and contrast information (e.g., "moved *to* from *Turdus* *before* finally being classified"). This necessitates accurate tracking of changes in classification over time.
*   **Specific Factual Recall:** The questions demand precise factual recall, often involving dates, names, and associations (e.g., "In which year was X awarded Y?").
*   **Entity Recognition & Disambiguation:** Questions involve named entities (people, organizations, awards) that require the LLM to correctly identify and disambiguate them. The failure highlights the importance of accurately mapping entities and their properties.
*   **Complex Relational Queries:** The questions often require understanding the relationship between multiple entities (e.g., person, achievement, organization). This goes beyond simple fact retrieval and requires the LLM to understand and reason about the relationships between different pieces of information.
*   **Compound Information:** Questions often contain multiple pieces of information (e.g., name, title, location), requiring the system to integrate and filter information accurately. *Example:* "In which month of 2005 was the Lal Bahadur Shastri Dam, also known as the Almatti Dam, located on the Krishna River in North Karnataka, India, completed?"
*   **Ambiguity in "First" or "Pioneer" Questions:** Questions asking about "first" or "pioneer" individuals or events may have multiple valid answers or lack a definitive answer, leading to discrepancies between the expected answer and the system's response.
*   **Temporal Specificity:** Many questions require extraction of specific dates or time periods. The questions are often phrased to directly ask "when" something occurred or "during which years" something happened. Questions often involve specific dates, years, or periods in history or academic careers (e.g., "...spend the year 1973-74...").  This necessitates the system to understand and reason about temporal relationships and be able to search/filter information accordingly.
*   **Entity-Rich Context:** The questions frequently include detailed contextual information about the entities involved (e.g., "Satyanarayan Gangaram Pitroda (an Indian telecommunication engineer and entrepreneur)"). This suggests a need for the model to filter relevant information based on these contextual clues.
*   **Fact Verification Challenge:** The questions target factual knowledge that may require precise lookup. This highlights the need for the model to differentiate between generally true statements and specific answers to the questions being asked, potentially requiring numerical reasoning.
*   **Specific Patch/Version Queries:** A significant portion of the questions requires pinpointing exact versions or patches for specific changes within games or software (e.g., "In what patch did..."). This demands precise information retrieval and accurate matching. The "Mechanical Glove" example demonstrates a failure to retrieve the specific patch number (1.2.3). This indicates a weakness in either the information retrieval component or the ability to synthesize information from retrieved text to extract the correct patch number. This might be due to the LLM not recognizing the specific format patch numbers typically take (e.g. 1.X.X).
*   **Complex Entities and Relationships:**  Questions frequently involve proper nouns (names of people, items, games, organizations), requiring the LLM to have a strong knowledge base and the ability to identify relationships between these entities. The Mehbooba Mufti question highlights this.
*   **Specific Fictional Worlds:** The dataset contains questions heavily reliant on knowledge of specific fictional worlds (e.g., "Severance"). Success hinges on having access to and correctly interpreting information from these very specific sources.
*   **Relationship Inference:** Some questions (e.g., parent-child relationships) require inference based on the provided context, rather than direct factual recall. The system needs to be able to identify relationships and infer answers, not just look for explicit statements.

## 2. EFFECTIVE TASK-SPECIFIC STRATEGIES

*   **NONE:** (Iteration 5 result) No aspect of the current strategy is working effectively for this dataset.
*   **Ineffective:** Direct LLM question answering without knowledge retrieval or verification (Baseline Experiment). Accuracy was only 10%.
*   **Ineffective:** Simple chain-of-thought approach using specialized LLM agents for each step (extraction, query generation, retrieval, answer generation, validation). Accuracy was 0%.
*   **Ineffective:** RAG implementation with current settings and the "explore" strategy is not effective (Iteration 4). RAG architecture with current validation loop (Iteration 5).
*   **Ineffective:** The validation loop alone, even with RAG, is insufficient for this dataset. While the validation loop might filter out some incorrect answers, it does not address the fundamental issue of the LLM's inability to extract specific temporal information when it's not explicitly stated in the retrieved snippets.
*   **Ineffective:** The RAG architecture, with its current validation loop, is failing to provide accurate answers for questions requiring precise factual recall from the simulated knowledge source. The validation loop isn't effective at correcting retrieval or generation errors. The `generate_query_and_validate` function isn't ensuring that the generated queries are precise enough to retrieve the exact information needed.
*   **Ineffective:** The accuracy of 0.67 in Iteration 7 indicates that simply decomposing the question and answering sub-questions is insufficient. The verification and, critically, the synthesis stages are hindering performance. The LLM can break down the problem, but can't reliably piece the answer back together. The hypothesis that recursive decomposition improves accuracy is not fully supported. The bottleneck seems to be in the later stages of verification and synthesis.
*    **Potentially Effective:** While overall accuracy is only 0.67, the LLM-ICE-FS (LLM Iterative Context Expansion with Focused Summarization) strategy from Iteration 8 does show promise in principle. Iterative expansion should improve performance.
*   **Potentially Effective:** The validation loop approach has promise, but it needs to be coupled with more robust information retrieval to be effective for this dataset. The idea of iterative refinement based on validation feedback is sound, but only if the initial information provided to the solver is accurate. However, iterations 2 and 5 revealed that the validation loop is failing to correct retrieval or generation errors, and the validator fails to detect answers that are partially correct, incorrect or missing.
*   **Potentially Effective:** The strategy of decomposing the question is likely useful considering the questions require multi-step reasoning.
*   **Untested:** Knowledge Base Retrieval (using LLM to formulate queries for external knowledge bases)
*   **Untested:** Hybrid Approach (LLM for query rephrasing and search results informing answer generation)
*   **Untested:** Entity and Relation Extraction before Knowledge Retrieval (though initial experiments highlight the need for *precise* extraction).
*   **Untested:** Structured Query Generation (e.g., SPARQL)
*   **Untested:** Few-Shot Learning, Chain-of-Thought Prompting, Answer Verification Prompting, Specialized "Who" question prompts.

## 3. COMMON FAILURE MODES ON THIS DATASET

*   **Factual Inaccuracy (Hallucination):** LLM provides incorrect facts not supported by evidence. Example: Incorrect details about "Barcelona corners" or the "Belmont purchaser" in the baseline experiment. This indicates the LLM is "hallucinating" facts. In Iteration 2, the LLM incorrectly stated "The Royal Society" instead of "American Academy of Arts and Science." This indicates the LLM's knowledge base or retrieval mechanism is unreliable for this specific information.
*   **Date Discrepancies:** Small differences in dates (e.g., "October 20" vs. "21 of October") are marked as incorrect, showing the need for high date precision.
*   **Incomplete Answers:** LLM fails to provide all parts of the answer requested by the question. Example: Providing only the month and year when the question asks for the day, month, and year.
*   **Ambiguity:** (Hypothesized from initial analysis) Some questions could be ambiguous if taken out of context.
*   **Multiple Valid Answers:** (Hypothesized from initial analysis) Some questions might have multiple correct answers, or answers that vary in specificity.
*   **Name Variations:** (Hypothesized from initial analysis) People's names can be written in different ways (e.g., "Robert" vs. "Bob").
*   **Cultural Differences:** (Hypothesized from initial analysis) Name formats differ across cultures. The dataset might contain names from various regions.
*   **Misspellings:** (Hypothesized from initial analysis) Questions might contain misspellings of names or terms, which could make retrieval difficult.
*   **Incorrect Entity/Relationship Extraction:** The system fails to accurately extract the precise entities and relationships needed to answer the question. For example, the system extracted "American University" which was not present in the gold answer, and was not relevant in Iteration 1. In Iteration 5, the system incorrectly provided "Robert P. Sharp" instead of "Carl Owen Dunbar".
*   **Inability to Discern Temporal Order:** The system struggles to correctly identify the sequence of events or changes over time. This is evident in the ruby-throated bulbul question, where the system failed to establish the order of genus classifications. The last visit to Europe question also shows this.
*   **Inaccurate Information Retrieval & Validation:** The system struggles to validate if information retrieved is in fact valid. This is demonstrated across all samples in Iteration 1, Iteration 3, Iteration 4, and Iteration 5.
*   **Lack of Reliable Source Attribution:** Since the responses aren't grounded in verifiable sources, it is difficult to determine the source of the error. This makes debugging and improving the system challenging.
*   **Incorrect Year Retrieval:** The system incorrectly retrieves the year in several questions, indicating a failure in the information retrieval or context validation steps. *Example:* For the question about Maharaj Kishan Bhan in Iteration 3, the system returned "1999" instead of "2013". This suggests a problem in filtering or prioritizing information within the retrieved context.
*   **Handling Ambiguity in "First" Questions:** The system struggled with questions seeking the "first" of something in Iteration 3. Instead of providing the exact expected answer ("Lydia Canaan"), it offered multiple possibilities or disclaimers about the lack of a single definitive answer. This indicates a failure in resolving ambiguity and selecting the most appropriate answer based on the context.
*   **Context Validation Inadequacy**: In Iteration 3, the LLM struggles to validate the context against the question and extracts the incorrect named entity, date, or other fact from the search results. The system reports "information not present" even when the information might be obtainable, pointing to overly conservative validation or poor snippet content comprehension (Iteration 4). In Iteration 5, the validator fails to detect answers that are partially correct, incorrect or missing.
*   **Script Errors:** Script errors encountered during attempted repairs, indicating fragility of the codebase and the need for more robust error handling. Examples:
    *   `ERROR: Answer not found in context`
    *   `ERROR: Gemini API call failed with AttributeError: 'module' object has no attribute 'GenerativeModel'`
    *   `ERROR: Gemini API key error and search snippets deemed irrelevant.`
    *   `ERROR: Could not find the answer.`
*   **Failure to Extract Numerical Answers:** The RAG approach fails when questions require numerical information. The system fails to extract the numerical answer, even when the relevant context may be available within the retrieved snippets. Example: For the question about Adolf Anderssen's losses, the system *might* have retrieved snippets discussing the match but failed to identify and extract the specific number "3". The validation step then incorrectly deemed the snippets unhelpful (Iteration 4).
*   **Granularity Mismatch in Date Retrieval:** (Iteration 5) The system fails when a question demands a precise date (e.g., "November 30, 1949") but the retrieved information provides a broader range (e.g., "1949 to February 21, 1974"). The retrieval process is not filtering with sufficient precision to isolate the exact date.
*   **"Answer Not Found" Errors for Existing Answers:** (Iteration 5) The system incorrectly reports "Answer Not found" even when the correct answer exists in the knowledge source. This suggests issues with query formulation, search strategy, or the ability to match question intent with relevant document content.
*   **Query Validation Ineffective:** The `generate_query_and_validate` function isn't ensuring that the generated queries are precise enough to retrieve the exact information needed. This is a critical bottleneck in the current approach. (Iteration 5)
*   **Date Extraction Bottleneck:** The system fails when it cannot find the exact date or time period mentioned in the golden answer within the retrieved snippets. The first error illustrates this clearly where the system returns "Answer not found in snippets," missing the "October 2009" answer. It seems unable to infer or synthesize temporal information.
*   **Insufficient Temporal Reasoning:** The system acknowledges Otto Schluter was a professor but cannot extract the date range (1911-1959). It appears the system lacks the ability to extract start and end dates and perform basic subtraction to derive a duration, it needs the exact answer in the snippets.
*   **Inability to Retrieve Patch Numbers:** The "Mechanical Glove" example demonstrates a failure to retrieve the specific patch number (1.2.3). This indicates a weakness in either the information retrieval component or the ability to synthesize information from retrieved text to extract the correct patch number. This might be due to the LLM not recognizing the specific format patch numbers typically take (e.g. 1.X.X).
*   **Answer Synthesis Breakdown:** The core issue is the failure to integrate the sub-question answers effectively. The system can decompose the question, but struggles to synthesize a coherent and accurate final answer from the individual parts. This means the `synthesize_answers` function is a critical point of failure. The error examples suggest a disconnect between the decomposed sub-questions and the expected final answer.
*   **Premature "Not Revealed" Conclusion:** The system incorrectly concludes that information is unavailable too quickly, especially when questions require inference or dealing with niche fictional settings. For instance, in the "Severance" example, the system gave up too soon instead of finding sources detailing the family relationships within the show. This is coupled with a flawed validation process that confirms this incorrect conclusion.
*   **Insufficient Contextual Depth:** The system likely doesn't expand the context deeply enough to uncover the relationships required for inference. The number of expansion iterations or the search scope might be too limited. It is therefore not capturing the nuanced relationships.
*   **Reliance on Negative Constraints:** The prompt relies on negative constraints in the validation step. These have been seen to often lead to false negatives.

## 4. EXPERIMENT LOG & FINDINGS

*   **Iteration 0 (Baseline):**
    *   **Approach:** Direct LLM call with a basic prompt.
    *   **Runtime:** Not explicitly recorded, but assumed to be fast due to direct prompting.
    *   **Accuracy:** 10%.
    *   **Key Findings:** Direct LLM prompting is insufficient. Requires knowledge retrieval and verification.
    *   **Error Analysis:** Factual inaccuracies and date discrepancies were the primary error types.

*   **Iteration 1:**
    *   **Approach:** Chain-of-thought approach using specialized LLM agents for each step (extraction, query generation, retrieval, answer generation, validation).
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** 0%.
    *   **Key Findings:** A simple chain-of-thought approach is not sufficient. Inaccuracies at the extraction stage propagate through the entire pipeline. The LLM struggles with understanding temporal information, tracking relationships across multiple sources, and performing accurate information validation.
    *   **Error Analysis:** Incorrect entity/relationship extraction, inability to discern temporal order, and inaccurate information retrieval and validation.

*   **Iteration 2:**
    *   **Approach:** Validation loop with specialized LLM agents. (Details of the individual components as in Iteration 1 were not re-stated but assumed).
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** Not explicitly recorded, but noted that validation alone was insufficient.
    *   **Key Findings:** The validation loop did not catch the factual inaccuracy in the example. This suggests the validator agent needs to be more rigorous in verifying factual claims, and needs access to reliable information to do so.
    *   **Error Analysis:** Incorrect Factual Recall. Validation alone is insufficient. Need for External Knowledge Injection. Lack of Reliable Source Attribution.

*   **Iteration 3:**
    *   **Approach:** LLM-based information retrieval and answer generation (details of the specific implementation not provided).
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** 0.33%.
    *   **Key Findings:** The current LLM-based information retrieval and answer generation approach, while conceptually sound, does not achieve satisfactory accuracy on this dataset. Contextual Understanding is Not Enough. The approach can be adapted by changing the prompt template, finetuning the LLM, or by giving it a validation step after information retrieval and before answer generation. More data alone may not be the solution. Prompt engineering or other improvements may be more beneficial.
    *   **Error Analysis:** Incorrect Year Retrieval. Handling Ambiguity in "First" Questions. Context Validation Inadequacy.

*   **Iteration 4:**
    *   **Approach:** RAG implementation (details not fully specified, but presumed to involve retrieval of context and generation of answer). "Explore" strategy used.
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** 0.67%.
    *   **Key Findings:** The "explore" strategy with the current RAG implementation is not effective for this dataset. The system struggles with questions requiring numerical information, and the validation step may be overly conservative, discarding useful information. The hypothesis that an LLM can effectively validate search snippets based on few-shot examples is not supported by these results.
    *   **Error Analysis:** Failure to extract numerical answers. Overly conservative snippet validation.

*   **Iteration 5:**
    *   **Approach:** RAG architecture with a validation loop (details not fully specified).
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** 0.00%.
    *   **Key Findings:** The RAG architecture, with its current validation loop, is failing to provide accurate answers for questions requiring precise factual recall from the simulated knowledge source. The validation loop isn't effective at correcting retrieval or generation errors. The `generate_query_and_validate` function isn't ensuring that the generated queries are precise enough to retrieve the exact information needed. This is a critical bottleneck in the current approach.
    *   **Error Analysis:** Granularity Mismatch in Date Retrieval. "Answer Not Found" Errors for Existing Answers. Incorrect Entity Resolution. Query Validation Ineffective. Validation Fails to Catch Inaccuracies.

*   **Iteration 6:**
    *   **Approach:** RAG architecture with validation loop (details not fully specified).
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** Not explicitly recorded, due to identified errors.
    *   **Key Findings:** The validation loop alone, even with RAG, is insufficient for this dataset. While the validation loop might filter out some incorrect answers, it does not address the fundamental issue of the LLM's inability to extract specific temporal information when it's not explicitly stated in the retrieved snippets. The hypothesis that iterative validation and refinement would overcome the limitations of initial answer generation is rejected. The system gets stuck because the initial answer (or retrieved snippets) doesn't contain enough precise information.
    *   **Error Analysis:** Date Extraction Bottleneck. Insufficient Temporal Reasoning.

*   **Iteration 7:**
    *   **Approach:** Decomposition Strategy (Details not fully specified, but assumed to decompose the original question into sub-questions, answer each, and then synthesize the answers).
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** 0.67%.
    *   **Key Findings:** The accuracy of 0.67 indicates that simply decomposing the question and answering sub-questions is insufficient. The verification and, critically, the synthesis stages are hindering performance. The LLM can break down the problem, but can't reliably piece the answer back together.
    *   **Error Analysis:** Inability to Retrieve Patch Numbers. Answer Synthesis Breakdown. The core issue is the failure to integrate the sub-question answers effectively. The system can decompose the question, but struggles to synthesize a coherent and accurate final answer from the individual parts. This means the `synthesize_answers` function is a critical point of failure. The error examples suggest a disconnect between the decomposed sub-questions and the expected final answer.
    *   **Script Errors:** `Error detected during script repair (attempt 1): ERROR: Could not find the answer.`

*   **Iteration 8:**
    *   **Approach:** LLM Iterative Context Expansion with Focused Summarization (LLM-ICE-FS). The hypothesis was that iterative context expansion and focused summarization can improve accuracy.
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** 0.67%.
    *   **Key Findings:** While overall accuracy is only 0.67, the LLM-ICE-FS strategy does show promise in principle. However, the current implementation struggles with questions needing in-depth knowledge or inference. The premature conclusion failure mode suggests that the current implementation over-prioritizes avoiding hallucination at the expense of recall, at least in this domain.
    *   **Error Analysis:** Premature "Not Revealed" Conclusion. Insufficient Contextual Depth. Relies on negative constraints in the validation step, leading to false negatives.
    *   **Specific Failure Example:** For the "Severance" example, the system gave up too soon instead of finding sources detailing the family relationships within the show. This is coupled with a flawed validation process that confirms this incorrect conclusion.

## 5. NEXT RESEARCH DIRECTIONS

*   **Refine Sub-Question Synthesis:** Focus on improving the `synthesize_answers` function. Implement strategies to ensure the individual answers to sub-questions are integrated logically and accurately. This may involve providing the LLM with explicit instructions on how to combine the answers, or developing a more structured approach to answer synthesis.
*   **Improve Patch Number Retrieval:** Enhance the system's ability to identify and extract patch numbers. This could involve:
    *   Training the LLM on examples of patch number formats.
    *   Using regular expressions to identify potential patch numbers in retrieved text.
*   **Implement Intermediate Reasoning Checks:** Add checks after the `answer_sub_question` step to ensure the answer is of the correct format and type. This could preemptively identify issues before the synthesis stage.
*   **Test Different Decomposition Strategies:** Experiment with different approaches to decomposing the original question. The current decomposition may not be optimal for all question types.
*   **Enhance Temporal Reasoning:** Implement a module specifically designed to extract and reason about dates and time periods. This could involve:
    *   Training a separate model to identify date entities and duration from text.
    *   Using regular expressions or other pattern matching techniques to extract date information from retrieved snippets.
    *   Incorporating a date normalization library to handle different date formats.
*   **Implement Date Inference:** Extend the system to infer dates or date ranges from contextual clues. For example, if a snippet mentions an event happening "in the early 20th century," the system could be trained to identify the potential range of years associated with that phrase.
*   **Fine-tune LLM on Temporal Tasks:** Fine-tune the base LLM on a dataset of question-answer pairs where the answers involve specific dates or time periods. This could improve the LLM's ability to extract and reason about temporal information.
*   **Query Expansion for Temporal Information:** Augment the search query to explicitly request temporal information. For example, instead of just searching for "Otto Schluter professor University of Halle," the query could be expanded to "Otto Schluter professor University of Halle years" or "Otto Schluter University of Halle professorship duration."
*   **Improve Query Formulation for Precision:** Focus on refining the query generation process to create more specific and targeted queries, especially when questions involve dates or named entities. Consider adding constraints to the query generation process to explicitly request the exact date/name.
*   **Enhance Retrieval Granularity:** Implement techniques to improve the granularity of the retrieval process. Explore methods for ranking search snippets based on the precision of date or entity matches. Experiment with chunking strategies that isolate key facts.
*   **Strengthen Entity Resolution:** Integrate entity linking or named entity recognition (NER) techniques to improve the system's ability to identify and disambiguate entities within the questions and the retrieved documents.
*   **Refine Validation Logic:** Re-evaluate the validation criteria and implementation. The validator needs to be more sensitive to partial matches, contradictions, and missing information. Explore using a separate, more robust LLM for validation. Remove reliance on negative constraints.
*   **Dataset Augmentation for Negative Examples:** Augment the dataset with negative examples, specifically questions paired with irrelevant or misleading snippets, to train the validation component to better identify incorrect answers.
*   **Improve Search Query Specificity:** Refine the prompt for query generation to emphasize the need for queries that specifically target numerical answers or quantifiable facts. For example, add phrases like "return the number of..." or "how many..." to the query generation prompt.
*   **Enhance Information Extraction:** Implement a more robust information extraction mechanism, potentially using regular expressions or specialized NER models, to identify and extract numerical answers from the validated snippets. Instead of relying solely on the LLM for answer generation, focus on extracting key facts.
*   **Refine Snippet Validation:** Loosen the validation criteria or explore alternative validation strategies. The current approach is likely discarding snippets that contain the necessary information but don't perfectly align with the expected format. Consider adding a scoring mechanism to evaluate snippets based on relevance instead of strict acceptance/rejection. Specifically, explore if retrieving more snippets will help, then re-rank them.
*   **Implement Numerical Reasoning Checks:** After extracting a numerical answer, add a simple check to ensure it makes sense in the context of the question (e.g., is it a reasonable number of losses in a chess match?). This could prevent nonsensical or hallucinated answers.
*   **Implement Knowledge Retrieval:** Integrate a search engine or knowledge base to retrieve supporting information *before* answer generation. This is crucial for addressing factual inaccuracies.
*   **Implement Answer Verification:** Verify the LLM's answer against a reliable external source and correct it if discrepancies are found. This should reduce hallucinations.
*   **Date Normalization:** Standardize date formats in questions and retrieved information to enable accurate comparison.
*   **Prompt Engineering for Completeness:** Revise the prompt to ensure the model provides all parts of the answer or responds with an appropriate error message (e.g., "Unable to determine the day."). Example prompt update: "Question: {question}. Answer: If all requested parts of the answer (day, month, year) are known, provide them all. Otherwise, state 'Insufficient Information'."
*   **Explore Hybrid Approach:** Use LLM for query rephrasing to improve search engine results, and then use those results to generate an answer.
*   **Enhance Entity and Relationship Extraction:** Implement more robust methods for entity and relationship extraction, potentially using named entity recognition (NER) models finetuned on similar datasets. Focus on precise extraction of the specific entities and relationships relevant to the question.
*   **Incorporate Temporal Reasoning:** Add a dedicated temporal reasoning module to track changes over time. This could involve using specialized data structures or algorithms to represent temporal relationships and perform reasoning about event sequences.
*   **Improve Answer Validation:** Implement more rigorous answer validation techniques, such as cross-referencing information from multiple sources and using a separate validation model to assess the answer's correctness. Implement a system that assesses the quality of the extracted entities *before* query generation.
*   **Iterative Refinement with Feedback:** Create a feedback loop where incorrect answers are analyzed to identify the specific errors made by each component in the pipeline. Use this feedback to iteratively refine the system and improve its performance.
*   **Test Structured Query Generation:** Convert the natural language question into a structured query (e.g., SPARQL) to retrieve information from knowledge graphs like Wikidata.
*   **Investigate Few-Shot Learning:** Provide the LLM with examples of question-answer pairs to improve performance.
*   **Evaluate Chain-of-Thought Prompting:** Prompt the LLM to explain its reasoning process step-by-step (though a simple version of this failed).
*   **Develop Answer Verification Prompting:** Use a separate prompt to ask the LLM to verify the answer's accuracy.
*   **Create Specialized "Who" Question Prompts:** Given the dominance of "Who" questions, design highly optimized prompts.
*   **Track Latency:** Measure and log the runtime (latency) of each experiment, to understand the performance impact of different strategies.
*   **Analyze Failure Cases:** Perform detailed analysis of failure cases to identify patterns and refine strategies.
*   **Implement Fact-Checking Mechanism:** Integrate a mechanism to fact-check the LLM's answers against a reliable knowledge base (e.g., Wikipedia, a curated database). The validator agent should use this to verify the LLM's claims.
*   **Implement Source Tracking:** Modify the `call_llm` function to include source tracking. This will allow the system to track where the information comes from and identify potential sources of error.
*   **RAG Implementation**: Explore Retrieval-Augmented Generation (RAG) to provide the LLM with relevant context from external sources during the solution generation phase. This will help to ground the LLM's answers in verifiable evidence. This should be implemented as part of the `call_llm` function.
*   **Improve Context Validation:** Implement a more robust context validation mechanism in the `retrieve_relevant_context` function. This could involve techniques like verifying the presence of key entities (names, dates) and assessing the overall relevance of the retrieved context to the specific question.
*   **Fine-tune Answer Selection Logic:** Refine the answer selection logic in the `generate_answer_with_context` function to prioritize precise factual matches and resolve ambiguity in "first" or "pioneer" questions. Explore techniques like confidence scoring or rule-based filtering to select the most appropriate answer.
*   **Prompt Engineering for Date Retrieval:** Optimize the prompts used for generating search queries and validating context to emphasize the importance of accurate date retrieval. Include explicit instructions to prioritize sources that provide specific dates.
*   **Prompt Engineering for validation:** Give the LLM the ability to "check its work" by validating its initial answer against the original question and the retrieved context. For example, "Does this answer the question completely? Are all the details consistent with the provided context?"
*   **Adjust Confidence Thresholds:** Tune the thresholds for concluding that information is unavailable. Make the system more persistent in searching for answers, particularly for questions about fictional worlds.
*   **Improve Context Expansion Depth/Breadth:** Increase the number of iterations in the `expand_context` function or broaden the search queries to explore more potential sources.
*   **Inference-Focused Summarization:** Modify the `summarize_context` prompt to explicitly instruct the LLM to identify and infer relationships between entities, not just summarize facts.
*   **Fictional World Specialization:** Consider a branch of the system specifically designed to handle questions about fictional works. This could involve using specialized knowledge bases or training the LLM on relevant scripts and summaries.
```
        
    
            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        
    
            MULTIPLE TOP PERFORMING APPROACHES TO SYNTHESIZE:
            
=== TOP PERFORMING APPROACH #1 ===
Iteration: 2
Accuracy: 0.67
Approach Summary: The script employs a validation-driven problem-solving approach using the Gemini LLM, where a problem is first solved and then iteratively refined based on validation feedback. The problem is decomposed into solution generation and solution validation steps. There are two agent roles: a solver and a validator, each with specific system instructions. `call_llm` is used to interact with the LLM by passing prompts and instructions, while `solve_with_validation_loop` manages the iterative solving and validation process, and `main` is the entry point that calls `solve_with_validation_loop`. The overall workflow involves generating an initial solution with `call_llm` using the solver agent, validating it with `call_llm` using the validator agent, and refining the solution using `call_llm` with feedback until it's deemed valid or the maximum number of attempts is reached.

FULL SCRIPT CODE:
```python
import os
import re
import json
import math # for react
from google import genai
from google.genai import types

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop.
    This is based on successful patterns from previous iterations, particularly in Iteration 0,
    but enhanced with iterative refinement for better accuracy."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions. Focus on factual accuracy and completeness."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements, ensuring factual correctness and completeness."

    # Initial solution generation - Enhanced with multi-example prompting
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements. Focus on factually accurate and complete answers.

    Example 1:
    Problem: What is the name of the university where Ana Figueroa, a political activist and government official, studies and graduates from?
    Solution: University of Chile

    Example 2:
    Problem: Which genus was the ruby-throated bulbul moved to from *Turdus* before finally being classified in the genus *Rubigula*?
    Solution: Genus Pycnonotus

    Example 3:
    Problem: In what year did Etta Cone last visit Europe?
    Solution: 1938

    Problem:
    {problem}
    """

    solution = call_llm(solution_prompt, system_instruction_solver)

    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution - Enhanced with specific validation examples
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem. Ensure factual correctness and completeness.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues, including specific factual errors or omissions]".

        Example 1:
        Problem: What is the capital of France?
        Solution: Paris
        Validation: VALID: The capital of France is indeed Paris.

        Example 2:
        Problem: Who painted the Mona Lisa?
        Solution: Leonardo DaVinci
        Validation: VALID: The Mona Lisa was painted by Leonardo da Vinci.

        Example 3:
        Problem: What year did World War II begin?
        Solution: 1940
        Validation: INVALID: World War II began in 1939, not 1940.

        Problem:
        {problem}

        Proposed Solution:
        {solution}
        """

        validation_result = call_llm(validation_prompt, system_instruction_validator)

        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution

        # If invalid, refine the solution - Provides multi-example based feedback to ensure robust refinement
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed. Ensure that you only use information from the original problem in your response, and ensure that the response is factually correct and as complete as possible.

        Problem:
        {problem}

        Your previous solution:
        {solution}

        Validation feedback:
        {validation_result}

        Example of a corrected solution based on validation feedback:

        Problem: When did the Titanic sink?
        Your previous solution: April 1912
        Validation Feedback: INVALID: The Titanic sank on April 15, 1912, include the day.

        Corrected Solution: April 15, 1912

        Please provide a completely revised solution that addresses all the issues mentioned. Be as factual as possible. Do not attempt to create new information that is not present in the original response.
        """

        solution = call_llm(refined_prompt, system_instruction_solver)

    return solution

def main(question):
    """
    Main function that orchestrates the solution process using solve_with_validation_loop.
    This function now incorporates the iterative validation loop for enhanced accuracy.
    This is a hybrid approach combining elements from Iteration 0 (direct LLM call) with the idea
    of iterative refinement from later iterations.
    """
    answer = solve_with_validation_loop(question)
    return answer
```

=== TOP PERFORMING APPROACH #2 ===
Iteration: 4
Accuracy: 0.67
Approach Summary: The script implements a retrieval-augmented generation (RAG) approach to answer questions using an LLM. It decomposes the problem into query generation, search snippet validation, and answer generation. The `generate_query_and_validate` function uses an LLM with the "expert at generating effective search queries" role to create a search query and then validates the query using another LLM call with the role "expert at validating search snippets" before proceeding; It uses a few-shot validation technique. The `generate_answer_with_snippets` function uses an LLM with the role "expert at answering questions given relevant search snippets" to formulate an answer based on the validated search snippets.

The overall workflow begins in `main` which calls `generate_query_and_validate` with a question, which in turn uses `call_llm` to generate a search query and validate the results. `main` then calls `generate_answer_with_snippets` with the original question and the validated search snippets, which in turn uses `call_llm` to generate the final answer. The `call_llm` function is used throughout the script to interface with the Gemini LLM for query generation, snippet validation, and answer generation.

FULL SCRIPT CODE:
```python
import os
import re
import math # for react
from google import genai
from google.genai import types

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def generate_query_and_validate(question, max_attempts=3):
    """
    Generates a search query from a question and validates its effectiveness by assessing
    if the top search snippets contain key entities and relationships needed to answer the question.
    Returns both the generated query and top search snippets.
    """
    system_instruction_query_gen = "You are an expert at generating effective search queries that help answer questions."
    system_instruction_search_validator = "You are an expert at validating whether a set of search snippets are relevant to answering the question"
    # Hypothesis: By generating and validating the query BEFORE retrieving the information, we can significantly improve the information retrieval and hallucination problems that are causing the pipeline to fail
    for attempt in range(max_attempts):
        # Step 1: Generate Search Query with Examples
        query_prompt = f"""
        Generate a search query to retrieve information needed to answer the question.

        Example 1:
        Question: What was the first name of Ralph E. Oesper?
        Search Query: Ralph E. Oesper first name

        Example 2:
        Question: In which year did Maharaj Kishan Bhan receive the Padma Bhushan for civil services?
        Search Query: Maharaj Kishan Bhan Padma Bhushan year

        Question: {question}
        Search Query:
        """
        search_query = call_llm(query_prompt, system_instruction_query_gen)
        # Step 2: Simulate Retrieving Top Search Snippets - IMPORTANT: IN A REAL SYSTEM THIS WOULD BE SEARCH API
        search_snippets = call_llm(f"Provide top 3 search snippets for: {search_query}", "You are a helpful search engine providing realistic search results.")

        # Step 3: Validate Relevance of Search Snippets with Examples
        validation_prompt = f"""
        Determine if the following search snippets are relevant to answering the question. If they are, respond with "RELEVANT: [brief explanation]". If not, respond with "IRRELEVANT: [detailed explanation]".

        Example 1:
        Question: What was the first name of Ralph E. Oesper?
        Search Snippets: Ralph Oesper was a professor...; His middle name was E...; There is no information on his first name.
        Validation: IRRELEVANT: The snippets don't reveal his first name.

        Example 2:
        Question: In which year did Maharaj Kishan Bhan receive the Padma Bhushan for civil services?
        Search Snippets: Maharaj Kishan Bhan received the Padma Bhushan in 2013; He was a scientist; He worked in civil services.
        Validation: RELEVANT: Snippets contain MKB and the year he received the award

        Question: {question}
        Search Snippets: {search_snippets}
        Validation:
        """
        validation_result = call_llm(validation_prompt, system_instruction_search_validator)

        if "RELEVANT:" in validation_result:
            return search_query, search_snippets # Return both the search query and relevant context
        else:
            print(f"Attempt {attempt + 1}: Search snippets deemed irrelevant. Trying again...")

    return None, None  # Return None if no relevant context is found
def generate_answer_with_snippets(question, search_snippets):
    """
    Generates an answer using the validated search snippets, ensuring that the answer
    is directly supported by the information in the snippets.
    """
    system_instruction = "You are an expert at answering question given relevant search snippets"
    # Now we leverage the search snippets to answer the question directly
    answer_prompt = f"""
    Answer the question using ONLY the information present in the search snippets.

    Example 1:
    Question: What was the first name of Ralph E. Oesper?
    Search Snippets: No results found.
    Answer: Answer not found.

    Example 2:
    Question: In which year did Maharaj Kishan Bhan receive the Padma Bhushan for civil services?
    Search Snippets: Maharaj Kishan Bhan was awarded the Padma Bhushan in 2013.; He was a famous scientist.
    Answer: 2013

    Question: {question}
    Search Snippets: {search_snippets}
    Answer:
    """
    answer = call_llm(answer_prompt, system_instruction)
    return answer

def main(question):
    """
    Main function to orchestrate the validated query generation, information retrieval (simulated),
    and answer generation process.
    """
    search_query, search_snippets = generate_query_and_validate(question)

    if search_query and search_snippets:
        answer = generate_answer_with_snippets(question, search_snippets)
        return answer
    else:
        return "Answer not found." # If not able to retrieve reliable context then return not found
```

=== TOP PERFORMING APPROACH #3 ===
Iteration: 7
Accuracy: 0.67
Approach Summary: The script implements LLM-Guided Recursive Decomposition & Verification (LLM-RDRV) to answer complex questions. It decomposes the original question into sub-questions, answers each sub-question individually, verifies the answers, and synthesizes them into a final answer. This involves agent roles like question decomposer, answerer, and validator. The functions used are `call_llm`, `decompose_question`, `answer_sub_question`, `verify_answer`, `synthesize_answers`, and `main`. The `main` function orchestrates the process by calling `decompose_question` to break down the initial question, then iterates through the sub-questions, using `answer_sub_question` to find answers, and `verify_answer` to check the validity of each response before finally using `synthesize_answers` to give the final output.

FULL SCRIPT CODE:
```python
import os
import re
import math # for react
from google import genai
from google.genai import types

# This script introduces a new approach: LLM-Guided Recursive Decomposition & Verification (LLM-RDRV)
# Hypothesis: By recursively decomposing complex questions into simpler sub-questions and verifying each intermediate answer,
# we can improve accuracy and handle complex queries more effectively.

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def decompose_question(question):
    """Decomposes a complex question into simpler, answerable sub-questions."""
    system_instruction = "You are an expert at breaking down complex questions into simpler sub-questions."
    prompt = f"""
    Decompose the following complex question into simpler, independent sub-questions that can be answered individually.

    Example 1:
    Complex Question: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    Sub-Questions:
    1. What country is the Great Barrier Reef located in?
    2. What is the capital of Australia?
    3. What is the population of Canberra?

    Example 2:
    Complex Question: In which month and year was Satyanarayan Gangaram Pitroda appointed as advisor to the Indian Prime Minister, and what was his rank?
    Sub-Questions:
    1. In which month and year was Satyanarayan Gangaram Pitroda appointed as advisor to the Indian Prime Minister?
    2. What was Satyanarayan Gangaram Pitroda's rank as advisor?

    Question: {question}
    Sub-Questions:
    """
    return call_llm(prompt, system_instruction)

def answer_sub_question(sub_question):
    """Answers a single sub-question using a direct LLM call."""
    system_instruction = "You are an expert at answering questions directly."
    prompt = f"""
    Answer the following question concisely and accurately.

    Example 1:
    Question: What is the capital of France?
    Answer: Paris

    Example 2:
    Question: In what year did World War II begin?
    Answer: 1939

    Question: {sub_question}
    Answer:
    """
    return call_llm(prompt, system_instruction)

def verify_answer(question, answer):
    """Verifies the answer against the original question to ensure relevance and accuracy."""
    system_instruction = "You are a critical validator who checks if an answer is factually correct and relevant to the question."
    prompt = f"""
    Verify if the following answer accurately and completely answers the question. Respond with VALID or INVALID, followed by a brief explanation.

    Example 1:
    Question: What is the capital of France?
    Answer: Paris
    Verification: VALID: Paris is indeed the capital of France.

    Example 2:
    Question: In what year did World War II begin?
    Answer: 1940
    Verification: INVALID: World War II began in 1939.

    Question: {question}
    Answer: {answer}
    Verification:
    """
    return call_llm(prompt, system_instruction)

def synthesize_answers(original_question, sub_questions_answers):
    """Synthesizes the answers to the sub-questions into a coherent answer to the original question."""
    system_instruction = "You are an expert at synthesizing information to answer complex questions."
    prompt = f"""
    Synthesize the following answers to sub-questions into a coherent and complete answer to the original question.

    Example 1:
    Original Question: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    Sub-Questions and Answers:
    1. What country is the Great Barrier Reef located in? Answer: Australia
    2. What is the capital of Australia? Answer: Canberra
    3. What is the population of Canberra? Answer: 431,500
    Synthesized Answer: The capital of Australia, where the Great Barrier Reef is located, is Canberra, and its population is 431,500.

   Example 2:
    Original Question: In which month and year was Satyanarayan Gangaram Pitroda appointed as advisor to the Indian Prime Minister, and what was his rank?
    Sub-Questions and Answers:
    1. In which month and year was Satyanarayan Gangaram Pitroda appointed as advisor to the Indian Prime Minister? Answer: October 2009
    2. What was Satyanarayan Gangaram Pitroda's rank as advisor? Answer: Cabinet Minister
    Synthesized Answer: Satyanarayan Gangaram Pitroda was appointed as advisor to the Indian Prime Minister in October 2009 with the rank of Cabinet Minister.

    Original Question: {original_question}
    Sub-Questions and Answers:
    {sub_questions_answers}
    Synthesized Answer:
    """
    return call_llm(prompt, system_instruction)

def main(question):
    """Main function to orchestrate the LLM-Guided Recursive Decomposition & Verification process."""
    # Step 1: Decompose the question
    sub_questions = decompose_question(question)
    print(f"Sub-questions: {sub_questions}")

    # Step 2: Answer each sub-question
    sub_questions_list = sub_questions.split("\n")
    sub_questions_answers = []
    all_valid = True

    for i, sub_question in enumerate(sub_questions_list):
        if sub_question.strip(): # Skip empty lines
            answer = answer_sub_question(sub_question)
            verification = verify_answer(sub_question, answer)
            print(f"Verification result: {verification}")
            if "INVALID" not in verification:
                sub_questions_answers.append(f"{i+1}. {sub_question} Answer: {answer}")
            else:
                all_valid = False
                break

    # Step 3: Synthesize the answers
    if all_valid:
        synthesized_answer = synthesize_answers(question, "\n".join(sub_questions_answers))
        print(f"Synthesized answer: {synthesized_answer}")

        return synthesized_answer
    else:
        return "Could not find the answer."
```

    
            EXPLOITATION SYNTHESIS GUIDANCE:
            1. ANALYZE EACH TOP SCRIPT to identify:
               - What specific techniques make each approach successful?
               - What unique strengths does each approach have?
               - What weaknesses or limitations does each approach have?
               - Which components could be combined effectively?
    
            2. IDENTIFY SYNTHESIS OPPORTUNITIES:
               - Which successful techniques from different scripts could work together?
               - How can you combine the best reasoning patterns from multiple approaches?
               - What hybrid approach would leverage strengths while avoiding weaknesses?
               - Can you create a multi-stage pipeline using the best parts of each?
    
            3. CREATE A HYBRID APPROACH that:
               - Takes the most effective reasoning techniques from each top script
               - Combines different successful verification/validation strategies
               - Integrates the best error handling approaches
               - Merges effective prompt engineering techniques from multiple scripts
               - Creates a more robust solution than any individual approach
    
            4. SPECIFIC SYNTHESIS STRATEGIES:
               - If Script A excels at information extraction and Script B excels at reasoning, combine both
               - If Script A has great verification and Script B has great generation, merge the pipelines
               - If multiple scripts use different successful prompting styles, create a multi-perspective approach
               - If different scripts handle different types of errors well, create comprehensive error handling
    
            5. AVOID SIMPLE COPYING:
               - Don't just take one script and make minor changes
               - Don't just concatenate approaches without thoughtful integration
               - Create something that's genuinely better than the sum of its parts
               - Ensure the hybrid approach addresses weaknesses that individual scripts had
    
            CRITICAL REQUIREMENTS FOR SYNTHESIS:
            1. The script MUST be a true hybrid that combines elements from multiple top approaches
            2. Include a clear comment explaining which elements came from which approaches
            3. EVERY LLM PROMPT must include embedded examples showing:
               - Sample input similar to the dataset
               - Expected reasoning steps
               - Desired output format
            4. The hybrid should be more robust than any individual approach
            5. Address the weaknesses identified in the capability assessment through synthesis
    
            Here's how to call the Gemini API. Use this example without modification:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
    
            SYNTHESIS IMPLEMENTATION:
            - Create a main function that orchestrates the combined approach
            - Integrate the best reasoning patterns from multiple scripts
            - Combine the most effective verification strategies
            - Merge successful prompt engineering techniques
            - Create comprehensive error handling that addresses issues from all approaches
    
            Return a COMPLETE, RUNNABLE Python script that represents a true synthesis of the top approaches:
            1. Has a main function that takes a question string as input and returns the answer string
            2. Combines reasoning techniques from multiple successful scripts
            3. Integrates the best verification and error handling from different approaches
            4. Includes embedded examples in EVERY LLM prompt
            5. Is COMPLETE - no missing code, no "..." placeholders
            6. Closes all string literals properly
            7. Includes comments explaining which techniques came from which top scripts
    
            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            CREATE A TRUE HYBRID THAT'S BETTER THAN ANY INDIVIDUAL APPROACH!
            