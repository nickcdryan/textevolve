
            You are developing a Python script to solve problems using LLM reasoning capabilities.
            You are in the EXPLORATION PHASE. You must generate a NEW approach that's different from previous approaches but informed by their successes and failures. With this approach, you will have a specific NEW HYPOTHESIS or variable you are trying to test. Your goal is to see if this new approach works, and you must add verification and validation steps to deduce if this new change is helpful. You may also test RADICAL NEW APPROACHES that are substantially different from previous approaches. 
            
            You should try NEW THINGS:
            
            Break down the problem into smaller pieces
            Think CREATIVELY about how to solve your problem if other approaches aren't working
            Transform data into different formats to see if it helps

            # YOUR TASK
            You are deeply familiar with prompting techniques and the agent works from the literature. 
            Your goal is to maximize the specified performance metrics by proposing interestingly new agents.
            Observe the past discovered agents and scripts carefully and think about what insights, lessons, or stepping stones can be learned from them.
            Be creative when thinking about the next interesting agent to try. You are encouraged to draw inspiration from related agent papers or academic papers from other research areas.
            Use the knowledge from the archive and inspiration from academic literature to propose the next interesting agentic system design.
            THINK OUTSIDE THE BOX.
            

            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "PASSAGE: Coming off their bye week, the Bills stayed at home for a fierce Week 7 intraconference duel with the Baltimore Ravens.  This match-up would be notable for RB Willis McGahee heading back to Buffalo to play against his former team. In the first quarter, the Bills got the first blood with kicker Rian Lindell getting a 29-yard field goal for the only score of the period.  In the second quarter, Buffalo increased its lead with Lindell nailing a 26-yard and a 35-yard field goal.  In the third quarter, the Ravens began to climb back into the game with McGahee getting a 46-yard TD run.  Fortunately, the Bills would respond with Lindell getting a 41-yard field goal, along with McGahee's successor, RB Marshawn Lynch, getting a 1-yard TD run.  In the fourth quarter, Baltimore drew closer as QB Kyle Boller completed a 15-yard TD pass to WR Derrick Mason.  Fortunately, Buffalo managed to hold on for a well-earned victory.\n\nQUESTION: How many more touchdowns were scored in the second half than the first?",
    "answer": "3"
  },
  {
    "id": 1,
    "question": "PASSAGE: After a tough loss, the Steelers went home to take on the 49ers.  They struck first in the first quarter when Ben Roethlisberger found Heath Miller on a 2-yard TD pass (with a successful 2-point conversion) for an 8-0 lead.  The Niners were able to get on the board in the second quarter when Phil Dawson kicked a 47-yard field goal shortening the lead to 8-3.  The Steelers however pulled away later on as De'Angelo Williams ran for a 2-yard TD (with another successful 2-point conversion) for a 16-3 lead followed up by Roethlisberger finding Darrius Heyward-Bay on a 35-yard TD pass (with a failed PAT) for a 22-3 lead and finally Williams rushing for another 2-yard TD before halftime to take a 29-3 lead.  After a scoreless third quarter, the Niners went to work in the fourth when Colin Kaepernick found Anquan Boldin on a 14-yard TD pass to move behind 29-10.  The Steelers however pulled away when Williams ran for a 1-yard TD to move on up 36-10.  The Niners drew closer when Kaepernick found Torrey Smith on a 75-yard TD pass (with a successful 2-point conversion) to make the score 36-18.  The Steelers however wrapped up the scoring of the game when Roethlisberger found Antonio Brown on a 7-yard TD pass for the final score 43-18. The defense had a field day on Kaepernick by sacking him 5 times and forcing a fumble which they recovered.\n\nQUESTION: How many yards was the longest touchdown pass?",
    "answer": "75"
  },
  {
    "id": 2,
    "question": "PASSAGE: Following their home loss from the Colts, the Titans flew to the Louisiana Superdome for a Monday Night fight with the New Orleans Saints. In the first quarter, Tennessee got the first punch with kicker Rob Bironas getting a 33-yard field goal for the only score of the period. In the second quarter, the Titans increased its lead with QB Vince Young completing a 35-yard TD pass to WR Brandon Jones. The Saints would respond with RB Reggie Bush getting a 1-yard TD run. In the third quarter, New Orleans would take the lead with another 1-yard TD run by Bush. Fortunately, Tennessee would regain the lead with RB LenDale White getting a 1-yard TD run. In the fourth quarter, the Titans closed out the game with Young completing a 3-yard TD pass to TE Bo Scaife, while DB Vincent Fuller returned an interception 61 yards for a touchdown. Pulling off the 3 other Saints interceptions was LB Keith Bulluck.\n\nQUESTION: How many touchdown runs were made for the same yardage?",
    "answer": "3"
  }
]

            HISTORICAL CONTEXT:
            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 9
        - Current explore/exploit balance: 70/30
        - Best accuracy achieved: 1.00 (iteration 1)

        APPROACH HISTORY (last 9 iterations):
        [
  {
    "iteration": 0,
    "strategy": "Exploration",
    "accuracy": 0.6666666666666666,
    "approach": "The script uses chain-of-thought reasoning by decomposing a question into sub-questions, extracting relevant information, and then synthesizing an answer, with verification steps at each stage. The agents involved are question decomposer, information extraction expert, and answer synthesis expert, each responsible for their respective tasks. The `main` function orchestrates the process by calling `decompose_question`, `extract_information`, and `synthesize_answer` sequentially, using `call_llm` to interact with the LLM for each step, and validation checks are performed after each step to ensure the validity of the generated content. The overall workflow is question decomposition, information extraction, and answer synthesis, each validated by the LLM before proceeding to the next step."
  },
  {
    "iteration": 1,
    "strategy": "Exploitation",
    "accuracy": 1.0,
    "approach": "The script uses a chain-of-thought approach with question decomposition and verification steps to answer a question. The problem is decomposed into sub-questions, then relevant information is extracted, and finally, an answer is synthesized. Three agent roles are used: question decomposer, information extraction expert, and answer synthesis expert, each implemented through specific prompts to the LLM. The functions used are `main` which orchestrates the process, `decompose_question` which breaks down the question, `extract_information` which gathers relevant information, `synthesize_answer` which formulates the final answer, and `call_llm` which interfaces with the Gemini API; these functions are called sequentially to solve the problem. The overall workflow is to decompose, extract, and synthesize, with validation steps at each stage to ensure correctness."
  },
  {
    "iteration": 2,
    "strategy": "Exploitation",
    "accuracy": 1.0,
    "approach": "The script addresses question answering by first determining the question type and then processing it accordingly. Numerical questions are processed by extracting numerical information and calculating the answer, while general questions are addressed through question decomposition, information extraction, and answer synthesis. The agent roles are: question type identifier, numerical information extractor, calculator, question decomposer, information extraction expert, and answer synthesis expert. Key functions include `determine_question_type`, `process_numerical_question`, `extract_numerical_info`, `calculate_answer`, `process_general_question`, `decompose_question`, `extract_information`, `synthesize_answer`, and `call_llm`; these functions are chained together to process the question and generate an answer using LLM-driven techniques with verification steps at each stage. The overall workflow involves determining question type, processing the question using a type-specific method, and returning the result or an error message."
  },
  {
    "iteration": 3,
    "strategy": "Exploitation",
    "accuracy": 0.8,
    "approach": "The script uses a chain-of-thought approach to answer questions by decomposing them into sub-questions, extracting relevant information, and synthesizing a final answer. Each step uses an LLM with a specific role (question decomposer, information extractor, answer synthesizer) and is validated for correctness. The core functions are `decompose_question`, `extract_information`, and `synthesize_answer`, which sequentially process the question. `decompose_question` breaks down the initial question, `extract_information` gathers needed data, and `synthesize_answer` formulates the final answer, with `call_llm` used to interface with the LLM."
  },
  {
    "iteration": 4,
    "strategy": "Exploitation",
    "accuracy": 1.0,
    "approach": "The script uses a chain-of-thought approach, decomposing a question into sub-questions, extracting information, and then synthesizing an answer. It employs three distinct agent roles: a question decomposer, an information extraction expert, and an answer synthesis expert. It leverages the `call_llm` function to interact with the Gemini model using specifically crafted prompts for each agent, and incorporates validation steps after each stage to ensure correctness. The script begins with the `main` function, which orchestrates the calls to `decompose_question`, `extract_information`, and `synthesize_answer`; these functions in turn use `call_llm` to generate responses which undergo validation at each step."
  },
  {
    "iteration": 5,
    "strategy": "Exploitation",
    "accuracy": 0.8,
    "approach": "The script uses a chain-of-thought approach, decomposing a question into sub-questions, extracting relevant information, and synthesizing an answer using the Gemini LLM. It employs verification steps at each stage to ensure validity. The script defines functions for question decomposition (`decompose_question`), information extraction (`extract_information`), answer synthesis (`synthesize_answer`), and calling the LLM (`call_llm`). The `main` function orchestrates the process by calling these functions sequentially, using the output of one as input for the next, to generate the final answer."
  },
  {
    "iteration": 6,
    "strategy": "Exploitation",
    "accuracy": 0.8,
    "approach": "The script uses chain-of-thought reasoning with a question decomposition, information extraction, and answer synthesis approach, where each step is validated by the LLM. It leverages three agent roles: question decomposer, information extraction expert, and answer synthesis expert, each guided by specific system instructions. The overall workflow involves `main` calling `decompose_question` to break down the question, then `extract_information` to find relevant information, and finally `synthesize_answer` to generate the final answer. The `call_llm` function is used to interact with the Gemini model, while the other functions decompose the problem and provide prompts to the LLM."
  },
  {
    "iteration": 7,
    "strategy": "Exploitation",
    "accuracy": 0.0,
    "approach": "The script uses a chain-of-thought approach with verification to answer questions, first determining the question type (numerical or general) and then processing it accordingly. Numerical questions are processed by extracting numerical information and then calculating the answer, while general questions are processed by decomposing the question into sub-questions, extracting information based on those sub-questions, and synthesizing the answer. The agent roles include a question type identifier, numerical information extractor, calculator, and (implicitly) a question decomposer and answer synthesizer.\n\nThe functions used are: `main` which orchestrates the process, `determine_question_type` to identify the type of question, `process_numerical_question` and `process_general_question` to handle the different question types, `extract_numerical_info` to extract numerical values, `calculate_answer` to perform calculations, `decompose_question` to break down general questions, `extract_information` to gather relevant details, and `synthesize_answer` to formulate the final response. The `call_llm` function is called within each of these functions. The overall workflow involves determining the question type, processing it based on its type (numerical or general), and returning the answer or an error message if any step fails."
  },
  {
    "iteration": 8,
    "strategy": "Exploitation",
    "accuracy": 0.0,
    "approach": "This script uses LLM-driven chain-of-thought reasoning and verification to answer questions, first determining if the question is numerical or general. Numerical questions are processed by extracting numbers and performing calculations, while general questions are decomposed into sub-questions, have information extracted for each, and then synthesize an answer. The script defines specialized agents for question type determination, numerical information extraction, calculation, question decomposition, information extraction, and answer synthesis; each leverages examples and a validation step.\n\nThe functions used are:\n- `main()`: Orchestrates the entire process.\n- `determine_question_type()`: Determines question type.\n- `process_numerical_question()`: Processes numerical questions.\n- `extract_numerical_info()`: Extracts numerical data.\n- `calculate_answer()`: Calculates numerical answers.\n- `process_general_question()`: Processes general questions.\n- `decompose_question()`: Decomposes complex questions.\n- `extract_information()`: Extracts information from decomposed questions.\n- `synthesize_answer()`: Synthesizes the final answer.\n- `call_llm()`: Calls the Gemini LLM with a prompt.\n\nThe overall workflow is as follows: `main()` calls `determine_question_type()`, then either `process_numerical_question()` which uses `extract_numerical_info()` and `calculate_answer()`, or `process_general_question()` which uses `decompose_question()`, `extract_information()`, and `synthesize_answer()`, with all *process, extract, calculate, decompose, and synthesize* functions using `call_llm()` to interact with the LLM."
  }
]

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 3 iterations):
        [
  {
    "iteration": 0,
    "issue": "The most critical problem is the **answer synthesis and validation logic's failure to consistently produce valid answers, even when the underlying reasoning and information extraction might be correct.** This is evidenced by the \"Error in answer synthesis\" message and the repeated validation failures. This is likely due to a mismatch between the expected answer format in the validation logic and the format being produced by the synthesis module."
  },
  {
    "iteration": 1,
    "issue": "Without error cases, the primary issue is a potential **lack of robustness in reasoning capabilities beyond simple information extraction.** The system is likely vulnerable to problems requiring more advanced reasoning, arithmetic, or inference."
  },
  {
    "iteration": 2,
    "issue": "The primary issue is the inability to determine the system's limitations due to the lack of error cases."
  },
  {
    "iteration": 3,
    "issue": "The most critical problem is the **lack of arithmetic reasoning capabilities**. The system can extract information but often fails to synthesize it by performing calculations, especially simple addition."
  },
  {
    "iteration": 4,
    "issue": "The primary issue is the system's inability to provide fully complete answers, which sometimes lack essential context, units, or modifiers that are present in the golden answer. While the extracted information is often accurate, the completeness of the response is not always guaranteed."
  },
  {
    "iteration": 5,
    "issue": "The most critical problem is the **misinterpretation of the question's specific intent**, leading the system to extract the wrong information or perform irrelevant calculations. In particular, the system's semantic processing needs improvement to handle subtle nuances in the question."
  },
  {
    "iteration": 6,
    "issue": "The primary issue is **inconsistent and unreliable arithmetic calculation, compounded by a lack of rigorous answer verification before output**. Even simple addition or subtraction sometimes fails, and the system doesn't catch these errors before finalizing the answer. This, coupled with inconsistent unit handling, reduces the trustworthiness of the entire process."
  },
  {
    "iteration": 7,
    "issue": "The single most critical problem is the undefined `call_llm` function. This function is presumably the core component responsible for interacting with the LLM, and its absence effectively disables the entire system. The definition or import of this function needs to be addressed immediately."
  },
  {
    "iteration": 8,
    "issue": "The most critical problem is the failure of the question type determination module. This module needs to be redesigned or debugged to correctly classify the types of questions being asked (e.g., comparison, extraction, calculation). The validation logic needs to be checked for correctness or adjusted."
  }
]

        TARGETED IMPROVEMENTS:
        [
  "Add Temporal Reasoning Module:** Implement a dedicated module for temporal reasoning that can handle date calculations, time intervals, and event ordering.",
  "Introduce Contrastive Learning:** Train the system using contrastive learning techniques, where it's presented with pairs of similar questions that require different answers. This will help the system learn to differentiate between subtle nuances in question wording.",
  "Refine Question Parsing:** Implement more sophisticated techniques for question parsing, including dependency parsing, semantic role labeling, and question type classification. This will enable the system to better understand the relationships between words and phrases in the question.",
  "Implement more print statements:** Add more print statements, especially when numerical computations are performed, so that one can track how numerical data and question data is extracted and utilized by the AI system.",
  "Implement Semantic Similarity Measures:** Utilize semantic similarity metrics (e.g., word embeddings, sentence embeddings) to compare the question with different parts of the passage. This will help the system identify the most relevant information and avoid focusing on irrelevant details.",
  "Introduce an arithmetic module:** Implement a dedicated module that takes a list of extracted numbers and an operator (e.g., \"sum\", \"difference\") as input and returns the result.",
  "Prompt Engineering:** Use prompt engineering to explicitly ask the LLM to extract all relevant numbers and their corresponding units and then perform the calculation. For example: \"Extract all numbers corresponding to the entities and their units. What arithmetic operations should be performed to arrive at the answer? Perform the calculation and give the final answer.\"",
  "Answer type classification:** Before generating the final answer, classify the question based on the expected answer type (e.g., numerical, descriptive, boolean). Use this classification to guide the answer generation process. For example, if the expected answer type is numerical, prioritize extracting numbers and performing calculations."
]
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity ✓
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity ✓
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity ✓
    4. All operations have O(1) average time complexity ✓
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        

            PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS). YOUR APPROACH MUST BE SUBSTANTIVELY DIFFERENT THAN THESE:
            
PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS):

=== SCRIPT FROM ITERATION 8 (Exploitation, ACCURACY: 0.00) ===
Approach: This script uses LLM-driven chain-of-thought reasoning and verification to answer questions, first determining if the question is numerical or general. Numerical questions are processed by extracting numbers and performing calculations, while general questions are decomposed into sub-questions, have information extracted for each, and then synthesize an answer. The script defines specialized agents for question type determination, numerical information extraction, calculation, question decomposition, information extraction, and answer synthesis; each leverages examples and a validation step.

The functions used are:
- `main()`: Orchestrates the entire process.
- `determine_question_type()`: Determines question type.
- `process_numerical_question()`: Processes numerical questions.
- `extract_numerical_info()`: Extracts numerical data.
- `calculate_answer()`: Calculates numerical answers.
- `process_general_question()`: Processes general questions.
- `decompose_question()`: Decomposes complex questions.
- `extract_information()`: Extracts information from decomposed questions.
- `synthesize_answer()`: Synthesizes the final answer.
- `call_llm()`: Calls the Gemini LLM with a prompt.

The overall workflow is as follows: `main()` calls `determine_question_type()`, then either `process_numerical_question()` which uses `extract_numerical_info()` and `calculate_answer()`, or `process_general_question()` which uses `decompose_question()`, `extract_information()`, and `synthesize_answer()`, with all *process, extract, calculate, decompose, and synthesize* functions using `call_llm()` to interact with the LLM.

```python
import os
import re

def main(question):
    """
    Solve the question by extracting relevant information from the passage and using chain-of-thought reasoning.
    This approach builds upon a prior attempt to use question decomposition, strengthens answer synthesis, and includes examples in all LLM prompts.
    It combines the strengths of Iteration 1 and 2, incorporating question-type determination and specialized processing.
    """
    try:
        # Step 1: Determine the question type (numerical or general).
        question_type_result = determine_question_type(question)
        if not question_type_result.get("is_valid"):
            return f"Error in determining question type: {question_type_result.get('validation_feedback')}"

        # Step 2: Process the question based on its type.
        if question_type_result["question_type"] == "numerical":
            process_result = process_numerical_question(question)
        else:
            process_result = process_general_question(question)

        return process_result

    except Exception as e:
        return f"An unexpected error occurred: {str(e)}"

def determine_question_type(question, max_attempts=3):
    """Determine if the question is numerical or general."""
    system_instruction = "You are an expert at classifying questions as either numerical or general."

    for attempt in range(max_attempts):
        type_prompt = f"""
        Classify the question as either "numerical" or "general". Numerical questions require numerical calculations. General questions do not.

        Example 1:
        Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Type: numerical

        Example 2:
        Question: Who caught the final touchdown of the game?
        Type: general

        Question: {question}
        Type:
        """

        type_result = call_llm(type_prompt, system_instruction)

        verification_prompt = f"""
        Verify if the question type classification is correct.

        Question: {question}
        Classification: {type_result}

        Example:
        Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Classification: numerical
        Validation: Valid

        Is the classification valid? Respond with 'Valid' or 'Invalid'.
        """

        verification_result = call_llm(verification_prompt, system_instruction)

        if "valid" in verification_result.lower() and type_result in ("numerical", "general"):
            return {"is_valid": True, "question_type": type_result}
        else:
            print(f"Question type validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")

    return {"is_valid": False, "validation_feedback": "Failed to determine the question type successfully."}

def process_numerical_question(question):
    """Process a numerical question."""
    try:
        extracted_info_result = extract_numerical_info(question)
        if not extracted_info_result.get("is_valid"):
            return f"Error in extracting numerical info: {extracted_info_result.get('validation_feedback')}"

        calculation_result = calculate_answer(question, extracted_info_result["numbers"])
        if not calculation_result.get("is_valid"):
            return f"Error in calculation: {calculation_result.get('validation_feedback')}"

        return calculation_result["answer"]

    except Exception as e:
        return f"Error processing numerical question: {str(e)}"

def extract_numerical_info(question, max_attempts=3):
    """Extract numerical information from the question."""
    system_instruction = "You are an expert at extracting numerical information."

    for attempt in range(max_attempts):
        extraction_prompt = f"""
        Extract all numbers from the question and their corresponding entities.

        Example 1:
        Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Numbers:
        6 yards (Chris Johnson's first touchdown)
        53 yards (Jason Hanson's first field goal)

        Example 2:
        Question: How many touchdowns did Brandon Jacobs rush for?
        Numbers:
        2 touchdowns (Brandon Jacobs)

        Question: {question}
        Numbers:
        """

        extracted_numbers = call_llm(extraction_prompt, system_instruction)

        verification_prompt = f"""
        Verify that the numbers extracted are correct and correspond to their entities.

        Question: {question}
        Extracted Numbers: {extracted_numbers}

        Example:
        Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Extracted Numbers: 6 yards (Chris Johnson's first touchdown), 53 yards (Jason Hanson's first field goal)
        Validation: Valid

        Is the extraction valid? Respond with 'Valid' or 'Invalid'.
        """

        verification_result = call_llm(verification_prompt, system_instruction)

        if "valid" in verification_result.lower():
            # Simple parsing
            numbers = re.findall(r'\d+', extracted_numbers)
            return {"is_valid": True, "numbers": numbers}
        else:
            print(f"Numerical info extraction validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")

    return {"is_valid": False, "validation_feedback": "Failed to extract numerical information successfully."}

def calculate_answer(question, numbers, max_attempts=3):
    """Calculate the answer to a numerical question."""
    system_instruction = "You are an expert calculator."

    for attempt in range(max_attempts):
        calculation_prompt = f"""
        Calculate the answer based on the extracted numbers and the original question. Show your work.

        Example:
        Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Numbers: 6, 53
        Calculation: 6 + 53 = 59
        Answer: 59

        Question: {question}
        Numbers: {numbers}
        Calculation:
        """

        calculation_result = call_llm(calculation_prompt, system_instruction)

        verification_prompt = f"""
        Verify if the calculation is correct.

        Question: {question}
        Numbers: {numbers}
        Calculation: {calculation_result}

        Example:
        Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Numbers: 6, 53
        Calculation: 6 + 53 = 59
        Validation: Valid

        Is the calculation valid? Respond with 'Valid' or 'Invalid'.
        """

        verification_result = call_llm(verification_prompt, system_instruction)

        if "valid" in verification_result.lower():
            answer = re.search(r'\d+$', calculation_result)
            if answer:
                return {"is_valid": True, "answer": answer.group(0)}
            else:
                return {"is_valid": False, "validation_feedback": "Could not find numerical answer."}
        else:
            print(f"Calculation validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")

    return {"is_valid": False, "validation_feedback": "Failed to calculate the answer successfully."}

def process_general_question(question):
    """Process a general question."""
    try:
        # Step 1: Decompose the question into sub-questions.
        decomposition_result = decompose_question(question)
        if not decomposition_result.get("is_valid"):
            return f"Error in question decomposition: {decomposition_result.get('validation_feedback')}"
        
        # Step 2: Extract relevant information based on sub-questions.
        information_extraction_result = extract_information(question, decomposition_result["sub_questions"])
        if not information_extraction_result.get("is_valid"):
            return f"Error in information extraction: {information_extraction_result.get('validation_feedback')}"

        # Step 3: Synthesize the answer from extracted information.
        answer_synthesis_result = synthesize_answer(question, information_extraction_result["extracted_info"])
        if not answer_synthesis_result.get("is_valid"):
            return f"Error in answer synthesis: {answer_synthesis_result.get('validation_feedback')}"
        
        return answer_synthesis_result["answer"]

    except Exception as e:
        return f"Error processing general question: {str(e)}"

def decompose_question(question, max_attempts=3):
    """Decompose the main question into smaller, answerable sub-questions."""
    system_instruction = "You are an expert question decomposer."
    
    for attempt in range(max_attempts):
        decomposition_prompt = f"""
        Decompose the given question into smaller, self-contained sub-questions that, when answered, will fully answer the original question.

        Example 1:
        Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions:
        1. How many yards was Chris Johnson's first touchdown?
        2. How many yards was Jason Hanson's first field goal?
        3. What is the sum of those two values?

        Example 2:
        Question: Who caught the final touchdown of the game?
        Sub-questions:
        1. Who scored the final touchdown of the game?

        Question: {question}
        Sub-questions:
        """
        
        decomposition_result = call_llm(decomposition_prompt, system_instruction)
        
        # Verify if the decomposition is valid
        verification_prompt = f"""
        Verify if these sub-questions are valid and sufficient to answer the original question.

        Original Question: {question}
        Sub-questions: {decomposition_result}

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions: 1. How many yards was Chris Johnson's first touchdown? 2. How many yards was Jason Hanson's first field goal? 3. What is the sum of those two values?
        Validation: Valid

        Is the decomposition valid and sufficient? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)
        
        if "valid" in verification_result.lower():
            return {"is_valid": True, "sub_questions": decomposition_result}
        else:
            print(f"Decomposition validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to decompose the question successfully."}

def extract_information(question, sub_questions, max_attempts=3):
    """Extract relevant information from the passage based on the sub-questions."""
    system_instruction = "You are an information extraction expert."
    
    for attempt in range(max_attempts):
        extraction_prompt = f"""
        Given the original question and its sub-questions, extract the relevant information from the passage required to answer the sub-questions.

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions:
        1. How many yards was Chris Johnson's first touchdown?
        2. How many yards was Jason Hanson's first field goal?
        Extracted Information:
        Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.

        Original Question: {question}
        Sub-questions: {sub_questions}
        Extracted Information:
        """
        
        extracted_info = call_llm(extraction_prompt, system_instruction)
        
        # Validate information extraction
        verification_prompt = f"""
        Verify if the extracted information is relevant and sufficient to answer the sub-questions.

        Original Question: {question}
        Sub-questions: {sub_questions}
        Extracted Information: {extracted_info}

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions: 1. How many yards was Chris Johnson's first touchdown? 2. How many yards was Jason Hanson's first field goal?
        Extracted Information: Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.
        Validation: Valid

        Is the extraction relevant and sufficient? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)
        
        if "valid" in verification_result.lower():
            return {"is_valid": True, "extracted_info": extracted_info}
        else:
            print(f"Information extraction validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to extract relevant information successfully."}

def synthesize_answer(question, extracted_info, max_attempts=3):
    """Synthesize the answer from the extracted information to answer the main question."""
    system_instruction = "You are an answer synthesis expert."

    for attempt in range(max_attempts):
        synthesis_prompt = f"""
        Given the original question and the extracted information, synthesize the final answer.

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Extracted Information: Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.
        Final Answer: 59

        Original Question: {question}
        Extracted Information: {extracted_info}
        Final Answer:
        """
        
        answer = call_llm(synthesis_prompt, system_instruction)

        # Answer checker
        verification_prompt = f"""
        Check if the answer is correct and answers the original question fully.

        Original Question: {question}
        Synthesized Answer: {answer}

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Synthesized Answer: 59
        Validation: Valid

        Is the answer correct and complete? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)

        if "valid" in verification_result.lower():
            return {"is_valid": True, "answer": answer}
        else:
            print(f"Answer synthesis validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to synthesize a valid answer."}

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types
        import os  # Import the os module

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
```

=== SCRIPT FROM ITERATION 7 (Exploitation, ACCURACY: 0.00) ===
Approach: The script uses a chain-of-thought approach with verification to answer questions, first determining the question type (numerical or general) and then processing it accordingly. Numerical questions are processed by extracting numerical information and then calculating the answer, while general questions are processed by decomposing the question into sub-questions, extracting information based on those sub-questions, and synthesizing the answer. The agent roles include a question type identifier, numerical information extractor, calculator, and (implicitly) a question decomposer and answer synthesizer.

The functions used are: `main` which orchestrates the process, `determine_question_type` to identify the type of question, `process_numerical_question` and `process_general_question` to handle the different question types, `extract_numerical_info` to extract numerical values, `calculate_answer` to perform calculations, `decompose_question` to break down general questions, `extract_information` to gather relevant details, and `synthesize_answer` to formulate the final response. The `call_llm` function is called within each of these functions. The overall workflow involves determining the question type, processing it based on its type (numerical or general), and returning the answer or an error message if any step fails.

```python
import os
import re

def main(question):
    """
    Solve the question by extracting relevant information from the passage and using chain-of-thought reasoning.
    This approach builds upon top-performing approaches by strengthening answer synthesis with a numerical reasoning module and improving verification,
    and uses multiple examples in all LLM prompts.
    """
    try:
        # Step 1: Determine question type
        question_type_result = determine_question_type(question)
        if not question_type_result.get("is_valid"):
            return f"Error in determining question type: {question_type_result.get('validation_feedback')}"

        # Step 2: Process question based on type
        if question_type_result["question_type"] == "numerical":
            process_result = process_numerical_question(question)
        else:
            process_result = process_general_question(question)

        return process_result

    except Exception as e:
        return f"An unexpected error occurred: {str(e)}"

def determine_question_type(question, max_attempts=3):
    """Determine if the question requires numerical reasoning or general information."""
    system_instruction = "You are an expert question type identifier."

    for attempt in range(max_attempts):
        type_prompt = f"""
        Determine if the question requires numerical reasoning (calculations) or general information extraction.

        Example 1:
        Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Type: numerical

        Example 2:
        Question: Who caught the final touchdown of the game?
        Type: general

        Question: {question}
        Type:
        """

        type_result = call_llm(type_prompt, system_instruction)

        verification_prompt = f"""
        Verify if the identified question type is correct.

        Question: {question}
        Identified Type: {type_result}

        Example:
        Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Identified Type: numerical
        Validation: Valid

        Is the identified type valid? Respond with 'Valid' or 'Invalid'.
        """

        verification_result = call_llm(verification_prompt, system_instruction)

        if "valid" in verification_result.lower():
            return {"is_valid": True, "question_type": type_result.lower()}
        else:
            print(f"Question type validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")

    return {"is_valid": False, "validation_feedback": "Failed to determine question type successfully."}

def process_numerical_question(question):
    """Process numerical questions by extracting numbers and performing calculations."""
    try:
        # Step 1: Extract numerical information
        extraction_result = extract_numerical_info(question)
        if not extraction_result.get("is_valid"):
            return f"Error in numerical information extraction: {extraction_result.get('validation_feedback')}"

        # Step 2: Calculate the answer
        calculation_result = calculate_answer(question, extraction_result["extracted_info"])
        if not calculation_result.get("is_valid"):
            return f"Error in calculation: {calculation_result.get('validation_feedback')}"

        return calculation_result["answer"]

    except Exception as e:
        return f"Error in processing numerical question: {str(e)}"

def extract_numerical_info(question, max_attempts=3):
    """Extract numerical information and units from the question."""
    system_instruction = "You are an expert at extracting numerical information and their units from text."

    for attempt in range(max_attempts):
        extraction_prompt = f"""
        Extract all numerical values and their corresponding units from the question.

        Example 1:
        Question: How many yards did Chris Johnson's first touchdown (6 yards) and Jason Hanson's first field goal (53 yards) combine for?
        Extracted Info:
        - 6 yards (touchdown)
        - 53 yards (field goal)

        Example 2:
        Question: The population increased by 12%, from 1000 to what number?
        Extracted Info:
        - 12% (increase)
        - 1000 (initial population)

        Question: {question}
        Extracted Info:
        """

        extracted_info = call_llm(extraction_prompt, system_instruction)

        verification_prompt = f"""
        Verify if the extracted numerical information is complete and accurate.

        Question: {question}
        Extracted Info: {extracted_info}

        Example:
        Question: How many yards did Chris Johnson's first touchdown (6 yards) and Jason Hanson's first field goal (53 yards) combine for?
        Extracted Info: - 6 yards (touchdown) - 53 yards (field goal)
        Validation: Valid

        Is the extracted information valid? Respond with 'Valid' or 'Invalid'.
        """

        verification_result = call_llm(verification_prompt, system_instruction)

        if "valid" in verification_result.lower():
            return {"is_valid": True, "extracted_info": extracted_info}
        else:
            print(f"Numerical info extraction failed (attempt {attempt+1}/{max_attempts}): {verification_result}")

    return {"is_valid": False, "validation_feedback": "Failed to extract numerical information successfully."}

def calculate_answer(question, extracted_info, max_attempts=3):
    """Calculate the answer based on the extracted numerical information."""
    system_instruction = "You are an expert calculator."

    for attempt in range(max_attempts):
        calculation_prompt = f"""
        Given the question and extracted numerical information, calculate the final answer.
        Identify the operation to perform (addition, subtraction, etc.) and then calculate it.

        Example:
        Question: How many yards did Chris Johnson's first touchdown (6 yards) and Jason Hanson's first field goal (53 yards) combine for?
        Extracted Info: - 6 yards (touchdown) - 53 yards (field goal)
        Calculation: 6 + 53 = 59
        Answer: 59

        Question: {question}
        Extracted Info: {extracted_info}
        Calculation:
        """

        calculation = call_llm(calculation_prompt, system_instruction)
        try:
            # Extract the numbers for the calculation from the LLM's calculation statement
            numbers = re.findall(r'\d+', calculation)
            if len(numbers) < 2:
                print("Not enough numbers were able to be extracted for the calculation")
                raise ValueError("Could not perform calculation with invalid numbers")
            num1 = int(numbers[0])
            num2 = int(numbers[1])

            # Extract the operator from the LLM's calculation statement
            operator_match = re.search(r'(\+|-|\*|/)', calculation)

            if not operator_match:
                print("No valid operator was able to be extracted for the calculation")
                raise ValueError("Invalid operator")
            operator = operator_match.group(1)

            if operator == "+":
                answer = num1 + num2
            elif operator == "-":
                answer = num1 - num2
            elif operator == "*":
                answer = num1 * num2
            elif operator == "/":
                answer = num1 / num2
            else:
                print("No known operator was selected")
                raise ValueError("Unknown operator")

            answer = str(answer)

        except Exception as e:
            print(f"Error performing calculation: {str(e)}")
            return {"is_valid": False, "validation_feedback": f"Failed to perform calculation: {str(e)}"}

        verification_prompt = f"""
        Verify if the calculated answer is correct based on the extracted information and question.

        Question: {question}
        Extracted Info: {extracted_info}
        Calculated Answer: {answer}

        Example:
        Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Extracted Info: - 6 yards (touchdown) - 53 yards (field goal)
        Calculated Answer: 59
        Validation: Valid

        Is the calculated answer valid? Respond with 'Valid' or 'Invalid'.
        """

        verification_result = call_llm(verification_prompt, system_instruction)

        if "valid" in verification_result.lower():
            return {"is_valid": True, "answer": answer}
        else:
            print(f"Calculation validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")

    return {"is_valid": False, "validation_feedback": "Failed to calculate a valid answer."}

def process_general_question(question):
    """Process general questions using decomposition, extraction, and synthesis."""
    try:
        # Step 1: Decompose the question into sub-questions.
        decomposition_result = decompose_question(question)
        if not decomposition_result.get("is_valid"):
            return f"Error in question decomposition: {decomposition_result.get('validation_feedback')}"

        # Step 2: Extract relevant information based on sub-questions.
        information_extraction_result = extract_information(question, decomposition_result["sub_questions"])
        if not information_extraction_result.get("is_valid"):
            return f"Error in information extraction: {information_extraction_result.get('validation_feedback')}"

        # Step 3: Synthesize the answer from extracted information.
        answer_synthesis_result = synthesize_answer(question, information_extraction_result["extracted_info"])
        if not answer_synthesis_result.get("is_valid"):
            return f"Error in answer synthesis: {answer_synthesis_result.get('validation_feedback')}"

        return answer_synthesis_result["answer"]

    except Exception as e:
        return f"An unexpected error occurred: {str(e)}"
```

=== SCRIPT FROM ITERATION 6 (Exploitation, ACCURACY: 0.80) ===
Approach: The script uses chain-of-thought reasoning with a question decomposition, information extraction, and answer synthesis approach, where each step is validated by the LLM. It leverages three agent roles: question decomposer, information extraction expert, and answer synthesis expert, each guided by specific system instructions. The overall workflow involves `main` calling `decompose_question` to break down the question, then `extract_information` to find relevant information, and finally `synthesize_answer` to generate the final answer. The `call_llm` function is used to interact with the Gemini model, while the other functions decompose the problem and provide prompts to the LLM.

```python
import os
import re

def main(question):
    """
    Solve the question by extracting relevant information from the passage and using chain-of-thought reasoning.
    This approach builds upon successful prior attempts, strengthens answer synthesis,
    and includes examples in all LLM prompts to improve performance and robustness.
    Addresses numerical reasoning and question interpretation issues.
    """
    try:
        # Step 1: Decompose the question into sub-questions.
        decomposition_result = decompose_question(question)
        if not decomposition_result.get("is_valid"):
            return f"Error in question decomposition: {decomposition_result.get('validation_feedback')}"
        
        # Step 2: Extract relevant information based on sub-questions.
        information_extraction_result = extract_information(question, decomposition_result["sub_questions"])
        if not information_extraction_result.get("is_valid"):
            return f"Error in information extraction: {information_extraction_result.get('validation_feedback')}"

        # Step 3: Synthesize the answer from extracted information. Incorporate unit handling and arithmetic.
        answer_synthesis_result = synthesize_answer(question, information_extraction_result["extracted_info"])
        if not answer_synthesis_result.get("is_valid"):
            return f"Error in answer synthesis: {answer_synthesis_result.get('validation_feedback')}"
        
        return answer_synthesis_result["answer"]

    except Exception as e:
        return f"An unexpected error occurred: {str(e)}"

def decompose_question(question, max_attempts=3):
    """Decompose the main question into smaller, answerable sub-questions."""
    system_instruction = "You are an expert question decomposer."
    
    for attempt in range(max_attempts):
        decomposition_prompt = f"""
        Decompose the given question into smaller, self-contained sub-questions that, when answered, will fully answer the original question.

        Example 1:
        Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions:
        1. How many yards was Chris Johnson's first touchdown?
        2. How many yards was Jason Hanson's first field goal?
        3. What arithmetic operation should be performed on these values to answer the original question?

        Example 2:
        Question: Who caught the final touchdown of the game?
        Sub-questions:
        1. Who scored the final touchdown of the game?

        Question: {question}
        Sub-questions:
        """
        
        decomposition_result = call_llm(decomposition_prompt, system_instruction)
        
        # Verify if the decomposition is valid
        verification_prompt = f"""
        Verify if these sub-questions are valid and sufficient to answer the original question.

        Original Question: {question}
        Sub-questions: {decomposition_result}

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions: 1. How many yards was Chris Johnson's first touchdown? 2. How many yards was Jason Hanson's first field goal? 3. What arithmetic operation should be performed on these values to answer the original question?
        Validation: Valid

        Is the decomposition valid and sufficient? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)
        
        if "valid" in verification_result.lower():
            return {"is_valid": True, "sub_questions": decomposition_result}
        else:
            print(f"Decomposition validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to decompose the question successfully."}

def extract_information(question, sub_questions, max_attempts=3):
    """Extract relevant information from the passage based on the sub-questions."""
    system_instruction = "You are an information extraction expert."
    
    for attempt in range(max_attempts):
        extraction_prompt = f"""
        Given the original question and its sub-questions, extract the relevant information from the passage required to answer the sub-questions. Include units where applicable.

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions:
        1. How many yards was Chris Johnson's first touchdown?
        2. How many yards was Jason Hanson's first field goal?
        Extracted Information:
        Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.

        Original Question: {question}
        Sub-questions: {sub_questions}
        Extracted Information:
        """
        
        extracted_info = call_llm(extraction_prompt, system_instruction)
        
        # Validate information extraction
        verification_prompt = f"""
        Verify if the extracted information is relevant and sufficient to answer the sub-questions.

        Original Question: {question}
        Sub-questions: {sub_questions}
        Extracted Information: {extracted_info}

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions: 1. How many yards was Chris Johnson's first touchdown? 2. How many yards was Jason Hanson's first field goal?
        Extracted Information: Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.
        Validation: Valid

        Is the extraction relevant and sufficient? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)
        
        if "valid" in verification_result.lower():
            return {"is_valid": True, "extracted_info": extracted_info}
        else:
            print(f"Information extraction validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to extract relevant information successfully."}

def synthesize_answer(question, extracted_info, max_attempts=3):
    """Synthesize the answer from the extracted information to answer the main question. Handle units and arithmetic."""
    system_instruction = "You are an expert answer synthesis expert. You must perform arithmetic and unit handling as necessary."

    for attempt in range(max_attempts):
        synthesis_prompt = f"""
        Given the original question and the extracted information, synthesize the final answer. Perform any necessary arithmetic.

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Extracted Information: Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.
        Final Answer: 59 yards

        Example 2:
        Original Question: How many points were scored in the first half?
        Extracted Information: Baltimore scored first with a field goal (3 points). Tampa Bay later tied it (3 points), but Baltimore scored two touchdowns (7 points each), to have a 17-3 halftime lead.
        Final Answer: 20 points

        Original Question: {question}
        Extracted Information: {extracted_info}
        Final Answer:
        """
        
        answer = call_llm(synthesis_prompt, system_instruction)

        # Answer checker
        verification_prompt = f"""
        Check if the answer is correct, complete, includes appropriate units, and fully answers the original question.

        Original Question: {question}
        Synthesized Answer: {answer}

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Synthesized Answer: 59 yards
        Validation: Valid

        Is the answer correct and complete? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)

        if "valid" in verification_result.lower():
            return {"is_valid": True, "answer": answer}
        else:
            print(f"Answer synthesis validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to synthesize a valid answer."}

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types
        import os  # Import the os module

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
```

=== SCRIPT FROM ITERATION 5 (Exploitation, ACCURACY: 0.80) ===
Approach: The script uses a chain-of-thought approach, decomposing a question into sub-questions, extracting relevant information, and synthesizing an answer using the Gemini LLM. It employs verification steps at each stage to ensure validity. The script defines functions for question decomposition (`decompose_question`), information extraction (`extract_information`), answer synthesis (`synthesize_answer`), and calling the LLM (`call_llm`). The `main` function orchestrates the process by calling these functions sequentially, using the output of one as input for the next, to generate the final answer.

```python
import os
import re

def main(question):
    """
    Solve the question by extracting relevant information from the passage and using chain-of-thought reasoning.
    This approach builds upon prior attempts to use question decomposition, strengthens answer synthesis, and includes examples in all LLM prompts.
    """
    try:
        # Step 1: Decompose the question into sub-questions.
        decomposition_result = decompose_question(question)
        if not decomposition_result.get("is_valid"):
            return f"Error in question decomposition: {decomposition_result.get('validation_feedback')}"
        
        # Step 2: Extract relevant information based on sub-questions.
        information_extraction_result = extract_information(question, decomposition_result["sub_questions"])
        if not information_extraction_result.get("is_valid"):
            return f"Error in information extraction: {information_extraction_result.get('validation_feedback')}"

        # Step 3: Synthesize the answer from extracted information.
        answer_synthesis_result = synthesize_answer(question, information_extraction_result["extracted_info"])
        if not answer_synthesis_result.get("is_valid"):
            return f"Error in answer synthesis: {answer_synthesis_result.get('validation_feedback')}"
        
        return answer_synthesis_result["answer"]

    except Exception as e:
        return f"An unexpected error occurred: {str(e)}"

def decompose_question(question, max_attempts=3):
    """Decompose the main question into smaller, answerable sub-questions."""
    system_instruction = "You are an expert question decomposer."
    
    for attempt in range(max_attempts):
        decomposition_prompt = f"""
        Decompose the given question into smaller, self-contained sub-questions that, when answered, will fully answer the original question.

        Example 1:
        Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions:
        1. How many yards was Chris Johnson's first touchdown?
        2. How many yards was Jason Hanson's first field goal?
        3. What is the sum of those two values?

        Example 2:
        Question: Who caught the final touchdown of the game?
        Sub-questions:
        1. Who scored the final touchdown of the game?

        Example 3:
        Question: Who had more casualties, the Colonists or the Abenakis?
        Sub-questions:
        1. How many casualties did the Colonists have?
        2. How many casualties did the Abenakis have?
        3. Who had more?

        Question: {question}
        Sub-questions:
        """
        
        decomposition_result = call_llm(decomposition_prompt, system_instruction)
        
        # Verify if the decomposition is valid
        verification_prompt = f"""
        Verify if these sub-questions are valid and sufficient to answer the original question.

        Original Question: {question}
        Sub-questions: {decomposition_result}

        Example 1:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions: 1. How many yards was Chris Johnson's first touchdown? 2. How many yards was Jason Hanson's first field goal? 3. What is the sum of those two values?
        Validation: Valid

        Example 2:
        Original Question: Who caught the final touchdown of the game?
        Sub-questions: 1. Who scored the final touchdown of the game?
        Validation: Valid
        
        Is the decomposition valid and sufficient? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)
        
        if "valid" in verification_result.lower():
            return {"is_valid": True, "sub_questions": decomposition_result}
        else:
            print(f"Decomposition validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to decompose the question successfully."}

def extract_information(question, sub_questions, max_attempts=3):
    """Extract relevant information from the passage based on the sub-questions."""
    system_instruction = "You are an information extraction expert."
    
    for attempt in range(max_attempts):
        extraction_prompt = f"""
        Given the original question and its sub-questions, extract the relevant information from the passage required to answer the sub-questions.

        Example 1:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions:
        1. How many yards was Chris Johnson's first touchdown?
        2. How many yards was Jason Hanson's first field goal?
        Extracted Information:
        Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.

        Example 2:
        Original Question: Who caught the final touchdown of the game?
        Sub-questions:
        1. Who scored the final touchdown of the game?
        Extracted Information:
        Wes Welker caught the final touchdown of the game.

        Original Question: {question}
        Sub-questions: {sub_questions}
        Extracted Information:
        """
        
        extracted_info = call_llm(extraction_prompt, system_instruction)
        
        # Validate information extraction
        verification_prompt = f"""
        Verify if the extracted information is relevant and sufficient to answer the sub-questions.

        Original Question: {question}
        Sub-questions: {sub_questions}
        Extracted Information: {extracted_info}

        Example 1:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions: 1. How many yards was Chris Johnson's first touchdown? 2. How many yards was Jason Hanson's first field goal?
        Extracted Information: Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.
        Validation: Valid

        Example 2:
        Original Question: Who caught the final touchdown of the game?
        Sub-questions: 1. Who scored the final touchdown of the game?
        Extracted Information: Wes Welker caught the final touchdown of the game.
        Validation: Valid

        Is the extraction relevant and sufficient? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)
        
        if "valid" in verification_result.lower():
            return {"is_valid": True, "extracted_info": extracted_info}
        else:
            print(f"Information extraction validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to extract relevant information successfully."}

def synthesize_answer(question, extracted_info, max_attempts=3):
    """Synthesize the answer from the extracted information to answer the main question."""
    system_instruction = "You are an answer synthesis expert. If the extracted information contains numbers, perform any necessary calculations to derive the final answer."

    for attempt in range(max_attempts):
        synthesis_prompt = f"""
        Given the original question and the extracted information, synthesize the final answer.

        Example 1:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Extracted Information: Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards.
        Final Answer: 59 yards

        Example 2:
        Original Question: Who caught the final touchdown of the game?
        Extracted Information: Wes Welker caught the final touchdown of the game.
        Final Answer: Wes Welker

        Example 3:
        Original Question: Who had more casualties, the Colonists or the Abenakis?
        Extracted Information: Colonists had 3 casualties. Abenakis had at least 31 casualties.
        Final Answer: Abenakis

        Original Question: {question}
        Extracted Information: {extracted_info}
        Final Answer:
        """
        
        answer = call_llm(synthesis_prompt, system_instruction)

        # Answer checker
        verification_prompt = f"""
        Check if the answer is correct and answers the original question fully.

        Original Question: {question}
        Synthesized Answer: {answer}

        Example 1:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Synthesized Answer: 59 yards
        Validation: Valid

        Example 2:
        Original Question: Who caught the final touchdown of the game?
        Synthesized Answer: Wes Welker
        Validation: Valid

        Is the answer correct and complete? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)

        if "valid" in verification_result.lower():
            return {"is_valid": True, "answer": answer}
        else:
            print(f"Answer synthesis validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to synthesize a valid answer."}

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types
        import os  # Import the os module

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
```

=== SCRIPT FROM ITERATION 4 (Exploitation, ACCURACY: 1.00) ===
Approach: The script uses a chain-of-thought approach, decomposing a question into sub-questions, extracting information, and then synthesizing an answer. It employs three distinct agent roles: a question decomposer, an information extraction expert, and an answer synthesis expert. It leverages the `call_llm` function to interact with the Gemini model using specifically crafted prompts for each agent, and incorporates validation steps after each stage to ensure correctness. The script begins with the `main` function, which orchestrates the calls to `decompose_question`, `extract_information`, and `synthesize_answer`; these functions in turn use `call_llm` to generate responses which undergo validation at each step.

```python
import os
import re
import math

def main(question):
    """
    Solve the question by extracting relevant information from the passage and using chain-of-thought reasoning.
    This approach builds upon a prior attempt to use question decomposition, strengthens answer synthesis, and includes examples in all LLM prompts.
    Includes arithmetic module and refines prompts for numerical questions.
    """
    try:
        # Step 1: Decompose the question into sub-questions.
        decomposition_result = decompose_question(question)
        if not decomposition_result.get("is_valid"):
            return f"Error in question decomposition: {decomposition_result.get('validation_feedback')}"
        
        # Step 2: Extract relevant information based on sub-questions.
        information_extraction_result = extract_information(question, decomposition_result["sub_questions"])
        if not information_extraction_result.get("is_valid"):
            return f"Error in information extraction: {information_extraction_result.get('validation_feedback')}"

        # Step 3: Synthesize the answer from extracted information.
        answer_synthesis_result = synthesize_answer(question, information_extraction_result["extracted_info"])
        if not answer_synthesis_result.get("is_valid"):
            return f"Error in answer synthesis: {answer_synthesis_result.get('validation_feedback')}"
        
        return answer_synthesis_result["answer"]

    except Exception as e:
        return f"An unexpected error occurred: {str(e)}"

def decompose_question(question, max_attempts=3):
    """Decompose the main question into smaller, answerable sub-questions."""
    system_instruction = "You are an expert question decomposer."
    
    for attempt in range(max_attempts):
        decomposition_prompt = f"""
        Decompose the given question into smaller, self-contained sub-questions that, when answered, will fully answer the original question.

        Example 1:
        Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions:
        1. How many yards was Chris Johnson's first touchdown?
        2. How many yards was Jason Hanson's first field goal?
        3. What arithmetic operation needs to be performed with the extracted data?

        Example 2:
        Question: Who caught the final touchdown of the game?
        Sub-questions:
        1. Who scored the final touchdown of the game?

        Example 3:
        Question: How many points did the Steelers score in the first quarter?
        Sub-questions:
        1. How many points did the Steelers score in the first quarter?

        Question: {question}
        Sub-questions:
        """
        
        decomposition_result = call_llm(decomposition_prompt, system_instruction)
        
        # Verify if the decomposition is valid
        verification_prompt = f"""
        Verify if these sub-questions are valid and sufficient to answer the original question.

        Original Question: {question}
        Sub-questions: {decomposition_result}

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions: 1. How many yards was Chris Johnson's first touchdown? 2. How many yards was Jason Hanson's first field goal? 3. What arithmetic operation needs to be performed with the extracted data?
        Validation: Valid

        Is the decomposition valid and sufficient? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)
        
        if "valid" in verification_result.lower():
            return {"is_valid": True, "sub_questions": decomposition_result}
        else:
            print(f"Decomposition validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to decompose the question successfully."}

def extract_information(question, sub_questions, max_attempts=3):
    """Extract relevant information from the passage based on the sub-questions."""
    system_instruction = "You are an information extraction expert."
    
    for attempt in range(max_attempts):
        extraction_prompt = f"""
        Given the original question and its sub-questions, extract the relevant information from the passage required to answer the sub-questions. Explicitly identify if an arithmetic operation is needed.

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions:
        1. How many yards was Chris Johnson's first touchdown?
        2. How many yards was Jason Hanson's first field goal?
        3. What arithmetic operation needs to be performed with the extracted data?
        Extracted Information:
        Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards. Operation Needed: Addition

        Original Question: {question}
        Sub-questions: {sub_questions}
        Extracted Information:
        """
        
        extracted_info = call_llm(extraction_prompt, system_instruction)
        
        # Validate information extraction
        verification_prompt = f"""
        Verify if the extracted information is relevant and sufficient to answer the sub-questions. Also confirm if the need for arithmetic operation is stated.

        Original Question: {question}
        Sub-questions: {sub_questions}
        Extracted Information: {extracted_info}

        Example:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Sub-questions: 1. How many yards was Chris Johnson's first touchdown? 2. How many yards was Jason Hanson's first field goal? 3. What arithmetic operation needs to be performed with the extracted data?
        Extracted Information: Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards. Operation Needed: Addition
        Validation: Valid

        Is the extraction relevant and sufficient? Also, is arithmetic operation needed specified? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)
        
        if "valid" in verification_result.lower():
            return {"is_valid": True, "extracted_info": extracted_info}
        else:
            print(f"Information extraction validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to extract relevant information successfully."}

def synthesize_answer(question, extracted_info, max_attempts=3):
    """Synthesize the answer from the extracted information to answer the main question."""
    system_instruction = "You are an answer synthesis expert. You can also perform arithmetic operations."

    for attempt in range(max_attempts):
        synthesis_prompt = f"""
        Given the original question and the extracted information, synthesize the final answer. If the extracted information includes numbers, perform any necessary arithmetic. If no numbers are present, respond directly based on the text.

        Example 1:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Extracted Information: Chris Johnson's first touchdown was 6 yards. Jason Hanson's first field goal was 53 yards. Operation Needed: Addition
        Final Answer: 59

        Example 2:
        Original Question: Who caught the final touchdown of the game?
        Extracted Information:  The passage states that Nate Burleson caught the final touchdown.
        Final Answer: Nate Burleson

        Original Question: {question}
        Extracted Information: {extracted_info}
        Final Answer:
        """
        
        answer = call_llm(synthesis_prompt, system_instruction)

        # Answer checker
        verification_prompt = f"""
        Check if the answer is correct, complete and based on the original question and extracted information.

        Original Question: {question}
        Synthesized Answer: {answer}

        Example 1:
        Original Question: How many yards did Chris Johnson's first touchdown and Jason Hanson's first field goal combine for?
        Synthesized Answer: 59
        Validation: Valid

        Example 2:
        Original Question: Who caught the final touchdown of the game?
        Synthesized Answer: Nate Burleson
        Validation: Valid

        Is the answer correct and complete? Respond with 'Valid' or 'Invalid'.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)

        if "valid" in verification_result.lower():
            return {"is_valid": True, "answer": answer}
        else:
            print(f"Answer synthesis validation failed (attempt {attempt+1}/{max_attempts}): {verification_result}")
            
    return {"is_valid": False, "validation_feedback": "Failed to synthesize a valid answer."}

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types
        import os  # Import the os module

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
```


            LEARNINGS FROM PREVIOUS ITERATIONS:
            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        ```
# DATASET-SPECIFIC LEARNINGS & EXPERIMENT LOG

This document tracks our evolving understanding and experimental findings related to the question answering task on the current dataset. It serves as a long-term memory to guide future research and development efforts.

## 1. DATASET PATTERNS & CHARACTERISTICS

*   **General:**
    *   The dataset consists of passages describing American football games, followed by questions about the game's details.
    *   Questions are a mix of numerical and general knowledge types, both requiring information extraction from the passage.
    *   Questions are fact-based, requiring information retrieval and processing directly from the passage.
    *   Answers are explicitly stated or directly derivable from the passage.
    *   Answers don't require external knowledge.
    *   Questions assume user access to and understanding of the passage.
    *   Passages can be lengthy, requiring careful reading.
    *   Case sensitivity is observed.
    *   The dataset includes questions requiring both numerical reasoning and general knowledge application.
    *   Questions often involve comparing or relating two or more pieces of information present in the passage.
    *   Questions frequently ask for numerical information.
    *   Questions often require the system to extract multiple numerical values from the text and perform basic arithmetic to arrive at the final answer.
    *   Questions often involve quantities, dates, and names of people or places, requiring precise extraction and comparison.
    *   Passages often present multiple entities or events, and questions require distinguishing between them and relating the correct information.
    *   Questions require synthesizing information from multiple sentences or even the entire passage. The answer isn't directly stated but needs to be inferred from disparate facts.
    *   Many questions involve interpreting percentages or proportions described in the text, requiring correct identification of the base number and appropriate conversion if needed.
    *   The questions often test the understanding of specific terminology within the context of the provided passage.
    *   Questions often require understanding of sports game summaries, particularly football.
    *   Questions often involve numerical reasoning based on events within the passage (e.g., counting touchdowns, calculating yardage differences).
    *   Questions can be complex, requiring differentiation between types of scoring events (e.g., TD pass vs. TD run) and temporal reasoning (e.g., "second half" vs. "first").

*   **Question Structure:**
    *   All questions follow the format: `"PASSAGE:\n[passage text]\n\nQUESTION: [question text]"`.
    *   Many questions involve numerical reasoning and comparison (e.g., "How many yards longer...", "Which star has a smaller mass...").
    *   Some questions ask for specific entities (e.g., "Who caught the final touchdown...", "Who threw the second longest touchdown pass?").
    *   Questions frequently ask for numerical information (e.g., "How many...?", "How many yards...?").

*   **Answer Structure:**
    *   Answers are typically short phrases, numbers, or names.

*   **Passage Structure:**
    *   Passages vary in content (sports summaries, scientific descriptions like astronomy).
    *   Sports passages often involve more complex event tracking (who did what, in what order), which pose extraction challenges.
    *   Passages are dense with numbers, requiring precise parsing, especially in sports narratives.

*   **Domain Knowledge:**
    *   Basic sports terminology (football positions, scoring) is needed for sports-related examples.
    *   General reading comprehension is crucial.
    *   For demographic examples, basic understanding of fertility rates is needed.

*   **Question Types:**
    *   Entity Extraction: Identifying specific players or entities.
    *   Numerical Comparison: Comparing numerical values (yards, scores, TFR, mass) to determine differences.
    *   Counting: Counting the number of occurrences of an event or entity.

*   **Reasoning Types:**
    *   Direct Extraction: Finding the answer directly stated in the passage.
    *   Simple Arithmetic: Performing basic calculations (addition, subtraction) using numbers from the passage.
    *   Logical Deduction: Combining information from different parts of the passage to arrive at the answer.
*   **Non-Obvious Patterns:**
    *   Chronological order of events is important. Time-related keywords (e.g., "final," "first," "later," "second") help narrow the search.

*   **Edge Cases/Complexities:**
    *   Passages with ambiguous or contradictory information.
    *   Questions requiring more complex arithmetic operations (e.g., multiplication, division).
    *   Questions with implicit rather than explicit answers, requiring deeper inference.
    *   Handling of units (yards, points, etc.) consistently.
    *   Sports narratives can induce errors because they require an understanding of event order and length, and they need accurate extraction of details such as players and actions.
    *   The questions require numerical reasoning and extraction from a provided passage. The answers are often derived through simple arithmetic operations on the extracted numbers.
    *   Many questions involve interpreting percentages or proportions described in the text, requiring correct identification of the base number and appropriate conversion if needed.
    *   The questions often test the understanding of specific terminology within the context of the provided passage.

## 2. EFFECTIVE TASK-SPECIFIC STRATEGIES

*   **Keyword-Based Retrieval:** Identify keywords from the question and search for those keywords in the passage to narrow down relevant sections.
*   **LLM-Based Extraction & Reasoning:** Use an LLM to directly answer the question based on the passage. Prompting is key here.
*   **Hybrid Approach:** Combine keyword retrieval with LLM reasoning. Use keywords to identify relevant sections, then use the LLM to extract the answer from those sections.
*   **Question Decomposition:** Decompose the question into smaller sub-questions that are easier to answer.
    *   Question decomposition appears effective because it breaks down complex questions into smaller, more manageable extraction tasks, suitable for straightforward information retrieval. This likely reduces the load on the LLM and enhances accuracy.
*   **Chain-of-Thought Reasoning (CoT):** Breaking down complex questions into smaller, more manageable steps (decomposition, information extraction, synthesis) improves the interpretability of the system's reasoning process and is generally helpful for this dataset. The chain-of-thought approach has some utility because many of the questions require multiple steps to solve. Decomposing the question into sub-questions allows the system to focus on specific aspects of the passage and break down the reasoning process. However, its effectiveness is limited by the system's ability to accurately understand the question's intent and extract relevant numerical data.
*   **Zero-Shot Reasoning:** Directly ask the LLM to answer the question based on the passage, using a well-crafted prompt.
*   **Text Splitting:** If passages are too long, split them into smaller chunks and process each chunk separately. Be mindful of context loss.
*   **Focus on Answer Synthesis:** Prioritize improving the `synthesize_answer` function.
*   **Specialized Agents:** The use of specialized agents (question decomposer, information extraction expert, answer synthesis expert) allows for a modular approach. Each agent can be fine-tuned for its specific role, leading to better performance. The individual role definitions for the LLMs (question decomposer, information extractor, answer synthesizer) needs more definition.
*   **Sequential Workflow:** The sequential workflow (decompose, extract, synthesize) enforces a structured problem-solving approach that helps to systematically work towards the answer.
*   **Question-Type Determination and Specialized Processing:** Determine the question type (numerical vs. general) and use specialized processing. Achieving 100% accuracy (in Iterations 1 and 2) suggests this strategy is highly effective. However, Iteration 8 demonstrates that a failure in question type determination can render the entire system useless.
*   **LLM-Driven Techniques with Verification:** Combining LLM-driven techniques with verification steps at each stage is a successful pattern. Verification steps at each stage can help to ensure the validity of the extracted information and the correctness of the reasoning process. This can help to prevent errors from propagating through the chain of thought. However, verification is inadequate for semantic errors that stem from misinterpreting the question.
*   **Dynamic Approach Selection:** Adapting the strategy based on the type of question being asked proves important.
*   **Extraction of Relevant Information:** The system has a strength in extracting relevant information from the passage.
*   **Leveraging distinct agent roles:** Question decomposer, information extraction expert, answer synthesizer help specialize the tasks and allows for more focused prompt engineering for each role.
*   **Chain-of-thought for focusing information extraction:** Aids in focusing the information extraction process, especially when dealing with questions requiring comparisons or multi-step reasoning.
*   **Validating intermediate steps:** Decomposition and extraction likely contributes to the overall accuracy, ensuring that incorrect or incomplete information doesn't propagate through the system.
*   Providing examples in LLM prompts seems to improve performance, as mentioned in the approach summary. It guides the LLM to perform numerical and logical operations in the desired way.
*   Explicitly prompting the LLM to show its work (intermediate calculations) helped in debugging and identifying where errors occurred in the numerical reasoning process.
*   **Intended Strategy (Iteration 7 - Unsuccessful):** Using a chain-of-thought approach to determine question type, extract information, and synthesize answers, combined with verification, is a reasonable starting point.
*   **(Iteration 8):** No strategies worked effectively due to a complete failure of the question type determination.

## 3. COMMON FAILURE MODES ON THIS DATASET

*   **General Script Errors:**
    *   `NameError: name 'call_llm' is not defined`. This single error prevents any actual processing of the questions and passages (ITERATION 7).
*   **Root Cause: Incorrect Question Type Determination (Iteration 8):** The entire system fails because the `determine_question_type()` function consistently returns an error. All questions, including those requiring numerical and general reasoning, could not be processed. This initial failure prevents any subsequent processing steps from executing correctly, leading to 0% accuracy. This highlights that the question type determination module is a single point of failure.
*   **Answer Synthesis Failure:** Inability to synthesize a valid answer. This is the most prominent failure mode. This suggests a problem with the agent responsible for generating the final answer from the extracted information or a mismatch between the expected answer format and the format being generated. Even when the system extracts the correct information, it sometimes fails to provide the answer in the format expected by the gold standard. It might provide a descriptive statement instead of a single numerical value (e.g., describing Jackson's pass instead of providing the yardage number).
    *   The system struggles to provide fully complete answers, especially when context, units, or modifiers from the original passage are needed. This suggests a failure in the synthesis stage to fully integrate the extracted information into a complete and nuanced answer.
*   **Sports narrative complexity:** The questions about sports narratives such as "Who threw the second longest touchdown pass?" can induce errors because they require an understanding of event order and length and they need accurate extraction of details such as players and actions. If the model fails to correctly extract who completed each pass and their distance, it will fail to determine the second-longest pass.
*   **Validation sensitivity:** Validation checks can be overly strict. For example, if the correct answer is `Brett Favre` but the agent responds `Brett Favre.`, the answer would be marked as incorrect.
*   **Information Extraction Failure:** Errors in information extraction contribute to failures in answer synthesis. Inaccurate or incomplete extracted information leads to incorrect final answers.
*   **Ambiguous Passages:** Passages with ambiguous or contradictory information can lead to incorrect answers.
*   **Missing Information:** If the answer cannot be found in the passage, this leads to a failure.
*   **Lack of Arithmetic Reasoning:** The most prominent failure mode observed in Iteration 3 is the inability to perform arithmetic operations, particularly simple addition. The system can identify the relevant numbers but fails to combine them to produce the correct answer (e.g., failing to add 12 and 10 to get 22 for the warship question). Arithmetic Errors continue to be a major failure mode, even simple addition/subtraction fails, leading to incorrect final answers.
*   **Potential for Masked Limitations:** With a 100% accuracy rate (in Iterations 1, 2, and 4), there is a potential that the current dataset isn't challenging enough and the system's vulnerability to more complex reasoning, arithmetic, or inference is masked.
*   **Potential Failure Modes (Unobserved in Iterations 1, 2 and 4 due to 100% accuracy):**
    *   **Numerical extraction errors:** Incorrectly identifying or extracting numerical values from the passage.
    *   **Calculation errors:** Performing incorrect arithmetic operations on extracted numbers.
    *   **Information extraction errors:** Failing to identify and extract the correct information to answer general questions.
    *   **Synthesis errors:** Generating a response that is not a coherent or accurate answer to the question.
    *   **Question type misclassification**: Incorrectly classifying a question as numerical or general.
*   **Unit Handling:** Numerical questions may suffer from a lack of proper unit handling, leading to incomplete or misinterpreted answers. For instance, failing to explicitly include "yards" when answering a question about field goal distances. The LLM sometimes fails to correctly interpret or apply units (e.g., percentages, numbers, counts), mixing them up or outputting them when only a raw number is expected.
*   **Misinterpretation of Question Intent:** The system struggles to accurately interpret the nuances of the question. In the example, "How many points got the Rams on the board?", the system calculates the *total* points from field goals instead of just the *number* of points for a single field goal. This highlights a failure in semantic understanding and correctly scoping the question's intent.
*   **Incorrect Numerical Calculation:** The system makes errors in numerical calculations, such as adding or subtracting values incorrectly, or using the wrong numbers from the passage. In the second error example, the system incorrectly calculates the time difference.
*   **Lack of Verification:** The system lacks a final answer verification step to check the reasonableness of the answer given the context of the question. For example, a significantly wrong numerical answer or the incorrect unit should be caught.
*   **Numerical Extraction in Dense Passages:** Difficulty in accurately extracting numerical values and associating them with the correct entities (players, events). The passages are dense with numbers, requiring precise parsing, especially in sports narratives.

## 4. EXPERIMENT LOG & FINDINGS

*   **2025-05-17 13:35:33: INITIAL DATASET ANALYSIS**
    *   Comprehensive analysis of dataset characteristics, challenges, potential approaches, and implementation recommendations.
    *   Identified key aspects of question and answer structure, domain knowledge required, and reasoning types.
    *   Highlighted potential solution strategies: Keyword-Based Retrieval, LLM-Based Extraction & Reasoning, Hybrid Approach.
    *   Outlined decomposition steps: Question Analysis, Passage Filtering, Information Extraction, Answer Generation.
    *   Proposed validation techniques: Consistency Checks, Unit Analysis, Numerical Validation, Fact Verification.
    *   Recommended implementation steps: Keyword Matching, Numerical Range Checks, Extracted Facts Representation, Relevant Sentences Storage.
    *   Prompt engineering techniques for effective text processing were proposed, including chain-of-thought prompting.
*   **ITERATION 0:**
    *   **Accuracy:** 0.67
    *   **Goal:** Improve performance using question decomposition and reasoning.
    *   **Finding:** The implementation of question decomposition has challenges with reliable answer synthesis.
    *   **Finding:** Frequent validation failures indicate problems with the downstream steps, even if decomposition is successful.
    *   **Finding:** Sports-related questions involving temporal reasoning (e.g., "Who threw the second longest touchdown pass?") pose significant challenges.
    *   **Insight:** Validation is helpful for diagnosing system performance, specifically highlighting failures in answer synthesis.
*   **ITERATION 1:**
    *   **Accuracy:** 1.00
    *   **Goal:** Improve performance using question decomposition and reasoning.
    *   **Finding:** The 100% accuracy rate indicates the system is very effective at information extraction and simple reasoning within the context of the dataset, likely due to the effectiveness of the chosen architecture (decomposition, extraction, synthesis).
    *   **Insight:** The high accuracy masks potential limitations related to more complex reasoning, arithmetic, or inference, which requires further investigation using more challenging datasets.
*   **ITERATION 2:**
    *   **Accuracy:** 1.00
    *   **Goal:** Improve performance using question-type determination and specialized processing.
    *   **Finding:** High accuracy suggests the strategy of question-type determination followed by specialized processing (numerical vs. general questions) is highly effective for this limited sample.
    *   **Finding:** The combination of rule-based processing (numerical calculations) and LLM-based reasoning (general questions) can effectively address question-answering in this domain.
    *   **Finding:** The successful implementation of the dynamic approach selection emphasizes the importance of adapting the strategy based on the type of question being asked.
    *   **Caveat:** The lack of error cases makes it impossible to determine the system's limitations definitively.
*   **ITERATION 3:**
    *   **Goal:** Improve performance, leveraging Chain-of-Thought (CoT) for question decomposition.
    *   **Finding:** CoT is a beneficial approach for this type of question answering task, particularly for breaking down complex questions.
    *   **Finding:** A significant limitation is the system's lack of arithmetic reasoning capabilities which prevents it from achieving higher accuracy on questions that require numerical calculations.
    *   **Finding:** The individual role definitions for the LLMs (question decomposer, information extractor, answer synthesizer) needs more definition.
*   **ITERATION 4:**
    *   **Accuracy:** 1.0
    *   **Goal:** Improve answer completeness, focusing on context, units, and modifiers.
    *   **Finding:** Achieving 1.0 accuracy suggests that the overall strategy of chain-of-thought, role-based agents, and validation is effective for this type of reading comprehension task, particularly for this dataset.
    *   **Finding:** The primary area for improvement is not in the accuracy of information extraction, but in the completeness and context of the final answer.
*   **ITERATION 5:**
    *   **Goal:** Improve semantic understanding and numerical reasoning capabilities.
    *   **Finding:** Chain-of-thought benefits are diminished by semantic errors in question interpretation.
    *   **Finding:** Verification steps are inadequate for addressing semantic errors.
    *   **Finding:** The system struggles to accurately interpret the nuances of the question. In the example, "How many points got the Rams on the board?", the system calculates the *total* points from field goals instead of just the *number* of points for a single field goal.
    *   **Finding:** The system makes errors in numerical calculations, such as adding or subtracting values incorrectly, or using the wrong numbers from the passage.
*   **ITERATION 6:**
    *   **Accuracy:** 0.80
    *   **Goal:** Improve semantic understanding and numerical reasoning capabilities by leveraging chain-of-thought, prompt examples, and showing work.
    *   **Finding:** The chain-of-thought approach with question decomposition, information extraction, and answer synthesis showed promise. The decomposition helps break down complex questions into manageable steps.
    *   **Finding:** Providing examples in LLM prompts seems to improve performance, guiding the LLM to perform numerical and logical operations in the desired way.
    *   **Finding:** Explicitly prompting the LLM to show its work (intermediate calculations) helped in debugging and identifying where errors occurred in the numerical reasoning process.
    *   **Finding:** Inconsistent and unreliable arithmetic calculations are a major failure mode. Even simple addition/subtraction fails, leading to incorrect final answers.
    *   **Finding:** The LLM sometimes fails to correctly interpret or apply units (e.g., percentages, numbers, counts), mixing them up or outputting them when only a raw number is expected.
    *   **Finding:** The system lacks a final answer verification step to check the reasonableness of the answer given the context of the question. For example, a significantly wrong numerical answer or the incorrect unit should be caught.
    *   **Finding:** The primary hypothesis, that breaking the problem into smaller steps and using examples would improve accuracy, was partially validated, but the arithmetic and unit errors indicate a need for more robust numerical reasoning capabilities.
*   **ITERATION 7:**
    *   **Accuracy:** 0.0
    *   **Goal:** Improve semantic understanding and numerical reasoning capabilities.
    *   **Finding:** This iteration provided no usable data due to the undefined `call_llm` function.
    *   **Finding:** The script is fundamentally broken without the `call_llm` function.
    *   **Finding:** The hypothesis that a chain-of-thought approach could solve these questions remains untested.
    *   **Finding:** The experiment results overwhelmingly indicate the importance of defining all necessary functions before running an experiment.
    *   **Error Log:**
        *   2025-05-17 13:51:39: INITIAL DATASET ANALYSIS - NameError: name 'call_llm' is not defined
        *   2025-05-17 13:51:51: INITIAL DATASET ANALYSIS - Question type validation failed and error in determining question type.
        *   2025-05-17 13:52:05: INITIAL DATASET ANALYSIS - Question type validation failed and question type could not be determined.
*   **ITERATION 8:**
    *   **Accuracy:** 0.0
    *   **Goal:** Improve semantic understanding and numerical reasoning capabilities.
    *   **Finding:** The `determine_question_type()` function consistently failed, leading to a complete system failure.
    *   **Finding:** The hypothesis that a modular approach with question-type determination followed by specialized processing would improve accuracy is rejected in its current form. The question type determination module is a single point of failure and renders the system useless when it fails.
    *   **Error Log:**
        *   2025-05-17 13:53:35: SCRIPT ERROR - Error detected during script repair (attempt 1): ERROR: Script failed to determine question type.
        *   2025-05-17 13:53:56: SCRIPT ERROR - Error detected during script repair (attempt 2): ERROR: Gemini API calls failing with BLOCK_NONE, question type validation failing, and inability to determine question type.
        *   2025-05-17 13:54:16: SCRIPT ERROR - Error detected during script repair (attempt 3): ERROR: Script failed due to google.genai module error and inability to determine question type.

## 5. NEXT RESEARCH DIRECTIONS

*   **Define or import `call_llm` immediately.** This is the absolute priority (from Iteration 7 findings).
*   **After fixing the `call_llm` issue, re-run the experiment to gather meaningful data (from Iteration 7 findings).**
*   **Critically, redesign/debug `determine_question_type()`.** (From Iteration 8 findings) Consider:
    *   **More examples:** Providing significantly more examples to the LLM prompt for question type classification. Ensure diverse examples covering all question types expected in the dataset.
    *   **Simpler prompt:** Attempting a simpler prompt focused solely on question classification without attempting more complex chain-of-thought.
    *   **Fine-tuning:** Fine-tuning a smaller, specialized LLM on question type classification, which might offer more consistent performance and lower latency than relying solely on prompts with a large general model.
    *   **Direct few-shot classification:** Prompt the LLM to directly classify questions with a few examples without chain-of-thought.
*   **Implement Validation & Fallback:** Add validation logic to `determine_question_type()` with a fallback mechanism. If the LLM classification is uncertain or contradicts simple heuristics (e.g., question contains "how many" implying a numerical question), use a default question type or trigger a different processing path. (From Iteration 8 findings).
*   **Error Handling:** Implement better error handling throughout the system to provide more informative debugging messages when individual steps fail. (From Iteration 8 findings).
*   **Analyze the results of the re-run. Focus on the accuracy of `determine_question_type`, `extract_numerical_info`, and `extract_information`. These are the modules most closely tied to the specifics of this dataset's structure (from Iteration 7 findings).**
*   **If the above modules are problematic, refine the prompts used by `call_llm` within each of those functions, providing more examples specific to football game summaries and question types. Pay attention to how well the LLM can distinguish between different numerical values and correctly attribute them to the appropriate entities (from Iteration 7 findings).**
*   **Implement a dedicated numerical reasoning module:** Integrate a calculator or numerical reasoning tool to perform arithmetic operations. This ensures accurate calculations instead of relying on the LLM for computation.
*   **Add Unit Verification:** Implement a module that checks the predicted unit against the expected unit based on the question. For example, if the question asks for a percentage, the output must include "%".
*   **Implement Answer Sanity Checks:** Add a final step to evaluate the answer for sanity. This could include checking if the answer falls within a reasonable range given the context. For example, if the question asks for the number of people, a negative number or a value exceeding the population size would be flagged.
*   **Enhance training examples for numerical and unit understanding:** Fine-tune the LLM with more examples that focus on numerical reasoning and unit conversions, highlighting correct arithmetic operations and unit handling.
*   **Test on edge cases:** Create specific test cases that target potential arithmetic errors, incorrect unit handling, and misunderstanding of specific terminology to proactively identify and address weaknesses.
*   **Enhance Semantic Understanding:** Focus on improving the system's ability to understand the nuances of the questions. This could involve using techniques like:
    *   Fine-tuning the LLM on a dataset of question-passage pairs with detailed annotations of the question's intent.
    *   Implementing a module that specifically focuses on paraphrasing the question in simpler terms to clarify its meaning.
*   **Improve Numerical Reasoning:** Implement more robust numerical reasoning techniques, such as:
    *   Using a dedicated numerical reasoning module that can perform calculations and comparisons.
    *   Implementing a unit-checking mechanism to ensure that the extracted values are in the correct units.
*   **Refine Verification Strategies:** Adapt the verification steps to specifically target potential semantic errors. For instance, the system could be trained to identify ambiguous words or phrases in the question and request clarification from the user.
*   **Enhance the answer synthesis stage:** Modify the prompt for the answer synthesis agent to explicitly request that it include all relevant context, units, and modifiers from the original passage. Use examples in the prompt that demonstrate the desired level of completeness.
*   **Improve unit handling:** Implement a more robust system for tracking and including units in numerical answers. This could involve a unit extraction step in the information extraction phase or a dedicated unit verification step in the answer synthesis phase.
*   **Post-processing for completeness:** Explore post-processing steps to check for missing information (e.g., units, quantities) and add them to the answer if missing. This could involve comparing the generated answer to the original passage and identifying any missing elements.
*   **Arithmetic Reasoning Module:** Integrate a dedicated arithmetic reasoning module into the answer synthesis step. This module should be capable of identifying numerical values in the extracted information and performing basic arithmetic operations.
*   **Refine Answer Synthesis Prompt:** Refine the prompt for the answer synthesis LLM to explicitly instruct it to perform calculations when necessary and provide the answer in the expected format (e.g., a single number when the question asks "How many...?").
*   **Include Examples of Arithmetic Reasoning in Prompts:** Add examples to the prompts for all LLM calls that explicitly show how to extract numbers and perform calculations, guiding the model to follow a similar pattern.
*   **Evaluate Different LLMs for Numerical Reasoning:** Evaluate different LLMs that are known to have better numerical reasoning capabilities to see if they can improve performance on this task.
*   **Answer Synthesis Improvement:**
    *   Debug and improve the `synthesize_answer` function.
    *   Examine the input it receives (extracted information) and the logic used to generate the final answer.
    *   Pay close attention to the expected answer format and ensure it is being correctly generated.
    *   Include thorough error logging to pinpoint where in the answer synthesis process the errors are occurring.
*   **Refine Validation Logic:**
    *   Review the validation checks to ensure they are not overly strict.
    *   Accurately reflect the expected answer format for this dataset, allowing for minor variations in phrasing or punctuation.
*   **Evaluate Information Extraction Success:**
    *   Assess how frequently information extraction fails.
    *   Use the validation component to diagnose problems with extraction.
*   **Dataset Split & Analysis:**
    *   Split the dataset by passage type (e.g., sports, science).
    *   Analyze performance on each subset to identify specific challenges.
    *   Focus on improving performance on the challenging sports scenarios that require temporal information and comparisons.
*   **Prompt Engineering:**
    *   Experiment with different prompt formulations for both information extraction and answer synthesis.
    *   Explore techniques like few-shot learning and chain-of-thought prompting to improve LLM reasoning.
*   **Context Window Management:**
    *   Implement strategies for handling long passages, such as text splitting and summarization.
    *   Ensure that relevant context is preserved when splitting passages.
*   **Error Analysis:**
    *   Manually analyze failed examples to identify the root cause of the errors.
    *   Categorize errors based on the type of question, passage, and reasoning required.
*   **External Knowledge Integration:**
    *   Investigate the use of external knowledge sources to augment the information provided in the passage.
    *   This may be helpful for resolving ambiguities or providing additional context.
*   **Introduce Complexity:** Adapt the dataset to include questions that require more sophisticated reasoning, multi-step calculations, or inference beyond simple information extraction. For example, asking for the *ratio* between two demographic groups, or asking a question that needs a calculation based on multiple sentences.
*   **Error Analysis on New Failures:** If/When the accuracy decreases after introducing more complex questions, conduct detailed error analysis on the failures to pinpoint the specific limitations of the current approach. For each failure, identify which agent failed (decomposer, extractor, or synthesizer) and why.
*   **Arithmetic/Reasoning Modules:** If quantitative reasoning is a common point of failure, consider integrating dedicated arithmetic or reasoning modules into the system.
*   **Introduce a more diverse set of questions:** Expand the dataset with more complex numerical reasoning problems (multi-step calculations, unit conversions) and more nuanced general knowledge questions requiring deeper understanding and inference.
*   **Focus on edge cases:** Craft specific test cases to target potential failure modes, especially in numerical extraction (e.g., questions involving ranges of numbers, percentages, or implied quantities) and information synthesis (e.g., questions requiring comparison of information from different parts of the passage).
*   **Implement error logging and analysis:** Add detailed logging to track the execution of each step (question type identification, information extraction, calculation, synthesis) and capture any errors or inconsistencies. This will provide valuable data for identifying and addressing failure modes.
*   **Evaluate the robustness of the question type classifier:** Test the question type identification module with ambiguous or borderline cases to assess its reliability and improve its accuracy.
*   **Stress-test with ambiguous passages:** Use passages that contain contradictory, incomplete, or misleading information to see how the system handles uncertainty and whether it can identify potential errors in the text.
```
        

            CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        

            EXPLORATION GUIDANCE:
            1. Review the historical approaches, error patterns, and accumulated learnings carefully
            2. Review the FULL CODE of previous scripts to understand what has already been tried
            3. Design a new approach that is DISTINCTLY DIFFERENT from previous attempts. This approach should have a specific NEW HYPOTHESIS or variable you are trying to test. 
            4. CRITICAL: Include EMBEDDED EXAMPLES directly within your LLM prompts
            5. For each key function, show a complete worked example, or include multiple examples, including:
               - Input example that resembles the dataset
               - Step-by-step reasoning through the example
               - Properly formatted output
            6. Apply the insights from the ACCUMULATED LEARNINGS section to avoid repeating past mistakes
            7. Pay SPECIAL ATTENTION to the weaknesses and improvement suggestions from the capability assessment
            8. Consider implementing one or more of these LLM usage patterns:
               - Repeated validation with feedback loops
               - Multi-perspective analysis with synthesis
               - Dynamic input-dependent routing with an orchestrator
               - Hybrid approaches combining LLM with deterministic functions
               - Best-of-n solution generation and selection
               - ReAct pattern for interactive reasoning and action
               - If it is unknown how successful a processing state or part of the pipeline is, include verification steps to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
               - Answer checkers to validate the final answer against the problem statement. If the answer is incorrect, the checker can send the answer back to an earlier part of the system for for refinement with feedback

            Here's how to call the Gemini API. Use this example without modification and don't invent configuration options:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

            Since this is an EXPLORATION phase:
            - Try a fundamentally different approach to reasoning about the problem. Test a NEW HYPOTHESIS or variable, and add verification steps to deduce if this new change is helpful.
            - THIS IS KEY: Break down the problem into new, distinct reasoning steps based on past performance before you start coding
            - For EACH key LLM prompt, include a relevant example with:
              * Sample input similar to the dataset
              * Expected reasoning steps
              * Desired output format
            - Apply a verifier call to different parts of the pipeline in order to understand what parts of the pipeline of calls is successful and where the system is breaking
            - Pay special attention to addressing the primary issues from previous iterations
            - Ensure your new approach addresses the weaknesses identified in the capability assessment

            CRITICAL REQUIREMENTS:
            1. The script MUST properly handle all string literals - be extremely careful with quotes and triple quotes
            2. The script MUST NOT exceed 150 lines of code to prevent truncation
            3. Include detailed comments explaining your reasoning approach
            4. EVERY SINGLE LLM PROMPT must include at least one embedded example showing:
               - Sample input with reasoning
               - Desired output format
            5. Make proper use of error handling
            6. Implement robust capabilities to address the specific weaknesses identified in the capability assessment
            7. Do NOT use json.loads() in the LLM calls to process input data. JSON formatting is good to use to structure information as inputs and outputs, but attempting to have functions process JSON data explicitly with strict built-in functionality is error prone due to formatting issues and additional text that appears as documentation, reasoning, or comments. When passing data into another LLM call, you can read it as plain text rather than trying to load it in strict json format, is the better approach.

            Return a COMPLETE, RUNNABLE Python script that:
            1. Has a main function that takes a question string as input and returns the answer string
            2. Makes multiple LLM calls for different reasoning steps
            3. Has proper error handling for API calls
            4. Includes embedded examples in EVERY LLM prompt
            5. Is COMPLETE - no missing code, no "..." placeholders
            6. Closes all string literals properly

            This should be FUNDAMENTALLY DIFFERENT from all previous approaches. Do not reuse the same overall structure.

            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            