
            You are developing a Python script to solve problems using LLM reasoning capabilities.
            You are in the EXPLORATION PHASE. You must generate a NEW approach that's different from previous approaches but informed by their successes and failures. With this approach, you will have a specific NEW HYPOTHESIS or variable you are trying to test. Your goal is to see if this new approach works, and you must add verification and validation steps to deduce if this new change is helpful. You may also test RADICAL NEW APPROACHES that are substantially different from previous approaches. 
            
            You should try NEW THINGS:
            
            Break down the problem into smaller pieces
            Think CREATIVELY about how to solve your problem if other approaches aren't working
            Transform data into different formats to see if it helps

            # YOUR TASK
            You are deeply familiar with prompting techniques and the agent works from the literature. 
            Your goal is to maximize the specified performance metrics by proposing interestingly new agents.
            Observe the past discovered agents and scripts carefully and think about what insights, lessons, or stepping stones can be learned from them.
            Be creative when thinking about the next interesting agent to try. You are encouraged to draw inspiration from related agent papers or academic papers from other research areas.
            Use the knowledge from the archive and inspiration from academic literature to propose the next interesting agentic system design.
            THINK OUTSIDE THE BOX.
            

            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "Let $n$ be a natural number with exactly 2 positive prime divisors.  If $n^2$ has 27 divisors, how many does $n$ have?",
    "answer": "Let $p$ and $q$ be the prime divisors of $n$, so we can write $n = p^a \\cdot q^b$ for positive integers $a$ and $b$.  This means $n^2 = p^{2a} \\cdot q^{2b}$, so $t(n^2) = (2a + 1)(2b + 1) = 27$.  Since $2a + 1$ and $2b + 1$ are both greater than 1 and are divisors of 27, we know they are 3 and 9 (in no particular order).  This means that $a$ and $b$ are 1 and 4 (in no particular order), so $$ t(n) = (a + 1)(b + 1) = (1 + 1)(4 + 1) = \\boxed{10}. $$"
  },
  {
    "id": 1,
    "question": "The point $P$ on the sphere of radius 3 and centered at the origin has spherical coordinate $\\left( 3, \\frac{3 \\pi}{8}, \\frac{\\pi}{5} \\right).$  Find the spherical coordinates of the point diametrically opposite $P.$  Enter your answer in the form $(\\rho,\\theta,\\phi),$ where $\\rho > 0,$ $0 \\le \\theta < 2 \\pi,$ and $0 \\le \\phi \\le \\pi.$",
    "answer": "The point $P$ is determined by the angles $\\theta$ and $\\phi,$ as shown below.\n\n[asy]\nimport three;\n\nsize(180);\ncurrentprojection = perspective(6,3,2);\n\ntriple sphericaltorectangular (real rho, real theta, real phi) {\n  return ((rho*Sin(phi)*Cos(theta),rho*Sin(phi)*Sin(theta),rho*Cos(phi)));\n}\n\ntriple O, P;\n\nO = (0,0,0);\nP = sphericaltorectangular(1,60,45);\n\ndraw((-1,0,0)--(1,0,0),Arrow3(6));\ndraw((0,-1,0)--(0,1,0),Arrow3(6));\ndraw((0,0,-1)--(0,0,1),Arrow3(6));\ndraw(surface(O--P--(P.x,P.y,0)--cycle),gray(0.7),nolight);\ndraw(O--P--(P.x,P.y,0)--cycle);\ndraw((0,0,0.5)..sphericaltorectangular(0.5,60,45/2)..sphericaltorectangular(0.5,60,45),Arrow3(6));\ndraw((0.4,0,0)..sphericaltorectangular(0.4,30,90)..sphericaltorectangular(0.4,60,90),Arrow3(6));\n\nlabel(\"$x$\", (1.1,0,0));\nlabel(\"$y$\", (0,1.1,0));\nlabel(\"$z$\", (0,0,1.1));\nlabel(\"$\\phi$\", (0.2,0.25,0.6));\nlabel(\"$\\theta$\", (0.6,0.15,0));\nlabel(\"$P$\", P, N);\n[/asy]\n\nFor the point diametrically opposite $P,$ $\\theta' = \\theta + \\pi$ and $\\phi' = \\pi - \\phi.$\n\n[asy]\nimport three;\n\nsize(180);\ncurrentprojection = perspective(6,3,2);\n\ntriple sphericaltorectangular (real rho, real theta, real phi) {\n  return ((rho*Sin(phi)*Cos(theta),rho*Sin(phi)*Sin(theta),rho*Cos(phi)));\n}\n\ntriple O, P, Q;\n\nO = (0,0,0);\nP = sphericaltorectangular(1,60,45);\nQ = sphericaltorectangular(1,240,135);\n\ndraw(surface(O--Q--(Q.x,Q.y,0)--cycle),gray(0.7),nolight);\ndraw((-1,0,0)--(1,0,0),Arrow3(6));\ndraw((0,-1,0)--(0,1,0),Arrow3(6));\ndraw((0,0,-1)--(0,0,1),Arrow3(6));\ndraw(O--P--(P.x,P.y,0)--cycle);\ndraw(O--Q--(Q.x,Q.y,0)--cycle);\ndraw((0,0,0.5)..sphericaltorectangular(0.5,240,135/2)..sphericaltorectangular(0.5,240,135),Arrow3(6));\ndraw((0.4,0,0)..sphericaltorectangular(0.4,120,90)..sphericaltorectangular(0.4,240,90),Arrow3(6));\n\nlabel(\"$x$\", (1.1,0,0));\nlabel(\"$y$\", (0,1.1,0));\nlabel(\"$z$\", (0,0,1.1));\nlabel(\"$\\phi'$\", (-0.2,-0.4,0.4));\nlabel(\"$\\theta'$\", (-0.6,0.25,0));\nlabel(\"$P$\", P, N);\n[/asy]\n\nHence, the spherical coordinates of the point diametrically opposite $P$ are $\\left( 3, \\frac{3 \\pi}{8} + \\pi, \\pi - \\frac{\\pi}{5} \\right) = \\boxed{\\left( 3, \\frac{11 \\pi}{8}, \\frac{4 \\pi}{5} \\right)}.$"
  },
  {
    "id": 2,
    "question": "How many fractions in the form $\\frac{n}{99}$, with $0<n<99$, are in lowest terms?",
    "answer": "The prime factorization of 99 is $3^2\\cdot11$. So for $\\frac{n}{99}$ to be in lowest terms, $n$ cannot be divisible by 3 or 11. The possible values of $n$ are from 1 to 98, inclusive, so there are 98 possible values for $n$. We can find the number of multiples of 3 and multiples of 11 and subtract from 98 to get the number of values that aren't divisible by 3 or 11. For multiples of 3, we go from 3 to 96, or $3\\cdot1$ to $3\\cdot32$, so there are 32 multiples of 3 from 1 to 98, inclusive. For multiples of 11, we go from 11 to 88, so there are 8 multiples of 11 from 1 to 98, inclusive. We have to make sure we don't double-count the numbers that are multiples of both 3 and 11: 33 and 66. So there are $32+8-2=38$ values of $n$ that are divisible by 3 or 11. That means there are $98-38=\\boxed{60}$ values of $n$ for which $\\frac{n}{99}$ is in lowest terms."
  }
]

            HISTORICAL CONTEXT:
            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 2
        - Current explore/exploit balance: 60/20
        - Best accuracy achieved: 0.50 (iteration 0)

        APPROACH HISTORY (last 2 iterations):
        [
  {
    "iteration": 0,
    "strategy": "baseline",
    "accuracy": 0.5,
    "approach": "Simple baseline script: Direct LLM call without sophisticated techniques"
  },
  {
    "iteration": 1,
    "strategy": "refine",
    "accuracy": 0.0,
    "approach": "The script implements a chain-of-thought approach to answer a question by breaking it down into sub-questions, answering each sub-question independently, and then synthesizing the individual answers into a final response. The `main` function orchestrates this process, using `call_llm` to interact with the Gemini model for question breakdown, answering sub-questions, and synthesizing the final answer. No agent roles are explicitly defined. The `call_llm` function is used to send prompts to the LLM and return the response, `main` takes the question and orchestrates the calls to `call_llm` to get the sub-questions, answers to sub-questions, and a final synthesis. The overall workflow involves question decomposition, answering sub-questions, and synthesizing the final answer."
  }
]

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 2 iterations):
        [
  {
    "iteration": 0,
    "issue": "The most critical problem to fix is the **inaccuracy in performing arithmetic and logical calculations**. This includes median calculation, LCM calculations, and general numerical manipulation errors within algebraic solutions. This undermines the entire solution process, even if the initial problem setup and equation formulation are correct."
  },
  {
    "iteration": 1,
    "issue": "The most critical problem is the consistent failure of the LLM call due to a `NoneType` argument. This suggests a data processing stage before the LLM call is producing a `None` value unexpectedly, which is then being passed as an argument when it should be an iterable. We need to identify *where* this `None` is originating and *why*."
  }
]

        TARGETED IMPROVEMENTS:
        [
  "Add more Print Statements for Future Debugging:** Add more print statements to show the intermediate stages of calculations so that in the future you can tell where things went wrong.",
  "Enhanced Constraint Handling:** Add explicit checks and validation steps to ensure all problem constraints are considered throughout the solution process. This could involve using automated constraint satisfaction techniques or incorporating constraint programming elements.",
  "Implement a Numerical Verification Module:** Integrate a numerical verification module to double-check the correctness of arithmetic computations. This could involve unit testing or using an external calculator to confirm results. For median calculation, explicitly check all values and their positions in sorted order.",
  "Return a default response indicating that the system was unable to solve the problem.",
  "Generate an error message that is more informative than the current \"Error calling LLM\" message.",
  "Retry the LLM call with slightly different parameters (e.g., a shorter question, a different prompt)."
]
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity ✓
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity ✓
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity ✓
    4. All operations have O(1) average time complexity ✓
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        

            PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS). YOUR APPROACH MUST BE SUBSTANTIVELY DIFFERENT THAN THESE:
            
PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS):

=== SCRIPT FROM ITERATION 1 (refine, ACCURACY: 0.00) ===
Approach: The script implements a chain-of-thought approach to answer a question by breaking it down into sub-questions, answering each sub-question independently, and then synthesizing the individual answers into a final response. The `main` function orchestrates this process, using `call_llm` to interact with the Gemini model for question breakdown, answering sub-questions, and synthesizing the final answer. No agent roles are explicitly defined. The `call_llm` function is used to send prompts to the LLM and return the response, `main` takes the question and orchestrates the calls to `call_llm` to get the sub-questions, answers to sub-questions, and a final synthesis. The overall workflow involves question decomposition, answering sub-questions, and synthesizing the final answer.

```python
import google.generativeai as genai
import os

# Replace with your actual Gemini API key or use environment variable
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
genai.configure(api_key=GOOGLE_API_KEY)

def call_llm(prompt, model_name="gemini-1.5-flash-002"):
    """Calls the LLM with error handling."""
    try:
        model = genai.GenerativeModel(model_name)
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"Error calling LLM: {str(e)}"

def main(question):
    """Main function to answer questions using LLM reasoning."""

    # Step 1: Break down the question into sub-questions
    breakdown_prompt = f"""
    Example:
    Question: What were the main causes of World War II?
    Breakdown:
    1. What were the political tensions in Europe before World War II?
    2. What were the economic factors contributing to the war?
    3. What were the key events that led to the outbreak of the war?

    Question: {question}
    Breakdown:
    """
    sub_questions = call_llm(breakdown_prompt)

    # Step 2: Answer each sub-question using LLM
    answers = []
    for sub_q in sub_questions.split("\n"):
        if not sub_q.strip():
            continue
        answer_prompt = f"""
        Example:
        Question: What were the political tensions in Europe before World War II?
        Answer: The Treaty of Versailles imposed harsh terms on Germany, leading to resentment and political instability.

        Question: {sub_q}
        Answer:
        """
        answer = call_llm(answer_prompt)
        answers.append(answer)

    # Step 3: Synthesize the answers into a final response
    synthesis_prompt = f"""
    Example:
    Sub-questions:
    1. What were the political tensions in Europe before World War II?
    2. What were the economic factors contributing to the war?
    3. What were the key events that led to the outbreak of the war?
    Answers:
    1. The Treaty of Versailles imposed harsh terms on Germany...
    2. The Great Depression created economic hardship...
    3. The invasion of Poland by Germany triggered declarations of war...
    Synthesis: World War II was caused by a combination of political tensions, economic factors, and aggressive actions...

    Sub-questions: {sub_questions}
    Answers: {answers}
    Synthesis:
    """
    final_answer = call_llm(synthesis_prompt)

    return final_answer

if __name__ == "__main__":
    question = "Explain the process of photosynthesis."
    answer = main(question)
    print(f"Question: {question}")
    print(f"Answer: {answer}")
```

=== SCRIPT FROM ITERATION 0 (baseline, ACCURACY: 0.50) ===
Approach: Simple baseline script: Direct LLM call without sophisticated techniques

```python
import os
from google import genai
from google.genai import types

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response"""
    try:
        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question):
    """
    Baseline script: Simple direct LLM call without sophisticated techniques.
    This establishes the baseline performance capability for this dataset.
    """
    system_instruction = "You are a helpful assistant. Answer the question directly and concisely based on the information provided."

    # Simple, direct call to LLM
    answer = call_llm(question, system_instruction)

    return answer
    
```


            LEARNINGS FROM PREVIOUS ITERATIONS:
            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        ```
# Math Question Dataset: Evolving Research Log

This document serves as a dynamic research log, capturing our evolving understanding, strategies, and findings related to the task of solving math questions from the provided dataset. It prioritizes concrete, task-specific insights.

## 1. DATASET PATTERNS & CHARACTERISTICS

*   **Question Content:** Predominantly multi-step mathematical reasoning problems. Requires a combination of algebra, geometry, and number theory. Questions range in complexity, requiring both computational and conceptual understanding.
*   **Mathematical Formulation:** Questions are primarily mathematical problems, often requiring symbolic manipulation (using LaTeX notation), number theory concepts, or geometric reasoning.
*   **Multi-Step Solutions:** The "expected" answers often involve multiple steps of logical deduction or calculation. This suggests that answering the questions requires breaking them down into smaller, manageable sub-problems, aligning with the CoT approach.
*   **Variety of Topics:** The sample questions span a range of mathematical topics (number theory, geometry, fractions), indicating a diverse knowledge base is needed.
*   **Answer Style:** Concise, step-by-step solutions using LaTeX. Final answers often boxed.
*   **Formatting:** Uses LaTeX for mathematical expressions and Asymptote code for diagrams. Accurate LaTeX interpretation is crucial.
*   **Numerical Focus:** Many questions require finding specific numerical values (e.g., smallest possible value, probability, arithmetic mean), demanding precise calculations.
*   **Reasoning Types:** Deductive, algebraic manipulation, spatial, computational, and logical.
*   **Examples:**
    *   Geometry problems involving area/circumference calculations, vector geometry.
    *   Number theory problems involving divisibility, digit sums.
    *   Algebra problems involving solving equations.

## 2. EFFECTIVE TASK-SPECIFIC STRATEGIES

*   **Ineffective (Baseline):** Direct LLM call. Accuracy ~50%. Insufficient for the complexity and precision required.
*   **(Untested) Chain-of-Thought (CoT):** Breaking down questions into smaller, manageable sub-problems appears promising due to the multi-step nature of the solutions. However, this strategy remains untested due to data processing errors preventing successful LLM calls.

## 3. COMMON FAILURE MODES ON THIS DATASET

*   **Arithmetic and Logical Calculation Errors:** Consistent errors in basic arithmetic and logical calculations (e.g., median, probability). *Example: Incorrectly calculating the median in the stem and leaf plot question.*
*   **Misinterpretation of Problem Context:** Failing to fully understand the constraints or conditions stated in the problem, leading to incorrect solution paths. *Example: LLM jumps directly to a numerical answer without proper justification in divisibility question.*
*   **LaTeX Interpretation Issues:** Subtle errors in interpreting LaTeX can lead to misconstrued equations and wrong answers.
*   **Difficulty:** Understanding the problem statement, which may involve complex mathematical notation. Choosing the right approach and applying the correct formulas. Performing accurate calculations. Dealing with multi-step problems requiring a sequence of logical deductions. Interpreting visual information from diagrams (when present).
*   **Edge Cases/Complexities:** Problems with subtle wording that can lead to misinterpretation. Questions requiring creative problem-solving or non-obvious insights. Diagrams that may be misleading or require careful analysis. Calculations involving fractions, radicals, or other potentially error-prone operations.
*   **`NoneType` Error in LLM Call:** The LLM call consistently fails due to receiving a `NoneType` argument. This suggests a problem with the script's data processing flow *before* the `call_llm` function. A variable expected to hold a string or iterable (likely the prompt or a list of sub-questions) is unexpectedly becoming `None`.
    *   **Script Error Log [2025-05-28 01:51:54]:** ERROR: TypeError: 'NoneType' is not iterable
    *   **Script Error Log [2025-05-28 01:51:59]:** ERROR: NoneType not iterable
    *   **Script Error Log [2025-05-28 01:52:04]:** ERROR: TypeError: 'NoneType' is not iterable

## 4. EXPERIMENT LOG & FINDINGS

*   **Experiment 0 (Baseline):**
    *   **Description:** Direct call to the LLM with the question.
    *   **Accuracy:** ~50%
    *   **Findings:** The baseline approach is inadequate. Requires more than just general knowledge; necessitates precise calculation and logical reasoning capabilities. Calculation errors and misinterpretations of context are frequent.
*   **Experiment 1:**
    *   **Description:** Attempt to implement a Chain-of-Thought (CoT) approach by breaking down the question into sub-questions.
    *   **Accuracy:** 0% (LLM call failed consistently)
    *   **Findings:** The core CoT strategy remains untested. The LLM call consistently fails due to a `NoneType` error originating *before* the `call_llm` function. Further debugging is needed to identify the source of the `None` value. Error handling only catches exceptions *during* the LLM call, not problems in data preparation *before* the call. The `NoneType` error halted script execution, preventing any assessment of the CoT's effectiveness on this dataset.

## 5. NEXT RESEARCH DIRECTIONS

*   **Debugging Data Flow:** Prioritize debugging to identify the exact location in the `main` function or related functions where the `None` value is being introduced. Add print statements or logging to track the values of variables at each step of the data processing pipeline.
*   **Input Validation:** Add checks before calling `call_llm` to ensure that the prompt (or any other input it receives) is not `None`. If it is, log an error message and potentially try to recover (e.g., by substituting a default prompt or skipping the question).
*   **Re-evaluate CoT after Fixing Errors:** Once the `NoneType` error is resolved, re-run the experiment to determine if the chain-of-thought approach is effective for this dataset. If the first iteration succeeds, evaluate the LLM's sub-question breakdown and answer synthesis quality.
*   **Implement Calculator Tool:** Offload arithmetic calculations to a tool for accurate numerical computation.
*   **Step-by-Step Reasoning:** Incorporate a step-by-step reasoning approach in the prompt to decompose problems into verifiable steps.
*   **Verifier Implementation:** Use a verifier to check the LLM's final answer against the problem's constraints and logical consistency.
*   **LaTeX Handling Improvement:** Improve LaTeX handling either via pre-processing or prompt engineering.
*   **Solution Strategies:**
    *   **Direct Calculation:** Solve the problem by applying relevant formulas and performing calculations directly.
    *   **Equation Solving:** Set up equations based on the problem statement and solve for the unknown variables.
    *   **Geometric Reasoning:** Use geometric properties and relationships to find the solution.
    *   **Casework:** Divide the problem into different cases and solve each case separately.
    *   **Pattern Recognition:** Identify patterns or relationships that can help solve the problem.
*   **Problem Decomposition:**
    1.  **Understand the Problem:** Carefully read the question and identify the knowns and unknowns. Translate the problem into mathematical notation.
    2.  **Develop a Plan:** Determine which formulas, theorems, or techniques are relevant to the problem.
    3.  **Execute the Plan:** Apply the chosen techniques to solve the problem.
    4.  **Check the Answer:** Verify that the answer is reasonable and consistent with the problem statement.
*   **Validation Techniques:**
    *   **Unit Analysis:** Check that the units of the answer are correct.
    *   **Estimation:** Estimate the answer to see if it's in the right ballpark.
    *   **Substitution:** Plug the answer back into the original problem to see if it works.
    *   **Dimensional Analysis:** Check that the dimensions of the quantities are consistent.
    *   **Consider extreme values:** check the answer works for extreme values of some variable.
*   **Creative Insights:**
    *   Sometimes, a geometric problem can be solved more easily using algebra, or vice versa.
    *   Looking for symmetries in the problem can simplify the solution.
    *   Rearranging the problem statement or using a different coordinate system can sometimes reveal a simpler solution path. Think about the problem from a different angle. Can you reframe the question or use a different representation? Instead of trying to solve the problem directly, try to solve a simpler version of the problem first. Draw analogies to other problem domains where similar concepts or techniques apply.
*   **Implementation Recommendations:**
    *   **Verification Steps:** Mathematical Correctness: The most crucial aspect. Verify that each step in the solution is mathematically sound. Consistency with Problem Statement: Ensure that the solution addresses the specific question asked and uses the given information correctly. Reasonableness of Answer: Check if the answer is reasonable in the context of the problem (e.g., a negative length is likely wrong). Edge Case Testing: Test the solution with edge cases or extreme values to ensure it holds true in all scenarios.
    *   **Intermediate Steps/Representations:** Symbolic Representation: Maintain the problem in symbolic form (using variables and equations) as long as possible to avoid premature numerical evaluation. Equation Tree: Represent the equations as a tree structure to facilitate manipulation and simplification. Diagrammatic Representation: (If applicable) Use a graph or diagram to represent the geometric relationships in the problem.
    *   **Text-Based Techniques:** LaTeX Parsing & Generation: While avoiding complex code generation, leverage LLMs' ability to understand and generate LaTeX. This is crucial for both interpreting questions and formatting answers. Step-by-Step Reasoning Chain: Prompt the LLM to explicitly state its reasoning in a step-by-step manner. This allows for easier debugging and verification. Each step should be a complete sentence. Formula Identification: Train the LLM to identify relevant formulas based on keywords in the problem statement. Equation Simplification: Use prompting to guide the LLM to simplify equations and expressions. Example-Based Learning: Fine-tune the LLM on a large dataset of similar problems and solutions. Verification Prompting: Use separate prompts to verify the correctness of each step in the solution and the final answer. For example, "Is this step logically valid based on the previous step?" "Does this answer make sense in the context of the problem?" Avoid Over-Reliance on Code: Don't try to offload the *reasoning* to external code. Use code only for arithmetic or symbolic manipulation if absolutely necessary, and always verify the results.
```
        

            CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        

            EXPLORATION GUIDANCE:
            1. Review the historical approaches, error patterns, and accumulated learnings carefully
            2. Review the FULL CODE of previous scripts to understand what has already been tried
            3. Design a new approach that is DISTINCTLY DIFFERENT from previous attempts. This approach should have a specific NEW HYPOTHESIS or variable you are trying to test. 
            4. CRITICAL: Include EMBEDDED EXAMPLES directly within your LLM prompts
            5. For each key function, show a complete worked example, or include multiple examples, including:
               - Input example that resembles the dataset
               - Step-by-step reasoning through the example
               - Properly formatted output
            6. Apply the insights from the ACCUMULATED LEARNINGS section to avoid repeating past mistakes
            7. Pay SPECIAL ATTENTION to the weaknesses and improvement suggestions from the capability assessment
            8. Consider implementing one or more of these LLM usage patterns:
               - Repeated validation with feedback loops
               - Multi-perspective analysis with synthesis
               - Dynamic input-dependent routing with an orchestrator
               - Hybrid approaches combining LLM with deterministic functions
               - Best-of-n solution generation and selection
               - ReAct pattern for interactive reasoning and action
               - If it is unknown how successful a processing state or part of the pipeline is, include verification steps to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
               - Answer checkers to validate the final answer against the problem statement. If the answer is incorrect, the checker can send the answer back to an earlier part of the system for for refinement with feedback

            Here's how to call the Gemini API. Use this example without modification and don't invent configuration options:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

            Since this is an EXPLORATION phase:
            - Try a fundamentally different approach to reasoning about the problem. Test a NEW HYPOTHESIS or variable, and add verification steps to deduce if this new change is helpful.
            - THIS IS KEY: Break down the problem into new, distinct reasoning steps based on past performance before you start coding
            - For EACH key LLM prompt, include a relevant example with:
              * Sample input similar to the dataset
              * Expected reasoning steps
              * Desired output format
            - Apply a verifier call to different parts of the pipeline in order to understand what parts of the pipeline of calls is successful and where the system is breaking
            - Pay special attention to addressing the primary issues from previous iterations
            - Ensure your new approach addresses the weaknesses identified in the capability assessment

            CRITICAL REQUIREMENTS:
            1. The script MUST properly handle all string literals - be extremely careful with quotes and triple quotes
            2. The script MUST NOT exceed 150 lines of code to prevent truncation
            3. Include detailed comments explaining your reasoning approach
            4. EVERY SINGLE LLM PROMPT must include at least one embedded example showing:
               - Sample input with reasoning
               - Desired output format
            5. Make proper use of error handling
            6. Implement robust capabilities to address the specific weaknesses identified in the capability assessment
            7. Do NOT use json.loads() in the LLM calls to process input data. JSON formatting is good to use to structure information as inputs and outputs, but attempting to have functions process JSON data explicitly with strict built-in functionality is error prone due to formatting issues and additional text that appears as documentation, reasoning, or comments. When passing data into another LLM call, you can read it as plain text rather than trying to load it in strict json format, is the better approach.

            Return a COMPLETE, RUNNABLE Python script that:
            1. Has a main function that takes a question string as input and returns the answer string
            2. Makes multiple LLM calls for different reasoning steps
            3. Has proper error handling for API calls
            4. Includes embedded examples in EVERY LLM prompt
            5. Is COMPLETE - no missing code, no "..." placeholders
            6. Closes all string literals properly

            This should be FUNDAMENTALLY DIFFERENT from all previous approaches. Do not reuse the same overall structure.

            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            