
            You are developing a Python script to solve problems using LLM reasoning capabilities.
            You are in the EXPLORATION PHASE. You must generate a NEW approach that's different from previous approaches but informed by their successes and failures. With this approach, you will have a specific NEW HYPOTHESIS or variable you are trying to test. Your goal is to see if this new approach works, and you must add verification and validation steps to deduce if this new change is helpful. You may also test RADICAL NEW APPROACHES that are substantially different from previous approaches. 
            
            You should try NEW THINGS:
            
            Break down the problem into smaller pieces
            Think CREATIVELY about how to solve your problem if other approaches aren't working
            Transform data into different formats to see if it helps

            # YOUR TASK
            You are deeply familiar with prompting techniques and the agent works from the literature. 
            Your goal is to maximize the specified performance metrics by proposing interestingly new agents.
            Observe the past discovered agents and scripts carefully and think about what insights, lessons, or stepping stones can be learned from them.
            Be creative when thinking about the next interesting agent to try. You are encouraged to draw inspiration from related agent papers or academic papers from other research areas.
            Use the knowledge from the archive and inspiration from academic literature to propose the next interesting agentic system design.
            THINK OUTSIDE THE BOX.
            

            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "Multi-hop reasoning task:\n\nQuestion: Who sang lead vocals on the Oasis hit single which had an acoustic debut in drummer Tony McCarroll's last concert ? \n\nSupporting Documents:\n=== Document 1: Oasis discography ===\nThe discography of the English rock band Oasis consists of seven studio albums, one live album, five compilation albums, six video albums, one extended play, twenty-nine singles, nineteen promotional singles and thirty-six music videos. The band have sold an estimated 70 million records worldwide and been cited by \"Guinness World Records\" as the most successful act in the United Kingdom between the years 1995 and 2005. Oasis was formed in 1991 by vocalist Liam Gallagher, guitarist Paul \"Bonehead\" Arthurs, bassist Paul \"Guigsy\" McGuigan and drummer Tony McCarroll \u2013 they were later joined by guitarist and songwriter Noel Gallagher. The band signed to Creation Records in May 1993 and released their debut single \"Supersonic\" the following year; it peaked at number 31 in the United Kingdom. Follow-up singles \"Shakermaker\" and \"Live Forever\" became UK top 15 hits, with the latter also attaining success in the United States. \" Definitely Maybe\", the band's debut studio album, topped the UK Albums Chart and went on to be certified seven times platinum by the British Phonographic Industry (BPI). \n\n=== Document 2: List of songs recorded by Oasis ===\nOasis were an English rock band formed in Manchester in 1991. Originally composed of vocalist Liam Gallagher, guitarists Noel Gallagher and Paul \"Bonehead\" Arthurs, bassist Paul \"Guigsy\" McGuigan and drummer Tony McCarroll, the band released their debut album \"Definitely Maybe\" in 1994, the material for which was entirely written by Noel Gallagher. The album topped the UK Albums Chart, and was supported by the release of \"Supersonic\", \"Shakermaker\", \"Live Forever\" and \"Cigarettes & Alcohol\" as singles. Later in the year, the band released the standalone single \"Whatever\", which reached number 3 in the UK Singles Chart. \n\n=== Document 3: Definitely Maybe ===\nDefinitely Maybe is the debut studio album by English rock band Oasis, released on 29 August 1994 by Creation Records. It was an immediate commercial and critical success in the UK, having followed on the heels of singles \"Supersonic\", \"Shakermaker\" and \"Live Forever\". It is their only full album to feature original drummer Tony McCarroll. \n\n=== Document 4: (What's the Story) Morning Glory? ===\n(What's the Story) Morning Glory? is the second studio album by English rock band Oasis, released on 2 October 1995 by Creation Records. It was produced by Owen Morris and the group's guitarist Noel Gallagher. The structure and arrangement style of the album were a significant departure from the group's previous record \"Definitely Maybe\". Gallagher's compositions were more focused in balladry and placed more emphasis on huge choruses, with the string arrangements and more varied instrumentation on the record contrasting with the rawness of the group's debut album. \" (What's the Story) Morning Glory?\" was the group's first album with drummer Alan White, who replaced Tony McCarroll. \n\n=== Document 5: Definitely Maybe Tour ===\nDefinitely Maybe Tour was a world concert tour by English band Oasis in support of their hugely successful debut album \"Definitely Maybe\". The tour, which spanned the UK, Europe, Japan, the US and Canada, included 143 shows over a period of several months in 1994 and 1995 amidst 10 different tour legs. The tour started on 6 February 1994 with a short concert at Gleneagles, Scotland, and ended on 22 April 1995 at the Sheffield Arena, which featured an acoustic debut of the future hit Don't Look Back in Anger and was also the last concert to feature original drummer Tony McCarroll. \n\n=== Document 6: Oasis (band) ===\nOasis were an English rock band formed in Manchester in 1991. Developed from an earlier group, the Rain, the band originally consisted of Liam Gallagher (vocals and tambourine), Paul \"Bonehead\" Arthurs (guitar), Paul \"Guigsy\" McGuigan (bass guitar), and Tony McCarroll (drums, percussion). They were later joined by Liam's older brother Noel Gallagher (lead guitar and vocals) as a fifth member, becoming the band's settled line-up until April 1995. \n\n=== Document 7: Don't Look Back in Anger ===\n\"Don't Look Back in Anger\" is a song by the English rock band Oasis. It was released on 19 February 1996 as the fifth single from their second studio album, \"(What's the Story) Morning Glory? \" (1995). The song was written by the band's guitarist and main songwriter, Noel Gallagher. It became the band's second single to reach number one on the UK Singles Chart, where it also went platinum. \"Don't Look Back in Anger\" was also the first Oasis single with lead vocals by Noel (who had previously only sung lead on B-sides) instead of his brother, Liam. \n\n=== Document 8: Alan White (Oasis drummer) ===\nAlan Victor White (born 26 May 1972 in Lewisham, South London) is an English rock drummer, best known as being the drummer of the English rock band Oasis from 1995 to 2004. Before Oasis, he was the drummer of Starclub from 1991 to 1994. He is the longest serving drummer in the band's history, performing on four studio albums, two compilation albums and one live album during his tenure. He joined the band in May 1995 after the band's original drummer Tony McCarroll was removed from the band. He was recommended to Noel Gallagher by Gallagher's friend Paul Weller. Notably, Alan's brother Steve has been longtime drummer for Weller. White left Oasis in early 2004 in somewhat unclear circumstances. He was replaced by Zak Starkey, drummer of The Who and son of The Beatles' drummer Ringo Starr. \n\n=== Document 9: List of Oasis band members ===\nOasis were an English rock band from Manchester. Formed in 1991, the group originally featured Gallagher brothers Liam (lead vocals) and Noel (guitar, vocals), as well as guitarist and keyboardist Paul \"Bonehead\" Arthurs, bassist Paul \"Guigsy\" McGuigan and drummer Tony McCarroll. After signing to Creation Records in 1993, the band released their debut album \"Definitely Maybe\" in 1994, which topped the UK Albums Chart and went on to sell over 15 million copies worldwide. In April 1995, after the recording and release of the single \"Some Might Say\", McCarroll was fired from Oasis. He was replaced by Alan White, who performed on the band's second album \"(What's the Story) Morning Glory? \", released in 1995. McGuigan briefly left the band during a tour in September 1995 and was temporarily replaced by Scott McLeod, although he returned a few weeks later. The band's third album \"Be Here Now\" was released in 1997, following the previous two releases by topping the UK Albums Chart. \n\n=== Document 10: List of awards and nominations received by Oasis ===\nOasis are a britpop band formed in Manchester by Liam Gallagher (vocals), Paul Arthurs (guitar), Paul McGuigan (bass) and Tony McCarroll (drums), who were soon joined by Liam's older brother Noel Gallagher (guitar, vocals). \n\n\nProvide your answer based on the information in the supporting documents.",
    "answer": "Noel Gallagher"
  },
  {
    "id": 1,
    "question": "Multi-hop reasoning task:\n\nQuestion: St. John's College, Belize offers an education in a tradition in which what three subjects were the core?\n\nSupporting Documents:\n=== Document 1: St. John's Regional Medical Center (California) ===\nSt. John's Regional Medical Center is a hospital located in Oxnard, California in the United States, and is operated by Dignity Health, along with its sister hospital, St. John's Pleasant Valley Hospital in Camarillo, California. The hospital was founded in 1912. St. John's Regional Medical Center and St. John's Pleasant Valley Hospital offer comprehensive medical services, including 24-hour emergency medical and surgical services and care, cancer and oncology care and support, cardiovascular care, community outreach and screenings, diagnostic imaging services, laboratory services, maternity and women's services, neonatal intensive care, palliative care, patient and family education, rehabilitation services, spine and orthopedic care, weight loss surgery, wound healing and oxygen therapy, and more. Together, St. John's Regional Medical Center and St. John's Pleasant Valley Hospital represent the largest acute-care health organization in Ventura County. St. John's hospitals serve all of Ventura County and beyond, including the cities of Camarillo, Moorpark, Oxnard, Port Hueneme, Ventura, and Somis. \n\n=== Document 2: Liberal arts education ===\nThe liberal arts (Latin: \"artes liberales\") are those subjects or skills that in classical antiquity were considered essential for a free person (Latin: \"liberalis\", \"worthy of a free person\") to know in order to take an active part in civic life, something that (for Ancient Greece) included participating in public debate, defending oneself in court, serving on juries, and most importantly, military service. Grammar, logic, and rhetoric were the core liberal arts, while arithmetic, geometry, the theory of music, and astronomy also played a (somewhat lesser) part in education. \n\n=== Document 3: Jack Kaiser ===\nJohn Warren Kaiser (born October 6, 1926) is Athletics Director Emeritus at St. John's University in Queens, NY. He was an American baseball player, college coach, and administrator. As a player, he helped St. John's to the 1949 College World Series. After a brief minor league career, he became head coach at St. John's and led the now-named St. John's Red Storm baseball team to eleven postseason appearances, including three trips to the College World Series in his 18-year career as head coach. He then became athletic director at St. John's, and was instrumental in the establishment of the Big East Conference. He was inducted into the ABCA Hall of Fame in 1979, and the Big East Conference Baseball Tournament Most Outstanding Player Award is named in his honor. Jack Kaiser Stadium, home baseball field of the Red Storm, is also named in his honor. \n\n=== Document 4: Aalborghus Gymnasium ===\nAalborghus Gymnasium is an upper secondary school in the city of Aalborg, in North Jutland in Denmark. It offers both the traditional three-year program and also the two-year Higher Preparatory Examination (HF) program. The subjects taught at the school range from Religion and Music to Spanish and Natural Geography. Aalborghus Gymnasium attempts to focus on the musical and creative side of students as well as taking an international perspective on issues. Students begin their studies at Aalborghus by selecting a stream of studies. Each stream has two or three subjects that are the focus of the studies. English/Social Studies is one example of a stream a student can select. \n\n=== Document 5: St. John's High School (South Carolina) ===\nSt. John's High School (SJHS) is a senior high school on Johns Island, South Carolina. It is a part of the Charleston County School District. St. John's is home to approximately 300 students and 30 faculty and staff. St. John's school mascot is The Mighty Islanders, sporting royal blue and maroon as the school colors. St. John's offers Advanced Placement and dual credit courses totaling at over 30 hours of offered college credit, as well as 3 career academies in Hospitality and Tourism, Computer Science, and Culinary Arts. St. John's competes at the A level in football, volleyball, basketball (boys and girls), wrestling, soccer, track, baseball, and softball. The Islanders also offer marching band, agriculture and green house, weightlifting, competitive academic team, and student council. \n\n=== Document 6: St. John's College High School, Belize ===\nSt. John's College High School is a high school for boys situated in Belize City, Belize. It was founded in 1887. The High School exists to educate academically talented young men in a Jesuit environment of self-discipline, love of learning, and service to others. The school 's curriculum is complemented by sports and extracurricular activities. The third and fourth form classes follow the Caribbean Secondary Education Certificate (CSEC) curricula and sit the regionally administered examinations at the end of their fourth year. \n\n=== Document 7: TriBond ===\nTriBond is a board game that has sold over 3 million copies in 14 countries since its release in 1990. It requires players to determine a common bond between three subjects. It follows in the tradition of \"Trivial Pursuit\", \"Outburst\" and other adult boardgames that require a wide range of knowledge but \"TriBond\" requires some problem solving ability as well. \n\n=== Document 8: St. John's College, Belize ===\nSt. John's College has three divisions, and a number of central academic centres and activities. Through its three divisions, it offers a wide variety of liberal arts and science courses at the secondary, British A-level, and United States junior college levels. St. John's College is a Roman Catholic institution in the Jesuit tradition, one of the oldest, largest, and most diverse educational institutions in Belize, founded by the Jesuits in 1887. \n\n=== Document 9: When Patty Went to College ===\nWhen Patty Went to College is Jean Webster's first novel, published in 1903. It is a humorous look at life in an all-girls college at the turn of the 20th century. Patty Wyatt, the protagonist of this story is a bright, fun loving, imperturbable girl who does not like to conform. The book describes her many escapades on campus during her senior year at college. Patty enjoys life on campus and uses her energies in playing pranks and for the entertainment of herself and her friends. An intelligent girl, she uses creative methods to study only as much as she feels necessary. Patty is, however, a believer in causes and a champion of the weak. She goes out of her way to help a homesick freshman Olivia Copeland who believes she will be sent home when she fails three subjects in the examination. \n\n=== Document 10: List of St. John's Seminary (California) people ===\nThe list of St. John's Seminary (California) people is a compilation of lists of notable alumni, faculty, and current students of St. John's Seminary in Camarillo, California, United States. St. John's Seminary grants graduate degrees for seminarians preparing for the priesthood, as well as a graduate degree for lay persons interested in pastoral ministry. The St. John's Seminary College was the undergraduate division of the seminary before it closed in the early 21st century. The table of notable alumni lists the date of graduation from St. John's college, seminary, or both, if applicable. It is not unusual for seminarians to have received their undergraduate education at a different institution than their seminary training. \n\n\nProvide your answer based on the information in the supporting documents.",
    "answer": "Grammar, logic, and rhetoric"
  },
  {
    "id": 2,
    "question": "Multi-hop reasoning task:\n\nQuestion: Robert Earl Holding owned an oil company that was originally founded by who?\n\nSupporting Documents:\n=== Document 1: A4 Holding ===\nA4 Holding S.p.A. known as Gruppo A4 Holding (previously as Serenissima Group), is an Italian holding company based in Verona, Veneto region. The company owned \"Autostrada Brescia Verona Vicenza Padova\" (100%), the operator of Brescia\u2013Padua section of Autostrada A4 and Autostrada A31 (Rovigo via Vicenza to Piovene Rocchette), as well as an equity interests in Autostrada del Brennero, the operator of Autostrada A22 (Modena to Brenner Pass; 4.2327% stake via \"Serenissima Partecipazioni\" which A4 Holding owned 99.999% stake) and Autostrade Lombarde, the parent company of the operator of Autostrada A35 (Brescia to Milan; 4.90% stake via \"Autostrada Brescia\u2013Padova\"). \n\n=== Document 2: Skelly Oil ===\nSkelly Oil Company was a medium-sized oil company founded in 1919 by William Grove (Bill) Skelly, Chesley Coleman Herndon and Frederick A. Pielsticker in Tulsa, Oklahoma. J.\u00a0Paul Getty acquired control of the company during the 1930s. Skelly Oil became part of Getty Oil Company, Mission Oil Company, Tidewater Oil Company. It became defunct when absorbed by Getty Oil Company in 1974, and the abandoned Skelly brand logo was revived by Nimmons-Joliet Development Corp. in 2012. \n\n=== Document 3: Robert Holding ===\nRobert Earl Holding (November 29, 1926 \u2013 April 19, 2013) was an American businessman who owned Sinclair Oil Corporation, the Little America Hotels, the Grand America Hotel, the Westgate Hotel in San Diego, California (directed by Georg Hochfilzer), and two ski resorts, Sun Valley in central Idaho since 1977, and Snowbasin near Ogden, Utah, since 1984. \n\n=== Document 4: Ahvaz Field ===\nThe Ahvaz oil field is an Iranian oil field located in Ahvaz, Khuzestan Province. It was discovered in 1953 and developed by National Iranian Oil Company. It began production in 1954. Ahvaz field is one of the richest oil fields in the world with an estimated proven reserves are around , and production is centered on 750000 oilbbl/d . The field is owned by state-owned National Iranian Oil Company (NIOC) and operated by National Iranian South Oil Company (NISOC). \n\n=== Document 5: Little America, Wyoming ===\nLittle America is a census-designated place (CDP) in Sweetwater County, Wyoming, United States. The population was 68 at the 2010 census. The community got its name from the Little America motel, which was purposefully located in a remote location as a haven, not unlike the base camp the polar explorer Richard E. Byrd set up in the Antarctic in 1928. However, being situated on a coast-to-coast highway and offering travel services, it thrived, launching a chain of travel facilities by the same name. Its developer, Robert Earl Holding, died on April 19, 2013, with a personal net worth of over $3 billion. \n\n=== Document 6: Aghajari oil field ===\nThe Aghajari oil field is an iranian oil field located in Khuzestan Province. It was discovered in 1938 and developed by National Iranian Oil Company. It began production in 1940 and produces oil. The total proven reserves of the Aghajari oil field are around 30 billion barrels (3758\u00d710tonnes), and production is centered on 300000 oilbbl/d . The field is owned by state-owned National Iranian Oil Company (NIOC) and operated by National Iranian South Oil Company (NISOC). \n\n=== Document 7: Carabobo Field ===\nCarabobo is an oil field located in Venezuela's Orinoco Belt. As one of the world's largest accumulations of recoverable oil, the recent discoveries in the Orinoco Belt have led to Venezuela holding the world's largest recoverable reserves in the world, surpassing Saudi Arabia in July 2010. The Carabobo oil field is majority owned by Venezuela's national oil company, Petroleos de Venezuela SA (PDVSA). Owning the majority of the Orinoco Belt, and its estimated 1.18 trillion barrels of oil in place, PDVSA is now the fourth largest oil company in the world. The field is well known for its extra Heavy crude oils, having an average specific gravity between 4 and 16 \u00b0API. The Orinoco Belt holds 90% of the world's extra heavy crude oils, estimated at 256 billion recoverable barrels. While production is in its early development, the Carabobo field is expected to produce 400,000 barrels of oil per day. \n\n=== Document 8: Sinclair Oil Corporation ===\nSinclair Oil Corporation is an American petroleum corporation, founded by Harry F. Sinclair on May 1, 1916, as the Sinclair Oil and Refining Corporation by combining the assets of 11 small petroleum companies. Originally a New York corporation, Sinclair Oil reincorporated in Wyoming in 1976. The corporation's logo features the silhouette of a large green dinosaur. \n\n=== Document 9: 101 Ranch Oil Company ===\nFounded in 1908 by oil exploration pioneer E. W. Marland, The 101 Ranch Oil Company was located on the Miller Brothers 101 Ranch and headquartered in Ponca City, Oklahoma. The company\u2019s 1911 oil discovery in North Eastern Oklahoma opened up oil development in a great region from Eastern Oklahoma west to Mervine, Newkirk, Blackwell, Billings and Garber and led to the founding of the Marland Oil Company, later renamed the Continental Oil Company, now known as Conoco. \n\n=== Document 10: Rag Sefid oil field ===\nThe Rag Sefid oil field is an oil field located in Khuzestan Province, approximately 6\u00a0km in nearest distance from the Persian Gulf, southwest Iran. It was discovered in 1964 and developed by National Iranian Oil Company and began production in 1966. The total proven reserves of the Rag Sefid oil field are around 14,5 billion barrels, and production is centered on 180000 oilbbl/d . The field is owned by state-owned National Iranian Oil Company (NIOC) and operated by National Iranian South Oil Company (NISOC). \n\n\nProvide your answer based on the information in the supporting documents.",
    "answer": "Harry F. Sinclair"
  }
]

            HISTORICAL CONTEXT:
            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 2
        - Current explore/exploit balance: 60/20
        - Best accuracy achieved: 0.80 (iteration 0)

        APPROACH HISTORY (last 2 iterations):
        [
  {
    "iteration": 0,
    "strategy": "baseline",
    "accuracy": 0.8,
    "approach": "Simple baseline script: Direct LLM call without sophisticated techniques"
  },
  {
    "iteration": 1,
    "strategy": "explore",
    "accuracy": 0.0,
    "approach": "The script addresses multi-hop reasoning by first summarizing each document and then reasoning across the summaries to answer a question. It employs a verification step to ensure the summaries retain relevant information, retrying summarization if the verification fails. The `main` function orchestrates the process, calling `summarize_document_with_verification` for each document and then `reason_across_summaries` to generate the final answer. `call_llm` is the core function that communicates with the Gemini LLM, while `summarize_document_with_verification` summarizes individual documents and verifies the summaries, and `reason_across_summaries` answers the final question based on the generated summaries."
  }
]

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 2 iterations):
        [
  {
    "iteration": 0,
    "issue": "The primary issue is the system's **inability to synthesize extracted information to provide precise and concise answers** that directly address the core question, even when relevant information is present in the documents."
  },
  {
    "iteration": 1,
    "issue": "The most critical problem is the `test_script_1.py` file calling the `main` function incorrectly. It is calling `answer = module.main(question)` when it should be calling `answer = module.main(question, supporting_documents)`."
  }
]

        TARGETED IMPROVEMENTS:
        []
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity ✓
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity ✓
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity ✓
    4. All operations have O(1) average time complexity ✓
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```\n\n```python\ndef solve_with_meta_programming(question):
            """
            Advanced: Script generates and executes its own code/prompts dynamically.
            The script acts as its own programmer and prompt engineer.
            """

            # Step 1: Analyze what approach is needed
            strategy_prompt = f"""
            For this problem: {question}

            What's the best approach?
            A) Generate Python code to calculate/process something
            B) Generate specialized LLM prompts for analysis  
            C) Use a hybrid approach with both code and LLM calls

            Explain your choice and what specific code or prompts I should generate.
            """


                analysis_system_prompt = """ 
                You are a problem analysis expert. You are a master of problem analysis and can 
                determine the best approach to solve a problem, understanding the strenghts and 
                weaknesses of LLMs for problem solving, when to delegate a more specific or problem 
                or subproblem to an additional LLM call, and when to write code to solve a problem.
            """
            strategy = call_llm(strategy_prompt, analysis_system_prompt)

            # Step 2: Generate and execute based on strategy
            if "###CODE_ONLY###" in strategy.lower():
                # Generate code dynamically
                code_gen_prompt = f"""
                Problem: {question}
                Strategy: {strategy}

                Write Python code to solve this problem. Include print statements for output.
                Return ONLY the Python code:
                """

                generated_code = call_llm(code_gen_prompt, "You are a Python programmer.")

                # Clean up code if wrapped in markdown
                import re
                code_match = re.search(r'```python\s*\n(.*?)\n```', generated_code, re.DOTALL)
                if code_match:
                    clean_code = code_match.group(1).strip()
                else:
                    clean_code = generated_code.strip()

                # Execute the generated code
                execution_result = execute_code(clean_code)

                # Interpret the execution result
                interpretation_prompt = f"""
                Original problem: {question}
                Generated code: {clean_code}
                Execution result: {execution_result}

                What is the final answer based on these results?
                """

                final_answer = call_llm(interpretation_prompt, "You are a solution interpreter.")
                return final_answer

            elif "###PROMPT_ONLY###" in strategy.lower():
                # Generate specialized prompts dynamically
                prompt_design = f"""
                For this problem: {question}
                Strategy: {strategy}

                Design the most effective prompt to solve this problem:
                """

                specialized_prompt = call_llm(prompt_design, "You are a prompt engineer.")

                # Use the generated prompt
                solution = call_llm(specialized_prompt, "You are an expert problem solver.")
                return solution

            else:  # Hybrid approach
                # Chain code and LLM calls dynamically
                current_result = question

                for step in range(3):
                    # Decide what to do at this step
                    step_decision = call_llm(f"""
                    Step {step + 1} of hybrid approach.
                    Current state: {current_result}

                    What should I do next?
                    - Generate and execute code
                    - Make an LLM analysis call
                    - Provide final answer

                    Choose one and explain exactly what to do.
                    """, "You are a workflow coordinator.")

                    if "final answer" in step_decision.lower():
                        return current_result
                    elif "code" in step_decision.lower():
                        # Generate code for this step
                        step_code_prompt = f"""
                        Based on this decision: {step_decision}
                        Current data: {current_result}

                        Write Python code to process this. Return only the code:
                        """
                        step_code = call_llm(step_code_prompt, "You are a Python programmer.")
                        code_result = execute_code(step_code)
                        current_result = f"Previous: {current_result}\nCode result: {code_result}"
                    else:
                        # Make LLM call for this step  
                        step_analysis = call_llm(f"Analyze this data: {current_result}\nBased on: {step_decision}", "You are an analyst.")
                        current_result = f"Previous: {current_result}\nAnalysis: {step_analysis}"

                return current_result\n```\n\n```python\ndef self_modifying_solver(problem):
            """
            A solver that rewrites its own approach based on intermediate results.
            Advanced meta-programming where the script evolves its strategy.
            """

            strategy = "direct_analysis"
            attempts = 0
            max_attempts = 3

            while attempts < max_attempts:
                attempts += 1

                if strategy == "direct_analysis":
                    # Try direct LLM analysis
                    result = call_llm(f"Solve this problem: {problem}", "You are an expert problem solver.")

                    # Evaluate if this worked
                    evaluation_prompt = f"""
                    Problem: {problem}
                    My attempt: {result}

                    Did this solve the problem correctly? If not, what approach should I try next?
                    Options: computational_approach, step_by_step_breakdown, code_generation
                    """

                    evaluation = call_llm(evaluation_prompt, "You are a solution evaluator.")

                    if "correct" in evaluation.lower() or "solved" in evaluation.lower():
                        return result
                    elif "computational" in evaluation.lower():
                        strategy = "computational_approach"
                    elif "step_by_step" in evaluation.lower():
                        strategy = "step_by_step_breakdown"  
                    else:
                        strategy = "code_generation"

                elif strategy == "computational_approach":
                    # Generate and execute computational code
                    comp_prompt = f"""
                    Problem: {problem}

                    Write Python code to solve this computationally. Include:
                    - Extract relevant numbers or data
                    - Perform calculations
                    - Print results clearly

                    Return only the Python code:
                    """

                    comp_code = call_llm(comp_prompt, "You are a computational programmer.")
                    comp_result = execute_code(comp_code)

                    # Interpret computational result
                    interpretation = call_llm(f"Problem: {problem}\nComputation result: {comp_result}\nFinal answer:", "You are an interpreter.")
                    return interpretation

                elif strategy == "step_by_step_breakdown":
                    # Generate step-by-step solution code
                    breakdown_prompt = f"""
                    Problem: {problem}

                    Write Python code that breaks this problem into steps and solves it methodically:
                    """

                    breakdown_code = call_llm(breakdown_prompt, "You are a systematic programmer.")
                    breakdown_result = execute_code(breakdown_code)

                    # Build final solution based on breakdown
                    final_solution = call_llm(f"Problem: {problem}\nStep-by-step result: {breakdown_result}\nFinal answer:", "You are a problem solver.")
                    return final_solution

                else:  # code_generation strategy
                    # Generate completely custom code for this problem
                    custom_prompt = f"""
                    Problem: {problem}

                    Write custom Python code specifically designed to solve this exact problem type:
                    """

                    custom_code = call_llm(custom_prompt, "You are a custom code generator.")
                    custom_result = execute_code(custom_code)

                    return f"Custom solution result: {custom_result}"

            return "Could not solve after multiple strategy attempts"\n```\n\n```python\ndef adaptive_chain_solver(question):
            """
            Chains multiple code generations and LLM calls adaptively.
            Each step decides what the next step should be.
            """

            current_data = question
            step_count = 0
            max_steps = 5

            while step_count < max_steps:
                step_count += 1

                # Decide what to do at this step
                decision_prompt = f"""
                Step {step_count}: Working with: {current_data}

                What should I do next to solve this problem?
                A) Generate and execute Python code to process/calculate something
                B) Generate a specialized LLM prompt for analysis
                C) I have enough information - provide final answer

                Choose A, B, or C and explain exactly what to do:
                """

                decision = call_llm(decision_prompt, "You are an adaptive workflow coordinator.")

                if "C)" in decision or "final answer" in decision.lower():
                    # Generate final answer
                    final_prompt = f"""
                    Original question: {question}
                    Current data/results: {current_data}

                    Based on all the processing done, what is the final answer?
                    """
                    return call_llm(final_prompt, "You are a solution synthesizer.")

                elif "A)" in decision or "code" in decision.lower():
                    # Generate and execute code
                    code_prompt = f"""
                    Current data: {current_data}
                    Decision: {decision}

                    Write Python code to process this data as suggested. Return only the code:
                    """

                    code = call_llm(code_prompt, "You are a Python programmer.")

                    # Execute and update current data
                    code_result = execute_code(code)
                    current_data = f"Step {step_count} result: {code_result}"

                else:  # Generate specialized LLM prompt
                    # Create specialized prompt
                    prompt_design = f"""
                    Current data: {current_data}
                    Decision: {decision}

                    Design a specialized prompt for this analysis:
                    """

                    specialized_prompt = call_llm(prompt_design, "You are a prompt engineer.")

                    # Use the specialized prompt
                    analysis_result = call_llm(specialized_prompt, "You are a specialized analyst.")
                    current_data = f"Step {step_count} analysis: {analysis_result}"

            return f"Final result after {max_steps} steps: {current_data}"\n```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        

            PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS). YOUR APPROACH MUST BE SUBSTANTIVELY DIFFERENT THAN THESE:
            
PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS):

=== SCRIPT FROM ITERATION 1 (explore, ACCURACY: 0.00) ===
Approach: The script addresses multi-hop reasoning by first summarizing each document and then reasoning across the summaries to answer a question. It employs a verification step to ensure the summaries retain relevant information, retrying summarization if the verification fails. The `main` function orchestrates the process, calling `summarize_document_with_verification` for each document and then `reason_across_summaries` to generate the final answer. `call_llm` is the core function that communicates with the Gemini LLM, while `summarize_document_with_verification` summarizes individual documents and verifies the summaries, and `reason_across_summaries` answers the final question based on the generated summaries.

```python
import os

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response"""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question, supporting_documents):
    """
    This script attempts to address multi-hop reasoning by:
    1. Summarizing each document independently to reduce information overload, then verifying the summarization.
    2. Reasoning across the summaries to find the answer.
    """

    # Hypothesis: Summarizing documents before reasoning will improve accuracy. This explores a document reduction strategy.
    # Addressing previous errors: Information Overload, Inability to Connect Disparate Facts
    # Verification goal: Verify the document summarization is both concise and accurate.
    
    summaries = []
    for i, doc in enumerate(supporting_documents):
        summary_result = summarize_document_with_verification(doc, i, question)
        if summary_result.get("is_valid"):
            summaries.append(summary_result["summary"])
        else:
            print(f"Error summarizing document {i}: {summary_result.get('validation_feedback')}")
            return f"Error summarizing document {i}"
    
    # Now, reason across the summaries to answer the question
    answer = reason_across_summaries(question, summaries)
    return answer

def summarize_document_with_verification(document, doc_id, question, max_attempts=3):
    """Summarizes a document and verifies the summary."""
    system_instruction = "You are an expert summarizer who creates concise, accurate summaries."
    
    #Attempt summarization, then verify
    for attempt in range(max_attempts):
        summary_prompt = f"""
        Summarize this document, focusing on information relevant to this question: {question}.
        Be concise and retain all critical information related to the question.
        
        Document: {document}
        
        Example 1:
        Document: The capital of Australia is Canberra. Canberra is located in the Australian Capital Territory.
        Summary: The capital of Australia is Canberra, located in the Australian Capital Territory.
        
        Example 2:
        Document:  Tommy's Honour is a 2016 historical drama film depicting the lives and careers of, and the complex relationship between, the pioneering Scottish golfing champions Old Tom Morris and his son Young Tom Morris.
        Summary: Tommy's Honour is a 2016 film about Scottish golfers Old Tom Morris and his son.

        Summary:
        """
        
        summary = call_llm(summary_prompt, system_instruction)
        
        # Verify the summary - does it retain relevant information?
        verification_prompt = f"""
        Verify that this summary of document {doc_id} retains all information relevant to the question: {question}.
        If not, explain what is missing.
        
        Document: {document}
        Summary: {summary}
        
        Respond with "VALID" if the summary is valid, or "INVALID: [reason]" if not.

        Example 1:
        Document: The Prime Minister of the UK is Rishi Sunak, who assumed office in 2022.
        Summary: Rishi Sunak is the UK Prime Minister.
        Verification: VALID
        
        Example 2:
        Document: Tommy's Honour is a film about Old Tom Morris and his son. Jack Lowden starred in it.
        Summary: Tommy's Honour is a film about Old Tom Morris.
        Verification: INVALID: The summary is missing the fact that Jack Lowden starred in it.
        """
        
        verification_result = call_llm(verification_prompt, system_instruction)
        
        if "VALID" in verification_result:
            return {"is_valid": True, "summary": summary}
        else:
            print(f"Summary verification failed for doc {doc_id}, attempt {attempt+1}: {verification_result}")
            if attempt < max_attempts-1:
                continue
            else:
                return {"is_valid": False, "summary": summary, "validation_feedback": verification_result}
    return {"is_valid": False, "summary": "", "validation_feedback": "Failed to generate a valid summary after multiple attempts."}

def reason_across_summaries(question, summaries):
    """Reasons across the summaries to answer the question."""
    system_instruction = "You are an expert at answering questions based on summaries of documents."
    
    reasoning_prompt = f"""
    Based on these summaries, answer the question: {question}. Synthesize information from multiple summaries if needed.
    
    Summaries:
    {summaries}

    Example 1:
    Question: What is the capital of Australia?
    Summaries: ['The capital of Australia is Canberra.']
    Answer: Canberra

    Example 2:
    Question: Tommy's Honour was a film that starred who?
    Summaries: ["Tommy's Honour is a 2016 film about Scottish golfers Old Tom Morris and his son.", "Jack Lowden starred in War & Peace"]
    Answer: Jack Lowden

    Answer:
    """
    
    answer = call_llm(reasoning_prompt, system_instruction)
    return answer

# Example usage (replace with actual data and document splitting)
if __name__ == "__main__":
    question = "Tommy's Honour was a drama film that included the actor who found success with what 2016 BBC miniseries?"
    supporting_documents = [
        "Tommy's Honour is a 2016 historical drama film depicting the lives and careers of, and the complex relationship between, the pioneering Scottish golfing champions Old Tom Morris and his son Young Tom Morris. The film is directed by Jason Connery, and the father and son are portrayed by Peter Mullan and Jack Lowden. The film won Best Feature Film at the 2016 British Academy Scotland Awards.",
        "Jack Andrew Lowden (born 2 June 1990) is a Scottish stage, television, and film actor. Following a highly successful and award-winning four-year stage career, his first major international onscreen success was in the 2016 BBC miniseries War & Peace, which led to starring roles in feature films."
    ]

    answer = main(question, supporting_documents)
    print(f"Answer: {answer}")
```

=== SCRIPT FROM ITERATION 0 (baseline, ACCURACY: 0.80) ===
Approach: Simple baseline script: Direct LLM call without sophisticated techniques

```python
import os
from google import genai
from google.genai import types

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response"""
    try:
        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question):
    """
    Baseline script: Simple direct LLM call without sophisticated techniques.
    This establishes the baseline performance capability for this dataset.
    """
    system_instruction = "You are a helpful assistant. Answer the question directly and concisely based on the information provided."

    # Simple, direct call to LLM
    answer = call_llm(question, system_instruction)

    return answer
    
```


            LEARNINGS FROM PREVIOUS ITERATIONS:
            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        Okay, here's the updated and synthesized version of our learnings, focusing on the specifics of this multi-hop reasoning dataset and task, organized into the requested sections:

**1. DATASET PATTERNS & CHARACTERISTICS**

*   **Explicit Multi-Hop Reasoning:** Questions are explicitly framed as multi-hop reasoning tasks, demanding information linkage across multiple documents. Single document retrieval is insufficient.
    *   *Example:* Questions require linking information about a person's career, location, and related events found in separate documents.
*   **Diverse Subject Matter:** Questions span various domains including history, geography, entertainment, and current events.
    *   *Example:* Questions range from historical figures to film trivia to geographical locations of organizations.
*   **Concise Factual Answers:** Answers are generally concise named entities, dates, or short phrases, directly responding to the question.
    *   *Example:* "1984", "Los Angeles", "Emilio Estevez".
*   **Answer Source Constraint:** Answers are *explicitly* within the provided supporting documents. The model should *not* require external knowledge. The core challenge is information retrieval and synthesis from provided texts.
*   **Information Overload:** "Supporting Documents" contain significant irrelevant information (noise). Effective models must filter and focus on relevant passages.
*   **Ambiguity:** Terms and entities can be ambiguous. Context is crucial for disambiguation.
*   **Synonymy and Paraphrasing:** Concepts are expressed differently in questions and supporting documents, requiring understanding of synonyms and paraphrases.
*   **Reasoning Depth Variation:** The number of inference "hops" to answer questions varies.
*   **Edge Cases Exist:**
    *   **Missing Information:** Documents *may not* contain the complete answer, even if relevant.
    *   **Contradictory Information:** Documents might contain conflicting information, requiring a resolution strategy.
    *   **Coreference Resolution:** Pronoun references must be resolved (e.g., "He" refers to whom?).
*   **Complex Multi-Hop Reasoning (Reinforced):** The dataset heavily relies on complex multi-hop reasoning. Answering a question often requires connecting information from multiple documents, sometimes in subtle ways.
    *   *Example:* Connecting "Emilio Estevez starred in Nightmares" with another document mentioning a film released in the same year.
*   **Information Synthesis Required (Reinforced):** Correct answers require synthesizing information rather than directly quoting a single document.
    *   *Example:* Combining facts, dates, names, and contexts to produce a derived answer.
*   **Real-World Knowledge Assumptions (Identified):** The questions often implicitly assume some basic real-world knowledge not explicitly in the documents.
    *   *Example:* Needing common-sense reasoning to understand the question's context even with supporting documents.
*   **Contextual Clues in Document Titles:** The document titles often provide crucial context for understanding the content of the document and its relevance to the question (e.g., "Oasis discography", "St. John's College, Belize").
*   **Varied Document Content:** The supporting documents encompass a wide range of formats, including discographies, lists of band members, historical context, and descriptions of albums and tours. This variety requires flexible information extraction.

**2. EFFECTIVE TASK-SPECIFIC STRATEGIES**

*   *(Initial State: No strategies have been proven effective yet as the baseline.)*
*   *(Update: No strategies have been proven effective. Experiment 1 resulted in accuracy 0.00 due to a critical error in the test script.)*

**3. COMMON FAILURE MODES ON THIS DATASET**

*   **Information Overload:** The LLM struggles to sift through the volume of information provided in the supporting documents.
*   **Inability to Connect Disparate Facts:** The system fails when the answer requires linking information that isn't explicitly connected in the documents.
    *   *Example:* Failing to connect Emilio Estevez and Nightmares with another document mentioning a different film released in the same year.
*   **Poor Summarization and Extraction:** The system struggles to extract the specific requested detail, returning more general information.
    *   *Example:* In the "Eric S. Pistorius" example, failing to extract the *concept* of his work, instead returning a more general description of his specializations as an attorney.
*   **Lack of Temporal Reasoning:** Weakness in temporal reasoning; the system can't easily determine which events occurred in the same year.
    *   *Example:* Failure involving Emilio Estevez demonstrates a weakness in temporal reasoning; the system can't easily determine which events occurred in the same year without more sophisticated processing.
*   **Basic Information Extraction is Not Enough:** Simply extracting facts from documents is insufficient. The system must be able to reason *with* those facts.
*   **Incorrect Function Call:** Critical failure due to incorrect function call in the test script, preventing the LLM from accessing the supporting documents.

**4. EXPERIMENT LOG & FINDINGS**

*   **Experiment 0: Baseline LLM Call**
    *   *Description:* Direct call to the LLM with the question and supporting documents.
    *   *Accuracy:* 80%
    *   *Findings:* Baseline performance indicates that the task complexity exceeds the capabilities of a simple LLM call without additional reasoning or information retrieval techniques. Highlights the need for a more structured approach to reasoning, potentially involving intermediate steps to identify relevant entities, relationships, and temporal information.
    *   *Failure Mode Examples:*
        *   Inability to connect disparate facts across documents.
        *   Poor summarization and extraction of specific details.
        *   Lack of temporal reasoning.
*   **Experiment 1: Summarization and Reasoning Pipeline**
    *   *Description:* Attempted to use `summarize_document_with_verification` to summarize supporting documents, then `reason_across_summaries` to answer the question.
    *   *Accuracy:* 0.00
    *   *Findings:* The experiment failed due to a critical error in the test script (`test_script_1.py`). The `main` function was called with only the question, omitting the `supporting_documents` argument. This prevented the model from accessing the necessary information to answer the questions, leading to zero accuracy. This highlights the importance of rigorous testing and validation to ensure the correct flow of information.
    *   *Error Logs:*
        ```
        === SCRIPT ERROR ENCOUNTERED [2025-06-01 07:02:35] ===
        Error detected during script repair (attempt 1): main() missing 1 required positional argument: 'supporting_documents'
        Traceback (most recent call last):
          File "/home/runner/workspace/scripts/test_script_1.py", line 261, in <module>
            answer = module.main(question)
                     ^^^^^^^^^^^^^^^^^^^^^
        TypeError: main() missing 1 required positional argument: 'supporting_documents'
        === END SCRIPT ERROR ===
        ```
        *(Similar error logs for attempt 2 and 3)*
    *   *Root Cause Identified:* Incorrect function call `module.main(question)` instead of `module.main(question, supporting_documents)`.

**5. NEXT RESEARCH DIRECTIONS**

*   **Correct the Function Call (CRITICAL):** Immediately fix the function call in `test_script_1.py` to `answer = module.main(question, supporting_documents)`. This is the most critical step to enable proper testing and move forward.
*   **Verify Input Handling:** Once the function call is corrected, add input validation within the `main` function to ensure that the `supporting_documents` are received and properly formatted before proceeding with summarization and reasoning. This validation should include checks for:
    *   Presence of the `supporting_documents` argument.
    *   Correct data type of `supporting_documents` (e.g., list of strings).
    *   Non-empty `supporting_documents` list.
*   **Evaluate Summarization Quality:** After correcting the input, analyze the summaries produced by `summarize_document_with_verification`. Determine if the verification process effectively retains relevant information from the original documents. If the summaries are consistently poor, refine the summarization prompts and verification criteria. Consider metrics to quantify the information retained in the summaries.
*   **Analyze Reasoning Chain:** Inspect the reasoning steps performed by `reason_across_summaries`. Identify any logical gaps or incorrect inferences made by the model. Experiment with different prompting strategies and reasoning frameworks to improve the accuracy of the final answer. Focus on the ability of the reasoning chain to synthesize information from multiple summaries.
*   **Implement Document Ranking/Filtering:** Implement a mechanism to rank or filter documents based on their relevance to the question *before* feeding them to the LLM.
    *   *Potential Techniques:* Keyword matching, semantic similarity, named entity recognition.
*   **Introduce a Chain-of-Thought Prompting:** Structure the LLM prompt to encourage chain-of-thought reasoning.
    *   *Example Prompt Structure:* Ask the LLM to first identify relevant entities, then identify relevant relationships between those entities, and finally answer the question based on those relationships.
*   **Fine-tune LLM (if feasible):** If resources allow, consider fine-tuning the LLM on a subset of the dataset to improve its ability to perform multi-hop reasoning and information synthesis. This would require a carefully constructed training set with questions and corresponding "reasoning paths".
*   **Incorporate External Knowledge (Cautiously):** Consider incorporating external knowledge sources (e.g., a knowledge graph or a database of facts) to augment the information provided in the documents. However, be careful to avoid introducing irrelevant or contradictory information.
*   **Implement a Temporal Reasoning Module:** Specifically address the temporal reasoning challenges by incorporating a module that can reason about dates, time intervals, and the order of events. This module could be a rule-based system or a machine learning model trained on temporal reasoning tasks.
        

            CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        

            EXPLORATION GUIDANCE:
            1. Review the historical approaches, error patterns, and accumulated learnings carefully
            2. Review the FULL CODE of previous scripts to understand what has already been tried
            3. Design a new approach that is DISTINCTLY DIFFERENT from previous attempts. This approach should have a specific NEW HYPOTHESIS or variable you are trying to test. 
            4. CRITICAL: Include EMBEDDED EXAMPLES directly within your LLM prompts
            5. For each key function, show a complete worked example, or include multiple examples, including:
               - Input example that resembles the dataset
               - Step-by-step reasoning through the example
               - Properly formatted output
            6. Apply the insights from the ACCUMULATED LEARNINGS section to avoid repeating past mistakes
            7. Pay SPECIAL ATTENTION to the weaknesses and improvement suggestions from the capability assessment
            8. Consider implementing one or more of these LLM usage patterns:
               - Repeated validation with feedback loops
               - Multi-perspective analysis with synthesis
               - Dynamic input-dependent routing with an orchestrator
               - Hybrid approaches combining LLM with deterministic functions
               - Best-of-n solution generation and selection
               - ReAct pattern for interactive reasoning and action
               - If it is unknown how successful a processing state or part of the pipeline is, include verification steps to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
               - Answer checkers to validate the final answer against the problem statement. If the answer is incorrect, the checker can send the answer back to an earlier part of the system for for refinement with feedback

            Here's how to call the Gemini API. Use this example without modification and don't invent configuration options:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

            Since this is an EXPLORATION phase:
            - Try a fundamentally different approach to reasoning about the problem. Test a NEW HYPOTHESIS or variable, and add verification steps to deduce if this new change is helpful.
            - THIS IS KEY: Break down the problem into new, distinct reasoning steps based on past performance before you start coding
            - For EACH key LLM prompt, include a relevant example with:
              * Sample input similar to the dataset
              * Expected reasoning steps
              * Desired output format
            - Apply a verifier call to different parts of the pipeline in order to understand what parts of the pipeline of calls is successful and where the system is breaking
            - Pay special attention to addressing the primary issues from previous iterations
            - Ensure your new approach addresses the weaknesses identified in the capability assessment

            CRITICAL REQUIREMENTS:
            1. The script MUST properly handle all string literals - be extremely careful with quotes and triple quotes
            2. The script MUST NOT exceed 150 lines of code to prevent truncation
            3. Include detailed comments explaining your reasoning approach
            4. EVERY SINGLE LLM PROMPT must include at least one embedded example showing:
               - Sample input with reasoning
               - Desired output format
            5. Make proper use of error handling
            6. Implement robust capabilities to address the specific weaknesses identified in the capability assessment
            7. Do NOT use json.loads() in the LLM calls to process input data. JSON formatting is good to use to structure information as inputs and outputs, but attempting to have functions process JSON data explicitly with strict built-in functionality is error prone due to formatting issues and additional text that appears as documentation, reasoning, or comments. When passing data into another LLM call, you can read it as plain text rather than trying to load it in strict json format, is the better approach.

            Return a COMPLETE, RUNNABLE Python script that:
            1. Has a main function that takes a question string as input and returns the answer string
            2. Makes multiple LLM calls for different reasoning steps
            3. Has proper error handling for API calls
            4. Includes embedded examples in EVERY LLM prompt
            5. Is COMPLETE - no missing code, no "..." placeholders
            6. Closes all string literals properly

            This should be FUNDAMENTALLY DIFFERENT from all previous approaches. Do not reuse the same overall structure.

            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            