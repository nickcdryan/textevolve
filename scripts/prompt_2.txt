
            You are creating a NEW Python script by SYNTHESIZING the best elements from multiple successful approaches.
            Your goal is to identify what makes each approach successful and combine these strengths into a superior hybrid solution.
    
            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "What is the name of the university where Ana Figueroa, a political activist and government official, studies and graduates from?",
    "answer": "University of Chile "
  },
  {
    "id": 1,
    "question": "Which genus was the ruby-throated bulbul moved to from *Turdus* before finally being classified in the genus *Rubigula*?",
    "answer": "Genus Pycnonotus"
  },
  {
    "id": 2,
    "question": "In what year did Etta Cone last visit Europe?",
    "answer": "1938"
  }
]
    
            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 2
        - Current explore/exploit balance: 60/20
        - Best accuracy achieved: 0.10 (iteration 0)

        APPROACH HISTORY (last 2 iterations):
        [
  {
    "iteration": 0,
    "strategy": "baseline",
    "accuracy": 0.1,
    "approach": "Simple baseline script: Direct LLM call without sophisticated techniques"
  },
  {
    "iteration": 1,
    "strategy": "explore",
    "accuracy": 0.0,
    "approach": "This script answers questions using LLMs by decomposing the problem into entity/relationship extraction, search query generation, information retrieval, answer generation, and answer validation. Each step leverages the Gemini LLM with a specific system instruction to act as a specialized agent (e.g., information extractor, query generator). The script uses a few functions: `call_llm` for interacting with the Gemini model, `extract_entities_and_relationships` to identify key information, `generate_search_query` to create a search query, `retrieve_information` to simulate a search engine, `generate_answer` to formulate an answer, and `validate_answer` to check the answer's correctness. The overall workflow involves sequentially calling these functions, passing the output of one as input to the next, ultimately returning the validated answer or an error message."
  }
]

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 2 iterations):
        [
  {
    "iteration": 0,
    "issue": "The most critical problem is the system's reliance on an unreliable knowledge source which leads to the retrieval and provision of factually incorrect information. The lack of a verification mechanism exacerbates this issue, as the system blindly trusts the incorrect information."
  },
  {
    "iteration": 1,
    "issue": "The most critical problem is the **inaccurate and unreliable information extraction process, coupled with insufficient validation and error detection**. The system needs to be significantly improved in its ability to pinpoint the specific information required to answer the question correctly and verify that the retrieved information is accurate and relevant."
  }
]

        TARGETED IMPROVEMENTS:
        [
  "Stage 2: Semantic Filtering:** Filter the retrieved information based on semantic relevance to the question. Use techniques like sentence similarity or knowledge graph traversal to identify the most relevant pieces of information.",
  "Stage 1: Focused Retrieval:** Retrieve a narrow set of potentially relevant information based on keywords and entity recognition.",
  "Stage 3: Verification and Validation:** Verify the accuracy and consistency of the filtered information by cross-referencing with other sources, applying logical rules, or using external validation tools."
]
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity ✓
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity ✓
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity ✓
    4. All operations have O(1) average time complexity ✓
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        
    
            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        ```
# Dataset-Specific Experiment Log: Question Answering

This document serves as a continuously updated log of patterns, strategies, and findings related to the question-answering task for this specific dataset. It prioritizes concrete, task-specific insights over general principles.

## 1. DATASET PATTERNS & CHARACTERISTICS

*   **Question Type Dominance:** The dataset predominantly features *Who* questions, seeking individuals or groups associated with specific events, creations, or awards. Example: "Who created Groove Coaster?". There is also at least one example of a "What" question focusing on a specific part of a person's name. Example: "What was the first name of Ralph E. Oesper?".
*   **Answer Type:** Answers are typically short-form, factual names of people, groups, or sometimes dates/numbers. They represent precise details directly related to the question.
*   **Knowledge Breadth:** Questions span a wide range of topics, requiring broad domain knowledge. Examples include music production (Groove Coaster), chemistry history (Ralph E. Oesper), oceanography (Jerlov Award), music band creation (Sho?), and TV series details ("El guardián invisible").
*   **Question Specificity:** Varies from very precise to allowing some interpretation.
*   **Structure and Format:** Questions are natural language sentences. Answers are simple noun phrases or names. Each entry has an ID field (string).
*   **Reasoning Type:** Primarily fact retrieval. Answers are facts needing extraction from a knowledge source.
*   **Entity and Relationship Focus:** Questions often contain specific entities (person, place, organization) and relationships (purchased, announced).
*   **Date Sensitivity:** Correctness is highly sensitive to dates; even slight variations are considered incorrect.
*   **Need for Completeness:** Answers must include all parts requested in the question (e.g., day, month, and year when asked for).
*   **Complex Relationships:** Questions frequently involve identifying relationships between entities across different time periods or classification schemes (e.g., genus changes, visit dates). This requires the system to track and correlate information from potentially disparate sources.
*   **Implicit Assumptions:** Some questions rely on implicit assumptions or background knowledge that the LLM might not possess. For example, understanding the taxonomic hierarchy to follow genus changes.
*   **Comparative Reasoning:** The dataset requires the system to compare and contrast information (e.g., "moved *to* from *Turdus* *before* finally being classified"). This necessitates accurate tracking of changes in classification over time.

## 2. EFFECTIVE TASK-SPECIFIC STRATEGIES

*   **Ineffective:** Direct LLM question answering without knowledge retrieval or verification (Baseline Experiment). Accuracy was only 10%.
*   **Ineffective:** Simple chain-of-thought approach using specialized LLM agents for each step (extraction, query generation, retrieval, answer generation, validation). Accuracy was 0%.
*   **Untested:** Knowledge Base Retrieval (using LLM to formulate queries for external knowledge bases)
*   **Untested:** Hybrid Approach (LLM for query rephrasing and search results informing answer generation)
*   **Untested:** Entity and Relation Extraction before Knowledge Retrieval (though initial experiments highlight the need for *precise* extraction).
*   **Untested:** Structured Query Generation (e.g., SPARQL)
*   **Untested:** Few-Shot Learning, Chain-of-Thought Prompting, Answer Verification Prompting, Specialized "Who" question prompts.

## 3. COMMON FAILURE MODES ON THIS DATASET

*   **Factual Inaccuracy (Hallucination):** LLM provides incorrect facts not supported by evidence. Example: Incorrect details about "Barcelona corners" or the "Belmont purchaser" in the baseline experiment. This indicates the LLM is "hallucinating" facts.
*   **Date Discrepancies:** Small differences in dates (e.g., "October 20" vs. "21 of October") are marked as incorrect, showing the need for high date precision.
*   **Incomplete Answers:** LLM fails to provide all parts of the answer requested by the question. Example: Providing only the month and year when the question asks for the day, month, and year.
*   **Ambiguity:** (Hypothesized from initial analysis) Some questions could be ambiguous if taken out of context.
*   **Multiple Valid Answers:** (Hypothesized from initial analysis) Some questions might have multiple correct answers, or answers that vary in specificity.
*   **Name Variations:** (Hypothesized from initial analysis) People's names can be written in different ways (e.g., "Robert" vs. "Bob").
*   **Cultural Differences:** (Hypothesized from initial analysis) Name formats differ across cultures. The dataset might contain names from various regions.
*   **Misspellings:** (Hypothesized from initial analysis) Questions might contain misspellings of names or terms, which could make retrieval difficult.
*   **Incorrect Entity/Relationship Extraction:** The system fails to accurately extract the precise entities and relationships needed to answer the question. For example, the system extracted "American University" which was not present in the gold answer, and was not relevant in Iteration 1.
*   **Inability to Discern Temporal Order:** The system struggles to correctly identify the sequence of events or changes over time. This is evident in the ruby-throated bulbul question, where the system failed to establish the order of genus classifications. The last visit to Europe question also shows this.
*   **Inaccurate Information Retrieval & Validation:** The system struggles to validate if information retrieved is in fact valid. This is demonstrated across all samples in Iteration 1.

## 4. EXPERIMENT LOG & FINDINGS

*   **Iteration 0 (Baseline):**
    *   **Approach:** Direct LLM call with a basic prompt.
    *   **Runtime:** Not explicitly recorded, but assumed to be fast due to direct prompting.
    *   **Accuracy:** 10%.
    *   **Key Findings:** Direct LLM prompting is insufficient. Requires knowledge retrieval and verification.
    *   **Error Analysis:** Factual inaccuracies and date discrepancies were the primary error types.

*   **Iteration 1:**
    *   **Approach:** Chain-of-thought approach using specialized LLM agents for each step (extraction, query generation, retrieval, answer generation, validation).
    *   **Runtime:** Not explicitly recorded.
    *   **Accuracy:** 0%.
    *   **Key Findings:** A simple chain-of-thought approach is not sufficient. Inaccuracies at the extraction stage propagate through the entire pipeline. The LLM struggles with understanding temporal information, tracking relationships across multiple sources, and performing accurate information validation.
    *   **Error Analysis:** Incorrect entity/relationship extraction, inability to discern temporal order, and inaccurate information retrieval and validation.

## 5. NEXT RESEARCH DIRECTIONS

*   **Implement Knowledge Retrieval:** Integrate a search engine or knowledge base to retrieve supporting information *before* answer generation. This is crucial for addressing factual inaccuracies.
*   **Implement Answer Verification:** Verify the LLM's answer against a reliable external source and correct it if discrepancies are found. This should reduce hallucinations.
*   **Date Normalization:** Standardize date formats in questions and retrieved information to enable accurate comparison.
*   **Prompt Engineering for Completeness:** Revise the prompt to ensure the model provides all parts of the answer or responds with an appropriate error message (e.g., "Unable to determine the day."). Example prompt update: "Question: {question}. Answer: If all requested parts of the answer (day, month, year) are known, provide them all. Otherwise, state 'Insufficient Information'."
*   **Explore Hybrid Approach:** Use LLM for query rephrasing to improve search engine results, and then use those results to generate an answer.
*   **Enhance Entity and Relationship Extraction:** Implement more robust methods for entity and relationship extraction, potentially using named entity recognition (NER) models finetuned on similar datasets. Focus on precise extraction of the specific entities and relationships relevant to the question.
*   **Incorporate Temporal Reasoning:** Add a dedicated temporal reasoning module to track changes over time. This could involve using specialized data structures or algorithms to represent temporal relationships and perform reasoning about event sequences.
*   **Improve Answer Validation:** Implement more rigorous answer validation techniques, such as cross-referencing information from multiple sources and using a separate validation model to assess the answer's correctness. Implement a system that assesses the quality of the extracted entities *before* query generation.
*   **Iterative Refinement with Feedback:** Create a feedback loop where incorrect answers are analyzed to identify the specific errors made by each component in the pipeline. Use this feedback to iteratively refine the system and improve its performance.
*   **Test Structured Query Generation:** Convert the natural language question into a structured query (e.g., SPARQL) to retrieve information from knowledge graphs like Wikidata.
*   **Investigate Few-Shot Learning:** Provide the LLM with examples of question-answer pairs to improve performance.
*   **Evaluate Chain-of-Thought Prompting:** Prompt the LLM to explain its reasoning process step-by-step (though a simple version of this failed).
*   **Develop Answer Verification Prompting:** Use a separate prompt to ask the LLM to verify the answer's accuracy.
*   **Create Specialized "Who" Question Prompts:** Given the dominance of "Who" questions, design highly optimized prompts.
*   **Track Latency:** Measure and log the runtime (latency) of each experiment, to understand the performance impact of different strategies.
*   **Analyze Failure Cases:** Perform detailed analysis of failure cases to identify patterns and refine strategies.
```
        
    
            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        
    
            MULTIPLE TOP PERFORMING APPROACHES TO SYNTHESIZE:
            
=== TOP PERFORMING APPROACH #1 ===
Iteration: 0
Accuracy: 0.10
Approach Summary: Simple baseline script: Direct LLM call without sophisticated techniques

FULL SCRIPT CODE:
```python
import os
from google import genai
from google.genai import types

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response"""
    try:
        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question):
    """
    Baseline script: Simple direct LLM call without sophisticated techniques.
    This establishes the baseline performance capability for this dataset.
    """
    system_instruction = "You are a helpful assistant. Answer the question directly and concisely based on the information provided."

    # Simple, direct call to LLM
    answer = call_llm(question, system_instruction)

    return answer
    
```

    
            EXPLOITATION SYNTHESIS GUIDANCE:
            1. ANALYZE EACH TOP SCRIPT to identify:
               - What specific techniques make each approach successful?
               - What unique strengths does each approach have?
               - What weaknesses or limitations does each approach have?
               - Which components could be combined effectively?
    
            2. IDENTIFY SYNTHESIS OPPORTUNITIES:
               - Which successful techniques from different scripts could work together?
               - How can you combine the best reasoning patterns from multiple approaches?
               - What hybrid approach would leverage strengths while avoiding weaknesses?
               - Can you create a multi-stage pipeline using the best parts of each?
    
            3. CREATE A HYBRID APPROACH that:
               - Takes the most effective reasoning techniques from each top script
               - Combines different successful verification/validation strategies
               - Integrates the best error handling approaches
               - Merges effective prompt engineering techniques from multiple scripts
               - Creates a more robust solution than any individual approach
    
            4. SPECIFIC SYNTHESIS STRATEGIES:
               - If Script A excels at information extraction and Script B excels at reasoning, combine both
               - If Script A has great verification and Script B has great generation, merge the pipelines
               - If multiple scripts use different successful prompting styles, create a multi-perspective approach
               - If different scripts handle different types of errors well, create comprehensive error handling
    
            5. AVOID SIMPLE COPYING:
               - Don't just take one script and make minor changes
               - Don't just concatenate approaches without thoughtful integration
               - Create something that's genuinely better than the sum of its parts
               - Ensure the hybrid approach addresses weaknesses that individual scripts had
    
            CRITICAL REQUIREMENTS FOR SYNTHESIS:
            1. The script MUST be a true hybrid that combines elements from multiple top approaches
            2. Include a clear comment explaining which elements came from which approaches
            3. EVERY LLM PROMPT must include embedded examples showing:
               - Sample input similar to the dataset
               - Expected reasoning steps
               - Desired output format
            4. The hybrid should be more robust than any individual approach
            5. Address the weaknesses identified in the capability assessment through synthesis
    
            Here's how to call the Gemini API. Use this example without modification:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"
    
            SYNTHESIS IMPLEMENTATION:
            - Create a main function that orchestrates the combined approach
            - Integrate the best reasoning patterns from multiple scripts
            - Combine the most effective verification strategies
            - Merge successful prompt engineering techniques
            - Create comprehensive error handling that addresses issues from all approaches
    
            Return a COMPLETE, RUNNABLE Python script that represents a true synthesis of the top approaches:
            1. Has a main function that takes a question string as input and returns the answer string
            2. Combines reasoning techniques from multiple successful scripts
            3. Integrates the best verification and error handling from different approaches
            4. Includes embedded examples in EVERY LLM prompt
            5. Is COMPLETE - no missing code, no "..." placeholders
            6. Closes all string literals properly
            7. Includes comments explaining which techniques came from which top scripts
    
            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            CREATE A TRUE HYBRID THAT'S BETTER THAN ANY INDIVIDUAL APPROACH!
            