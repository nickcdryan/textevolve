
            You are developing a Python script to solve problems using LLM reasoning capabilities.
            You are in the EXPLORATION PHASE. You must generate a NEW approach that's different from previous approaches but informed by their successes and failures. With this approach, you will have a specific NEW HYPOTHESIS or variable you are trying to test. Your goal is to see if this new approach works, and you must add verification and validation steps to deduce if this new change is helpful. You may also test RADICAL NEW APPROACHES that are substantially different from previous approaches. 
            
            You should try NEW THINGS:
            
            Break down the problem into smaller pieces
            Think CREATIVELY about how to solve your problem if other approaches aren't working
            Transform data into different formats to see if it helps

            # YOUR TASK
            You are deeply familiar with prompting techniques and the agent works from the literature. 
            Your goal is to maximize the specified performance metrics by proposing interestingly new agents.
            Observe the past discovered agents and scripts carefully and think about what insights, lessons, or stepping stones can be learned from them.
            Be creative when thinking about the next interesting agent to try. You are encouraged to draw inspiration from related agent papers or academic papers from other research areas.
            Use the knowledge from the archive and inspiration from academic literature to propose the next interesting agentic system design.
            THINK OUTSIDE THE BOX.
            

            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "Who was the guest star who played Carter on S5 E9 of \"The Dukes of Hazzard\"?",
    "answer": "Brett Halsey"
  },
  {
    "id": 1,
    "question": "What month, day, and year did the second session of the 4th Parliament of Singapore commence?",
    "answer": "December 26, 1978"
  },
  {
    "id": 2,
    "question": "How many siblings did Elizabeth Spencer, Baroness Hunsdon, have?",
    "answer": "6"
  }
]

            HISTORICAL CONTEXT:
            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 34
        - Current explore/exploit balance: 80/20
        - Best accuracy achieved: 0.67 (iteration 22)

        APPROACH HISTORY (last 10 iterations):
        [
  {
    "iteration": 24,
    "strategy": "Exploration",
    "accuracy": 0.6666666666666666,
    "approach": "The script implements a QA system using two simulated knowledge bases (KB1 and KB2) and LLM-driven answer extraction and validation, using the `gemini-2.0-flash` model. The problem is decomposed into knowledge retrieval, answer extraction, and answer validation, with KB1 having higher authority in cases of conflict. The agent roles involved are a knowledge base simulator, an answer extractor, and a validator.\n\nThe functions used are:\n1.  `call_llm`: Used to interface with the LLM for all tasks.\n2.  `simulate_knowledge_base`: Simulates retrieving information from two knowledge bases, KB1 and KB2.\n3.  `extract_answer`: Extracts the answer from the knowledge base results, giving precedence to KB1.\n4.  `validate_answer`: Validates the extracted answer against the knowledge bases, also prioritizing KB1.\n5.  `main`: Orchestrates the entire QA process.\n\nThe workflow starts with `main` calling `simulate_knowledge_base` twice to get results from KB1 and KB2, then calls `extract_answer` to extract an answer from these results, and then uses `validate_answer` to validate the extracted answer."
  },
  {
    "iteration": 25,
    "strategy": "Exploration",
    "accuracy": 0.3333333333333333,
    "approach": "The script employs a multi-agent system to answer questions by simulating multiple search engines and reconciling their results, ultimately validating the final answer. It uses the `call_llm` function to interact with the Gemini LLM, assigning specific roles to different agents via system instructions. The problem is decomposed into simulating search (`simulate_search`), extracting answers (`extract_answer`), reconciling answers (`reconcile_answers`), and verifying sources (`source_verifier`). The `main` function orchestrates the workflow, calling `simulate_search` to get results, `extract_answer` to extract the answers from the search results, `reconcile_answers` to get a final answer, and `source_verifier` to validate the answer."
  },
  {
    "iteration": 26,
    "strategy": "Exploitation",
    "accuracy": 0.0,
    "approach": "The script employs multiple LLM-based techniques, including simulating multiple search engines and answer reconciliation, to improve fact-checking. The problem is decomposed into simulating search results, extracting answers, reconciling them, and finally validating the reconciled answer. Several agent roles are defined through system instructions (search engine, answer extraction expert, reconciliation expert, validator). Other functions used include `call_llm`, which interfaces with the Gemini model. The script uses the following functions: `call_llm` is used to interact with the Gemini model, `simulate_search` simulates search engine results, `extract_answer` extracts the answer from the search results, `reconcile_answers` reconciles the answers from different search engines, and `validate_answer` validates the reconciled answer. The overall workflow involves simulating multiple searches, extracting answers from each, reconciling those answers, and validating the final reconciled answer."
  },
  {
    "iteration": 27,
    "strategy": "Exploration",
    "accuracy": 0.0,
    "approach": "This script uses multiple LLM-based agents to answer questions with a layered validation approach. The problem is decomposed into search, answer extraction, and validation steps. The agents include a search engine simulator, an answer extractor, a fact validator, a temporal validator, and a source reliability validator.\n\nThe core functions are `call_llm`, `simulate_search`, `extract_answer`, `fact_validator`, `temporal_validator`, `source_reliability_validator`, and `main`. `main` calls `simulate_search` to get results, then `extract_answer` to get an answer and its source. Finally, it calls `fact_validator`, `temporal_validator`, and `source_reliability_validator` to validate the answer.\n\nThe workflow involves simulating a search, extracting an answer, and then validating the answer using multiple specialized validators focusing on factuality, temporal consistency, and source reliability, returning the validated answer or error message."
  },
  {
    "iteration": 28,
    "strategy": "Exploration",
    "accuracy": 0.0,
    "approach": "This script employs a two-stage retrieval and focused summarization approach, enhanced with a self-reflection mechanism, to answer questions using the Gemini LLM. It decomposes the problem into three distinct stages: initial information retrieval, focused summarization, and self-reflection validation. The script defines the functions `initial_info_retrieval`, `focused_summarization`, and `self_reflection_validation`, each acting as an agent with a specific role (information retrieval, summarization, and validation, respectively), while `call_llm` sends the prompt to the LLM and retrieves a response, and `main` orchestrates the entire process. The `main` function first calls `initial_info_retrieval` to get background information, then passes this and the original question to `focused_summarization`, and finally uses `self_reflection_validation` to validate and answer the question."
  },
  {
    "iteration": 29,
    "strategy": "Exploration",
    "accuracy": 0.3333333333333333,
    "approach": "The script uses a multi-agent system with a \"Knowledge Navigator\" and a \"Fact Checker\" to answer questions accurately. The Knowledge Navigator iteratively refines search queries and extracts candidate answers and sources, while the Fact Checker validates the answer. The core functions are `call_llm` to interact with the LLM, `knowledge_navigator` to find a candidate answer, and `fact_checker` to validate the answer. The `main` function orchestrates the process by calling `knowledge_navigator` to obtain a candidate answer and source, then calls `fact_checker` to validate and return the final answer."
  },
  {
    "iteration": 30,
    "strategy": "Exploration",
    "accuracy": 0.0,
    "approach": "This script uses a \"Chain of Knowledge\" approach, iteratively refining information through LLM calls with Adaptive Source Selection. The problem is decomposed into source selection, information retrieval, verification, and answer extraction, each handled by a specialized agent. The workflow involves `select_source` to choose a source, `retrieve_information` to get data, `verify_information` to ensure accuracy, and `extract_answer` to provide a concise response, using `call_llm` for all LLM interactions. The main function orchestrates this process, printing debug information and returning the final answer."
  },
  {
    "iteration": 31,
    "strategy": "Exploration",
    "accuracy": 0.3333333333333333,
    "approach": "The script implements a Chain of Thought approach with specialized LLM roles for information extraction and question answering. The problem is decomposed into information extraction, search query generation, information retrieval, answer extraction, and validation steps. The agent roles involved are information extraction expert, validation expert, search query generator, search engine simulator, answer extraction expert, and fact validator.\n\nThe functions used are `extract_info` (extracts key information), `validate_extraction` (validates the extracted information), `generate_search_query` (generates a search query), `retrieve_info` (retrieves information using the search query), `extract_answer` (extracts the answer), and `validate_answer` (validates the final answer). The overall workflow involves extracting information, validating the extraction, generating a search query, retrieving information, extracting the answer, and validating the answer, using `call_llm` to call the LLM in each function."
  },
  {
    "iteration": 32,
    "strategy": "Exploration",
    "accuracy": 0.3333333333333333,
    "approach": "This script employs a question decomposition approach, breaking down a complex question into simpler sub-questions, then synthesizing information for each sub-question using targeted information retrieval and validating answers using a simulated knowledge graph. It involves agents with roles such as question decomposer, search engine simulator, answer extraction expert, and knowledge graph validator. The `call_llm` function interacts with the Gemini API, `decompose_question` breaks down the initial question, `retrieve_information` fetches information for each sub-question, `extract_answer` extracts the answer from the retrieved information, and `knowledge_graph_validator` validates the extracted answer. The `main` function orchestrates the entire process: it calls `decompose_question`, iterates through the decomposed questions calling `retrieve_information`, `extract_answer`, and `knowledge_graph_validator` for each, and then synthesizes the final answer."
  },
  {
    "iteration": 33,
    "strategy": "Exploration",
    "accuracy": 0.0,
    "approach": "The script implements a \"Knowledge Source Navigator with Chain-of-Verification\" approach to answer questions using an LLM. It decomposes the problem into sequential steps: generating a search query, retrieving information, extracting the answer, and validating the answer, using an LLM as a tool in each step. Each step uses a specific agent role (search query generator, search engine simulator, answer extractor, and fact validator) via prompting with multiple few-shot examples. The core functions used are `generate_search_query`, `retrieve_info`, `extract_answer`, and `validate_answer`, which sequentially refine the LLM's output to produce a validated answer to the input question; these all rely on the `call_llm` function which executes the requests from the google/genai API."
  }
]

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 3 iterations):
        [
  {
    "iteration": 24,
    "issue": "The most critical problem is **inaccurate fact retrieval.** The system retrieves and presents information that is factually incorrect with respect to the question asked. This is evidenced by the incorrect spouse name in the error case."
  },
  {
    "iteration": 25,
    "issue": "The most critical problem is the **lack of capability to verify and refine answers against common knowledge** or to perform simple logical deductions to arrive at the golden answer if it is not directly stated in the provided source. The current system focuses heavily on retrieving and presenting information directly from the source without critical evaluation or synthesis. Furthermore, there seems to be an issue defining what constitutes semantic equivalence with respect to numerical gold answers when the system response contains supporting details."
  },
  {
    "iteration": 26,
    "issue": "The most critical problem is **inaccurate information retrieval and integration, particularly related to dates, compounded by a lack of reliable source prioritization**. The system needs a more robust mechanism for validating and prioritizing information from different sources before using it to generate answers."
  },
  {
    "iteration": 27,
    "issue": "The most critical problem is the **lack of a robust information verification process**. The system needs to go beyond simply retrieving information from a source and implement strategies for validating its accuracy, such as cross-referencing multiple sources, applying logical constraints, or using common sense reasoning."
  },
  {
    "iteration": 28,
    "issue": "The single most critical problem is the inability to reliably retrieve and validate information required to answer questions. The vague error message \"Could not be validated\" masks the underlying cause, be it a failure in searching, parsing, or verifying the extracted information."
  },
  {
    "iteration": 29,
    "issue": "The single most critical problem is the **failure of the validation component to accurately assess the correctness of candidate answers.** It's reporting \"VALID\" even when the retrieved information is incorrect, indicating a fundamental flaw in its logic and/or the data it uses to perform the validation."
  },
  {
    "iteration": 30,
    "issue": "The most critical problem is the **failure to accurately extract the specific, desired information from the retrieved text**, particularly when the answer should be a specific data type such as a year or a numerical value. The system stops at finding a relevant passage, but it doesn't have the necessary logic to pinpoint and return the *exact* answer."
  },
  {
    "iteration": 31,
    "issue": "The most critical problem is the system's **failure to perform precise information retrieval based on contextual understanding.** This leads to the selection of plausible but ultimately incorrect answers. The system's information retrieval process must be refined to consider the specific context and constraints presented in the question."
  },
  {
    "iteration": 32,
    "issue": "The primary issue is the **unreliable validation logic**, which frequently mislabels correct information as invalid and vice versa. This causes the system to discard relevant information and accept incorrect facts, ultimately leading to inaccurate and incoherent answers."
  },
  {
    "iteration": 33,
    "issue": "The primary issue is the **flawed answer validation process**. The system incorrectly flags incorrect answers as \"VALID\", leading to the acceptance and selection of wrong solutions. This suggests a deficiency in the logic and criteria used for validating the extracted information."
  }
]

        TARGETED IMPROVEMENTS:
        [
  "After generating an initial answer, query multiple knowledge sources (e.g., Wikidata, DBpedia, a custom knowledge base) for the same information.",
  "Semantic similarity between the answer and the question.",
  "The presence of relevant keywords and entities.",
  "Score each source based on its reliability (e.g., based on the number of citations or the reputation of the source).",
  "The fulfillment of any explicit or implicit constraints (e.g., temporal constraints, specific dates).",
  "Compare the answers from different sources. If there is significant disagreement, flag the answer as potentially incorrect and try to find a more reliable answer.",
  "For questions involving numerical values, identify reasonable ranges for the answer. For example, the depth of a lake is unlikely to be more than a few hundred meters.",
  "Use structured data more effectively:** If the underlying knowledge base contains structured data (e.g., tables, knowledge graphs), leverage this structure to improve the accuracy of knowledge retrieval. This involves designing queries that target specific fields and relationships within the structured data.",
  "Reject answers that fall outside of these ranges and try to find a more reasonable answer.",
  "The source reliability for the candidate answer."
]
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity ✓
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity ✓
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity ✓
    4. All operations have O(1) average time complexity ✓
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        

            PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS). YOUR APPROACH MUST BE SUBSTANTIVELY DIFFERENT THAN THESE:
            
PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS):

=== SCRIPT FROM ITERATION 33 (Exploration, ACCURACY: 0.00) ===
Approach: The script implements a "Knowledge Source Navigator with Chain-of-Verification" approach to answer questions using an LLM. It decomposes the problem into sequential steps: generating a search query, retrieving information, extracting the answer, and validating the answer, using an LLM as a tool in each step. Each step uses a specific agent role (search query generator, search engine simulator, answer extractor, and fact validator) via prompting with multiple few-shot examples. The core functions used are `generate_search_query`, `retrieve_info`, `extract_answer`, and `validate_answer`, which sequentially refine the LLM's output to produce a validated answer to the input question; these all rely on the `call_llm` function which executes the requests from the google/genai API.

```python
import os
import re
import math

# Hypothesis: Implement a "Knowledge Source Navigator with Chain-of-Verification" approach, focusing on targeted search query generation,
# multi-example prompting, and intermediate validation to enhance answer accuracy. The primary change is using multiple few-shot examples in the prompt, and adding validation checks through the different parts of the pipeline.

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response."""
    try:
        from google import genai
        from google.genai import types

        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def generate_search_query(question):
    """Generate a targeted search query based on the question."""
    system_instruction = "You are an expert search query generator, designing effective queries to find answers."
    prompt = f"""
    Generate a targeted search query based on the question.
    
    Example 1:
    Question: In what year did Charlton Publications sell Hit Parader?
    Search Query: "Charlton Publications" sold "Hit Parader" year
    
    Example 2:
    Question: What is the peak brightness of the Asus ROG Phone 5s Pro in nits?
    Search Query: "Asus ROG Phone 5s Pro" peak brightness nits
    
    Example 3:
    Question: Who played Creon in Antigone at the Epidaurus Festival 2022?
    Search Query: Creon Antigone Epidaurus Festival 2022 cast
    
    Question: {question}
    Search Query:
    """
    search_query = call_llm(prompt, system_instruction)
    print(f"Generated search query: {search_query}") # Debugging
    return search_query

def retrieve_info(search_query):
    """Retrieve relevant information using the generated search query."""
    system_instruction = "You are a search engine simulator providing factual and concise information."
    prompt = f"""
    Simulate search results for the query.
    
    Example 1:
    Search Query: "Charlton Publications" sold "Hit Parader" year
    Search Results: Charlton Publications sold Hit Parader in 1991.
    
    Example 2:
    Search Query: "Asus ROG Phone 5s Pro" peak brightness nits
    Search Results: The peak brightness of the Asus ROG Phone 5s Pro is 1200 nits.
    
    Example 3:
    Search Query: Creon Antigone Epidaurus Festival 2022 cast
    Search Results: Vasilis Bisbikis played Creon in Antigone at the Epidaurus Festival in 2022.
    
    Search Query: {search_query}
    Search Results:
    """
    retrieved_info = call_llm(prompt, system_instruction)
    print(f"Retrieved info: {retrieved_info}") # Debugging
    return retrieved_info

def extract_answer(question, retrieved_info):
    """Extract the answer from the retrieved information."""
    system_instruction = "You are an expert at extracting precise answers from text. Focus on accuracy."
    prompt = f"""
    Extract the concise answer from the search results.
    
    Example 1:
    Question: In what year did Charlton Publications sell Hit Parader?
    Search Results: Charlton Publications sold Hit Parader in 1991.
    Answer: 1991
    
    Example 2:
    Question: What is the peak brightness of the Asus ROG Phone 5s Pro in nits?
    Search Results: The peak brightness of the Asus ROG Phone 5s Pro is 1200 nits.
    Answer: 1200 nits
    
    Example 3:
    Question: Who played Creon in Antigone at the Epidaurus Festival 2022?
    Search Results: Vasilis Bisbikis played Creon in Antigone at the Epidaurus Festival in 2022.
    Answer: Vasilis Bisbikis
    
    Question: {question}
    Search Results: {retrieved_info}
    Answer:
    """
    extracted_answer = call_llm(prompt, system_instruction)
    print(f"Extracted answer: {extracted_answer}") # Debugging
    return extracted_answer

def validate_answer(question, answer):
    """Validate the extracted answer against the question."""
    system_instruction = "You are a fact validator, ensuring the answer is correct and complete."
    prompt = f"""
    Validate if the answer accurately and completely answers the question.
    
    Example 1:
    Question: In what year did Charlton Publications sell Hit Parader?
    Answer: 1991
    Validation: VALID
    
    Example 2:
    Question: What is the peak brightness of the Asus ROG Phone 5s Pro in nits?
    Answer: 1200 nits
    Validation: VALID
    
    Example 3:
    Question: Who played Creon in Antigone at the Epidaurus Festival 2022?
    Answer: Vasilis Bisbikis
    Validation: VALID
    
    Question: {question}
    Answer: {answer}
    Validation:
    """
    validation_result = call_llm(prompt, system_instruction)
    print(f"Validation result: {validation_result}") # Debugging
    return validation_result

def main(question):
    """Solve questions by generating a search query, retrieving info, extracting, and validating."""
    try:
        # 1. Generate Search Query
        search_query = generate_search_query(question)
        
        # 2. Retrieve Information
        retrieved_info = retrieve_info(search_query)
        if "Error" in retrieved_info:
          return "Error retrieving information."
        
        # 3. Extract Answer
        answer = extract_answer(question, retrieved_info)
        if "Error" in answer:
          return "Error extracting answer."

        # 4. Validate Answer
        validation_result = validate_answer(question, answer)
        if "Error" in validation_result:
          return "Error validating answer."
        
        if "VALID" in validation_result:
            return answer
        else:
            return "Could not be validated."

    except Exception as e:
        return f"Error: {str(e)}"
```

=== SCRIPT FROM ITERATION 32 (Exploration, ACCURACY: 0.33) ===
Approach: This script employs a question decomposition approach, breaking down a complex question into simpler sub-questions, then synthesizing information for each sub-question using targeted information retrieval and validating answers using a simulated knowledge graph. It involves agents with roles such as question decomposer, search engine simulator, answer extraction expert, and knowledge graph validator. The `call_llm` function interacts with the Gemini API, `decompose_question` breaks down the initial question, `retrieve_information` fetches information for each sub-question, `extract_answer` extracts the answer from the retrieved information, and `knowledge_graph_validator` validates the extracted answer. The `main` function orchestrates the entire process: it calls `decompose_question`, iterates through the decomposed questions calling `retrieve_information`, `extract_answer`, and `knowledge_graph_validator` for each, and then synthesizes the final answer.

```python
import os
import re
import math

# Hypothesis: Using a "Question Decomposition with Targeted Information Retrieval and a Knowledge Graph Validator" approach.
# This script introduces a decomposition approach, where a complex question is broken down into smaller questions, for each question, knowledge sources are queried to synthesize an answer. The knowledge graph is used to check if the answer is consistent.

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def decompose_question(question):
    """Decompose a complex question into simpler questions."""
    system_instruction = "You are an expert question decomposer, breaking down questions into simpler, answerable parts."
    prompt = f"""
    Decompose the following complex question into simpler, answerable questions.

    Example 1:
    Complex Question: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    Decomposed Questions:
    1. Where is the Great Barrier Reef located?
    2. What is the capital of Australia?
    3. What is the population of Canberra?

    Complex Question: {question}
    Decomposed Questions:
    """
    return call_llm(prompt, system_instruction)

def retrieve_information(question):
    """Retrieve information from a simulated search engine."""
    system_instruction = "You are a search engine simulator providing factual and concise information. Be concise."
    prompt = f"""
    Simulate search results for the query: '{question}'.

    Example:
    Query: capital of Australia
    Search Results: Canberra is the capital of Australia.

    Query: {question}
    Search Results:
    """
    return call_llm(prompt, system_instruction)

def extract_answer(question, retrieved_info):
    """Extract the answer from the retrieved information."""
    system_instruction = "You are an answer extraction expert, focusing on precision."
    prompt = f"""
    Extract the concise answer from the search results.

    Example:
    Question: What is the capital of Australia?
    Search Results: Canberra is the capital of Australia.
    Answer: Canberra

    Question: {question}
    Search Results: {retrieved_info}
    Answer:
    """
    return call_llm(prompt, system_instruction)

def knowledge_graph_validator(question, answer):
    """Validates the answer against a simulated knowledge graph."""
    system_instruction = "You are a knowledge graph validator. Check if the answer is consistent with known facts."
    prompt = f"""
    Validate if the answer is consistent with a knowledge graph of facts.

    Example:
    Question: What is the capital of Australia?
    Answer: Canberra
    Validation: VALID - Canberra is the capital of Australia.

    Question: {question}
    Answer: {answer}
    Validation:
    """
    return call_llm(prompt, system_instruction)

def main(question):
    """Solve questions by decomposing, retrieving, extracting, and validating."""
    try:
        # 1. Decompose the question
        decomposed_questions = decompose_question(question)
        print(f"Decomposed questions: {decomposed_questions}") #debugging

        answers = []
        # 2. Process each sub-question
        for sub_question in decomposed_questions.split("\n"):
            if not sub_question.strip():
                continue  # Skip empty lines

            # 3. Retrieve Information
            retrieved_info = retrieve_information(sub_question)
            print(f"Retrieved info: {retrieved_info}") #debugging

            # 4. Extract Answer
            answer = extract_answer(sub_question, retrieved_info)
            print(f"Extracted answer: {answer}") #debugging

            # 5. Validate Answer
            validation_result = knowledge_graph_validator(sub_question, answer)
            print(f"Validation result: {validation_result}") #debugging

            if "VALID" not in validation_result:
                return "Could not be validated."
            answers.append(answer)

        # 6. Synthesize the final answer (simple concatenation for now)
        final_answer = ", ".join(answers)
        return final_answer

    except Exception as e:
        return f"Error: {str(e)}"
```

=== SCRIPT FROM ITERATION 31 (Exploration, ACCURACY: 0.33) ===
Approach: The script implements a Chain of Thought approach with specialized LLM roles for information extraction and question answering. The problem is decomposed into information extraction, search query generation, information retrieval, answer extraction, and validation steps. The agent roles involved are information extraction expert, validation expert, search query generator, search engine simulator, answer extraction expert, and fact validator.

The functions used are `extract_info` (extracts key information), `validate_extraction` (validates the extracted information), `generate_search_query` (generates a search query), `retrieve_info` (retrieves information using the search query), `extract_answer` (extracts the answer), and `validate_answer` (validates the final answer). The overall workflow involves extracting information, validating the extraction, generating a search query, retrieving information, extracting the answer, and validating the answer, using `call_llm` to call the LLM in each function.

```python
import os
import re
import math

# Hypothesis: A "Chain of Thought with Expert Roles and Validation" approach to improve information extraction and validation.
# The key idea is to use specialized LLM roles for each step of the process (extraction, synthesis, validation)
# and embed validation steps after each to improve overall reliability.

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def extract_info(question):
    """Extract key information from the question using a specialized LLM."""
    system_instruction = "You are an information extraction expert, skilled at identifying key details from questions."
    prompt = f"""
    Extract the key entities, constraints, and expected answer format from the question.

    Example:
    Question: In what year did Charlton Publications sell Hit Parader?
    Extracted Information:
    {{
        "entities": ["Charlton Publications", "Hit Parader"],
        "constraints": [],
        "answer_format": "year"
    }}

    Question: What is the peak brightness of the Asus ROG Phone 5s Pro in nits?
    Extracted Information:
    {{
        "entities": ["Asus ROG Phone 5s Pro"],
        "constraints": ["peak brightness"],
        "answer_format": "numerical value with unit nits"
    }}

    Question: {question}
    Extracted Information:
    """
    return call_llm(prompt, system_instruction)

def validate_extraction(question, extracted_info):
    """Validate the extracted information."""
    system_instruction = "You are a strict validation expert who validates extracted information."
    prompt = f"""
    Validate if the extracted information correctly captures the key entities, constraints, and answer format.

    Example:
    Question: In what year did Charlton Publications sell Hit Parader?
    Extracted Information: {{ "entities": ["Charlton Publications", "Hit Parader"], "constraints": [], "answer_format": "year" }}
    Validation: VALID

    Question: What is the peak brightness of the Asus ROG Phone 5s Pro in nits?
    Extracted Information: {{ "entities": ["Asus ROG Phone 5s Pro"], "constraints": ["peak brightness"], "answer_format": "numerical value with unit nits" }}
    Validation: VALID

    Question: {question}
    Extracted Information: {extracted_info}
    Validation:
    """
    return call_llm(prompt, system_instruction)

def generate_search_query(extracted_info):
    """Generate a search query based on the extracted information."""
    system_instruction = "You are an expert search query generator, designing effective queries to find answers."
    prompt = f"""
    Generate a search query based on the extracted information, focusing on retrieving factual answers.

    Example:
    Extracted Information: {{ "entities": ["Charlton Publications", "Hit Parader"], "constraints": [], "answer_format": "year" }}
    Search Query: "Charlton Publications sell Hit Parader year"

    Extracted Information: {{ "entities": ["Asus ROG Phone 5s Pro"], "constraints": ["peak brightness"], "answer_format": "numerical value with unit nits" }}
    Search Query: "Asus ROG Phone 5s Pro peak brightness nits"

    Extracted Information: {extracted_info}
    Search Query:
    """
    return call_llm(prompt, system_instruction)

def retrieve_info(search_query):
    """Retrieve relevant information using the generated search query."""
    system_instruction = "You are a search engine simulator providing factual and concise information."
    prompt = f"""
    Simulate search results for the query.

    Example:
    Search Query: "Charlton Publications sell Hit Parader year"
    Search Results: Charlton Publications sold Hit Parader in 1991.

    Search Query: "Asus ROG Phone 5s Pro peak brightness nits"
    Search Results: The peak brightness of the Asus ROG Phone 5s Pro is 1200 nits.

    Search Query: {search_query}
    Search Results:
    """
    return call_llm(prompt, system_instruction)

def extract_answer(retrieved_info, extracted_info):
    """Extract the answer from the retrieved information, considering the expected format."""
    system_instruction = "You are an answer extraction expert, focusing on accuracy and formatting."
    prompt = f"""
    Extract the answer from the retrieved information, based on the expected answer format.

    Example:
    Retrieved Information: Charlton Publications sold Hit Parader in 1991.
    Extracted Information: {{ "entities": ["Charlton Publications", "Hit Parader"], "constraints": [], "answer_format": "year" }}
    Answer: 1991

    Retrieved Information: The peak brightness of the Asus ROG Phone 5s Pro is 1200 nits.
    Extracted Information: {{ "entities": ["Asus ROG Phone 5s Pro"], "constraints": ["peak brightness"], "answer_format": "numerical value with unit nits" }}
    Answer: 1200 nits

    Retrieved Information: {retrieved_info}
    Extracted Information: {extracted_info}
    Answer:
    """
    return call_llm(prompt, system_instruction)

def validate_answer(question, answer):
    """Validate the extracted answer against the question."""
    system_instruction = "You are a fact validator, ensuring the answer is correct and complete."
    prompt = f"""
    Validate if the answer accurately and completely answers the question.

    Example:
    Question: In what year did Charlton Publications sell Hit Parader?
    Answer: 1991
    Validation: VALID

    Question: What is the peak brightness of the Asus ROG Phone 5s Pro in nits?
    Answer: 1200 nits
    Validation: VALID

    Question: {question}
    Answer: {answer}
    Validation:
    """
    return call_llm(prompt, system_instruction)

def main(question):
    """Orchestrate the question-answering process."""
    try:
        # 1. Extract Information
        extracted_info = extract_info(question)
        validation_result = validate_extraction(question, extracted_info)

        if "VALID" not in validation_result:
            return "Could not extract information."

        # 2. Generate Search Query
        search_query = generate_search_query(extracted_info)

        # 3. Retrieve Information
        retrieved_info = retrieve_info(search_query)

        # 4. Extract Answer
        answer = extract_answer(retrieved_info, extracted_info)

        # 5. Validate Answer
        final_validation = validate_answer(question, answer)

        if "VALID" not in final_validation:
            return "Could not be validated."

        return answer

    except Exception as e:
        return f"Error: {str(e)}"
```

=== SCRIPT FROM ITERATION 30 (Exploration, ACCURACY: 0.00) ===
Approach: This script uses a "Chain of Knowledge" approach, iteratively refining information through LLM calls with Adaptive Source Selection. The problem is decomposed into source selection, information retrieval, verification, and answer extraction, each handled by a specialized agent. The workflow involves `select_source` to choose a source, `retrieve_information` to get data, `verify_information` to ensure accuracy, and `extract_answer` to provide a concise response, using `call_llm` for all LLM interactions. The main function orchestrates this process, printing debug information and returning the final answer.

```python
import os
import re
import math

# Hypothesis: Implementing a "Chain of Knowledge" approach with Iterative Fact Verification and Adaptive Source Selection
# This script introduces a "Chain of Knowledge" approach, where information is iteratively refined through a chain of LLM calls,
# focusing on Adaptive Source Selection. Source is selected based on previous reliability.

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. """
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def select_source(question, previous_sources=None):
    """Select the best source based on the question and past reliability."""
    system_instruction = "You are an expert source selector, choosing the most reliable information source."
    prompt = f"""
    Select the best source for answering the question, considering past source reliability.

    Example 1:
    Question: What is the capital of Australia?
    Past Sources: None
    Best Source: Wikipedia (General Knowledge, High Reliability)

    Example 2:
    Question: In what year was the praying mantis species Eremiaphila bifasciata described?
    Past Sources: Wikipedia (Reliability: High)
    Best Source: Zoological Record (Zoology, High Reliability)

    Question: {question}
    Past Sources: {previous_sources or "None"}
    Best Source:
    """
    return call_llm(prompt, system_instruction)

def retrieve_information(question, source):
    """Retrieve information from the selected source."""
    system_instruction = f"You are an information retriever, extracting relevant data from {source}."
    prompt = f"""
    Retrieve the most relevant information from {source} to answer the question.

    Example:
    Question: What is the capital of Australia?
    Source: Wikipedia
    Retrieved Information: Canberra is the capital of Australia.

    Question: {question}
    Source: {source}
    Retrieved Information:
    """
    return call_llm(prompt, system_instruction)

def verify_information(question, retrieved_info, source):
    """Verify the accuracy of the retrieved information."""
    system_instruction = "You are a fact verifier, ensuring the accuracy of information from {source}."
    prompt = f"""
    Verify if the information from {source} accurately answers the question.

    Example:
    Question: What is the capital of Australia?
    Retrieved Information: Canberra is the capital of Australia.
    Source: Wikipedia
    Verification: VALID - Canberra is indeed the capital of Australia.

    Question: {question}
    Retrieved Information: {retrieved_info}
    Source: {source}
    Verification:
    """
    verification_result = call_llm(prompt, system_instruction)
    return verification_result

def extract_answer(question, verified_info):
    """Extract a concise answer from the verified information."""
    system_instruction = "You are a concise answer extractor, focusing on precision."
    prompt = f"""
    Extract a concise answer from the verified information.

    Example:
    Question: What is the capital of Australia?
    Verified Information: VALID - Canberra is indeed the capital of Australia.
    Answer: Canberra

    Question: {question}
    Verified Information: {verified_info}
    Answer:
    """
    return call_llm(prompt, system_instruction)

def main(question):
    """Solve questions using the Chain of Knowledge approach."""
    try:
        # Initialize variables
        previous_sources = None
        source = None

        # Select the best initial source
        source = select_source(question, previous_sources)
        print(f"Source: {source}")  # Debug print

        # Retrieve information from the selected source
        retrieved_info = retrieve_information(question, source)
        print(f"Retrieved info: {retrieved_info}") # Debug print

        # Verify the retrieved information
        verification_result = verify_information(question, retrieved_info, source)
        print(f"Verification: {verification_result}") # Debug print

        # Extract a concise answer
        answer = extract_answer(question, verification_result)
        print(f"Answer: {answer}") # Debug print

        # Return the answer
        return answer

    except Exception as e:
        return f"Error: {str(e)}"
```

=== SCRIPT FROM ITERATION 29 (Exploration, ACCURACY: 0.33) ===
Approach: The script uses a multi-agent system with a "Knowledge Navigator" and a "Fact Checker" to answer questions accurately. The Knowledge Navigator iteratively refines search queries and extracts candidate answers and sources, while the Fact Checker validates the answer. The core functions are `call_llm` to interact with the LLM, `knowledge_navigator` to find a candidate answer, and `fact_checker` to validate the answer. The `main` function orchestrates the process by calling `knowledge_navigator` to obtain a candidate answer and source, then calls `fact_checker` to validate and return the final answer.

```python
import os
import re
import math

# Hypothesis: Implement a multi-agent system with a "Knowledge Navigator" that uses iterative refinement and source validation to address the problem of factual accuracy.
# The Knowledge Navigator will:
# 1. Formulate initial search queries
# 2. Evaluate initial search results for relevance and source credibility
# 3. Refine search queries based on initial results and source credibility
# 4. Extract a candidate answer and source
# 5. Validate the candidate answer against external knowledge and internal consistency
# 6. Use a second "Fact Checker" agent to confirm or deny findings.

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def knowledge_navigator(question, max_iterations=3):
    """Navigate knowledge sources to find a reliable answer."""
    system_instruction = "You are a Knowledge Navigator, tasked with finding accurate information from multiple sources. You will analyze, refine, and validate information."

    search_query = question  # Initial search query
    candidate_answer = "No answer found." # initialize the candidate answer
    candidate_source = "None" # initialize the candidate source

    for i in range(max_iterations):
        # Step 1: Simulate search and extract potential answers

        search_results = call_llm(f"Search for: {search_query}", system_instruction="You are a search engine simulator. Provide concise, fact-based answers with source URLs.")

        # Step 2: Evaluate relevance and source credibility
        evaluation_prompt = f"""
        Evaluate these search results for relevance and source credibility.
        Question: {question}
        Search Results: {search_results}

        Example 1:
        Question: What is the capital of Australia?
        Search Results: Canberra is the capital of Australia. Source: wikipedia.org
        Relevance: Very Relevant
        Credibility: High

        Example 2:
        Question: What is the capital of Australia?
        Search Results: A blog post about visiting Sydney, Australia. Source: travelblog.com
        Relevance: Not Relevant
        Credibility: Low

        Relevance:
        Credibility:
        """

        evaluation = call_llm(evaluation_prompt, system_instruction="You are an expert at judging the relevancy and credibility of sources.")

        # Step 3: Extract potential answer

        extract_prompt = f"""
        From these search results, extract a concise answer and source.
        Question: {question}
        Search Results: {search_results}

        Example 1:
        Question: What is the capital of Australia?
        Search Results: Canberra is the capital of Australia. Source: wikipedia.org
        Answer: Canberra, Source: wikipedia.org

        Example 2:
        Question: Babymetal's song "Road of Resistance" charted at what number...?
        Search Results: Road of Resistance peaked at number 22 on the Billboard... Source: billboard.com
        Answer: 22, Source: billboard.com

        Answer:
        """
        extracted_answer = call_llm(extract_prompt, system_instruction="You are an expert answer extractor, focus on accuracy and succinctness.")

        if "Source:" in extracted_answer:
            candidate_answer = extracted_answer.split("Source:")[0].strip()
            candidate_source = extracted_answer.split("Source:")[1].strip()

        # Step 4: Refine search query (only if needed)
        if "Not Relevant" in evaluation:
            search_query = call_llm(f"Refine the search query for: {question}", system_instruction="You are an expert query refiner, use all known info to make queries more specific.")

    return candidate_answer, candidate_source

def fact_checker(question, answer, source):
    """Verify the answer with an external source."""
    system_instruction = "You are a Fact Checker, verifying information against reliable external sources."
    validation_prompt = f"""
    Verify this answer against a reliable external source.
    Question: {question}
    Answer: {answer}
    Source: {source}

    Example 1:
    Question: What is the capital of Australia?
    Answer: Canberra
    Source: wikipedia.org
    Validation: VALID

    Example 2:
    Question: Babymetal's song "Road of Resistance" charted at what number...?
    Answer: 22
    Source: billboard.com
    Validation: VALID

    Validation:
    """

    validation_result = call_llm(validation_prompt, system_instruction)
    return validation_result

def main(question):
    """Solve questions using a Knowledge Navigator and Fact Checker."""
    try:
        # Step 1: Knowledge Navigation

        candidate_answer, candidate_source = knowledge_navigator(question)
        print(f"Candidate answer is {candidate_answer}") # debug
        print(f"Candidate source is {candidate_source}") #debug
        # Step 2: Fact Checking

        validation_result = fact_checker(question, candidate_answer, candidate_source)
        print(f"Validation result is {validation_result}") #debug
        if "VALID" in validation_result:
            return candidate_answer
        else:
            return "Could not be validated."

    except Exception as e:
        return f"Error: {str(e)}"
```


            LEARNINGS FROM PREVIOUS ITERATIONS:
            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        Okay, here's the updated and synthesized research log, incorporating the new learnings from Iteration 33, focusing on maintaining a comprehensive and actionable record of our work with this specific dataset, while staying within the token limit by condensing and prioritizing key insights.

## 1. DATASET PATTERNS & CHARACTERISTICS

*   **Factual and Specific Questions:** Questions are factual and seek specific information across a wide range of topics. Require recalling specific factual information, sometimes involving multiple parts (first, middle, last names).
*   **Emphasis on Proper Nouns and Named Entities:** Questions frequently contain proper nouns, specific dates, and locations, making them amenable to information retrieval. Answers are often short phrases. Need to handle generational titles correctly (e.g., "James Vernon the Younger"). Many questions center around entities like people, places, creative works, organizations (universities), and biological classifications (genus, species). The questions often directly ask *for the name of* these things. Examples: "Who was the first cardiologist in Kashmir?" "Who murdered the supervillain Monsieur Mallah?". Entity Recognition and Disambiguation are Key. Questions often involve specific entities (e.g., a particular praying mantis species, a specific politician).
*   **Temporal Specificity (CRITICAL):** Questions often require precise temporal information (month, year, range of years), with many questions requiring events within a specific year or even date (e.g., "February 27, 2022"). The dataset is brittle with even small discrepancies leading to incorrect responses. Temporal Reasoning required. Questions frequently involve ordinal numbers (e.g., 21st) or specific years, which should be critical signals. **Date-centric questions are prevalent, requiring precise dates (day, month, year). Synonymity (different date formats) and ambiguity are present.** Example: Kunming Metro's Line 6 opening date requires "23 September 2020". **Small differences (e.g., 1940 vs. 1941, June vs. October, "December 26, 1978" vs. "January 9, 1978") are critical.** Questions frequently include a temporal element (e.g., "in Season 5," "in 2000," "in 2022").
*   **Fact Retrieval Focus:** The dataset primarily consists of fact retrieval questions, specifically those that require identifying a person associated with a specific role, event, or award. Requires external knowledge beyond pre-existing model knowledge.
*   **Extraction of Specific Dates/Names From Historical Contexts:** The questions often require extracting specific dates or names from historical contexts ("month, day, and year of philosopher Edmund Burke's last day in office").
*   **"Which" Questions Targeting Specific Individuals/Entities with Constraints:** Several questions begin with "Which" and seek a specific individual or entity associated with a particular role or accomplishment, often including specific constraints related to entities, timeframes, or categories (e.g., "Which architect was tasked with finishing the chapel...?"). The constraints are crucial to identifying the correct answer. "Dukes of Hazzard" question requires identifying guest stars, not just any actor.
*   **"Specific Instance" Questions:** The questions frequently ask for the specific instance of a general category ("Which *specific* ocean was the asteroid 224 Oceana named after?"). This requires high precision.
*   **Complex Noun Phrases:** The questions often include complex noun phrases that require accurate parsing and understanding (e.g., "Aomori University Men’s Rhythmic Gymnastics Team").
*   **Question Types:** Primarily fact-retrieval questions asking for specific details about people, places, or things ("Who...", "What...", "Name of...", "Which..."). The questions often begin with "In what year..." or "What is...".
*   **Need for Precise Factual Recall:** Questions require precise factual recall, often involving dates, numbers, or names (e.g., "Champions League semi-final match between Barcelona and Milan on April 27, 2006", "What month and year did Apple release the Hey Siri feature?"). The Babymetal question requires understanding the song title, the specific chart, and the date. Precision is paramount; approximations are incorrect.
*   **Contextual Specificity:** Questions often contain specific contextual details to narrow the scope of the answer. Complex event queries require integrating information from multiple sources (e.g., sports matches, product announcements). Questions rely on specific contextual information such as research paper titles, authors, and publication years.
*   **Information Extraction Task:** The dataset tests the ability to extract *very specific* pieces of information from potentially larger contexts. It's not simply about broad topic understanding. **Many questions require answers in a highly structured format (e.g., "day, month, year").** Correct answers are often embedded within larger text passages, requiring precise extraction rather than general topic summarization ("Needle-in-a-Haystack Answers").
*   **Definitive Answers Expected:** The questions expect precise answers (specific dates, names), not general descriptions or related concepts.
*   **Complex Factual Questions:** The dataset contains complex factual questions that often require multi-hop reasoning or accessing multiple pieces of information. Questions often involve multiple entities or relationships (e.g., "the younger daughter of Mehbooba Mufti"). This requires the system to correctly identify and relate these elements to retrieve the final answer.
*   **Multi-part Answers Common:** Several questions require multi-part answers, such as specifying both the award and the category (e.g., "Which award and in which category..."). This necessitates the system to identify and extract multiple pieces of information that are linked together.
*   **Indirect Relationships:** Questions often involve indirect relationships. The answer isn't directly stated but needs to be inferred from the provided information.
*   **Broad Subject Matter:** The questions are varied in subject matter, ranging from history and geography to academia and linguistics, politics, zoology, and performing arts. This requires a broad knowledge base and adaptable information retrieval. The questions span a variety of topics (e.g., publications, electronics, biographies), suggesting diverse source types are necessary.
*   **Fact-Seeking and Precise Information:** The questions are fact-seeking, requiring precise information retrieval (dates, ages, publication details).
*   **Name Disambiguation:** Many questions revolve around specific individuals, requiring accurate identification and disambiguation (e.g., Satyanarayan Gangaram Pitroda). The system needs to differentiate between similar names. Questions involving people's names (spouse of Silas A. Holcomb) require disambiguation.
*   **Complex Attribute Retrieval:** The task involves retrieving specific attributes (e.g., award received, position held, professorship duration) associated with entities.
*   **Fact-based, Detail-Oriented Questions:** The questions require precise factual recall, often related to dates, names, or specific versions/patches. Even slight inaccuracies are penalized.
*   *\[Condensing for space: Removed redundant repetition of patterns like Subject Matter Specificity, Implicit Context, Numerical Data, Niche Knowledge Domain, Complex Sentence Structure, Potential for Contradictory Information, Communist Party Emphasis, Unit Awareness, Numerical Reasoning, Implied Answers/Common Knowledge, Diverse Information Types, Specificity and Fandom Knowledge, Quote Identification, Temporal Reasoning. These patterns remain important and are implicitly understood.]*

## 2. EFFECTIVE TASK-SPECIFIC STRATEGIES

*   **Current Accuracy Very Low:** With a maximum accuracy of 0.67 (Iteration 22) no current strategies can be considered effectively "working" consistently. The underlying framework shows potential but requires significant refinement.
*   **RAG Framework Insufficient:** The initial hypothesis that a basic RAG framework with Gemini is sufficient for this task is rejected.
*   **Fact Verification Approach Needs Active Information Seeking**: The fact verification approach with multi-source integration shows promise, since it decomposes the problem into generating multiple search queries, simulating context retrieval from these queries while verifying their relevance, extracting answers from each context, synthesizing a final answer, and validating that final answer. However, this strategy needs an active information-seeking loop.
*   Decomposing the task into sub-tasks such as information extraction, search query generation, answer extraction, and validation seems like a reasonable approach and may provide a good starting point if further refined.
*   **Concept Expansion is Insufficient:** The approach's reliance on concept expansion alone isn't sufficient to guarantee accurate answer extraction.
*   **NONE:** The accuracy in most experiments is 0.00, suggesting no part of the current strategy is effective.
*   **Knowledge Base Selection:** The knowledge base selection *seems* to be functioning reasonably well (but needs further validation).
*   **Decomposition Potential:** Using LLMs to decompose the problem into KB selection, query generation, answer extraction, and fact verification shows potential as a framework.
*   **Decomposition Isn't Always Helpful:** Decomposing the question into sub-questions without a robust mechanism for ensuring that the sub-questions are helpful and comprehensively answered can lead to failures. LLM confidence scores are not reliable.
*   **Simulated Search Diversity (Potentially):** The core idea of using multiple simulated search engines with different biases has the *potential* to be a strength. However, it's only effective if the simulated biases are truly diverse and relevant to the questions asked.
*   *\[Condensing for space: Removed redundant statements about Source Verification Alone being Insufficient, the "Exploitation" strategy failure, consistently low or 0% accuracy in Iterations 27-32, the "Chain of Knowledge" Potential, and the CoT with Expert Roles concept. These points remain valid and are considered implicitly.]*

## 3. COMMON FAILURE MODES ON THIS DATASET

*   **Incorrect Entity Association:** The system retrieves information related to the correct event but attributes it to the wrong entity (George W. Wetherill instead of Leon Mestel for Eddington Medal). Entity Confusion is a key failure mode. Iteration 24: retrieved *a* spouse's name but not the *correct* spouse (Martha Alice Brinson) for the Holcomb example. The LLM hallucinates a plausible but incorrect answer. Entities (e.g., "Theresa," "John Tuzo Wilson Medal") might have multiple meanings, requiring precise disambiguation. In Iteration 33, "Tom Wopat" was validated instead of "Brett Halsey" for the Dukes of Hazzard question.
*   **Incorrect Fact Retrieval:** The system struggles to retrieve the correct facts from sources (Gliese 146, Karl Polanyi examples). Iteration 28 primary failure: inability to reliably retrieve the correct information.
*   **Inability to Extract Precise Information:** The system often fails to extract the *exact* piece of information required (e.g., "Billboard Magazine archives" instead of "1991" for Hit Parader question). `extract_answer` step is not effectively isolating the required data type. Answer extraction struggles to isolate the precise answer from the retrieved text and includes irrelevant information.
*   **Insufficient Answer Granularity:** The system often returns answers that are correct but lack the precision required (e.g., answering "2023" when the expected answer was "Sep 07, 2023.").
*   **Inability to Perform Temporal Reasoning:** The system fails when a question requires comparing or understanding dates (Edmund Burke question).
*   *\[Condensing for space: Removed redundant statements about Lack of Granularity/Specificity, Incorrect Granularity, Inference Limitations, Dependency on Perfect Information Retrieval, Inability to resolve conflicting information, Failure to identify the correct entity in search results, Poor Simulated Search Results, Inaccurate Entity Resolution, Knowledge Graph Misidentification/Hallucination, Information Extraction Bottleneck, Insufficient Context Detection Failure, Passive Behavior Regarding Missing Information, Lack of Numerical Precision, Inability to handle ambiguity and semantic equivalence, Poor Query Generation, Incorrect Entity Attribution, Temporal Reasoning Errors, Inability to Handle Partial Information, Ineffective Search Query Formulation, Lack of Domain Specificity in Search, Failure to Handle Implicit Context, Answer Extraction Failure, Lack of Information Retrieval Linkage, Incorrect Precision.]*
*   **Validation Sensitivity INSUFFICIENT (CRITICAL):** Validation often fails to catch incorrect answers, especially on dates. The `fact_checker` fails to recognize discrepancies in dates or years, even when the retrieved answer is numerically close to the correct answer (e.g., validating "1941" when the expected answer is "1940"). Validation Logic Errors: The core failure often lies in the validation logic incorrectly flagging relevant information as invalid.
*   **Lack of Contextual Understanding During Validation:** The validator likely struggles to relate the retrieved information to the specific constraints in the question. Contextual misinterpretation occurs in event queries. Exact Match Validation Failure: The validation stage requires an exact match, leading to failures even with near-correct answers. The validator needs to be robust to handle different date formats, synonymity, and numeric tolerance. **Erroneous validation of alternative facts occurs.** Fails to recognize the *specific* recipient from 2000 (Donald M. Gray).
*   Iteration 33: System validated "Tom Wopat" as the guest star instead of "Brett Halsey" for the "Dukes of Hazzard" question, highlighting a major flaw in validation logic. The system failed to distinguish between significantly different dates (e.g., "December 26, 1978" vs. "January 9, 1978") and numerical values (e.g., "Four" vs. "6"). Validation doesn't consider the specific context (guest star vs. regular cast member).
*   *\[Condensing for space: Removed redundant statements about Reliance on Simulated Results, Incorrect Fact Retrieval and Ranking, Inability to Find Relevant Facts, Synthesis of Incorrect Information, Lack of Robust Verification, Contextual Misdirection (Location), Numerical Extraction Failure (Version Number), General Numerical Extraction, Unit Conversion and Precision, Incomplete Sub-question Answering, Inability to Disambiguate Related but Incorrect Information, Insufficient Error Handling when Search Fails, Failure to Retrieve Precise Numerical Data, Insufficient Information Retrieval for Niche Topics, Inability to Verify Semantic Equivalence with Numerical Values, "No Answer" Response, Numerical Discrepancy Failure, Lack of Verifiable Source, Lack of Disambiguation, Failure to perform numerical reasoning, Reliance on direct extraction, not inference, Inability to Verify and Refine Answers, Conflicting Dates and Poor Reconciliation, Incorrect Date Extraction, No Source Reliability Prioritization, Date Extraction/Handling Errors, Lack of Cross-Referencing, Parsing and Extraction Failure, Returning the Source Instead of the Information, Value Confusion (Numerical), Misinterpretation of Context/Nuance.]*
*   **Insufficient Validation (Iteration 31):** The validation steps are not robust enough to catch the errors resulting from incorrect information retrieval or misinterpretation. The validators are being fooled by plausible but incorrect answers.

## 4. EXPERIMENT LOG & FINDINGS

*   **Experiment 0-31:** *\[Previous Experiment Log entries, see original document for full list. These have been condensed to save space, but the core findings remain represented in the sections above.]*
*   **Experiment 30:** Strategy: "Chain of Knowledge" approach with modular source selection, information retrieval, verification, and answer extraction. Result: 0.00 accuracy. Finding: The hypothesis of the "Chain of Knowledge" approach improving accuracy is rejected. The current implementation, especially the `extract_answer` and `verify_information` components, is insufficient. Adaptive Source Selection remains untested.
*   **Experiment 31:** Strategy: Chain of Thought with Expert Roles (Information Extraction Expert, Fact Checker Expert, Knowledge Navigator Expert). Result: 0.00 accuracy. Finding: The hypothesis of the "Chain of Thought with Expert Roles and Validation" improving accuracy is rejected. The individual experts are not expert enough, and the validation steps did not prevent incorrect answers. The underlying "Chain of Thought with Experts" *concept* remains potentially viable, but the *implementation* failed.
*   **Experiment 32:** Strategy: Question decomposition, targeted information retrieval, and knowledge graph validation. Result: 0.33 accuracy. Finding: The initial hypothesis that question decomposition, targeted information retrieval, and knowledge graph validation would improve accuracy is **rejected** in its current implementation. The validation component's unreliability negates any potential benefits.
*   **Experiment 33:** Strategy: Targeted search query generation, multi-example prompting, intermediate validation. Result: 0.00 accuracy. Finding: The hypothesis of improving accuracy through targeted search query generation, multi-example prompting, and intermediate validation was rejected. The flawed validation process negated any potential benefits. Validation is the critical bottleneck.

## 5. NEXT RESEARCH DIRECTIONS

*   **Revamp Validation Logic (CRITICAL & IMMEDIATE):** Overhaul the answer validation logic. Implement more rigorous comparison methods. Test the logic *independently* before re-integration. Create unit tests to prevent regressions.
    *   **Data Type-Specific Validation:** Implement specific validation routines for dates, numbers, names, etc. For dates, ensure the system considers the full date (month, day, year). For numbers, require exact matches or define acceptable tolerances. Implement string similarity measures (e.g., Levenshtein distance) for textual answers, but use cautiously.
    *   **Contextual Validation:** Incorporate checks to ensure the extracted answer aligns with the question's context (e.g., verifying that the extracted name is indeed a guest star for "guest star" questions).
    *   **String Similarity Measures:** For textual answers (e.g., names), implement string similarity measures (e.g., Levenshtein distance) to allow for minor variations in spelling or phrasing while still identifying correct answers. However, be careful with these to avoid accepting incorrect answers.
    *   **Negative Examples:** Add negative examples to the few-shot prompts for the validation agent.
*   **Improve Search Query Precision (CRITICAL):** Generate *more specific* search queries that include key details from the question. Focus on terms that uniquely identify the desired information. Experiment with different search query strategies (e.g., adding quotes around specific phrases).
    *   Example: For the Space: 1999 question, use "Space: 1999 Series 1 Episode 17 title" as a query.
*   **Enhance Answer Extraction (CRITICAL - Numerical & Temporal):** Significantly improve the `extract_answer` function to specifically identify and extract the target data type (year, numerical value, etc.). This might involve:
    *   **Regular Expressions:** Using regular expressions or more sophisticated parsing techniques to isolate the desired information.
    *   **Data Type Specification:** Providing the `extract_answer` function with the expected data type (e.g., "year," "number in nits") as additional context to guide its extraction process.
    *   **Focused Summarization on Entity Extraction:** Modify the `focused_summarization` function to specifically extract the entity (date, number, name) required to answer the question. Use a more robust method to parse the document, with clear entity definitions.
*   **Improve Information Retrieval Accuracy (CRITICAL):** The immediate and primary focus should be on improving the accuracy of the `initial_info_retrieval` function. The retrieval stage is a significant bottleneck. This might involve:
    *   **Query Expansion:** Use LLM to generate multiple search queries from a single question.
    *   **Contextualized Search:** Incorporate the question context into the search query to refine the search results.
    *   **Specialized Search Engines:** Explore using specialized search engines or APIs relevant to specific question types (e.g., a music database for the Babymetal question).
    *   **Fine-tuning embedding model:** If using embeddings, fine-tune the model on question/answer pairs from this dataset.
*   **Implement Entity Disambiguation (CRITICAL):** Integrate an entity disambiguation module into the retrieval process. This module should identify the specific entity being queried and filter the knowledge base results accordingly. **Improve initial search queries to include details that help to clarify the entity that is the subject of the question.**
*   **Enhance Answer Extraction with Cross-Validation (CRITICAL):** Implement a more robust answer extraction mechanism that considers multiple sources and cross-validates information *before* presenting a final answer. This should include explicitly comparing information from different sources and resolving discrepancies.
*   **Implement External Validation (CRITICAL):** Augment the `validate_answer` function to compare the LLM's answer with a reliable external knowledge base (e.g., Wikidata, a fact-checked database) to verify its accuracy *after* reconciliation. This is crucial for addressing the lack of verifiable sources.
*   *\[Condensing for space: Removed redundant statements about Improve Information Verification, Implement Flexible Answer Validation, Refine Source Selection Prompts, Add intermediate logging, Strengthen Source Reliability Validation, Implement Numerical Reasoning Module, Incorporate Common Knowledge Base, Introduce an Inference/Deduction Step, Improve Search Simulation Fidelity, Focus on Numerical Answer Accuracy, Implement a Verification Loop, Refine Temporal Validation, Add a negative constraint capability, Dataset-Specific Fine-Tuning, Test with Simpler Validation, Implement Temporal Filtering.]*
        

            CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        

            EXPLORATION GUIDANCE:
            1. Review the historical approaches, error patterns, and accumulated learnings carefully
            2. Review the FULL CODE of previous scripts to understand what has already been tried
            3. Design a new approach that is DISTINCTLY DIFFERENT from previous attempts. This approach should have a specific NEW HYPOTHESIS or variable you are trying to test. 
            4. CRITICAL: Include EMBEDDED EXAMPLES directly within your LLM prompts
            5. For each key function, show a complete worked example, or include multiple examples, including:
               - Input example that resembles the dataset
               - Step-by-step reasoning through the example
               - Properly formatted output
            6. Apply the insights from the ACCUMULATED LEARNINGS section to avoid repeating past mistakes
            7. Pay SPECIAL ATTENTION to the weaknesses and improvement suggestions from the capability assessment
            8. Consider implementing one or more of these LLM usage patterns:
               - Repeated validation with feedback loops
               - Multi-perspective analysis with synthesis
               - Dynamic input-dependent routing with an orchestrator
               - Hybrid approaches combining LLM with deterministic functions
               - Best-of-n solution generation and selection
               - ReAct pattern for interactive reasoning and action
               - If it is unknown how successful a processing state or part of the pipeline is, include verification steps to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
               - Answer checkers to validate the final answer against the problem statement. If the answer is incorrect, the checker can send the answer back to an earlier part of the system for for refinement with feedback

            Here's how to call the Gemini API. Use this example without modification and don't invent configuration options:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

            Since this is an EXPLORATION phase:
            - Try a fundamentally different approach to reasoning about the problem. Test a NEW HYPOTHESIS or variable, and add verification steps to deduce if this new change is helpful.
            - THIS IS KEY: Break down the problem into new, distinct reasoning steps based on past performance before you start coding
            - For EACH key LLM prompt, include a relevant example with:
              * Sample input similar to the dataset
              * Expected reasoning steps
              * Desired output format
            - Apply a verifier call to different parts of the pipeline in order to understand what parts of the pipeline of calls is successful and where the system is breaking
            - Pay special attention to addressing the primary issues from previous iterations
            - Ensure your new approach addresses the weaknesses identified in the capability assessment

            CRITICAL REQUIREMENTS:
            1. The script MUST properly handle all string literals - be extremely careful with quotes and triple quotes
            2. The script MUST NOT exceed 150 lines of code to prevent truncation
            3. Include detailed comments explaining your reasoning approach
            4. EVERY SINGLE LLM PROMPT must include at least one embedded example showing:
               - Sample input with reasoning
               - Desired output format
            5. Make proper use of error handling
            6. Implement robust capabilities to address the specific weaknesses identified in the capability assessment
            7. Do NOT use json.loads() in the LLM calls to process input data. JSON formatting is good to use to structure information as inputs and outputs, but attempting to have functions process JSON data explicitly with strict built-in functionality is error prone due to formatting issues and additional text that appears as documentation, reasoning, or comments. When passing data into another LLM call, you can read it as plain text rather than trying to load it in strict json format, is the better approach.

            Return a COMPLETE, RUNNABLE Python script that:
            1. Has a main function that takes a question string as input and returns the answer string
            2. Makes multiple LLM calls for different reasoning steps
            3. Has proper error handling for API calls
            4. Includes embedded examples in EVERY LLM prompt
            5. Is COMPLETE - no missing code, no "..." placeholders
            6. Closes all string literals properly

            This should be FUNDAMENTALLY DIFFERENT from all previous approaches. Do not reuse the same overall structure.

            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            