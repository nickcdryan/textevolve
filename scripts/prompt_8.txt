
            You are developing a Python script to solve problems using LLM reasoning capabilities.
            You are in the EXPLORATION PHASE. You must generate a NEW approach that's different from previous approaches but informed by their successes and failures. With this approach, you will have a specific NEW HYPOTHESIS or variable you are trying to test. Your goal is to see if this new approach works, and you must add verification and validation steps to deduce if this new change is helpful. You may also test RADICAL NEW APPROACHES that are substantially different from previous approaches. 
            
            You should try NEW THINGS:
            
            Break down the problem into smaller pieces
            Think CREATIVELY about how to solve your problem if other approaches aren't working
            Transform data into different formats to see if it helps

            # YOUR TASK
            You are deeply familiar with prompting techniques and the agent works from the literature. 
            Your goal is to maximize the specified performance metrics by proposing interestingly new agents.
            Observe the past discovered agents and scripts carefully and think about what insights, lessons, or stepping stones can be learned from them.
            Be creative when thinking about the next interesting agent to try. You are encouraged to draw inspiration from related agent papers or academic papers from other research areas.
            Use the knowledge from the archive and inspiration from academic literature to propose the next interesting agentic system design.
            THINK OUTSIDE THE BOX.
            

            Here are example problems from previously seen data:
            [
  {
    "id": 0,
    "question": "How old was Lillian Marie Bounds when her father passed away?",
    "answer": "17 years old. "
  },
  {
    "id": 1,
    "question": "What day, month, and year was the first Sazae-san strip run by the Asahi Shimbun published?",
    "answer": "November 30, 1949"
  },
  {
    "id": 2,
    "question": "In which month and year did \"Flying\" magazine publish \"Battling the Big Lie: John King's Crusade to Change Aviation's Culture\"?",
    "answer": "March 2001"
  }
]

            HISTORICAL CONTEXT:
            
        ITERATION HISTORY SUMMARY:
        - Total iterations completed: 8
        - Current explore/exploit balance: 65/35
        - Best accuracy achieved: 0.33 (iteration 1)

        APPROACH HISTORY (last 8 iterations):
        [
  {
    "iteration": 0,
    "strategy": "Exploration",
    "accuracy": 0.0,
    "approach": "The script uses an LLM to answer factual questions through iterative refinement, incorporating chain-of-thought reasoning and validation. The problem is decomposed into generating a search query, simulating information retrieval, extracting an answer with a confidence score, and validating the extracted answer. The agent roles involved are a search query generator, a question answering expert, and an answer validator. The script defines a `call_llm` function, which is used to call the Gemini LLM with different prompts, and a `main` function that orchestrates the entire process by calling `call_llm` multiple times to generate search queries, extract answers, and perform validation. The overall workflow is: question -> search query generation -> simulated information retrieval -> answer extraction -> validation -> final answer."
  },
  {
    "iteration": 1,
    "strategy": "Exploitation",
    "accuracy": 0.3333333333333333,
    "approach": "The script uses LLM-based techniques with agent roles for information extraction, search query generation, answer extraction, and answer validation to answer questions. It decomposes the problem into extracting information, generating a search query, simulating search results using the LLM, extracting the answer, and validating the answer. The functions used are `call_llm`, `extract_information`, `generate_search_query`, `extract_answer`, `validate_answer`, and `main`. The `main` function orchestrates the process by calling the functions sequentially, starting with information extraction and ending with answer validation, using the output of one function as input to the next."
  },
  {
    "iteration": 2,
    "strategy": "Exploration",
    "accuracy": 0.0,
    "approach": "The script uses a multi-stage reasoning approach with LLMs to answer factual questions, simulating knowledge graph integration. The problem is decomposed into entity/relationship extraction, knowledge graph lookup, answer synthesis, and validation, each handled by the `call_llm` function with specific system instructions to define agent roles. The `call_llm` function is used to query the Gemini LLM for each stage, passing tailored prompts and system instructions to guide the LLM's behavior. The overall workflow involves extracting entities, looking up information, synthesizing an answer, validating it, and returning the answer if valid."
  },
  {
    "iteration": 3,
    "strategy": "Exploration",
    "accuracy": 0.0,
    "approach": "The script implements a RAG approach with explicit source identification and verification, using the Gemini LLM to answer factual questions. The problem is decomposed into generating a context query, retrieving context from a simulated knowledge base, extracting the answer with source citation, and verifying the extracted answer against the cited source. Three agent roles are involved: one for generating context queries, one for extracting answers, and one for verifying answers. The `call_llm` function is used to interact with the Gemini model, `main` orchestrates the overall workflow, calling `call_llm` to generate a query, extract an answer, and verify the answer based on a knowledge base."
  },
  {
    "iteration": 4,
    "strategy": "Exploration",
    "accuracy": 0.3333333333333333,
    "approach": "The script uses a fact verification approach with multi-source integration, leveraging the Gemini LLM to answer factual questions. It decomposes the problem into generating multiple search queries, simulating context retrieval from these queries while verifying their relevance, extracting answers from each context, synthesizing a final answer, and validating that final answer. The agent roles include an expert at generating diverse search queries, an agent for validating retrieved context, an expert at extracting and synthesizing answers, and an expert answer validator. The functions used are `call_llm` which interacts with the Gemini LLM, and `main` which orchestrates the overall workflow involving prompt generation, LLM calls, and result validation."
  },
  {
    "iteration": 5,
    "strategy": "Exploration",
    "accuracy": 0.0,
    "approach": "The script addresses factual questions using an iterative refinement and extraction approach driven by the Gemini LLM. It starts by generating an initial search query from the input question and simulating information retrieval. If the retrieved information is insufficient, the question is refined using the LLM, followed by another round of information retrieval. Finally, the answer is extracted and verified.\n\nThe agent roles involved are search query generator, information sufficiency assessor, question refiner, question answering system, and validation expert.\n\nKey functions include:\n- `call_llm`: Used to interact with the Gemini LLM to generate search queries, assess sufficiency, refine questions, extract answers, and verify the extracted answer.\n- `main`: Orchestrates the entire process, iteratively refining the question, retrieving information, and extracting the answer."
  },
  {
    "iteration": 6,
    "strategy": "Exploitation",
    "accuracy": 0.3333333333333333,
    "approach": "The script implements a question-answering system using the Gemini LLM through a series of steps involving information extraction, search query generation, answer extraction, and validation. Each step is handled by a distinct LLM agent with a specific role defined by a system instruction. The problem is decomposed into these subtasks, leveraging the LLM's capabilities for each. The function names used are `call_llm` to interact with the LLM, `extract_information` to extract entities and constraints from the question, `generate_search_query` to generate a search query, `extract_answer` to extract the answer and a confidence score from search results, `validate_answer` to validate the extracted answer, and `main` to orchestrate the entire process. The workflow involves sequentially calling these functions, passing the output of one as input to the next, with the final answer being validated before being returned."
  },
  {
    "iteration": 7,
    "strategy": "Exploration",
    "accuracy": 0.0,
    "approach": "The script uses LLM-based decomposition and targeted retrieval to answer factual questions. It decomposes the question into known and unknown parts, generates a targeted search query based on the unknown information, simulates retrieval, and extracts a concise answer, then validates if the extracted answer answers the question. The LLM acts as an expert at breaking down questions, generating targeted search queries, concise answer extraction, and as an expert validator.\n\nThe functions used are:\n- `call_llm`: Calls the Gemini API with a given prompt and optional system instruction.\n- `main`: Orchestrates the question-answering process by decomposing the question, generating a targeted query, simulating retrieval, extracting the answer, and validating it.\n\nThe overall workflow involves `main` calling `call_llm` multiple times to perform decomposition, query generation, answer extraction, and validation, with the results of each call influencing the subsequent steps."
  }
]

        COMMON ERROR PATTERNS:
        []

        PRIMARY ISSUES (last 3 iterations):
        [
  {
    "iteration": 0,
    "issue": "The single most critical problem is the **overly strict or flawed validation process.** The system rejects potentially correct answers, leading to a \"Could not be validated\" response even when a valid answer exists. This is likely due to a high confidence threshold for what constitutes a valid answer, incorrect logic in comparing retrieved information with the question's requirements, or an incomplete knowledge base."
  },
  {
    "iteration": 1,
    "issue": "The primary issue is the system's **lack of robust reasoning and inference capabilities when processing search results**. It needs to be able to synthesize information from multiple sources, resolve conflicts, and verify answers against a broader knowledge base or logic."
  },
  {
    "iteration": 2,
    "issue": "The single most critical problem is **inaccurate knowledge retrieval** leading to the generation of incorrect answers. This manifests in the system providing a different answer than the provided \"golden answer,\" which indicates an issue with the accuracy of the data the system accesses or the way it queries and interprets the data."
  },
  {
    "iteration": 3,
    "issue": "The single most critical problem to fix is the **ineffective information extraction**. The system's current approach is not successfully identifying and retrieving the answer from the context provided, leading to a consistent \"no answer\" response."
  },
  {
    "iteration": 4,
    "issue": "The primary issue is the system's passive behavior when faced with insufficient information. It needs to be augmented with the capability to actively seek out the necessary information via external knowledge sources (e.g., internet search). It needs a mechanism to detect if information is lacking, formulate a relevant query, retrieve information, extract the answer, and then respond to the original question."
  },
  {
    "iteration": 5,
    "issue": "The most critical problem is the **inability to reliably retrieve and validate information from the knowledge source**. This leads to \"Could not be validated\" or \"Unavailable\" responses, hindering the system's ability to answer questions. The validation logic is likely too sensitive or the knowledge retrieval is inaccurate."
  },
  {
    "iteration": 6,
    "issue": "The most critical problem is the **lack of a robust, multi-source validation mechanism** for extracted answers. The system's current validation process is too simplistic and doesn't account for potentially conflicting information or the need to prioritize credible sources."
  },
  {
    "iteration": 7,
    "issue": "The single most critical problem is the **ineffective information retrieval** due to poorly constructed queries and a lack of iterative search refinement. This leads to an inability to find the required information to answer the questions, regardless of the complexity of the question decomposition logic."
  }
]

        TARGETED IMPROVEMENTS:
        [
  "Implement a confidence score for each extracted answer and only reject answers with very low confidence scores.",
  "Implement Error Feedback Loop:** When the system fails to validate an answer, implement a mechanism to analyze the failed query and automatically improve the knowledge retrieval process.",
  "Relax the validation criteria to allow for slight variations in answer format. For example, allow \"March, 2001\" and \"March 2001\" to be considered equivalent.",
  "Relax Validation Criteria (Cautiously):** Experiment with less strict validation criteria to reduce the number of false negatives. However, be careful not to introduce more incorrect answers.",
  "Add a component that analyzes the question type and tailors the query generation strategy accordingly (e.g., date-related questions should generate queries that specifically target dates).",
  "Set a maximum number of iterations to prevent infinite loops.",
  "Refine Knowledge Base Indexing and Search:** Evaluate the efficiency and accuracy of the knowledge base indexing and search mechanisms. Consider using techniques like semantic search or entity linking to improve retrieval performance.",
  "After the initial query, analyze the search results (even if no direct answer is found).",
  "If no direct answer is found, modify the query based on the initial results. This could involve adding related terms, removing less relevant terms, or trying different search engines.",
  "Implement a broader range of query generation strategies, including using keywords from the question, identifying key entities, and incorporating related terms."
]
        

EXAMPLE OF EFFECTIVE LLM USAGE PATTERNS:

```python
def extract_information_with_examples(text):
    """Extract key information from the input text using embedded examples."""
    system_instruction = "You are an information extraction specialist focusing on identifying key entities and relationships."
    
    prompt = f"""
    Extract key information from this text. Focus on identifying all entities, relationships, and important attributes.
    
    Example usage:
    
    Input Text:
    The company XYZ Corp reported quarterly earnings of $3.5 million, which represents a 12% increase from last year. The CEO, Jane Smith, attributed this growth to their new product line launched in March, which has already captured 8% of the market share. They expect to expand their operations to Europe by Q2 2023.
    
    Let's think step by step.
    
    The key entities are:
    - XYZ Corp (company)
    - Jane Smith (person, CEO)
    - New product line (product)
    
    The key information points are:
    - Financial: Quarterly earnings of $3.5 million
    - Performance: 12% increase from previous year
    - Product: New product line launched in March
    - Market: 8% market share for new product
    - Plans: Expansion to Europe by Q2 2023
    
    Extracted Information:
    {{
      "entities": [
        {{"name": "XYZ Corp", "type": "company"}},
        {{"name": "Jane Smith", "type": "person", "role": "CEO"}},
        {{"name": "New product line", "type": "product", "launch_date": "March"}}
      ],
      "financial_data": {{
        "quarterly_earnings": "$3.5 million",
        "growth_rate": "12%"
      }},
      "market_data": {{
        "product_market_share": "8%"
      }},
      "future_plans": [
        {{"type": "expansion", "region": "Europe", "timeline": "Q2 2023"}}
      ]
    }}
    
    Now, extract information from this new text:
    {text}
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def verify_solution_with_examples(problem, proposed_solution):
    """Verify if the proposed solution satisfies all requirements using embedded examples."""
    system_instruction = "You are a critical evaluator who verifies if solutions correctly address problems."
    
    prompt = f"""
    Verify if this proposed solution correctly addresses all aspects of the problem.
    
    Example usage:
    
    Problem:
    Design a data structure that can efficiently perform the following operations:
    1. Insert a value
    2. Delete a value
    3. Get a random value with equal probability for all stored values
    All operations should have average time complexity of O(1).
    
    Proposed Solution:
    I'll use a combination of a hashmap and an array. The hashmap will store the value as the key and its index in the array as the value. The array will store all the inserted values.
    
    For insert: Add the value to the end of the array and update the hashmap with the value and its index. O(1) time.
    
    For delete: Look up the index of the value in the hashmap, swap the value with the last element in the array, update the hashmap for the swapped element, remove the last element from the array, and remove the value from the hashmap. O(1) time.
    
    For get random: Generate a random index within the array's bounds and return the value at that index. O(1) time.
    
    Verification:
    Let me check each requirement:
    1. Insert operation: The solution adds the value to the end of the array and updates the hashmap with O(1) time complexity ✓
    2. Delete operation: The solution uses the hashmap to find the index, then swaps with the last element and updates accordingly with O(1) time complexity ✓
    3. Get random operation: The solution generates a random index within the array bounds with O(1) time complexity ✓
    4. All operations have O(1) average time complexity ✓
    
    Result: VALID - The solution correctly addresses all requirements with the specified time complexity.
    
    Problem:
    {problem}
    
    Proposed Solution:
    {proposed_solution}
    
    Verification:
    """
    
    return call_llm(prompt, system_instruction)
```

```python
def solve_with_validation_loop(problem, max_attempts=3):
    """Solve a problem with iterative refinement through validation feedback loop."""
    system_instruction_solver = "You are an expert problem solver who creates detailed, correct solutions."
    system_instruction_validator = "You are a critical validator who carefully checks solutions against all requirements."
    
    # Initial solution generation
    solution_prompt = f"""
    Provide a detailed solution to this problem. Be thorough and ensure you address all requirements.
    
    Problem:
    {problem}
    """
    
    solution = call_llm(solution_prompt, system_instruction_solver)
    
    # Validation loop
    for attempt in range(max_attempts):
        # Validate the current solution
        validation_prompt = f"""
        Carefully validate if this solution correctly addresses all aspects of the problem.
        If the solution is valid, respond with "VALID: [brief reason]".
        If the solution has any issues, respond with "INVALID: [detailed explanation of issues]".
        
        Problem:
        {problem}
        
        Proposed Solution:
        {solution}
        """
        
        validation_result = call_llm(validation_prompt, system_instruction_validator)
        
        # Check if solution is valid
        if validation_result.startswith("VALID:"):
            return solution
        
        # If invalid, refine the solution
        refined_prompt = f"""
        Your previous solution to this problem has some issues that need to be addressed.
        
        Problem:
        {problem}
        
        Your previous solution:
        {solution}
        
        Validation feedback:
        {validation_result}
        
        Please provide a completely revised solution that addresses all the issues mentioned.
        """
        
        solution = call_llm(refined_prompt, system_instruction_solver)
    
    return solution
```

```python
def multi_perspective_analysis(problem):
    """Analyze a problem from multiple specialized perspectives and synthesize the insights."""
    # Define specialized analysis functions
    def analyze_factual_content(problem):
        system_instruction = "You are a factual analyst who focuses on identifying key facts and data points."
        prompt = f"""
        Analyze this problem for factual content only. Identify explicit facts, constraints, and requirements.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    def analyze_structure(problem):
        system_instruction = "You are a structural analyst who specializes in problem organization and patterns."
        prompt = f"""
        Analyze the structure of this problem. Identify its components, relationships, and patterns.
        
        Problem:
        {problem}
        """
        return call_llm(prompt, system_instruction)
    
    # Execute parallel analyses
    factual_analysis = analyze_factual_content(problem)
    structural_analysis = analyze_structure(problem)
    
    # Synthesize the results
    synthesis_prompt = f"""
    Synthesize these two different analyses of the same problem into a comprehensive understanding.
    
    Factual Analysis:
    {factual_analysis}
    
    Structural Analysis:
    {structural_analysis}
    
    Provide a unified analysis that leverages both perspectives.
    """
    
    return call_llm(synthesis_prompt, "You are an insight synthesizer who combines multiple analyses.")
```

```python
def best_of_n_approach(problem, n=3):
    """Generate multiple solutions and select the best one based on a quality evaluation."""
    system_instruction_solver = "You are an expert problem solver who provides detailed, correct solutions."
    system_instruction_evaluator = "You are a quality evaluator who assesses solutions based on correctness, completeness, and clarity."
    
    # Generate n different solutions
    solutions = []
    for i in range(n):
        diversity_factor = f"Solution approach {i+1}/{n}: Use a different perspective from previous solutions."
        solution_prompt = f"""
        Provide a detailed solution to this problem.
        {diversity_factor if i > 0 else ""}
        
        Problem:
        {problem}
        """
        
        solutions.append(call_llm(solution_prompt, system_instruction_solver))
    
    # Evaluate each solution
    evaluations = []
    for i, solution in enumerate(solutions):
        evaluation_prompt = f"""
        Evaluate this solution on correctness, completeness, and clarity (1-10 scale).
        
        Problem:
        {problem}
        
        Solution {i+1}:
        {solution}
        
        Provide your evaluation as a JSON with scores and explanation.
        """
        
        evaluations.append(call_llm(evaluation_prompt, system_instruction_evaluator))
    
    # Find the best solution
    comparison_prompt = f"""
    Compare these solutions and their evaluations. Select the best one.
    
    Problem:
    {problem}
    
    {["Solution " + str(i+1) + ": " + solutions[i] + "\n\nEvaluation: " + evaluations[i] for i in range(n)]}
    
    Which solution is best? Respond with the solution number and explanation.
    """
    
    best_solution_index = int(call_llm(comparison_prompt, "You are a solution selector.").split()[1]) - 1
    return solutions[best_solution_index]
```

```python
def solve_with_react_pattern(problem):
    """Solve problems through iterative Reasoning and Acting (ReAct) approach."""
    system_instruction = "You are a problem-solving agent that follows the ReAct pattern: Reason about the current state, take an Action, observe the result, and repeat until reaching a solution."
    
    # Initialize ReAct process
    prompt = f"""
    Solve this problem using the ReAct pattern - alternate between Reasoning and Acting until you reach a final answer.
    
    Example usage:
    
    Problem: What is the capital of the country where the Great Barrier Reef is located, and what is the population of that capital?
    
    Thought 1: I need to determine which country the Great Barrier Reef is in, then find its capital, and finally the population of that capital.
    Action 1: Search[Great Barrier Reef location]
    Observation 1: The Great Barrier Reef is located off the coast of Queensland in northeastern Australia.
    
    Thought 2: Now I know the Great Barrier Reef is in Australia. I need to find Australia's capital city.
    Action 2: Search[capital of Australia]
    Observation 2: The capital of Australia is Canberra.
    
    Thought 3: Now I need to find the population of Canberra.
    Action 3: Search[population of Canberra]
    Observation 3: As of 2021, the population of Canberra is approximately 431,500.
    
    Thought 4: I have found all the required information. The capital of Australia (where the Great Barrier Reef is located) is Canberra, and its population is approximately 431,500.
    Action 4: Finish[The capital of Australia is Canberra, with a population of approximately 431,500.]
    
    Now solve this new problem:
    {problem}
    
    Start with Thought 1:
    """
    
    # Initial reasoning and action planning
    react_response = call_llm(prompt, system_instruction)
    
    # Extract the action from the response
    action = extract_action(react_response)
    
    # Continue the ReAct loop until we reach a "Finish" action
    while not action["type"] == "Finish":
        # Perform the requested action and get an observation
        if action["type"] == "Search":
            observation = perform_search(action["query"])
        elif action["type"] == "Calculate":
            observation = perform_calculation(action["expression"])
        elif action["type"] == "Lookup":
            observation = perform_lookup(action["term"])
        else:
            observation = f"Unknown action type: {action['type']}"
        
        # Continue the ReAct process with the new observation
        continuation_prompt = f"""
        {react_response}
        Observation {action["step_number"]}: {observation}
        
        Continue with the next thought and action:
        """
        
        # Get the next reasoning step and action
        react_response += "\n" + call_llm(continuation_prompt, system_instruction)
        
        # Extract the next action
        action = extract_action(react_response)
    
    # Extract the final answer from the Finish action
    final_answer = action["answer"]
    return final_answer

def extract_action(text):
    """Parse the ReAct response to extract the current action."""
    # Find the last action in the text
    action_matches = re.findall(r"Action (\d+): (\w+)\[(.*?)\]", text)
    if not action_matches:
        return {"type": "Error", "step_number": 0, "query": "No action found"}
    
    # Get the most recent action
    last_action = action_matches[-1]
    step_number = int(last_action[0])
    action_type = last_action[1]
    action_content = last_action[2]
    
    # Handle different action types
    if action_type == "Finish":
        return {"type": "Finish", "step_number": step_number, "answer": action_content}
    elif action_type in ["Search", "Lookup", "Calculate"]:
        return {"type": action_type, "step_number": step_number, "query": action_content}
    else:
        return {"type": "Unknown", "step_number": step_number, "query": action_content}

def perform_search(query):
    """Simulate a search action in the ReAct pattern."""
    # In a real implementation, this would call an actual search API
    return call_llm(f"Provide a factual answer about: {query}", "You are a helpful search engine that provides concise, factual information.")

def perform_calculation(expression):
    """Perform a calculation action in the ReAct pattern."""
    try:
        # Safely evaluate the expression
        result = eval(expression, {"__builtins__": {}}, {"math": math})
        return f"The result is {result}"
    except Exception as e:
        return f"Error in calculation: {str(e)}"

def perform_lookup(term):
    """Simulate a lookup action for specific information."""
    # In a real implementation, this would query a knowledge base or database
    return call_llm(f"Provide specific information about: {term}", "You are a knowledge base that provides specific factual information.")
```MULTI-EXAMPLE PROMPTING GUIDANCE:
        1. CRITICAL: Use MULTIPLE examples (2-5) in EVERY LLM prompt, not just one
        2. Vary the number of examples based on task complexity - more complex tasks need more examples
        3. Select diverse examples that showcase different patterns and edge cases
        4. Structure your few-shot examples to demonstrate clear step-by-step reasoning
        5. Consider using both "easy" and "challenging" examples to help the LLM learn from contrasts
        6. The collection of examples should collectively cover all key aspects of the problem
        7. When available, use examples from previous iterations that revealed specific strengths or weaknesses.
        8. USE REAL EXAMPLES FROM THE DATASET WHERE POSSIBLE!!

        Example of poor single-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        Example of effective multi-example prompting:
        ```python
        def extract_entities(text):
            prompt = f'''
            Extract entities from this text.

            Example 1:
            Text: John will meet Mary at 3pm on Tuesday.
            Entities: {{"people": ["John", "Mary"], "time": "3pm", "day": "Tuesday"}}

            Example 2:
            Text: The team needs to submit the report by Friday at noon.
            Entities: {{"people": ["the team"], "time": "noon", "day": "Friday", "object": "report"}}

            Example 3:
            Text: Alex cannot attend the conference from Jan 3-5 due to prior commitments.
            Entities: {{"people": ["Alex"], "event": "conference", "date_range": ["Jan 3-5"], "reason": "prior commitments"}}

            Text: {text}
            Entities:
            '''
            return call_llm(prompt)
        ```

        === DIRECT LLM REASONING APPROACH ===

        CRITICAL: Previous scripts have shown that complex code generation with JSON parsing and multi-step pipelines often 
        leads to errors and low performance. Instead, focus on leveraging the LLM's natural reasoning abilities:

        1. SIMPLIFY YOUR APPROACH:
           - Minimize the number of processing steps - simpler is better
           - Directly use LLM for pattern recognition rather than writing complex code
           - Avoid trying to parse or manipulate JSON manually - pass it as text to the LLM

        2. DIRECT TRANSFORMATION:
           - Instead of trying to extract features and then apply them, use the LLM to do the transformation directly
           - Use examples to teach the LLM the pattern, then have it apply that pattern to new inputs
           - Avoid attempting to write complex algorithmic solutions when pattern recognition will work better

        3. ROBUST ERROR HANDLING:
           - Include multiple approaches in case one fails (direct approach + fallback approach)
           - Use simple validation to check if outputs are in the expected format
           - Include a last-resort approach that will always return something valid

        4. AVOID COMMON PITFALLS:
           - Do NOT attempt to use json.loads() or complex JSON parsing - it often fails
           - Do NOT create overly complex Python pipelines that require perfect indentation
           - Do NOT create functions that generate or execute dynamic code
           - Do NOT create unnecessarily complex data transformations

        5. SUCCESSFUL EXAMPLES:
           - The most successful approaches have used direct pattern matching with multiple examples
           - Scripts with simple validation and fallback approaches perform better
           - Scripts with fewer processing steps have higher success rates
        
        IMPLEMENTATION STRATEGIES:
        1. Maintain a "example bank" of successful and failed examples to select from
        2. Implement n-shot prompting with n=3 as default, but adapt based on performance
        3. For complex tasks, use up to 5 examples; for simpler tasks, 2-3 may be sufficient
        4. Include examples with a range of complexity levels, rather than all similar examples



        VALIDATION AND VERIFICATION GUIDANCE:
        1. CRITICAL: Consider implementing validation loops for EACH key processing step, not just final outputs
        2. Design your system to detect, diagnose, and recover from specific errors. This will help future learnings
        3. For every LLM extraction or generation, add a verification step that checks:
           - Whether the output is well-formed and complete
           - Whether the output is logically consistent with the input
           - Whether all constraints are satisfied
        4. Add feedback loops that retry failures with specific feedback
        5. Include diagnostic outputs that reveal exactly where failures occur. Add print statements and intermediate outputs such that you can see them later to determine why things are going wrong.
        6. Include capability to trace through execution steps to identify failure points

        Example of pipeline without verification:
        ```python
        def process_question(question):
            entities = extract_entities(question)
            constraints = identify_constraints(question)
            solution = generate_solution(entities, constraints)
            return solution
        ```

        Example of robust pipeline with verification:
        ```python
        def process_question(question, max_attempts=3):
            # Step 1: Extract entities with verification
            entities_result = extract_entities_with_verification(question)
            if not entities_result.get("is_valid"):
                print(f"Entity extraction failed: {entities_result.get('validation_feedback')}")
                return f"Error in entity extraction: {entities_result.get('validation_feedback')}"

            # Step 2: Identify constraints with verification
            constraints_result = identify_constraints_with_verification(question, entities_result["entities"])
            if not constraints_result.get("is_valid"):
                print(f"Constraint identification failed: {constraints_result.get('validation_feedback')}")
                return f"Error in constraint identification: {constraints_result.get('validation_feedback')}"

            # Step 3: Generate solution with verification
            solution_result = generate_solution_with_verification(
                question, 
                entities_result["entities"], 
                constraints_result["constraints"]
            )
            if not solution_result.get("is_valid"):
                print(f"Solution generation failed: {solution_result.get('validation_feedback')}")
                return f"Error in solution generation: {solution_result.get('validation_feedback')}"

            return solution_result["solution"]

        def extract_entities_with_verification(question, max_attempts=3):
            #Extract entities and verify their validity with feedback loop.
            system_instruction = "You are an expert at extracting and validating entities."

            for attempt in range(max_attempts):
                # First attempt at extraction
                extraction_prompt = f'''
                Extract key entities from this question. 
                Return a JSON object with the extracted entities.

                Example 1: [example with entities]
                Example 2: [example with different entities]
                Example 3: [example with complex entities]

                Question: {question}
                Extraction:
                '''

                extracted_data = call_llm(extraction_prompt, system_instruction)

                try:
                    # Parse the extraction
                    data = json.loads(extracted_data)

                    # Verification step
                    verification_prompt = f'''
                    Verify if these extracted entities are complete and correct:

                    Question: {question}
                    Extracted entities: {json.dumps(data, indent=2)}

                    Check if:
                    1. All relevant entities are extracted
                    2. No irrelevant entities are included
                    3. All entity values are correct

                    Return a JSON with:
                    {{
                      "is_valid": true/false,
                      "validation_feedback": "detailed explanation",
                      "missing_entities": ["entity1", "entity2"],
                      "incorrect_entities": ["entity3"]
                    }}
                    '''

                    verification_result = call_llm(verification_prompt, system_instruction)
                    verification_data = json.loads(verification_result)

                    if verification_data.get("is_valid", False):
                        data["is_valid"] = True
                        data["validation_feedback"] = "All entities are valid."
                        return data

                    # If not valid and we have attempts left, refine with feedback
                    if attempt < max_attempts - 1:
                        feedback = verification_data.get("validation_feedback", "")
                        print(f"Validation failed (attempt {attempt+1}/{max_attempts}): {feedback}")
                        continue

                    # If we're out of attempts, return the best we have with validation info
                    data["is_valid"] = False
                    data["validation_feedback"] = verification_data.get("validation_feedback", "Unknown validation error")
                    return data

                except Exception as e:
                    print(f"Error in extraction/validation (attempt {attempt+1}/{max_attempts}): {str(e)}")
                    if attempt >= max_attempts - 1:
                        return {
                            "is_valid": False,
                            "validation_feedback": f"Error during processing: {str(e)}"
                        }

            return {
                "is_valid": False,
                "validation_feedback": "Failed to extract valid entities after multiple attempts."
            }
        ```

        VALIDATION IMPLEMENTATION STRATEGIES:
        1. Create detailed verification functions for each major processing step
        2. Implement max_attempts limits on all retry loops (typically 3-5 attempts)
        3. Pass specific feedback from verification to subsequent retry attempts
        4. Log all verification failures to help identify systemic issues
        5. Design fallback behaviors when verification repeatedly fails

        

            PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS). YOUR APPROACH MUST BE SUBSTANTIVELY DIFFERENT THAN THESE:
            
PREVIOUSLY TRIED APPROACHES (LAST 5 SCRIPTS):

=== SCRIPT FROM ITERATION 7 (Exploration, ACCURACY: 0.00) ===
Approach: The script uses LLM-based decomposition and targeted retrieval to answer factual questions. It decomposes the question into known and unknown parts, generates a targeted search query based on the unknown information, simulates retrieval, and extracts a concise answer, then validates if the extracted answer answers the question. The LLM acts as an expert at breaking down questions, generating targeted search queries, concise answer extraction, and as an expert validator.

The functions used are:
- `call_llm`: Calls the Gemini API with a given prompt and optional system instruction.
- `main`: Orchestrates the question-answering process by decomposing the question, generating a targeted query, simulating retrieval, extracting the answer, and validating it.

The overall workflow involves `main` calling `call_llm` multiple times to perform decomposition, query generation, answer extraction, and validation, with the results of each call influencing the subsequent steps.

```python
import os
import re
import math

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question, max_attempts=3):
    """Solve factual questions using a new approach: Decomposition and targeted retrieval with active feedback."""

    # Hypothesis: Answering questions effectively requires a focused retrieval strategy, explicitly asking for *what we don't know* to guide the search process. This strategy will actively seek missing information, improving on the passive behavior observed in previous attempts.
    # 1. Decompose into known and unknown
    # 2. Target retrieval based on what is unknown
    # 3. Iteratively refine search strategy by focusing on what is *still* unknown

    # Step 1: Decompose question into KNOWN and UNKNOWN elements (with examples)
    decomposition_prompt = f"""
    Decompose the following question into what information is ALREADY KNOWN and what information is UNKNOWN and needs to be retrieved.

    Example 1:
    Question: What is the capital of Australia?
    Known: Australia, has a capital
    Unknown: The name of the capital

    Example 2:
    Question: In what year did Etta Cone last visit Europe?
    Known: Etta Cone visited Europe
    Unknown: The specific year of her last visit

    Question: {question}
    Known:
    Unknown:
    """

    decomposition_result = call_llm(decomposition_prompt, system_instruction="You are an expert at breaking down questions.")

    try:
        known = decomposition_result.split('Unknown:')[0].replace('Known:','').strip()
        unknown = decomposition_result.split('Unknown:')[1].strip()
    except:
        return "Error in decomposing knowns and unknowns"
    print (f"Known: {known}")
    print (f"Unknown: {unknown}")

    # Step 2: Generate targeted retrieval query focused on the UNKNOWN (with examples)
    targeted_query_prompt = f"""
    Based on the KNOWN and UNKNOWN elements, generate a highly targeted search query focused on retrieving the UNKNOWN.

    Example 1:
    Known: Australia, has a capital
    Unknown: The name of the capital
    Query: "capital of Australia"

    Example 2:
    Known: Etta Cone visited Europe
    Unknown: The specific year of her last visit
    Query: "Etta Cone last visit Europe year"

    Known: {known}
    Unknown: {unknown}
    Query:
    """

    targeted_query = call_llm(targeted_query_prompt, system_instruction="You are an expert at generating highly targeted search queries.")
    print (f"Targeted Query: {targeted_query}")

    # Step 3: Simulate Retrieval
    retrieved_info = f"Simulated web search results for: {targeted_query}. Placeholder for real search functionality."

    # Step 4: Extract the answer (with examples)
    answer_extraction_prompt = f"""
    Given the original question and the retrieved information, extract a CONCISE answer.
    Example 1:
    Question: What is the capital of Australia?
    Retrieved Info: Canberra is the capital city of Australia.
    Answer: Canberra

    Example 2:
    Question: In what year did Etta Cone last visit Europe?
    Retrieved Info: Etta Cone's last visit to Europe was in 1951.
    Answer: 1951

    Question: {question}
    Retrieved Info: {retrieved_info}
    Answer:
    """
    extracted_answer = call_llm(answer_extraction_prompt, system_instruction="You are an expert at concise answer extraction.")
    print (f"Extracted Answer: {extracted_answer}")

    # Step 5: VALIDATION: Does extracted answer actually answer the question?
    validation_prompt = f"""
    Does the extracted answer actually answer the ORIGINAL question? Respond with "YES" or "NO".

    Question: {question}
    Extracted Answer: {extracted_answer}

    Example 1:
    Question: What is the capital of Australia?
    Extracted Answer: Canberra
    Does the extracted answer actually answer the ORIGINAL question? Respond with "YES" or "NO".
    YES

    Example 2:
    Question: In what year did Etta Cone last visit Europe?
    Extracted Answer: 1951
    Does the extracted answer actually answer the ORIGINAL question? Respond with "YES" or "NO".
    YES

    Original Question: {question}
    Extracted Answer: {extracted_answer}
    Does the extracted answer actually answer the ORIGINAL question? Respond with "YES" or "NO".
    """
    validation_result = call_llm(validation_prompt, system_instruction="You are an expert validator who determines if the question is answered.").strip()

    if "YES" in validation_result.upper():
      return extracted_answer
    else:
      return "Could not be validated. Not an answer."
```

=== SCRIPT FROM ITERATION 6 (Exploitation, ACCURACY: 0.33) ===
Approach: The script implements a question-answering system using the Gemini LLM through a series of steps involving information extraction, search query generation, answer extraction, and validation. Each step is handled by a distinct LLM agent with a specific role defined by a system instruction. The problem is decomposed into these subtasks, leveraging the LLM's capabilities for each. The function names used are `call_llm` to interact with the LLM, `extract_information` to extract entities and constraints from the question, `generate_search_query` to generate a search query, `extract_answer` to extract the answer and a confidence score from search results, `validate_answer` to validate the extracted answer, and `main` to orchestrate the entire process. The workflow involves sequentially calling these functions, passing the output of one as input to the next, with the final answer being validated before being returned.

```python
import os
import re
import math

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def extract_information(question):
    """Extract key information from the question, including entities and constraints."""
    system_instruction = "You are an expert information extractor."
    prompt = f"""
    Extract the key entities and constraints from the following question.

    Example 1:
    Question: What is the capital of the country where the Great Barrier Reef is located?
    Entities: Great Barrier Reef, Australia (country)
    Constraints: Location is a country, seeking its capital

    Example 2:
    Question: How many corners did Barcelona take in the Champions League semi-final match between Barcelona and Milan on April 27, 2006?
    Entities: Barcelona, Champions League, Milan, April 27, 2006
    Constraints: Corners taken by Barcelona, in that specific match

    Question: {question}
    Entities and Constraints:
    """
    return call_llm(prompt, system_instruction)

def generate_search_query(question, extracted_info):
    """Generate a search query based on the question and extracted information."""
    system_instruction = "You are a search query generator."
    prompt = f"""
    Generate a search query to answer the question, using the extracted information.

    Example 1:
    Question: What is the capital of Australia?
    Extracted Info: Australia, capital
    Search Query: "capital of Australia"

    Example 2:
    Question: How many corners did Barcelona take in the Champions League semi-final match between Barcelona and Milan on April 27, 2006?
    Extracted Info: Barcelona, Champions League, Milan, April 27, 2006, corners
    Search Query: "Barcelona Milan Champions League April 27 2006 corner kicks statistics"

    Question: {question}
    Extracted Info: {extracted_info}
    Search Query:
    """
    return call_llm(prompt, system_instruction)

def extract_answer(question, search_results):
    """Extract the answer from the search results and provide a confidence score."""
    system_instruction = "You are an answer extraction expert."
    prompt = f"""
    Extract the answer to the question from the search results and provide a confidence score (1-10).
    Provide the answer even if you are not 100% sure - the validator will check.

    Example 1:
    Question: What is the capital of Australia?
    Search Results: Canberra is the capital city of Australia.
    Answer: Canberra (Confidence: 9)

    Example 2:
    Question: How many corners did Barcelona take in the Champions League semi-final match between Barcelona and Milan on April 27, 2006?
    Search Results: Barcelona took 3 corners in the match.
    Answer: 3 (Confidence: 8)

    Question: {question}
    Search Results: {search_results}
    Answer:
    """
    return call_llm(prompt, system_instruction)

def validate_answer(question, answer, search_results):
    """Validate if the extracted answer is correct and satisfies the question's requirements."""
    system_instruction = "You are an answer validator. Be strict. Use the search results to validate your answer."
    prompt = f"""
    Validate if the extracted answer is correct and satisfies the question's requirements, using the search results.
    Provide a detailed explanation of your validation process.

    Example 1:
    Question: What is the capital of Australia?
    Answer: Canberra
    Search Results: Canberra is the capital city of Australia.
    Validation: VALID - The answer Canberra is correct according to the search results.

    Example 2:
    Question: How many corners did Barcelona take in the Champions League semi-final match between Barcelona and Milan on April 27, 2006?
    Answer: 3
    Search Results: Barcelona took 3 corners in the match.
    Validation: VALID - The answer 3 is correct according to the search results.

    Question: {question}
    Answer: {answer}
    Search Results: {search_results}
    Validation:
    """
    return call_llm(prompt, system_instruction)

def main(question):
    """Main function to answer the question."""
    try:
        # Step 1: Extract information
        extracted_info = extract_information(question)
        print(f"Extracted Info: {extracted_info}")

        # Step 2: Generate search query
        search_query = generate_search_query(question, extracted_info)
        print(f"Search Query: {search_query}")

        # Step 3: Simulate information retrieval
        search_results = call_llm(search_query, "You are a helpful search engine that provides concise, factual information.")
        print(f"Search Results: {search_results}")

        # Step 4: Extract answer
        extracted_answer_raw = extract_answer(question, search_results)
        print(f"Extracted Answer (raw): {extracted_answer_raw}")
        
        #Split out answer and confidence score
        try:
            extracted_answer = extracted_answer_raw.split('(Confidence:')[0].strip()
            confidence = int(extracted_answer_raw.split('(Confidence:')[1].replace(')','').strip())
        except:
            extracted_answer = extracted_answer_raw
            confidence = 5 #low confidence score to force validation to work

        # Step 5: Validate answer
        validation_result = validate_answer(question, extracted_answer, search_results)
        print(f"Validation Result: {validation_result}")

        if "VALID" in validation_result:
            return extracted_answer
        else:
            return "Could not be validated."
    except Exception as e:
        print(f"Error: {str(e)}")
        return f"Error: {str(e)}"
```

=== SCRIPT FROM ITERATION 5 (Exploration, ACCURACY: 0.00) ===
Approach: The script addresses factual questions using an iterative refinement and extraction approach driven by the Gemini LLM. It starts by generating an initial search query from the input question and simulating information retrieval. If the retrieved information is insufficient, the question is refined using the LLM, followed by another round of information retrieval. Finally, the answer is extracted and verified.

The agent roles involved are search query generator, information sufficiency assessor, question refiner, question answering system, and validation expert.

Key functions include:
- `call_llm`: Used to interact with the Gemini LLM to generate search queries, assess sufficiency, refine questions, extract answers, and verify the extracted answer.
- `main`: Orchestrates the entire process, iteratively refining the question, retrieving information, and extracting the answer.

```python
import os
import re
import math

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question, max_attempts=3):
    """Solve factual questions using an iterative question refinement and information extraction approach."""

    # Hypothesis: Refining the question itself based on initial retrieval failures, and using this refined question to get more specific information, will improve accuracy.
    # This addresses the passive behavior and insufficient context detection issues from previous iterations.

    # Step 1: Generate an initial search query based on the question (with examples)
    initial_search_query_prompt = f"""
    Given a factual question, generate a concise and effective initial search query.

    Example 1:
    Question: What was the first name of the wife of the American chemist Ralph E. Oesper?
    Search Query: Ralph E. Oesper wife

    Example 2:
    Question: Who formed the Dubai-based band Sho? in June 2009?
    Search Query: Dubai band Sho formed 2009
    
    Question: {question}
    Search Query:
    """
    initial_search_query = call_llm(initial_search_query_prompt, "You are a search query generator.")

    # Step 2: Simulate information retrieval with a limited context (with an example)
    retrieved_info = f"Simulated web search results for: {initial_search_query}. Limited context available."  # Replace with actual search API call
    
    # Step 3: Determine if the retrieved info is sufficient to answer the question (with example and validation)
    sufficiency_check_prompt = f"""
    Given a question and retrieved information, determine if the information is sufficient to answer the question.

    Example:
    Question: What was the first name of the wife of the American chemist Ralph E. Oesper?
    Retrieved Information: Ralph E. Oesper's wife was a chemist.
    Sufficient: No. The first name is missing.

    Question: {question}
    Retrieved Information: {retrieved_info}
    Sufficient:
    """
    sufficiency_result = call_llm(sufficiency_check_prompt, "You are a helpful expert at assessing information sufficiency.")

    # Step 4: If not sufficient, refine the question (with examples)
    if "No" in sufficiency_result:
        refine_question_prompt = f"""
        Given a question and the reason why the initial information was insufficient, refine the question to get a more specific answer.
        
        Example:
        Original Question: What was the first name of the wife of the American chemist Ralph E. Oesper?
        Reason: The first name is missing.
        Refined Question: What was the *first name* of Ralph E. Oesper's wife?

        Question: {question}
        Reason: {sufficiency_result}
        Refined Question:
        """
        refined_question = call_llm(refine_question_prompt, "You are an expert at refining questions.")

        # Step 5: Retrieve information using refined question.
        refined_search_query_prompt = f"""
        Given a refined question, generate a search query.
        Question: {refined_question}
        Search Query:
        """
        refined_search_query = call_llm(refined_search_query_prompt, "You are an search query generator.")

        retrieved_info = f"Simulated web search results for: {refined_search_query}. Specific information available."
    else:
        refined_question = question # If the sufficiency test passed

    # Step 6: Extract the answer from retrieved information (with examples)
    answer_extraction_prompt = f"""
    Given a question and retrieved information, extract the answer.
    Example:
    Question: What was the *first name* of Ralph E. Oesper's wife?
    Relevant Information: Helen Oesper was the wife of Ralph E. Oesper.
    Answer: Helen

    Question: {refined_question}
    Relevant Information: {retrieved_info}
    Answer:
    """
    extracted_answer = call_llm(answer_extraction_prompt, "You are an expert question answering system.")
    
    # Step 7: Verify answer with original question
    verification_prompt = f"""
    Verify that the following answer accurately addresses the *original* question:
    Original question: {question}
    Extracted Answer: {extracted_answer}
    Verification (Correct/Incorrect):
    """
    verification_result = call_llm(verification_prompt, "You are a validation expert")

    if "Correct" in verification_result:
        return extracted_answer
    else:
        return "Could not be validated."
```

=== SCRIPT FROM ITERATION 4 (Exploration, ACCURACY: 0.33) ===
Approach: The script uses a fact verification approach with multi-source integration, leveraging the Gemini LLM to answer factual questions. It decomposes the problem into generating multiple search queries, simulating context retrieval from these queries while verifying their relevance, extracting answers from each context, synthesizing a final answer, and validating that final answer. The agent roles include an expert at generating diverse search queries, an agent for validating retrieved context, an expert at extracting and synthesizing answers, and an expert answer validator. The functions used are `call_llm` which interacts with the Gemini LLM, and `main` which orchestrates the overall workflow involving prompt generation, LLM calls, and result validation.

```python
import os
import re
import math

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question, max_attempts=3):
    """Solve factual questions using a fact verification with multi-source integration approach."""

    # Hypothesis: Explicitly searching for validating sources and integrating information from multiple sources before answering will increase accuracy. This addresses the previous issues of inaccurate knowledge retrieval and ineffective information extraction.

    # Step 1: Generate multiple search queries (n=3) to find validating sources.
    search_query_prompt = f"""
    Generate three diverse search queries to find independent validating sources for the following question.

    Example 1:
    Question: What is the name of the individual who was awarded the Paul Karrer Gold Medal in 2004?
    Queries:
    1. "Paul Karrer Gold Medal 2004 recipient"
    2. "Who won Paul Karrer Gold Medal 2004"
    3. "Awardees of Paul Karrer Gold Medal in 2004"

    Question: {question}
    Queries:
    """
    search_queries = call_llm(search_query_prompt, system_instruction="You are an expert at generating diverse search queries.").split("\n")

    # Step 2: Simulate retrieval of context from the web for each query and VERIFY that a source exists
    retrieved_contexts = []
    for query in search_queries:
        context = f"Simulated web search results for: {query}. Placeholder for real search functionality."
        # Verify that results are not empty
        verification_prompt = f"""Question: {question} Search query: {query}. Retrieved context: {context}. Is the context useful to answering the question? Answer 'yes' or 'no'."""
        verification_result = call_llm(verification_prompt, "Validating retrieved context")
        retrieved_contexts.append(context if "yes" in verification_result.lower() else "No relevant context found.")
    
    # Step 3: Extract answers from *each* context, and then synthesize them.
    answer_extraction_prompt = f"""
    Given the question and retrieved contexts from multiple sources, extract an answer from each. Then, synthesize a final answer, considering the consistency and reliability of the sources.
    Question: {question}

    Context 1: {retrieved_contexts[0]}
    Context 2: {retrieved_contexts[1]}
    Context 3: {retrieved_contexts[2]}

    Example 1:
    Question: What is the capital of Australia?
    Context 1: Canberra is the capital city of Australia.
    Context 2: Australia's capital is Canberra.
    Context 3: Canberra serves as the capital of the Commonwealth of Australia.
    Answer: Canberra, based on multiple consistent sources.

    Answer:
    """
    final_answer = call_llm(answer_extraction_prompt, system_instruction="You are an expert at extracting and synthesizing answers from multiple sources.")

    # Step 4: Final validation that the synthesized answer answers the question
    validation_prompt = f"""
    Validate that the following extracted and synthesized answer correctly answers the question.

    Question: {question}
    Answer: {final_answer}

    Example:
    Question: What is the capital of Australia?
    Answer: Canberra, based on multiple consistent sources.
    Validation: Correct; Canberra is the capital of Australia.

    Validation:
    """

    validation_result = call_llm(validation_prompt, system_instruction="You are an expert answer validator.")

    if "Correct" in validation_result:
        return final_answer
    else:
        return "Could not be validated."
```

=== SCRIPT FROM ITERATION 3 (Exploration, ACCURACY: 0.00) ===
Approach: The script implements a RAG approach with explicit source identification and verification, using the Gemini LLM to answer factual questions. The problem is decomposed into generating a context query, retrieving context from a simulated knowledge base, extracting the answer with source citation, and verifying the extracted answer against the cited source. Three agent roles are involved: one for generating context queries, one for extracting answers, and one for verifying answers. The `call_llm` function is used to interact with the Gemini model, `main` orchestrates the overall workflow, calling `call_llm` to generate a query, extract an answer, and verify the answer based on a knowledge base.

```python
import os
import re
import math

def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

def main(question):
    """Solve factual questions using a new approach: RAG with explicit source identification and verification."""

    # Hypothesis: Providing the LLM with specific context from a simulated knowledge base, and then asking it to explicitly cite the source for its answer, will improve accuracy.
    # This addresses the previous issues of inaccurate knowledge retrieval and overly strict validation by giving the LLM more focused information and requiring transparency.

    # Step 1: Generate a query to retrieve relevant context from a simulated knowledge base (with examples)
    context_query_prompt = f"""
    Generate a concise query to retrieve relevant context from a knowledge base to answer the following question.

    Example 1:
    Question: Who was the lead programmer of Project Firebreak who helped create CYAN in Horizon Zero Dawn: The Frozen Wilds?
    Context Query: Project Firebreak lead programmer Horizon Zero Dawn CYAN

    Example 2:
    Question: In which month and year did Apple add the ability for users to speak "Hey Siri" to enable the assistant without the requirement of physically handling the device?
    Context Query: Apple Hey Siri release date

    Question: {question}
    Context Query:
    """
    context_query = call_llm(context_query_prompt, system_instruction="You are an expert at generating context queries.")

    # Step 2: Simulate retrieval of context from a knowledge base (replace with actual retrieval mechanism if available)
    simulated_knowledge_base = {
        "Project Firebreak lead programmer Horizon Zero Dawn CYAN": "Anita Sandoval was the lead programmer of Project Firebreak, which helped create CYAN in Horizon Zero Dawn: The Frozen Wilds.",
        "Apple Hey Siri release date": "Apple added the 'Hey Siri' feature in September 2014.",
        "ISCB Accomplishment by a Senior Scientist Award 2019 recipient": "Bonnie Berger was the recipient of the ISCB Accomplishment by a Senior Scientist Award in 2019."
    }
    retrieved_context = simulated_knowledge_base.get(context_query, "No relevant context found.")

    # Step 3: Extract the answer from the context, *explicitly citing the source* (with examples)
    answer_extraction_prompt = f"""
    Given the question and the retrieved context, extract the answer and explicitly cite the source from which the answer was derived.

    Example 1:
    Question: Who was the lead programmer of Project Firebreak who helped create CYAN in Horizon Zero Dawn: The Frozen Wilds?
    Retrieved Context: Anita Sandoval was the lead programmer of Project Firebreak, which helped create CYAN in Horizon Zero Dawn: The Frozen Wilds.
    Answer: Anita Sandoval (Source: Anita Sandoval was the lead programmer of Project Firebreak, which helped create CYAN in Horizon Zero Dawn: The Frozen Wilds.)

    Example 2:
    Question: In which month and year did Apple add the ability for users to speak "Hey Siri" to enable the assistant without the requirement of physically handling the device?
    Retrieved Context: Apple added the 'Hey Siri' feature in September 2014.
    Answer: September 2014 (Source: Apple added the 'Hey Siri' feature in September 2014.)

    Question: {question}
    Retrieved Context: {retrieved_context}
    Answer:
    """
    answer_extraction_response = call_llm(answer_extraction_prompt, system_instruction="You are an expert at extracting answers from context and citing the source.")

    # Step 4: Verify that the extracted answer is supported by the cited source.
    verification_prompt = f"""
    Verify if the extracted answer is supported by the cited source.

    Example 1:
    Question: Who was the lead programmer of Project Firebreak who helped create CYAN in Horizon Zero Dawn: The Frozen Wilds?
    Extracted Answer: Anita Sandoval (Source: Anita Sandoval was the lead programmer of Project Firebreak, which helped create CYAN in Horizon Zero Dawn: The Frozen Wilds.)
    Verification: The answer is supported by the source. VALID.

    Example 2:
    Question: In which month and year did Apple add the ability for users to speak "Hey Siri" to enable the assistant without the requirement of physically handling the device?
    Extracted Answer: September 2014 (Source: Apple added the 'Hey Siri' feature in September 2014.)
    Verification: The answer is supported by the source. VALID.

    Question: {question}
    Extracted Answer: {answer_extraction_response}
    Verification:
    """
    verification_result = call_llm(verification_prompt, system_instruction="You are an expert at verifying answers based on provided sources.")

    if "VALID" in verification_result:
        return answer_extraction_response.split('(Source:')[0].strip()
    else:
        return "Could not be validated."
```


            LEARNINGS FROM PREVIOUS ITERATIONS:
            
        ACCUMULATED LEARNINGS FROM PREVIOUS ITERATIONS:
        Okay, here's the updated and synthesized version of our learnings, focusing specifically on the given dataset and task. This document will serve as our evolving research log.

## 1. DATASET PATTERNS & CHARACTERISTICS

*   **Factual and Specific Questions:** Questions are factual and seek specific pieces of information across a wide range of topics (music, chemistry, oceanography, bands, television, technology, awards, and biological classifications).
*   **Emphasis on Proper Nouns and Named Entities:** Questions frequently contain proper nouns and specific dates, making them amenable to information retrieval. Answers are often short phrases, often consisting of one or a few proper nouns (names of people or things). Examples: "Who was the first cardiologist in Kashmir?" "Who murdered the supervillain Monsieur Mallah?", "Who was the lead programmer for Project Firebreak?". "Name of person awarded a specific prize in a specific year", "Genus transitions of a species". The questions often involve named entities (people, organizations, titles) that may have ambiguity or require precise matching in a knowledge base. The "James Vernon the Younger" example highlights the need to handle generational titles correctly.
*   **Explicit Named Entities:** Questions frequently focus on named entities like people, organizations (universities), and biological classifications (genus, species). The questions directly ask *for the name of* these things.
*   **Specific Year Requirements:** The dataset frequently requires identifying a specific year associated with an event or person (e.g., "In what year did Etta Cone last visit Europe?", "In which year was Jamini Roy awarded the Padma Bhushan?", "In 1993, Vaughan Jones was elected to which academy?"). This suggests a strong reliance on temporal reasoning and access to date-related information.
*   **"Which" Questions Targeting Specific Individuals/Entities:** Several questions begin with "Which" and seek a specific individual or entity associated with a particular role or accomplishment (e.g., "Which architect was tasked with finishing the chapel...?"). These questions require precise identification from a potentially large pool of candidates.
*   **Question Types:** Primarily fact-retrieval questions asking for specific details about people, places, or things ("Who...", "What...", "Name of...", "Which...").
*   **Need for Precise Factual Recall:** Questions require precise factual recall, often involving dates, numbers, or names (e.g., "Champions League semi-final match between Barcelona and Milan on April 27, 2006", "What month and year did Apple release the Hey Siri feature?").
*   **Contextual Specificity:** Questions often contain specific contextual details (e.g., "Champions League semi-final match between Barcelona and Milan on April 27, 2006") to narrow the scope of the answer. Questions often include specific contextual details, such as time periods or specific publications, that heavily influence the correct answer. For example, the question about Lois Griffiths specifies the time "immediately following the award of her doctorate".
*   **Information Extraction Task:** The dataset tests the ability to extract *very specific* pieces of information from potentially larger contexts. It's not simply about broad topic understanding.
*   **"Who" questions requiring specific names:** Many questions (e.g., "Who was the first cardiologist in Kashmir?") require identifying specific individuals, making accurate name retrieval crucial. A significant portion of questions targets **specific names, titles, or locations.** The answers often involve precise string matching. The question about the linguist's name is a prime example, where accuracy hinges on identifying the exact name from potentially noisy search results.
*   **Questions requiring temporal context:** Some questions ("What day did India land its first spacecraft on the moon?", "Before the New 52, who murdered the supervillain Monsieur Mallah?", "What month and year did Apple release the Hey Siri feature?", "In what year did Etta Cone last visit Europe?", "In which year was Jamini Roy awarded the Padma Bhushan?") involve temporal qualifiers. This demands understanding of historical or fictional timelines to extract correct information.
*   **Questions expecting definitive answers:** The questions expect precise answers (specific dates, names), not general descriptions or related concepts.
*   **Complex Factual Questions:** The dataset contains complex factual questions that often require multi-hop reasoning or accessing multiple pieces of information. This is evident in questions like the "Hey Siri" query, which requires understanding the feature and the company (Apple).
*   **Entity Specificity:** Many questions revolve around specific entities like people (e.g., "Anita Sandoval", "Bonnie Berger"), projects ("Project Firebreak"), awards ("ISCB Accomplishment by a Senior Scientist Award", "Paul Karrer Gold Medal"), and products ("Horizon Zero Dawn"). The correctness depends heavily on identifying these entities accurately.
*   **Temporal Reasoning:** Some questions (e.g., "Hey Siri" question, "In what year did Etta Cone last visit Europe?", "In which year was Jamini Roy awarded the Padma Bhushan?") require temporal reasoning, understanding dates, and relating events to specific time periods.
*   **Complex Question Structure:** The questions are fact-retrieval based but require understanding relationships and potentially hierarchical information (e.g., name of a person awarded a specific prize in a specific year, genus transitions of a species).
*   **Context Dependency:** The answers are not directly present in the question itself; they require accessing and processing external information (simulated knowledge base) and connecting it to the question terms.
*   **Assumed Prior Knowledge and Lack of Context:** The questions often assume background knowledge not explicitly stated in the question itself. The system is failing when the initial, limited context it constructs is insufficient to answer the question directly.
*   **Varied Answer Types:** The expected answers encompass different data types, including strings (names, academy titles) and numerical values (years), which necessitate a flexible validation approach.
*   **Multi-hop Reasoning:** The dataset features questions that often require **multi-hop reasoning** and synthesis of information from multiple sources. Questions often include specific contextual details, such as time periods or specific publications, that heavily influence the correct answer.
*   **Broad Subject Matter:** The questions are varied in subject matter, ranging from history and geography (Patiala and East Punjab States Union) to academia (Lois Griffiths' appointment) and linguistics (onymic genitives paper). This requires a broad knowledge base and adaptable information retrieval.
*   **Fact-Seeking and Precise Information:** The questions are fact-seeking, requiring precise information retrieval (dates, ages, publication details).
*   **Named Entities and Specific Dates:** Questions frequently involve named entities (people, magazines, publications) and specific dates or periods related to those entities. Example: "How old was Lillian Marie Bounds when her father passed away?"
*   **High Degree of Precision:** The questions demand a high degree of precision; approximate or related information is insufficient. The answers must be exact facts.

## 2. EFFECTIVE TASK-SPECIFIC STRATEGIES

*   **Initial Attempts Ineffective:** The initial strategy of using an LLM for iterative refinement with a validation step resulted in 0% accuracy, indicating the need for significant improvements.
*   **Current Accuracy Very Low:** With an accuracy of 0.33 in experiment 1 and experiment 4 and 0.00 in experiments 2, 3, 5 & 6 & 7, no current strategies can be considered effectively "working". The underlying framework shows potential but requires significant refinement. The highest accuracy has been 0.33 (Experiment 1 and Experiment 4).
*   **RAG Framework Insufficient:** The initial hypothesis that a basic RAG framework with Gemini is sufficient for this task is rejected. The current implementation struggles to extract answers, even with explicit source citation and verification steps. The decomposition into multiple agents (query generation, extraction, verification) doesn't appear to be effective with the current implementation.
*   **Fact Verification Approach with Multi-Source Integration Needs Active Information Seeking**: The fact verification approach with multi-source integration shows promise, since it decomposes the problem into generating multiple search queries, simulating context retrieval from these queries while verifying their relevance, extracting answers from each context, synthesizing a final answer, and validating that final answer. However, this strategy needs an active information-seeking loop.
*   Decomposing the task into sub-tasks such as information extraction, search query generation, answer extraction, and validation seems like a reasonable approach and may provide a good starting point if further refined.
*   (Currently, no specific strategies are working, given the 0% accuracy in Experiments 2, 3, 5, 6 & 7. This section remains largely blank, awaiting the identification of truly effective approaches.)

## 3. COMMON FAILURE MODES ON THIS DATASET

*   **Overly Strict Validation:** The primary failure mode is the "Could not be validated" response. This suggests the validation logic is too rigid.
    *   **Example:** The date question "Specify the day, month, and year..." could fail if the extracted answer includes the day of the week, even if the core date is correct. The "Hey Siri" question also resulted in "Could not be validated," indicating that the system failed to extract the correct month and year from its knowledge source. If the system retrieves "American Academy of Arts and Sciences" (plural) when the expected answer is "American Academy of Arts and Science" (singular), it fails validation.
*   **Validation Sensitivity:** The primary failure stems from over-sensitive validation logic. The system frequently reports "Could not be validated" even when the answer *might* be present but not in the exact expected format.
*   **Lack of Contextual Understanding During Validation:** The validator likely struggles to relate the retrieved information to the specific constraints in the question.
    *   **Example:** For the corner kicks question, the validator might not be able to verify that the retrieved number of corners specifically relates to the *Barcelona* team, the *Champions League* match, and the specified date.
*   **Dependency on Perfect Information Retrieval:** The system's reliance on simulated retrieval means a slightly off search query can lead to no relevant information, and thus, a validation failure even if the LLM could have reasoned with broader knowledge.
*   **Ambiguity:** A question might have multiple possible answers depending on context.
*   **Synonyms/Paraphrasing:** The answer might be expressed differently in the source text than in the question.
*   **Misspellings/Typos:** Both in the question and the potential answer source.
*   **Cultural References:** Some questions may rely on understanding cultural or historical context that is not explicitly stated.
*   **Multiple entities with the same name:** disambiguation may be necessary.
*   **UnboundLocalError:** Error detected during script repair (attempt 1): cannot access local variable 'extracted_answer' where it is not associated with a value. This highlights the need for robust error handling and ensuring variables are properly initialized.
*   **Inability to resolve conflicting information:** In the "first cardiologist" question, the LLM extracted "Dr. Sheikh Jalal" instead of the correct "Dr Upendra Kaul," indicating a failure in discerning accurate information from potentially conflicting (simulated) search results. The system needs a more robust method for selecting the correct answer when multiple possibilities are presented in the search context.
*   **Failure to identify the correct entity in search results:** For the "Monsieur Mallah" question, the system returned "Unknown from search result" when the correct answer was "Gorilla Grodd." This suggests the model struggled to extract the specific entity (Gorilla Grodd) related to the murder event from the provided context, even if the information was present in the simulated search results. This can be due to named entity recognition failure, relationship extraction failure, or even failures in question understanding. For the "Project Firebreak" question, the LLM returned "Naoya Oda" instead of "Anita Sandoval," indicating a failure to accurately link the project to the correct lead programmer.
*   **Poor Simulated Search Results:** The low accuracy (0.33 in experiment 1 and experiment 4 and 0.00 in experiments 2, 3, 5, 6 & 7) reinforces the possibility that the simulated search results are not accurate or relevant enough, hindering both information extraction and validation.
*   **Inaccurate Entity Resolution:** The primary failure mode is the inability to resolve entities correctly. The "James Vernon the Younger" example highlights the need to handle generational titles correctly.
*   **Knowledge Graph Misidentification/Hallucination:** The "ISCB Accomplishment by a Senior Scientist Award" example showcases a failure to retrieve the correct information from the knowledge source. The LLM returns "Janet Thornton" instead of "Bonnie Berger," suggesting that either the lookup process is flawed, or the LLM is hallucinating information not present in the knowledge source.
*   **Information Extraction Bottleneck:** The primary failure mode is the inability of the system to extract the correct answer from the provided context. The LLM consistently returns "No answer can be extracted," even when the ground truth answer is present within the knowledge base. This suggests a problem with the *answer extraction* agent, rather than the *context query generation* agent.
*   **Lack of Understanding Contextual Relationships:** The error examples suggest the system struggles to identify and connect the relevant information from the context to the specific question. For example, identifying "Ada Yonath" as the recipient of a particular award in a given year requires understanding the relationship between the person, the award, and the year within the context.
*   **Potential impact of agent prompting**: The prompt to the extraction agent is likely not optimized for extracting the precise answer from the context.
*   **Information extraction as a critical failure point**: We've identified that information extraction specifically, rather than the RAG setup, is the point of failure.
*   **Insufficient Context Detection Failure:** The agent struggles when the initially retrieved context (simulated or otherwise) lacks the information required to answer the question. The system passively reports the inability to answer instead of actively seeking the missing information. This is especially evident in questions like "In what year did Etta Cone last visit Europe?" where the context doesn't contain the specific year.
*   **Passive Behavior Regarding Missing Information:** The system's most significant failure mode is its inability to actively address information gaps. Even with a decomposition strategy including search queries, it does not detect its own informational shortcomings and reformulate queries to fill those gaps. It simply gives up.
*   **Inaccurate Information Retrieval:** The "Unavailable" response in the Miss USA example suggests a failure in information retrieval. The system likely couldn't find the answer to this specific question, indicating potential gaps in the knowledge source or ineffective search queries.
*   **Lack of Numerical Precision:** The James Vernon example demonstrates a failure in matching numerical answers. The validation likely requires an exact string match, failing if there are any slight variations.
*   **Insufficient validation leads to incorrect answers:** As the primary identified issue is a lack of a robust validation mechanism, the system frequently provides answers that are plausible but incorrect, as demonstrated by the Lois Griffiths example where the system identifies "University of North Dakota" instead of the expected "Northwestern." This indicates a failure to adequately cross-reference or prioritize the most reliable source.
*   **Reliance on single-source extraction results in errors**: The Tanja Ackermann example showcases that the system picks up on a different name (Jochen Bepler) instead of the right one. This suggest that the current system does not make use of evidence from multiple sources in order to improve reliability. This also suggest a failure to contextualize the search result within the specific paper in question.
*   **Inability to handle ambiguity and semantic equivalence.** The system struggles with answers that are semantically equivalent but not identical strings. The explanations suggest that the system is too strict in its comparison between predicted and expected outputs.
*   **Poor Query Generation:** The generated search queries consistently fail to retrieve the necessary information. This is evident in all the error examples from iteration 7, where the system fails to provide *any* answer, indicating a failure to find relevant information in the simulated retrieval step. The root cause is the inability to translate the decomposed question into an effective search query that targets the specific facts needed (age, date, etc.). For instance, "How old was Lillian Marie Bounds when her father passed away?" requires a query that precisely connects Lillian Marie Bounds, her father, and their respective dates (birth/death).

## 4. EXPERIMENT LOG & FINDINGS

*   **Experiment 0 (Initial Exploration):**
    *   **Strategy:** LLM-based iterative refinement with validation.
    *   **Result:** 0% accuracy.
    *   **Finding:** The initial configuration of prompts, validation logic, or information retrieval simulation is inadequate. The hypothesis that the LLM, with its inherent knowledge, can directly answer complex questions involving precise factual data and validation is rejected in its current form.
*   **Experiment 1 (Agent-based LLM Approach):**
    *   **Strategy:** Basic framework of information extraction, query generation, simulated search, answer extraction, and validation.
    *   **Result:** 0.33 accuracy.
    *   **Finding:** The agent-based LLM approach, in its current form, struggles with reasoning and information synthesis from (simulated) search results. The failure to accurately answer questions involving specific entities (names, dates) indicates a weakness in linking entities to the events described in the questions. This is the highest accuracy achieved so far.
*   **Experiment 2:**
    *   **Strategy:** Multi-stage reasoning with explicit instructions for entity extraction, knowledge lookup, synthesis, and validation.
    *   **Result:** 0.00 accuracy.
    *   **Finding:** The experiment rejected the hypothesis that a multi-stage reasoning approach with LLMs, even with explicit instructions for each stage (entity extraction, knowledge lookup, synthesis, validation), is sufficient to answer complex factual questions without more robust knowledge retrieval mechanisms. The decomposition into stages did not compensate for the base-level problem of incorrect information retrieval. The experiment also confirmed the reliance of LLMs on external knowledge sources when answering factual questions. Inaccurate information from these sources directly leads to incorrect answers, regardless of the sophistication of the reasoning process.
*   **Experiment 3:**
    *   **Strategy:** RAG (Retrieval Augmented Generation) framework with Gemini model. Multiple agents for query generation, information extraction, and verification with explicit source citation.
    *   **Result:** 0.00 accuracy.
    *   **Finding:** The initial hypothesis that a basic RAG framework with Gemini is sufficient for this task is rejected. The current implementation struggles to extract answers, even with explicit source citation and verification steps. The decomposition into multiple agents (query generation, extraction, verification) doesn't appear to be effective with the current implementation. Information extraction, specifically, is a critical failure point. The generic LLM is insufficient to reliably perform this task, or at least the current prompting strategy is insufficient.
*   **Experiment 4:**
    *   **Strategy:** Fact verification with multi-source integration. Decomposed the problem into generating multiple search queries, simulating context retrieval, extracting answers, synthesizing a final answer, and validating that final answer.
    *   **Result:** 0.33 accuracy.
    *   **Finding:** The simulation of context retrieval proves insufficient. The "simulated search results" are not detailed enough to answer the questions. The attempt to solve the problem using fact verification *without* equipping the system with active information seeking proves ineffective. The accuracy of 0.33 confirms the hypothesis that the system cannot passively rely on initial context. This approach needs an active information-seeking loop.
*   **Experiment 5:**
    *   **Strategy:** Iterative refinement and extraction, leveraging LLMs for query formulation, information retrieval, and answer extraction, followed by stringent validation.
    *   **Result:** 0% accuracy.
    *   **Finding:** The initial hypothesis that the LLM-driven iterative refinement and extraction approach would be effective for this factual question answering task is rejected. The current implementation struggles with reliable retrieval and validation. The experiment highlights the validation step as a major bottleneck. Even if the system retrieves potentially correct information, the strict validation logic prevents it from being recognized as such.
*   **Experiment 6:**
    *   **Strategy:** Multi-source fact verification with an attempt to improve validation
    *   **Result:** 0.00 accuracy.
    *   **Finding:** The experiment rejects the hypothesis that a decomposed approach using multiple sources with enhanced validation provides sufficient accuracy on this task. The low accuracy (0.33) strongly indicates that the current exploitation strategy, while structurally sound in its decomposition, is significantly underperforming. The individual components (information extraction, query generation, answer extraction, validation) are not functioning effectively enough to provide high accuracy. The hypothesis that decomposing the question answering task into smaller, LLM-driven subtasks would lead to accurate results is *rejected* at the current performance level. While the approach may have merit, the current implementation has major shortcomings.
*   **Experiment 7:**
    *   **Strategy:** Exploration strategy, relying on LLM-based decomposition and targeted retrieval with the *existing* prompt engineering.
    *   **Result:** 0% accuracy.
    *   **Finding:** The current exploration strategy, relying on LLM-based decomposition and targeted retrieval with the *existing* prompt engineering, is **unsuccessful** for this dataset. The hypothesis that the LLM can effectively decompose questions and generate targeted queries leading to accurate answer extraction is **rejected**. The primary bottleneck is the inability to formulate effective search queries.
*   **Script Repair Errors:**
    *   **Error 1:** `UnboundLocalError: cannot access local variable 'extracted_answer' where it is not associated with a value`. Occurred during script repair attempt 1. Indicates a logic error where `extracted_answer` was used before being assigned a value.
    *   **Error 2 & 3:** "ERROR: Validation failure" during script repair attempts 2 & 3. This reinforces the issues with the overly strict validation process.
    *   **Error 4:** "list index out of range" occurred during script repair attempt 2, indicating an issue with accessing elements in a list beyond its bounds. Specifically, the error occurred while trying to access `retrieved_contexts[1]`, suggesting that only one context was retrieved.
    *   **Error 5:** "Gemini API call failed due to missing attribute" occurred during script repair attempt 3, indicating a problem with the API call, potentially due to missing parameters or an incorrect API configuration.

## 5. NEXT RESEARCH DIRECTIONS

*   **Implement a Multi-Source Validation Mechanism:** Augment the `validate_answer` function to cross-reference information from multiple search results and prioritize more authoritative sources. Develop a scoring mechanism that assigns confidence levels to different sources and answers.
*   **Enhance Answer Extraction with Context Awareness:** Modify the `extract_answer` function to better contextualize extracted answers within the source document. For example, when extracting a name, ensure it is associated with the correct publication or institution. This will help reduce false positives.
*   **Improve Semantic Comparison in Validation:** Refine the validation process to account for semantic equivalence. For example, use an LLM to determine if two university names are referring to the same institution, or if two names represent the same person.
*   **Refine Information Extraction prompts**: Since dataset questions often contain important time frames and context, ensure that the `extract_information` function is accurately extracting and including this information when generating search queries.
*   **Implement an Active Information Seeking Loop:** Augment the system with a mechanism to:
    1.  **Detect** when it lacks sufficient information to answer a question with confidence.
    2.  **Generate** new, targeted search queries designed to fill specific information gaps. For example, if the initial query "Etta Cone Europe visits" doesn't return a year, the system should generate a follow-up query like "Etta Cone last visit to Europe year".
    3.  **Incorporate** the information retrieved from these new queries into its knowledge base.
*   **Relax Validation Criteria:** Implement more flexible validation. Consider:
    *   Semantic similarity checks instead of exact string matching.
    *   Allowing for answers that contain more information than the minimal expected answer, as long as the core information is present and correct.
    *   Introducing a confidence score threshold for validation, where answers with high confidence, even if not perfectly matching the expected answer, are accepted.
    *   **Fuzzy Matching:** Using techniques like Levenshtein distance or semantic similarity to compare the system's answer with the expected answer, allowing for minor variations in wording or spelling.
    *   **Type Checking:** Implementing logic to specifically compare numerical answers without requiring an exact string match.
*   **Improve Relevance Validation:** The "relevance" checks on retrieved information need to be more stringent. The system should not proceed with information extraction if the retrieved context is only tangentially related to the question.
*   **Refine Query Generation:** The initial query generation step needs improvement to create more specific and focused queries. Explore techniques to expand the initial search query to include synonyms or related terms, increasing the chances of finding relevant information.
*   **Focus on Temporal Reasoning:** Since many questions require specific years, develop specialized modules/prompts that focus on extracting and reasoning about temporal information. For questions asking for years, ensure that the validator correctly assesses the validity of the extracted year against the expected year.
*   **Improve Answer Extraction Prompting:** Focus on refining the prompt used by the answer extraction agent. Experiment with more specific instructions, few-shot examples demonstrating the desired extraction format, and constraints on the output format (e.g., "Only provide the name of the person/university/genus").
*   **Incorporate Relationship Extraction Techniques:** Explore incorporating relationship extraction techniques into the answer extraction agent. This could involve using the LLM to explicitly identify relationships between entities in the context and then using this information to extract the answer. For example, prompt the LLM to identify the 'awardee' relationship for the 'Paul Karrer Gold Medal' in '2004'.
*   **Fine-tune extraction**: Consider fine-tuning a smaller, more efficient LLM specifically for the information extraction task. This could improve the accuracy and speed of the system.
*   **Test Context Quality**: While the focus is on extraction, verify that the retrieved context contains the necessary information *before* optimizing the extraction prompt. There's an implicit assumption the context is adequate, but this should be explicitly validated.
*   **Analyze Failure Cases in Detail**: Conduct a more in-depth analysis of the failure cases. For each failure, examine the question, the retrieved context, the LLM's response, and the ground truth answer to identify the root cause of the failure. This will help to identify specific areas where the system needs improvement.
*   **Improve Entity Linking/Resolution:** Implement more robust entity linking strategies, such as using a named entity disambiguation module before querying the LLM. This could involve cross-referencing extracted entities against a known database or knowledge graph to ensure accurate identification.
*   **Refine Knowledge Graph Retrieval:** Investigate the knowledge graph lookup process. This could involve:
    *   Evaluating the quality and coverage of the underlying knowledge graph.
    *   Optimizing the query formulation to the knowledge graph based on extracted entities and relationships. Ensure the queries are precise and target the specific information needed.
*   **Knowledge Source Augmentation:** Consider augmenting the knowledge source with additional data to address gaps in coverage, specifically for less commonly known facts.
*   **Implement a Fact Verification Module:** Before answer synthesis, add a step to verify the extracted information against multiple sources (if available) to reduce the impact of incorrect information from a single source.
*   **Improve Contextual Understanding in Validation:** Enhance the validation prompt to explicitly consider the context provided in the original question. For example, provide the validator with the original question and ask it to confirm that the extracted answer directly addresses all parts of the question (team, event, date, etc.).
*   **Evaluate Information Retrieval Simulation:** Examine the quality of the simulated information retrieval. Is the search query being generated effectively capturing the essence of the question? Does the "retrieved" information contain the answer (even if the extraction fails)? Consider adding a step to evaluate the quality of the retrieved information independently of the answer extraction.
*   **Implement Explicit Error Handling/Reporting:** Instead of simply outputting "Could not be validated," provide more detailed feedback on *why* the validation failed. This will help diagnose issues with the validation process itself and prevent similar errors (like the `UnboundLocalError`) in the future.
*   **Prompt Engineering for Answer Extraction:** Refine the prompt used for answer extraction. Ensure it clearly instructs the LLM to extract the *specific* information requested and to provide a confidence score.
*   **Address the `UnboundLocalError`:** Implement a default value for `extracted_answer` or refactor the code to guarantee it is always assigned before being used. Add logging to track the flow of execution and variable assignments.
*   **Focus on RAG (Retrieval Augmented Generation):** Prioritize techniques that combine information retrieval with LLM generation to leverage external knowledge and reduce hallucination. Start with basic RAG and iteratively refine the retrieval and generation prompts.
*   **Improve Search Query Generation:** Refine the `generate_search_query` function to produce more targeted queries that specifically address the key entities and relationships in the question. For the "Monsieur Mallah" example, ensure the query explicitly targets the murderer.
*   **Enhance Answer Extraction with Contextual Reasoning:** Enhance the `extract_answer` function to consider the context within the simulated search results. Implement strategies to weigh different sources of information based on reliability or relevance, and to resolve conflicts between sources.
*   **Strengthen Validation Logic:** The `validate_answer` function needs more sophisticated logic to identify semantic equivalence. For example, it should be able to recognize that "Gorilla Grodd" is the murderer of Monsieur Mallah even if the search results don't explicitly state "Gorilla Grodd murdered Monsieur Mallah" but contain information strongly implying it.
*   **Data Augmentation with Known Facts:** Augment the simulated search results with known facts, or use an LLM specifically trained for QA to extract more accurate information. This is to overcome the issue of poor simulated search results.
*   **Implement Explicit Reasoning Steps:** Force the LLM to verbalize its reasoning steps (e.g., "I found X in source Y. Source Z contradicts this, but source Y is more reliable. Therefore, the answer is X."). This makes debugging and error analysis easier. This could be done in the prompt, or by adding another LLM step for this explicit reasoning.
*   **Improve validation stage:** The "Could not be validated" error suggests a problem with determining the correctness of the final answer. Improve this stage by explicitly prompting for the reasoning behind the answer.
*   **Implement RAG with specific knowledge base:** Instead of letting the LLM find the information, use Retrieval Augmented Generation to provide the context to the LLM. This involves creating or selecting a relevant knowledge base and designing a retrieval mechanism to fetch the most relevant passages based on the question.
*   **Need for enhanced fine-tuning**: The generic LLM is insufficient to reliably perform this task, or at least the current prompting strategy is insufficient.
*   **Address List Index Out of Range Errors**: Add error handling to prevent "list index out of range" errors. Specifically, ensure that the code checks if the list has at least two elements before accessing `retrieved_contexts[1]`. Also log the number of returned contexts and the question asked when such an error occurs.
*   **Fix Gemini API Call Issues**: Review the Gemini API call implementation and ensure that all required parameters are present and correctly configured. Implement error handling to catch API call failures and provide informative error messages. Log the specific parameters sent to the API before the call.
*   **Error Analysis:** Implement more detailed logging to capture the exact information retrieved by the system and the reason for validation failure. This will allow for a more fine-grained analysis of the error patterns.
*   **Specific example handling:** For questions asking for years, ensure that the validator correctly assesses the validity of the extracted year against the expected year.
*   **Improve Query Generation:** The immediate focus should be on refining the query generation process.
        *   **Prompt Engineering:** Experiment with different prompts for query generation, emphasizing the need for precision and the inclusion of relevant named entities. Consider prompts that explicitly instruct the LLM to include keywords like "age," "date," or "publication details" when relevant. Example question: "How old was Lillian Marie Bounds when her father passed away?"
        *   **Query Refinement Loop:** Implement a query refinement loop where the initial query's results are analyzed, and the query is iteratively improved based on the content found (or not found). If the initial simulated retrieval yields no relevant information, the query should be automatically revised (potentially by the LLM) before proceeding.
    *   **Dataset-Specific Fine-tuning (Optional):** If feasible, fine-tune a smaller language model on a set of question-answer pairs from this dataset to improve query generation.
    *   **Hybrid Retrieval (Future):** If the refined query generation remains insufficient, consider a hybrid retrieval approach that combines keyword-based search with semantic search to capture both precise terms and contextual meaning.
        

            CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
            
        CAPABILITY ASSESSMENT & IMPROVEMENT GUIDANCE:
        SYSTEM ANALYSIS & GUIDANCE


        

            EXPLORATION GUIDANCE:
            1. Review the historical approaches, error patterns, and accumulated learnings carefully
            2. Review the FULL CODE of previous scripts to understand what has already been tried
            3. Design a new approach that is DISTINCTLY DIFFERENT from previous attempts. This approach should have a specific NEW HYPOTHESIS or variable you are trying to test. 
            4. CRITICAL: Include EMBEDDED EXAMPLES directly within your LLM prompts
            5. For each key function, show a complete worked example, or include multiple examples, including:
               - Input example that resembles the dataset
               - Step-by-step reasoning through the example
               - Properly formatted output
            6. Apply the insights from the ACCUMULATED LEARNINGS section to avoid repeating past mistakes
            7. Pay SPECIAL ATTENTION to the weaknesses and improvement suggestions from the capability assessment
            8. Consider implementing one or more of these LLM usage patterns:
               - Repeated validation with feedback loops
               - Multi-perspective analysis with synthesis
               - Dynamic input-dependent routing with an orchestrator
               - Hybrid approaches combining LLM with deterministic functions
               - Best-of-n solution generation and selection
               - ReAct pattern for interactive reasoning and action
               - If it is unknown how successful a processing state or part of the pipeline is, include verification steps to different parts of the pipeline in order to help deduce which parts are successful and where the system is breaking
               - Answer checkers to validate the final answer against the problem statement. If the answer is incorrect, the checker can send the answer back to an earlier part of the system for for refinement with feedback

            Here's how to call the Gemini API. Use this example without modification and don't invent configuration options:
            def call_llm(prompt, system_instruction=None):
    """Call the Gemini LLM with a prompt and return the response. DO NOT deviate from this example template or invent configuration options. This is how you call the LLM."""
    try:
        from google import genai
        from google.genai import types

        # Initialize the Gemini client
        client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

        # Call the API with system instruction if provided
        if system_instruction:
            response = client.models.generate_content(
                model="gemini-2.0-flash", 
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                ),
                contents=prompt
            )
        else:
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )

        return response.text
    except Exception as e:
        print(f"Error calling Gemini API: {str(e)}")
        return f"Error: {str(e)}"

            Since this is an EXPLORATION phase:
            - Try a fundamentally different approach to reasoning about the problem. Test a NEW HYPOTHESIS or variable, and add verification steps to deduce if this new change is helpful.
            - THIS IS KEY: Break down the problem into new, distinct reasoning steps based on past performance before you start coding
            - For EACH key LLM prompt, include a relevant example with:
              * Sample input similar to the dataset
              * Expected reasoning steps
              * Desired output format
            - Apply a verifier call to different parts of the pipeline in order to understand what parts of the pipeline of calls is successful and where the system is breaking
            - Pay special attention to addressing the primary issues from previous iterations
            - Ensure your new approach addresses the weaknesses identified in the capability assessment

            CRITICAL REQUIREMENTS:
            1. The script MUST properly handle all string literals - be extremely careful with quotes and triple quotes
            2. The script MUST NOT exceed 150 lines of code to prevent truncation
            3. Include detailed comments explaining your reasoning approach
            4. EVERY SINGLE LLM PROMPT must include at least one embedded example showing:
               - Sample input with reasoning
               - Desired output format
            5. Make proper use of error handling
            6. Implement robust capabilities to address the specific weaknesses identified in the capability assessment
            7. Do NOT use json.loads() in the LLM calls to process input data. JSON formatting is good to use to structure information as inputs and outputs, but attempting to have functions process JSON data explicitly with strict built-in functionality is error prone due to formatting issues and additional text that appears as documentation, reasoning, or comments. When passing data into another LLM call, you can read it as plain text rather than trying to load it in strict json format, is the better approach.

            Return a COMPLETE, RUNNABLE Python script that:
            1. Has a main function that takes a question string as input and returns the answer string
            2. Makes multiple LLM calls for different reasoning steps
            3. Has proper error handling for API calls
            4. Includes embedded examples in EVERY LLM prompt
            5. Is COMPLETE - no missing code, no "..." placeholders
            6. Closes all string literals properly

            This should be FUNDAMENTALLY DIFFERENT from all previous approaches. Do not reuse the same overall structure.

            BE EXTREMELY CAREFUL TO PROPERLY CLOSE ALL STRING QUOTES AND TRIPLE QUOTES!
            